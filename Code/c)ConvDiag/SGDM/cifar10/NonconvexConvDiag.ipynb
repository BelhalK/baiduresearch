{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/karimimohammedbelhal/Desktop/ML_Research/BaiduResearch/Research/baiduresearch/Code/c)ConvDiag/SGDM/cifar10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import pytorch_optim as optim_local\n",
    "import os\n",
    "import argparse\n",
    "from models import *\n",
    "from logger import Logger, savefig\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data prep\n",
    "transform_train = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trainset = torchvision.datasets.MNIST('../data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.MNIST('../data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "batch_size = 128\n",
    "dataset = \"MNIST\"\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "gamma = 0.1\n",
    "weight_decay = 5e-4\n",
    "beta2 = 0.999\n",
    "burnin = 10\n",
    "window = 10 #stationary test window (epochs)\n",
    "sim = 'ip' #['ip', 'cosine'] inner prod. similarity\n",
    "schedule = [80, 120] # 'Either str diagnostic / plateau or epochs to decrease learning rate at.')\n",
    "momentum_switch = False #Momentum reduction boolean\n",
    "early_threshold = 0.2 #threshold for norm-based momentum switch\n",
    "num_reduce = 1 #if diagnostic, number of times to reduce LR\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test loads\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = MnistNet()\n",
    "model = \"MnistNetSmall\"\n",
    "net = MnistNetSmall()\n",
    "device ='cuda:{}'.format(args.gpu[0]) if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create checkpoints log file\n",
    "logname = model\n",
    "title = '{}-{}'.format(dataset, logname)\n",
    "checkpoint_dir = 'checkpoint/checkpoint_{}'.format(dataset)\n",
    "logger = Logger('{}/log{}.txt'.format(checkpoint_dir, logname), title=title)\n",
    "logger.set_names(['Learning Rate', 'Momentum', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.',\n",
    "                      'IP Sum', 'IP Mean', 'IP Std', 'Grad Norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZATION params\n",
    "optimizer = \"ADAM\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if optimizer =='signSGD':\n",
    "    optimizer = optim_local.sign_SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, sim=sim)\n",
    "elif optimizer == 'SGDM':\n",
    "    optimizer = optim_local.SGD_Diagnostic_Nonconvex(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, sim=sim)\n",
    "elif optimizer == 'ADAM':\n",
    "    optimizer = optim_local.Adam_Diagnostic(net.parameters(), lr=1e-3, betas=(momentum, beta2), eps=1e-8, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'plateau' in schedule:\n",
    "    scheduler = optim_local.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=burnin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_norm = []\n",
    "train_loss_ls = []\n",
    "test_stat = 0.0\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0; global train_loss_ls\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ip_loss = []; global test_stat\n",
    "    grad_loss = []; global grad_norm\n",
    "    ind = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        ind += 1\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        _, diag_args = optimizer.step()\n",
    "        ip_loss.append(diag_args['ip_loss'])\n",
    "        grad_loss.append(diag_args['grad_loss'])\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if not debug:\n",
    "            if ind%100 == 0:\n",
    "                print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d) | IP_sum: %.3f'\n",
    "                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total, np.sum(ip_loss)))\n",
    "#            progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d) | IP_sum: %.3f'\n",
    "#                        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total, np.sum(ip_loss)))\n",
    "\n",
    "    # convergence tests based on inner product of loss list from epoch\n",
    "    diag_stats = {'ip_loss_sum':np.sum(ip_loss), 'ip_loss_mean':np.mean(ip_loss), 'ip_loss_std':np.std(ip_loss),\n",
    "                  'grad_norm_mean':np.mean(grad_loss)}\n",
    "    grad_norm.append(diag_stats['grad_norm_mean'])\n",
    "    train_loss_ls.append(train_loss)\n",
    "\n",
    "    if (momentum_switch and momentum_ind == -1) or (not momentum_switch and epoch > burnin):\n",
    "        test_stat += np.sum(ip_loss)\n",
    "\n",
    "    return (train_loss, 100.*correct/total, diag_stats)\n",
    "\n",
    "def test(epoch):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        ind = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            ind +=1\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if not debug:\n",
    "                if ind%100 == 0:\n",
    "                    print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d) | test stat: %.3f'\n",
    "                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total, test_stat))\n",
    "#                progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d) | test stat: %.3f'\n",
    "#                             % (test_loss/(batch_idx+1), 100.*correct/total, correct, total, test_stat))\n",
    "\n",
    "    return (test_loss, 100.*correct/total)\n",
    "\n",
    "def save_checkpoint(state, test_acc):\n",
    "    # Save checkpoint.\n",
    "    global best_acc\n",
    "    if test_acc > best_acc:\n",
    "        print('Saving..')\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './{}/ckpt.{}'.format(checkpoint_dir, logname))\n",
    "        best_acc = test_acc\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, diag_stats):#, grad_norm, ip_loss):\n",
    "    global lr\n",
    "    global momentum\n",
    "    global num_reduce\n",
    "    global momentum_ind\n",
    "\n",
    "    def change_momentum(m_new, opt):\n",
    "        '''toggles momentum indicator and changes momentum parameter'''\n",
    "        global momentum\n",
    "        global momentum_ind\n",
    "\n",
    "        opt.change_momentum(m_new)\n",
    "        momentum_ind *= -1\n",
    "        momentum = m_new\n",
    "        print('Momentum change from %.2f to %.2f after epoch %d' % (momentum, final_momentum, epoch))\n",
    "\n",
    "    # Momentun Reduction\n",
    "    if momentum_switch and epoch > 0 and momentum_ind == 1 and \\\n",
    "        np.abs(train_loss_ls[epoch]-train_loss_ls[epoch-1])/train_loss_ls[epoch-1] < early_threshold:\n",
    "        #np.abs(grad_norm[epoch]-grad_norm[epoch-1])/grad_norm[epoch-1] < early_threshold:\n",
    "        change_momentum(final_momentum, optimizer)\n",
    "\n",
    "    if 'plateau' in schedule and num_reduce >= 1 and momentum_ind == -1:\n",
    "        if scheduler.step(diag_stats['ip_loss_sum']):\n",
    "            lr *= gamma\n",
    "            num_reduce -= 1\n",
    "            change_momentum(momentum, optimizer)\n",
    "        # if diag_stats['adf_convg'] and epoch > burnin:\n",
    "        #     lr *= gamma\n",
    "        #     burnin = epoch + burnin\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = lr\n",
    "    elif 'diagnostic' in schedule and num_reduce >= 1 and momentum_ind == -1 and epoch > burnin:\n",
    "        if np.sum(test_stat) < 0.0:\n",
    "            print('LR reduce from %.2f to %.2f at epoch %d' % (lr, lr*gamma, epoch))\n",
    "            lr *= gamma\n",
    "            num_reduce -= 1\n",
    "            change_momentum(momentum, optimizer)\n",
    "    elif (epoch+1) in schedule and momentum_switch:\n",
    "        lr *= gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        change_momentum(momentum, optimizer)\n",
    "        momentum_ind = 1 #only matters for momentum_switch=True case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train Loss: 0.475 | Train Acc: 85.945% (11001/12800) | IP_sum: 25.625\n",
      "Train Loss: 0.333 | Train Acc: 90.250% (23104/25600) | IP_sum: 43.294\n",
      "Train Loss: 0.266 | Train Acc: 92.224% (35414/38400) | IP_sum: 58.711\n",
      "Train Loss: 0.228 | Train Acc: 93.328% (47784/51200) | IP_sum: 75.307\n",
      "Test Loss: 0.076 | Test Acc: 97.630% (9763/10000) | test stat: 0.000\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.081 | Train Acc: 97.656% (12500/12800) | IP_sum: 16.533\n",
      "Train Loss: 0.081 | Train Acc: 97.742% (25022/25600) | IP_sum: 30.606\n",
      "Train Loss: 0.077 | Train Acc: 97.789% (37551/38400) | IP_sum: 47.159\n",
      "Train Loss: 0.075 | Train Acc: 97.838% (50093/51200) | IP_sum: 60.866\n",
      "Test Loss: 0.058 | Test Acc: 98.090% (9809/10000) | test stat: 0.000\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(start_epoch, start_epoch+epochs):\n",
    "    train_loss, train_acc, diag_stats = train(epoch)\n",
    "    test_loss, test_acc = test(epoch)\n",
    "\n",
    "    # append logger file\n",
    "    logger.append([lr, momentum, train_loss, test_loss, train_acc, test_acc,\n",
    "                   #diag_stats['ip_loss_sum']\n",
    "                   test_stat, diag_stats['ip_loss_mean'], diag_stats['ip_loss_std'], diag_stats['grad_norm_mean']])\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch, diag_stats)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'net': net.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'epoch': epoch,\n",
    "    }, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-72410df58ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoint/checkpoint_MNIST/logMnistNetSmall.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "file = 'checkpoint/checkpoint_MNIST/logMnistNetSmall.txt'\n",
    "run = pd.read_table(file, index_col = 0, sep = '\\t')\n",
    "p = plt.plot(range(epochs), run['Train Loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baiduenv",
   "language": "python",
   "name": "baiduenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
