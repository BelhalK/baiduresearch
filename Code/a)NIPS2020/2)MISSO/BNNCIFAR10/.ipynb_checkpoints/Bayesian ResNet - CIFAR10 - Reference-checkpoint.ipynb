{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from models.bayesian_resnet import bayesian_resnet\n",
    "from models.bayesian_vgg import bayesian_vgg\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [32, 32, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                         batch_size, valid_size):\n",
    "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
    "  x_train = x_train.astype(\"float32\")\n",
    "  x_test = x_test.astype(\"float32\")\n",
    "\n",
    "  x_train /= 255\n",
    "  x_test /= 255\n",
    "\n",
    "  y_train = y_train.flatten()\n",
    "  y_test = y_test.flatten()\n",
    "\n",
    "  if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "  print(\"x_train shape:\" + str(x_train.shape))\n",
    "  print(str(x_train.shape[0]) + \" train samples\")\n",
    "  print(str(x_test.shape[0]) + \" test samples\")\n",
    "\n",
    "  # Build an iterator over training batches.\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_train, np.int32(y_train)))\n",
    "  training_batches = training_dataset.shuffle(\n",
    "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n",
    "\n",
    "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
    "  # i.e., return the entire heldout set as a constant.\n",
    "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_test, np.int32(y_test)))\n",
    "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
    "  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_batches)\n",
    "\n",
    "  # Combine these into a feedable iterator that can switch between training\n",
    "  # and validation inputs.\n",
    "  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
    "  feedable_iterator = tf.compat.v1.data.Iterator.from_string_handle(\n",
    "      handle, training_batches.output_types, training_batches.output_shapes)\n",
    "  images, labels = feedable_iterator.get_next()\n",
    "\n",
    "  return images, labels, handle, training_iterator, heldout_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fake_data():\n",
    "  \"\"\"Build fake CIFAR10-style data for unit testing.\"\"\"\n",
    "  num_examples = 10\n",
    "  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"bnnmodels/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate fake data for now before switching to CIFAR10\n",
    "fake_data = True\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "epochs = 3\n",
    "data_dir = \"data/\"\n",
    "eval_freq = 400\n",
    "num_monte_carlo = 50\n",
    "architecture = \"resnet\" # or \"vgg\"\n",
    "kernel_posterior_scale_mean = 0.9\n",
    "kernel_posterior_scale_constraint = 0.2\n",
    "kl_annealing = 50\n",
    "subtract_pixel_mean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Warning: deleting old log directory at bnnmodels/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_fake_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ad57a8ae9b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_fake_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_fake_data' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if tf.io.gfile.exists(model_dir):\n",
    "      tf.compat.v1.logging.warning(\n",
    "          \"Warning: deleting old log directory at {}\".format(model_dir))\n",
    "      tf.io.gfile.rmtree(model_dir)\n",
    "    tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "    if fake_data:\n",
    "        (x_train, y_train), (x_test, y_test) = build_fake_data()\n",
    "    else:\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    (images, labels, handle,\n",
    "     training_iterator,\n",
    "     heldout_iterator) = build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                                              batch_size, 500)\n",
    "\n",
    "    if architecture == \"resnet\":\n",
    "        model_fn = bayesian_resnet\n",
    "    else:\n",
    "        model_fn = bayesian_vgg\n",
    "\n",
    "    model = model_fn(\n",
    "        IMAGE_SHAPE,\n",
    "        num_classes=10,\n",
    "        kernel_posterior_scale_mean=kernel_posterior_scale_mean,\n",
    "        kernel_posterior_scale_constraint=kernel_posterior_scale_constraint)\n",
    "    logits = model(images)\n",
    "    labels_distribution = tfd.Categorical(logits=logits)\n",
    "    t = tf.compat.v2.Variable(0.0)\n",
    "    kl_regularizer = t / (kl_annealing * len(x_train) / batch_size)\n",
    "\n",
    "    # Compute the -ELBO as the loss. The kl term is annealed from 0 to 1 over\n",
    "    # the epochs specified by the kl_annealing flag.\n",
    "    log_likelihood = labels_distribution.log_prob(labels)\n",
    "    neg_log_likelihood = -tf.reduce_mean(input_tensor=log_likelihood)\n",
    "    kl = sum(model.losses) / len(x_train) * tf.minimum(1.0, kl_regularizer)\n",
    "    loss = neg_log_likelihood + kl\n",
    "\n",
    "    # Build metrics for evaluation. Predictions are formed from a single forward\n",
    "    # pass of the probabilistic layers. They are cheap but noisy\n",
    "    # predictions.\n",
    "    predictions = tf.argmax(input=logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.compat.v1.name_scope(\"train\"):\n",
    "      train_accuracy, train_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions)\n",
    "      opt = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "      train_op = opt.minimize(loss)\n",
    "      update_step_op = tf.compat.v1.assign(t, t + 1)\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"valid\"):\n",
    "      valid_accuracy, valid_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions)\n",
    "\n",
    "    init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n",
    "                       tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "    stream_vars_valid = [\n",
    "        v for v in tf.compat.v1.local_variables() if \"valid/\" in v.name\n",
    "    ]\n",
    "    reset_valid_op = tf.compat.v1.variables_initializer(stream_vars_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "training_steps = int(\n",
    "      round(epochs * (len(x_train) / batch_size)))\n",
    "training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:   0 Loss: 285062.438 Accuracy: 0.094 KL: 153683.234\n",
      "Step:   1 Loss: 438545.438 Accuracy: 0.105 KL: 307377.406\n",
      "Step:   2 Loss: 601105.375 Accuracy: 0.130 KL: 461074.938\n",
      "Step:   3 Loss: 727282.188 Accuracy: 0.133 KL: 600420.312\n",
      "Step:   4 Loss: 747203.500 Accuracy: 0.120 KL: 600428.062\n",
      "Step:   5 Loss: 735255.625 Accuracy: 0.115 KL: 600434.875\n",
      "Step:   6 Loss: 738430.375 Accuracy: 0.117 KL: 600440.812\n",
      "Step:   7 Loss: 734095.438 Accuracy: 0.117 KL: 600446.000\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "\n",
    "  # Run the training loop\n",
    "  train_handle = sess.run(training_iterator.string_handle())\n",
    "  heldout_handle = sess.run(heldout_iterator.string_handle())\n",
    "  training_steps = int(\n",
    "      round(epochs * (len(x_train) / batch_size)))\n",
    "  for step in range(training_steps):\n",
    "    _ = sess.run([train_op,\n",
    "                  train_accuracy_update_op,\n",
    "                  update_step_op],\n",
    "                 feed_dict={handle: train_handle})\n",
    "\n",
    "    # Manually print the frequency\n",
    "    if step % 1 == 0:\n",
    "      loss_value, accuracy_value, kl_value = sess.run(\n",
    "          [loss, train_accuracy, kl], feed_dict={handle: train_handle})\n",
    "      print(\n",
    "          \"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f} KL: {:.3f}\".format(\n",
    "              step, loss_value, accuracy_value, kl_value))\n",
    "\n",
    "    if (step + 1) % eval_freq == 0:\n",
    "      # Compute log prob of heldout set by averaging draws from the model:\n",
    "      # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
    "      #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
    "      # where model_i is a draw from the posterior\n",
    "      # p(model|train).\n",
    "      probs = np.asarray([sess.run((labels_distribution.probs),\n",
    "                                   feed_dict={handle: heldout_handle})\n",
    "                          for _ in range(num_monte_carlo)])\n",
    "      mean_probs = np.mean(probs, axis=0)\n",
    "\n",
    "      _, label_vals = sess.run(\n",
    "          (images, labels), feed_dict={handle: heldout_handle})\n",
    "      heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]),\n",
    "                                             label_vals.flatten()]))\n",
    "      print(\" ... Held-out nats: {:.3f}\".format(heldout_lp))\n",
    "\n",
    "      # Calculate validation accuracy\n",
    "      for _ in range(20):\n",
    "        sess.run(\n",
    "            valid_accuracy_update_op, feed_dict={handle: heldout_handle})\n",
    "      valid_value = sess.run(\n",
    "          valid_accuracy, feed_dict={handle: heldout_handle})\n",
    "\n",
    "      print(\n",
    "          \" ... Validation Accuracy: {:.3f}\".format(valid_value))\n",
    "    sess.run(reset_valid_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baiduenv",
   "language": "python",
   "name": "baiduenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
