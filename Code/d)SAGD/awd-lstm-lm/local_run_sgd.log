nohup: ignoring input
Loading cached dataset...
train total num  929589
start running 0 repeat
Applying weight drop of 0.5 to weight_hh_l0
Applying weight drop of 0.5 to weight_hh_l0
Applying weight drop of 0.5 to weight_hh_l0
[WeightDrop(
  (module): LSTM(400, 1150)
), WeightDrop(
  (module): LSTM(1150, 1150)
), WeightDrop(
  (module): LSTM(1150, 400)
)]
Args: Namespace(alpha=2, batch_size=20, beta=1, beta1=0.9, beta2=0.999, bptt=70, clip=0.25, cuda=True, data='data/penn', device='cuda:0', disable_asgd=False, dropout=0.4, dropoute=0.1, dropouth=0.25, dropouti=0.4, emsize=400, epochs=500, final_lr=0.1, gamma=0.001, log_interval=200, lr=30, model='LSTM', momentum=0, nhid=1150, nlayers=3, noise_coe=1, nonmono=5, optimizer='sgd', repeat=3, resume='', save='output/PTB.pt', seed=141, tied=True, wdecay=1.2e-06, wdrop=0.5, when=[-1])
Model total parameters: 24221600
/home/yujinxing/software/anaconda3/envs/torch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  result = self.forward(*input, **kwargs)
| epoch   1 |   200/  663 batches | lr 30.00000 | ms/batch 269.85 | loss  7.10 | ppl  1212.31 | bpc   10.244
| epoch   1 |   400/  663 batches | lr 30.00000 | ms/batch 263.69 | loss  6.33 | ppl   563.30 | bpc    9.138
| epoch   1 |   600/  663 batches | lr 30.00000 | ms/batch 267.48 | loss  6.03 | ppl   414.06 | bpc    8.694
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 192.56s | valid loss  5.83 | valid ppl   338.79 | valid bpc    8.404
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   2 |   200/  663 batches | lr 30.00000 | ms/batch 272.15 | loss  5.81 | ppl   334.77 | bpc    8.387
| epoch   2 |   400/  663 batches | lr 30.00000 | ms/batch 266.44 | loss  5.65 | ppl   285.45 | bpc    8.157
| epoch   2 |   600/  663 batches | lr 30.00000 | ms/batch 269.45 | loss  5.54 | ppl   255.66 | bpc    7.998
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 191.76s | valid loss  5.32 | valid ppl   205.24 | valid bpc    7.681
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   3 |   200/  663 batches | lr 30.00000 | ms/batch 269.79 | loss  5.46 | ppl   235.74 | bpc    7.881
| epoch   3 |   400/  663 batches | lr 30.00000 | ms/batch 267.78 | loss  5.37 | ppl   215.48 | bpc    7.751
| epoch   3 |   600/  663 batches | lr 30.00000 | ms/batch 264.92 | loss  5.30 | ppl   199.48 | bpc    7.640
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 192.33s | valid loss  5.17 | valid ppl   175.90 | valid bpc    7.459
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   4 |   200/  663 batches | lr 30.00000 | ms/batch 264.53 | loss  5.27 | ppl   194.27 | bpc    7.602
./local_run_sgd.sh: line 4:  5326 Killed                  python -u main.py --batch_size 20 --data data/penn --dropouti 0.4 --dropouth 0.25 --wdrop 0.5 --seed 141 --epoch 500 --save output/PTB.pt --optimizer sgd --device $1
