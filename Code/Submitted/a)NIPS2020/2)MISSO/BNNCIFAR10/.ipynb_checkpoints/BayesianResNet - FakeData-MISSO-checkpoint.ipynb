{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from models.bayesian_resnet import bayesian_resnet\n",
    "from models.bayesian_vgg import bayesian_vgg\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [32, 32, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                         batch_size, valid_size):\n",
    "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
    "  x_train = x_train.astype(\"float32\")\n",
    "  x_test = x_test.astype(\"float32\")\n",
    "\n",
    "  x_train /= 255\n",
    "  x_test /= 255\n",
    "\n",
    "  y_train = y_train.flatten()\n",
    "  y_test = y_test.flatten()\n",
    "\n",
    "  if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "  print(\"x_train shape:\" + str(x_train.shape))\n",
    "  print(str(x_train.shape[0]) + \" train samples\")\n",
    "  print(str(x_test.shape[0]) + \" test samples\")\n",
    "\n",
    "  # Build an iterator over training batches.\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_train, np.int32(y_train)))\n",
    "  training_batches = training_dataset.shuffle(\n",
    "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n",
    "\n",
    "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
    "  # i.e., return the entire heldout set as a constant.\n",
    "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_test, np.int32(y_test)))\n",
    "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
    "  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_batches)\n",
    "\n",
    "  # Combine these into a feedable iterator that can switch between training\n",
    "  # and validation inputs.\n",
    "  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
    "  feedable_iterator = tf.compat.v1.data.Iterator.from_string_handle(\n",
    "      handle, training_batches.output_types, training_batches.output_shapes)\n",
    "  images, labels = feedable_iterator.get_next()\n",
    "\n",
    "  return images, labels, handle, training_iterator, heldout_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fake_data():\n",
    "  \"\"\"Build fake CIFAR10-style data for unit testing.\"\"\"\n",
    "  num_examples = 10\n",
    "  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"bnnmodels/\"\n",
    "loss_dir = 'lossesFAKE/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate fake data for now before switching to CIFAR10\n",
    "fake_data = True\n",
    "learning_rate = 0.001\n",
    "data_dir = \"data/\"\n",
    "eval_freq = 400\n",
    "num_monte_carlo = 50\n",
    "architecture = \"resnet\" # or \"vgg\"\n",
    "kernel_posterior_scale_mean = 0.9\n",
    "kernel_posterior_scale_constraint = 0.2\n",
    "kl_annealing = 50\n",
    "subtract_pixel_mean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:(10, 32, 32, 3)\n",
      "10 train samples\n",
      "10 test samples\n",
      "WARNING:tensorflow:From <ipython-input-2-26c3b33fe1c3>:40: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
      "WARNING:tensorflow:From <ipython-input-2-26c3b33fe1c3>:40: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if fake_data:\n",
    "        (x_train, y_train), (x_test, y_test) = build_fake_data()\n",
    "    else:\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    (images, labels, handle,\n",
    "     training_iterator,\n",
    "     heldout_iterator) = build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                                              batch_size, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_misso(algo,fake_data, batch_size, epochs, verbose):\n",
    "    with tf.Session() as sess:\n",
    "            \n",
    "        #instanciate model (bayesian resnet)\n",
    "        model_fn = bayesian_resnet\n",
    "        model = model_fn(\n",
    "            IMAGE_SHAPE,\n",
    "            num_classes=10,\n",
    "            kernel_posterior_scale_mean=kernel_posterior_scale_mean,\n",
    "            kernel_posterior_scale_constraint=kernel_posterior_scale_constraint)\n",
    "        logits = model(images)\n",
    "        labels_distribution = tfd.Categorical(logits=logits)\n",
    "        t = tf.compat.v2.Variable(0.0)\n",
    "        kl_regularizer = t / (kl_annealing * len(x_train) / batch_size)\n",
    "\n",
    "        log_likelihood = labels_distribution.log_prob(labels)\n",
    "        neg_log_likelihood = -tf.reduce_mean(input_tensor=log_likelihood)\n",
    "        kl = sum(model.losses) / len(x_train) * tf.minimum(1.0, kl_regularizer)\n",
    "        loss = neg_log_likelihood + kl\n",
    "\n",
    "        predictions = tf.argmax(input=logits, axis=1)\n",
    "\n",
    "        with tf.compat.v1.name_scope(\"train\"):\n",
    "            train_accuracy, train_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "                labels=labels, predictions=predictions)\n",
    "            opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        \n",
    "            train_op = opt.minimize(loss)\n",
    "            update_step_op = tf.compat.v1.assign(t, t + 1)\n",
    "\n",
    "        with tf.compat.v1.name_scope(\"valid\"):\n",
    "            valid_accuracy, valid_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "                labels=labels, predictions=predictions)\n",
    "\n",
    "            init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n",
    "                           tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "            stream_vars_valid = [v for v in tf.compat.v1.local_variables() if \"valid/\" in v.name]\n",
    "            reset_valid_op = tf.compat.v1.variables_initializer(stream_vars_valid)\n",
    "    \n",
    "    \n",
    "        sess.run(init_op)\n",
    "        indivgrads = []\n",
    "        indivvar = []\n",
    "        # Run the training loop\n",
    "        train_handle = sess.run(training_iterator.string_handle())\n",
    "        heldout_handle = sess.run(heldout_iterator.string_handle())\n",
    "        training_steps = int(\n",
    "          round(epochs * (len(x_train) / batch_size)))\n",
    "        listloss = []\n",
    "        listaccuracy = []\n",
    "        print(training_steps)\n",
    "        for indiv in range(0,total,batch_size):\n",
    "            print(indiv)\n",
    "            gradss = tf.gradients(loss, tf.trainable_variables())\n",
    "            grads = [x for x in gradss if x is not None]\n",
    "            #set_trace()\n",
    "            var_updates = []\n",
    "            var_list = tf.trainable_variables()\n",
    "            for grad, var in zip(grads, var_list):\n",
    "                var_updates.append(var.assign_sub(0.001 * grad))\n",
    "            train_op = tf.group(*var_updates)\n",
    "            indivgrads.append(grads)\n",
    "            indivvar.append(var_list)\n",
    "#        for step in range(training_steps):\n",
    "        for epoch in range(epochs):\n",
    "            for step in range(0,int(total/batch_size)):\n",
    "                gradss = tf.gradients(loss, tf.trainable_variables())\n",
    "                grads = [x for x in gradss if x is not None]\n",
    "                indivgrads[step] = grads\n",
    "                var_updates = []\n",
    "                var_list = tf.trainable_variables()\n",
    "                for gradstemp, varlist in zip(indivgrads, indivvar):\n",
    "                    for grad, var in zip(gradstemp, varlist):\n",
    "                        var_updates.append(var.assign_sub(0.001 * grad)) #\\theta^{\\tau_i^k} - \\grad f_{\\tau_i^k}\n",
    "                _ = sess.run([train_op,\n",
    "                              train_accuracy_update_op,\n",
    "                              update_step_op],\n",
    "                              feed_dict={handle: train_handle})\n",
    "                # Print loss values\n",
    "                loss_value, accuracy_value, kl_value = sess.run(\n",
    "                  [loss, train_accuracy, kl], feed_dict={handle: train_handle})\n",
    "                if step % 100 == 0:\n",
    "                    print(\"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f} KL: {:.3f}\".format(\n",
    "                      step, loss_value, accuracy_value, kl_value))\n",
    "                listloss.append(loss_value)\n",
    "                listaccuracy.append(accuracy_value)\n",
    "                \n",
    "        sess.run(reset_valid_op)\n",
    "    return listloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "batch_size = 1\n",
    "nb_runs = 3\n",
    "seed0 = 23456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misso_loss = []\n",
    "misso_kl = []\n",
    "for _ in range(nb_runs):\n",
    "    print(\"Run Number: {:.0f}\".format(_))\n",
    "    tf.random.set_random_seed(_*seed0)\n",
    "    loss, kl = run_experiment_misso(algo='misso', fake_data=fake_data, batch_size = batch_size, epochs=epochs, verbose= True)\n",
    "    \n",
    "    misso_loss.append(loss)\n",
    "    misso_kl.append(kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SAVE LOSSES\n",
    "with open(loss_dir+'missoloss', 'wb') as fp: \n",
    "    pickle.dump(misso_loss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(loss_dir+'missokl', 'wb') as fp: \n",
    "    pickle.dump(misso_kl, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplotseveral(x, y, n=20, percentile_min=1, percentile_max=99, color='r', plot_mean=True, plot_median=False, line_color='k', **kwargs):\n",
    "    line_colors=['r','b','g','y','black']\n",
    "    colors=['r','b','g','y','black']\n",
    "    labels= ['ADAM','ADAGRAD','ADADELTA','RMSPROP','SAG']\n",
    "    i = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(16, 3.5))\n",
    "    axes.set_facecolor('white')\n",
    "    axes.grid(linestyle='-', linewidth='0.2', color='grey')\n",
    "    axes.spines['bottom'].set_color('black')\n",
    "    axes.spines['top'].set_color('black') \n",
    "    axes.spines['right'].set_color('black')\n",
    "    axes.spines['left'].set_color('black')\n",
    "    \n",
    "    for element in y:\n",
    "      perc1 = np.percentile(element, np.linspace(percentile_min, 50, num=n, endpoint=False), axis=0)\n",
    "      perc2 = np.percentile(element, np.linspace(50, percentile_max, num=n+1)[1:], axis=0)\n",
    "\n",
    "\n",
    "      if 'alpha' in kwargs:\n",
    "          alpha = kwargs.pop('alpha')\n",
    "      else:\n",
    "          alpha = 1/n\n",
    "      alpha = 0.005\n",
    "      # fill lower and upper percentile groups\n",
    "      for p1, p2 in zip(perc1, perc2):\n",
    "          plt.fill_between(x, p1, p2, alpha=alpha, color=colors[i], edgecolor=None)\n",
    "\n",
    "\n",
    "      if plot_mean:\n",
    "          plt.plot(x, np.mean(element, axis=0), color=line_colors[i],label=labels[i])\n",
    "\n",
    "\n",
    "      if plot_median:\n",
    "          plt.plot(x, np.median(element, axis=0), color=line_colors[i],label=labels[i])\n",
    "      i += 1\n",
    "    leg = plt.legend(fontsize=18,fancybox=True, loc=0,ncol=3)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.xlabel('Epoch', fontsize=20)\n",
    "    plt.ylabel('ELBO', fontsize=20)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.grid(linestyle='dotted',linewidth=2)\n",
    "    pylab.ticklabel_format(axis='y',style='sci',scilimits=(1,4))\n",
    "    fig.tight_layout()\n",
    "    return plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = len(misso_loss[0])\n",
    "iterations\n",
    "itera = np.linspace(0,iterations,iteratio\"ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplotloss = [misso_loss]\n",
    "toplotkl = [misso_kl]\n",
    "tsplotseveral(itera,toplotloss, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baiduenv",
   "language": "python",
   "name": "baiduenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
