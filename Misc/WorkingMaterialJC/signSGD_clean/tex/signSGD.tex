\documentclass[12pt]{article}    
\input{macros}

\title{
Convergence diagnostic for communication efficient SGD
}
\author{Jerry Chee}
\author{Ping Li}
\affil[]{Cognitive Computing Lab, Baidu USA}

\begin{document}
\pagestyle{empty}
\maketitle

\begin{abstract} 
%Abstract
Convergence detection in stochastic iterative optimization methods remains more art than science.
As training of machine learning algorithms becomes ever more resource intensive with larger data sets and greater computing power, it becomes even more important to determine when such methods should terminate.
In this paper we focus on signSGD, a variant of SGD where the gradient values are truncated to just their sign components.
There has been much recent work in gradient compression to reduce the communication of distributed SGD implementations.
We provide a non-asymptotic convergence analysis to show that signSGD moves from a transient phase where iterates quickly remove their dependence on initial conditions, to a stationary phase of bounded radius around a minimum point.
To detect this phase transition we present a principled statistical convergence diagnostic for signSGD. 
We provide theoretical and empirical evidence supporting that the proposed diagnostic works well. 

\vfill
\textbf{Keywords: Convergence detection, signSGD, gradient compression
} 
\end{abstract}
\if

%\newpage
%\tableofcontents

%\newpage
\pagestyle{plain}
\setcounter{page}{1}
%\doublespacing

\section{Introduction}
The future of machine learning will require even larger models to train on even larger data sets. 
Stochastic gradient descent (SGD) is the workhorse training algorithm.
It was originally developed as a serial algorithm, and much recent work has been done to speedup SGD through parallelization.
A popular approach has been to use a parameter-server framework to split data amongst workers and individually compute gradient updates which much be gathered, averaged, and then re-distributed. 
This gradient communication can be prohibitive to increased training speeds.
To alleviate this, there has been recent interest in gradient compression to reduce the communication overhead. 

One simple way to compress the gradient is to take the sign of each element.
These methods have been studied and were called 1-bit methods for speech models, and more recently have been called signSGD.
This gradient compression is easy to implement, and does not suffer from the variance explosions of other unbiased gradient compression schemes.
However, the sign of the gradient is no longer an unbiased estimate of the true gradient.
Bernstein et al. 2018 presented a convergence analysis under a non-convex loss.
We focus on the (strongly) convex case; at some point signSGD will enter a local minima which is (strongly) convex. 
We present a non-asymptotic convergence analysis under (strongly) convex loss.
This analysis will show the existence of a transient and stationary phase for signSGD.
We focus on a constant learning rate as this is commonly used in practice, and it makes the transition from transient to stationary phase explicitly clear. 
For constant rate signSGD, in the transient phase it moves quickly towards the minimum point to forget its initial conditions.
In the stationary phase, signSGD with constant rate remains in a bounded region around a local minima. 
Thus once the stationary region has been reached, it is pointless to keep running.

Inspiration is drawn from the work of Chee and Toulis (2018), which present a convergence diagnostic for vanilla SGD.
At a high level, the idea is that in the transient phase successive gradients are likely to be pointing in the same direction, and thus the inner product of successive gradients is likely to be positive on average.
When the stationary phase is reached, iterates are likely to be oscillating around the minimum, and thus the inner product of successive gradients is likely to be negative on average.
Though the gradient information has been compressed to just the sign, we believe that the same intuition holds. 

\subsection{Related work}
Error correction. Maybe say its ok doesn't converge because use constant learning rate?

\subsection{Our contributions}
Convexity is a key assumption behind our analysis.
Intuitively, our proposed diagnostic detects when signSGD is oscillating as a result of the curvature of the loss around a minimum point.
Even for non-convex loss, it is assumed that there are convex regions around the minimum points.
We present a non-asymptotic convergence analysis for signSGD under (strongly) convex loss.
We present a principled statistical convergence diagnostic for signSGD. 
It keeps a running mean of the inner product between successive gradients, and activates when less than zero.

\section{Convergence analysis}
%\textcolor{red}{I think for practical convergence analysis I still need a bound on $E \| \theta_n - \theta_\star \|^2$, which is still too hard to estimate while running sgd to be reliable enough.}
%Two convergence analyses.
%The first a more standard one, with lipschitz smoothness, convexity assumptions.
%While it can be theoretically insightful for these bounds to depend on characterizations of the loss, such as convexity of lipschitz continuity, it is precisely these constants that make it difficult to use these bounds in practice.
%The second convergence analysis only uses constants which are easy to estimate while SGD is running.
%It is precisely the simple form of the gradient (just the sign) which makes derivation of these bounds possible.


Consider the stochastic optimization problem:
\begin{equation}
\theta_\star = \arg \min_{\theta \in \mathbb{R}^d} f ( \theta ) = \Ex{ \ell ( \theta, \xi ) }
\end{equation}

The $n$-th estimate of $\theta_\star$ is:
\begin{equation}
\label{eq:signSGD}
\theta_n = \theta_{n-1} - \gamma sign ( \nabla \ell ( \theta_{n-1}, \xi_n ) )
\end{equation}


We consider a class of loss functions slightly more general than convex.

\begin{assumption}
\label{assump:sign_convex}
%Weaker convexity assumption.
Let $f$ satisfy for all $x, y$
\begin{equation*}
\biggl( sign ( \nabla f (y) ) - sign ( \nabla f(x) ) \biggr)^\top ( y - x ) \geq 0
\end{equation*}
At the minimum point $f_\star$, we define $sign ( \nabla f_\star ) = 0$.
\end{assumption}

This class of functions has a unique minimum, but does not have to be convex.
For example, consider the sin function on the bounded interval $(-1.5 \pi, 0.5 \pi)$.
This function is not convex, and yet it satisfies Assumption~\ref{assump:sign_convex}.
We can view the sign of the gradient as imposing a convex structure upon functions which have a unique minimum, but are not convex.


\begin{theorem}
\label{thm:convex}
Let $f$ satisfy Assumption~\ref{assump:sign_convex}. 
The distance of the iterates from $\theta_\star$ is bounded by $\| \theta_0 - \theta_\star \|^2$.
The learning rate satisfies $\gamma < 1 / 2 \mu_{\theta_0}$ for some $\mu_{\theta_0} > 0$.
Then,

\begin{equation*}
\Ex{ \| \theta_n - \theta_\star \|^2 } \leq ( 1 - 2 \gamma \mu_{\theta_0} )^n \ \Ex{ \| \theta_{0} - \theta_\star \|^2 } + \frac{d ( \gamma + 4)}{2 \mu_{\theta_0}}
\end{equation*}
\end{theorem}

\begin{proof}
We follow the approach taken by Bach and Moulines 2011, and adapt it for signSGD.
For brevity of notation let $\nabla \ell_{n-1} \equiv \nabla \ell ( \theta_{n-1}, \xi_n )$.
First derive a recursive relation for $\| \theta_n - \theta_\star \|^2$.
Unless otherwise noted, $\| \cdot \|$ represents the L2 norm.

\begin{align*}
\theta_n - \theta_\star &= \theta_{n-1} - \theta_\star - \gamma sign ( \nabla \ell_{n-1} ) \\
\| \theta_n - \theta_\star \|^2 &= \| \theta_{n-1} - \theta_\star \|^2  - 2 \gamma sign ( \nabla \ell_{n-1} )^\top ( \theta_{n-1} - \theta_\star) + \gamma^2 \| sign ( \nabla \ell_{n-1} ) \|^2 \\
\Ex{ \| \theta_n - \theta_\star \|^2 } &= \| \theta_{n-1} - \theta_\star \|^2  - 2 \gamma \Ex{ sign ( \nabla \ell_{n-1} ) }^\top ( \theta_{n-1} - \theta_\star ) + \gamma^2 \Ex{ \| sign ( \nabla \ell_{n-1} ) \|^2 } \\
\end{align*}

The third term is equal to $\gamma^2 d$.
In Bach and Moulines 2011 this term is bounded with a bound on the variance of the stochastic gradient.
However this is not needed for signSGD as the gradients only contain the sign information.
To bound the second term, we use a technique from Bernstein et al 2018 to deal with the fact that the sign gradient is now a biased estimate of the true gradient.
First decompose the gradient estimate.
\comment{Do I use the dot product or element-wise sum for the second term bellow?}

\begin{align*}
\Ex{ sign ( \nabla \ell_{n-1} ) }^\top ( \theta_{n-1} - \theta_\star)
&= sign ( \nabla f ( \theta_{n-1} ) )^\top ( \theta_{n-1} - \theta_\star ) \\
&\quad - 2 sign( \nabla f ( \theta_{n-1} ) )^\top \ \mathbb{P} [ sign( \nabla \ell_{n-1} ) \neq sign( \nabla f ( \theta_{n-1} ) ) ] \\
%&\geq \mu_{\theta_0} \| \theta_{n-1} - \theta_\star \|^2
\end{align*}

The probability represents a vector where the $i$-th element is equal to $\mathbb{P} [ sign( \nabla \ell_{n-1} )_i \neq sign( \nabla f ( \theta_{n-1} ) )_i ]$.
By Assumption~\ref{assump:sign_convex}, there exists constant $\mu_{\theta_0} > 0$ such that $sign( f ( \theta_{n-1} ) )^\top ( \theta_{n-1} - \theta_\star ) \geq \mu_{\theta_0} \| \theta_{n-1} - \theta_\star \|^2 $.
We can rewrite the term that captures the amount that the sign of the gradient estimate is incorrect, and use Markov's inequality.

\begin{align*}
\mathbb{P} [ sign( \nabla \ell_{n-1} )_i \neq sign( \nabla f ( \theta_{n-1} )_i ) ] &= \mathbb{P} [ | sign( \nabla \ell_{n-1} )_i - sign( \nabla f ( \theta_{n-1} )_i ) | \geq 1 ] \\
&\leq \Ex{ | sign( \nabla \ell_{n-1} )_i - sign( \nabla f ( \theta_{n-1} )_i ) | } \\
&\leq 2
\end{align*}

\comment{Could instead bound with $\sqrt{ \Ex{ | sign( \nabla \ell_{n-1} )_i - sign( \nabla f ( \theta_{n-1} )_i ) |^2 } } \leq \sigma^2$}
Combining these bounds and taking expectation on both sides we get the following recursive relation:
\begin{align*}
\Ex{ \| \theta_n - \theta_\star \|^2 } &\leq ( 1 - 2 \gamma \mu_{\theta_0} ) \ \Ex{ \| \theta_{n-1} - \theta_\star \|^2 } + d (\gamma^2 + 4 \gamma) \\
&= ( 1 - 2 \gamma \mu_{\theta_0} )^n \ \Ex{ \| \theta_{0} - \theta_\star \|^2 } + d (\gamma^2 + 4 \gamma) \sum_{i=0}^{n-1} ( 1 - 2 \gamma \mu_{\theta_0} )^i \\
&\leq ( 1 - 2 \gamma \mu_{\theta_0} )^n \ \Ex{ \| \theta_{0} - \theta_\star \|^2 } + \frac{d ( \gamma + 4)}{2 \mu_{\theta_0}}
\end{align*}
\end{proof}

\emph{Remarks}.
$\mu_{\theta_0}$ is similar to the strong convexity parameter, however it also depends on the initial conditions.
It is reasonable to assume that the distance from $\theta_\star$ as the initial parameter $\theta_0$ begins a fixed distance from $\theta_\star$, and the procedure would only have unbounded iterates if it diverged.
This convergence analysis is not sensitive to the lipschitz parameter of the gradients, or to the noise level of the gradients.
The dimension in the bound is not prohibitive as the L2 norm is proportional to the dimension $d$.

Theorem~\ref{thm:convex} supports the existence of the transient and stationary phase for signSGD under functions described by Assumption~\ref{assump:sign_convex}, which includes convex functions.
The first term of the bound dominates in the transient phase when the initial conditions are forgotten exponentially fast, this is when the bias dominates.
The second term dominates in the stationary phase when signSGD remains trapped in a region of radius $O( \sqrt{d (\gamma+4) / 2 \mu_{\theta_0}} )$.
We also see the tradeoff for the constant learning rate which has been widely observed, such as in Bach and Moulies 2011.
A higher learning rate increases the rate at which initial conditions are forgotten.
However, this enacts a tradeoff where the radius of the stationary phase is much larger.

While Theorem~\ref{thm:convex} provides theoretical insights into the runtime behavior of signSGD, it cannot be practically used.
The data dependent constant $\mu_{\theta_0}$ is too difficult to estimate reliably.
%However, as we have seen in Theorem~\ref{thm:convex} that the sign gradient effectively removes enough information such that lipschitz smoothness or bounded gradient noise are no longer needed to be assumed.
%SignSGD has enough information removed from the gradient that we can derive a bound with only easy to derive parameters.
%Moreover, we can estimate these parameters while we run signSGD.


\section{Convergence diagnostic}
A theoretical convergence analysis helpful for understanding, but is difficult to use in practical scenarios because of the need to estimate data-dependent constants.

\begin{algorithm}[t]
\Input{Learning rate $\gamma$, 
Data-set $D$ with $N$ data points $(x_n, y_n)$,
Initial point $\theta$,
Burnin period burnin,
Initial point $\theta_0$.
}
$\theta \gets \theta_0$ \\
S $\gets 0$ \\
\While {Not converged} {
%s $\gets 0$ \\
\For {$n$ in 1 to $N$} {
$\theta \gets \theta - \gamma sign( \nabla \ell ( \theta, \xi_{n} ) )$ \\
\If {burnin done} {
S $\gets S +  \langle  \nabla \ell ( \theta, \xi_{n} ),  \nabla \ell ( \theta, \xi_{n-1} ) \rangle$ \\
\If {S $< 0$} {
\Return{$\theta$}
}
}
}
}
\caption{Convergence diagnostic for signSGD}
\label{alg:diagnostic}
\end{algorithm}

\begin{theorem}
Let $f$ satisfy Assumption~\ref{assump:sign_convex}. 
\comment{Need to be careful difference between $\| \theta_n - \theta_\star \|^2$ and $\| \theta_{n-1} - \theta_n \|^2$.}
Suppose that Theorem~\ref{thm:convex} holds, such that $\Ex{ \| \theta_{n-1} - \theta_n \|^2 } \leq \gamma M$ for some positive $M$ and large enough $n$.
Let $\delta_\star > 0$ such that $\Ex{ f( \theta_n ) - f( \theta_\star ) } \leq \delta_\star$ for large enough $n$.
It holds that $\gamma < (2d - \delta_\star) / M$ where $d$ is the dimension.
Then,

\begin{equation*}
\frac{1}{\gamma} \Ex{ sign( \nabla \ell( \theta_n, \xi_{n+1} ) ) }^\top ( \theta_{n-1} - \theta_n ) 
%\leq \frac{1}{\gamma} [ \delta_\star + \gamma M - 2d ]
< 0
\end{equation*}
\end{theorem}

\begin{proof}
Re-arrange the update in Equation~\ref{eq:signSGD} to get $sign( \nabla \ell ( \theta_{n-1}, \xi_n ) ) = \frac{1}{\gamma} ( \theta_{n-1} - \theta_n )$.
We use this to rewrite the inner product

\begin{align}
\label{eq:diag1}
sign( \nabla \ell( \theta_n, \xi_{n+1} ) )^\top sign( \nabla \ell( \theta_{n-1}, \xi_n ) ) &= \frac{1}{\gamma} sign( \nabla \ell( \theta_n, \xi_{n+1} ) )^\top ( \theta_{n-1} - \theta_n )
\end{align}

Apply expectation to both sides of Equation~\ref{eq:diag1}, we decompose the expectation into the expected value and offset.

\begin{align}
\label{eq:diag2}
\frac{1}{\gamma} \Ex{ sign( \nabla \ell( \theta_n, \xi_{n+1} ) ) }^\top ( \theta_{n-1} - \theta_n ) 
&= \frac{1}{\gamma} sign ( \nabla f ( \theta_{n} ) )^\top ( \theta_{n-1} - \theta_n ) \\
&\quad - \frac{2}{\gamma} \sum_{i=1}^{d} sign( \nabla f ( \theta_{n} )_i ) \ \mathbb{P} [ sign( \nabla \ell( \theta_n, \xi_{n+1} ) )_i \neq sign( \nabla f ( \theta_{n} )_i ) ] \nonumber
\end{align}

From Assumption~\ref{assump:sign_convex} it follows that for all $y, x \in \mathbb{R}^d$, $f (y) \geq f (x) + sign( \nabla f (x) )^\top (y - x) - \| y - x \|^2$.
In addition, the second term in Equation~\ref{eq:diag2} can be bounded bellow by $- 2 d / \gamma$.
Applying these bounds,

\begin{align*}
\frac{1}{\gamma} \Ex{ sign( \nabla \ell( \theta_n, \xi_{n+1} ) ) }^\top ( \theta_{n-1} - \theta_n ) 
&\leq \frac{1}{\gamma} [ f( \theta_{n-1} ) - f( \theta_n ) + \| \theta_{n-1} - \theta_n \|^2 ] - \frac{2 d}{\gamma} \\
&\leq \frac{1}{\gamma} [ f( \theta_{n-1} ) - f( \theta_\star ) + \| \theta_{n-1} - \theta_n \|^2 ] - \frac{2 d}{\gamma}
\end{align*}

The second inequality is due to the assumption that $f (\theta_\star)$ is the minimum point.
Now apply expectation to both sides, and apply the assumption bounds.
\begin{align*}
\frac{1}{\gamma} \Ex{ sign( \nabla \ell( \theta_n, \xi_{n+1} ) ) }^\top ( \theta_{n-1} - \theta_n ) 
&\leq \frac{1}{\gamma} [ \delta_\star + \gamma M - 2d ] \\
&< 0
\end{align*}
By our condition on $\gamma$, the bound is negative.

\end{proof}



\section{Quadratic loss}

\section{GLM?}

\section{Experiments}
Try it on CIFAR10, see if behaves better with sign gradient.



% section introduction (end)
%\clearpage
%\newpage
%\singlespacing
%\bibliographystyle{chicago}
%\bibliography{convg_diag}

%\newpage
%\section{Appendix}
%\label{sec:appendix}
%\input{appendix} 
%\fi
\end{document}  