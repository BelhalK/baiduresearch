{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training BNN using SAEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns  # pylint: disable=g-import-not-at-top\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "NUM_TRAIN_EXAMPLES = 60000\n",
    "NUM_HELDOUT_EXAMPLES = 10000\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n",
    "  \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n",
    "\n",
    "  Args:\n",
    "    names: A Python `iterable` of `str` variable names.\n",
    "      qm_vals: A Python `iterable`, the same length as `names`,\n",
    "      whose elements are Numpy `array`s, of any shape, containing\n",
    "      posterior means of weight varibles.\n",
    "    qs_vals: A Python `iterable`, the same length as `names`,\n",
    "      whose elements are Numpy `array`s, of any shape, containing\n",
    "      posterior standard deviations of weight varibles.\n",
    "    fname: Python `str` filename to save the plot to.\n",
    "  \"\"\"\n",
    "  fig = figure.Figure(figsize=(6, 3))\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "\n",
    "  ax = fig.add_subplot(1, 2, 1)\n",
    "  for n, qm in zip(names, qm_vals):\n",
    "    sns.distplot(tf.reshape(qm, shape=[-1]), ax=ax, label=n)\n",
    "  ax.set_title('weight means')\n",
    "  ax.set_xlim([-1.5, 1.5])\n",
    "  ax.legend()\n",
    "\n",
    "  ax = fig.add_subplot(1, 2, 2)\n",
    "  for n, qs in zip(names, qs_vals):\n",
    "    sns.distplot(tf.reshape(qs, shape=[-1]), ax=ax)\n",
    "  ax.set_title('weight stddevs')\n",
    "  ax.set_xlim([0, 1.])\n",
    "\n",
    "  fig.tight_layout()\n",
    "  canvas.print_figure(fname, format='png')\n",
    "  print('saved {}'.format(fname))\n",
    "\n",
    "\n",
    "def plot_heldout_prediction(input_vals, probs,\n",
    "                            fname, n=10, title=''):\n",
    "  \"\"\"Save a PNG plot visualizing posterior uncertainty on heldout data.\n",
    "\n",
    "  Args:\n",
    "    input_vals: A `float`-like Numpy `array` of shape\n",
    "      `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.\n",
    "    probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n",
    "      num_heldout, num_classes]` containing Monte Carlo samples of\n",
    "      class probabilities for each heldout sample.\n",
    "    fname: Python `str` filename to save the plot to.\n",
    "    n: Python `int` number of datapoints to vizualize.\n",
    "    title: Python `str` title for the plot.\n",
    "  \"\"\"\n",
    "  fig = figure.Figure(figsize=(9, 3*n))\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "  for i in range(n):\n",
    "    ax = fig.add_subplot(n, 3, 3*i + 1)\n",
    "    ax.imshow(input_vals[i, :].reshape(IMAGE_SHAPE[:-1]), interpolation='None')\n",
    "\n",
    "    ax = fig.add_subplot(n, 3, 3*i + 2)\n",
    "    for prob_sample in probs:\n",
    "      sns.barplot(np.arange(10), prob_sample[i, :], alpha=0.1, ax=ax)\n",
    "      ax.set_ylim([0, 1])\n",
    "    ax.set_title('posterior samples')\n",
    "\n",
    "    ax = fig.add_subplot(n, 3, 3*i + 3)\n",
    "    sns.barplot(np.arange(10), tf.reduce_mean(probs[:, i, :], axis=0), ax=ax)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title('predictive probs')\n",
    "  fig.suptitle(title)\n",
    "  fig.tight_layout()\n",
    "\n",
    "  canvas.print_figure(fname, format='png')\n",
    "  print('saved {}'.format(fname))\n",
    "\n",
    "\n",
    "def create_model():\n",
    "  \"\"\"Creates a Keras model using the LeNet-5 architecture.\n",
    "\n",
    "  Returns:\n",
    "      model: Compiled Keras model.\n",
    "  \"\"\"\n",
    "  # KL divergence weighted by the number of training samples, using\n",
    "  # lambda function to pass as input to the kernel_divergence_fn on\n",
    "  # flipout layers.\n",
    "  kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /  # pylint: disable=g-long-lambda\n",
    "                            tf.cast(NUM_TRAIN_EXAMPLES, dtype=tf.float32))\n",
    "\n",
    "  # Define a LeNet-5 model using three convolutional (with max pooling)\n",
    "  # and two fully connected dense layers. We use the Flipout\n",
    "  # Monte Carlo estimator for these layers, which enables lower variance\n",
    "  # stochastic gradients than naive reparameterization.\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tfp.layers.Convolution2DFlipout(\n",
    "          6, kernel_size=5, padding='SAME',\n",
    "          kernel_divergence_fn=kl_divergence_function,\n",
    "          activation=tf.nn.relu),\n",
    "      tf.keras.layers.MaxPooling2D(\n",
    "          pool_size=[2, 2], strides=[2, 2],\n",
    "          padding='SAME'),\n",
    "      tfp.layers.Convolution2DFlipout(\n",
    "          16, kernel_size=5, padding='SAME',\n",
    "          kernel_divergence_fn=kl_divergence_function,\n",
    "          activation=tf.nn.relu),\n",
    "      tf.keras.layers.MaxPooling2D(\n",
    "          pool_size=[2, 2], strides=[2, 2],\n",
    "          padding='SAME'),\n",
    "      tfp.layers.Convolution2DFlipout(\n",
    "          120, kernel_size=5, padding='SAME',\n",
    "          kernel_divergence_fn=kl_divergence_function,\n",
    "          activation=tf.nn.relu),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tfp.layers.DenseFlipout(\n",
    "          84, kernel_divergence_fn=kl_divergence_function,\n",
    "          activation=tf.nn.relu),\n",
    "      tfp.layers.DenseFlipout(\n",
    "          NUM_CLASSES, kernel_divergence_fn=kl_divergence_function,\n",
    "          activation=tf.nn.softmax)\n",
    "  ])\n",
    "\n",
    "  # Model compilation.\n",
    "  optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "  # We use the categorical_crossentropy loss since the MNIST dataset contains\n",
    "  # ten labels. The Keras API will then automatically add the\n",
    "  # Kullback-Leibler divergence (contained on the individual layers of\n",
    "  # the model), to the cross entropy loss, effectively\n",
    "  # calcuating the (negated) Evidence Lower Bound Loss (ELBO)\n",
    "  model.compile(optimizer, loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "model_dir = 'bayesian_neural_network/'\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/karimimohammedbelhal/opt/anaconda3/lib/python3.7/site-packages/tensorflow_probability/python/layers/util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    }
   ],
   "source": [
    "tf.io.gfile.makedirs(model_dir)\n",
    "train_set, heldout_set = tf.keras.datasets.mnist.load_data()\n",
    "train_seq = MNISTSequence(data=train_set, batch_size=batch_size)\n",
    "heldout_seq = MNISTSequence(data=heldout_set, batch_size=batch_size)\n",
    "\n",
    "model = create_model()\n",
    "# TODO(b/149259388): understand why Keras does not automatically build the\n",
    "# model correctly.\n",
    "model.build(input_shape=[None, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 2\n",
    "num_monte_carlo=10\n",
    "viz_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch index: 0, Loss: 27.690, Accuracy: 0.109\n",
      " ... Running monte carlo inference\n",
      "79/79 [==============================] - 4s 46ms/step\n",
      "79/79 [==============================] - 3s 40ms/step\n",
      "79/79 [==============================] - 3s 40ms/step\n",
      "79/79 [==============================] - 3s 41ms/step\n",
      "79/79 [==============================] - 3s 40ms/step\n",
      "79/79 [==============================] - 3s 40ms/step\n",
      "79/79 [==============================] - 3s 40ms/step\n",
      "79/79 [==============================] - 3s 41ms/step\n",
      "79/79 [==============================] - 3s 40ms/step\n",
      "79/79 [==============================] - 3s 40ms/step\n",
      " ... Held-out nats: -5.500\n",
      "saved bayesian_neural_network/epoch0_step00099_weights.png\n",
      "saved bayesian_neural_network/epoch0_step99_pred.png\n",
      "Epoch: 0, Batch index: 100, Loss: 22.153, Accuracy: 0.537\n",
      " ... Running monte carlo inference\n",
      "79/79 [==============================] - 3s 42ms/step\n",
      "79/79 [==============================] - 4s 46ms/step\n",
      "79/79 [==============================] - 3s 43ms/step\n",
      "79/79 [==============================] - 3s 43ms/step\n",
      "79/79 [==============================] - 3s 43ms/step\n",
      "79/79 [==============================] - 3s 43ms/step\n",
      "79/79 [==============================] - 3s 43ms/step\n",
      "79/79 [==============================] - 3s 42ms/step\n",
      "79/79 [==============================] - 3s 42ms/step\n",
      "79/79 [==============================] - 4s 46ms/step\n",
      " ... Held-out nats: -6.769\n",
      "saved bayesian_neural_network/epoch0_step00199_weights.png\n",
      "saved bayesian_neural_network/epoch0_step199_pred.png\n",
      "Epoch: 0, Batch index: 200, Loss: 21.465, Accuracy: 0.692\n",
      " ... Running monte carlo inference\n",
      "47/79 [================>.............] - ETA: 1s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dc73ea8fd874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' ... Running monte carlo inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       probs = tf.stack([model.predict(heldout_seq, verbose=1)\n\u001b[0;32m---> 23\u001b[0;31m                         for _ in range(num_monte_carlo)], axis=0)\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mmean_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mheldout_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-dc73ea8fd874>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' ... Running monte carlo inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       probs = tf.stack([model.predict(heldout_seq, verbose=1)\n\u001b[0;32m---> 23\u001b[0;31m                         for _ in range(num_monte_carlo)], axis=0)\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mmean_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mheldout_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(x, y, sample_weights)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;31m# 1, 2, or 3-tuples from generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1262\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  epoch_accuracy, epoch_loss = [], []\n",
    "  for step, (batch_x, batch_y) in enumerate(train_seq):\n",
    "    batch_loss, batch_accuracy = model.train_on_batch(\n",
    "        batch_x, batch_y)\n",
    "    epoch_accuracy.append(batch_accuracy)\n",
    "    epoch_loss.append(batch_loss)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      print('Epoch: {}, Batch index: {}, '\n",
    "            'Loss: {:.3f}, Accuracy: {:.3f}'.format(\n",
    "                epoch, step,\n",
    "                tf.reduce_mean(epoch_loss),\n",
    "                tf.reduce_mean(epoch_accuracy)))\n",
    "\n",
    "    if (step+1) % viz_steps == 0:\n",
    "      # Compute log prob of heldout set by averaging draws from the model:\n",
    "      # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
    "      #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
    "      # where model_i is a draw from the posterior p(model|train).\n",
    "      print(' ... Running monte carlo inference')\n",
    "      probs = tf.stack([model.predict(heldout_seq, verbose=1)\n",
    "                        for _ in range(num_monte_carlo)], axis=0)\n",
    "      mean_probs = tf.reduce_mean(probs, axis=0)\n",
    "      heldout_log_prob = tf.reduce_mean(tf.math.log(mean_probs))\n",
    "      print(' ... Held-out nats: {:.3f}'.format(heldout_log_prob))\n",
    "\n",
    "      if HAS_SEABORN:\n",
    "        names = [layer.name for layer in model.layers\n",
    "                 if 'flipout' in layer.name]\n",
    "        qm_vals = [layer.kernel_posterior.mean()\n",
    "                   for layer in model.layers\n",
    "                   if 'flipout' in layer.name]\n",
    "        qs_vals = [layer.kernel_posterior.stddev()\n",
    "                   for layer in model.layers\n",
    "                   if 'flipout' in layer.name]\n",
    "        plot_weight_posteriors(names, qm_vals, qs_vals,\n",
    "                               fname=os.path.join(\n",
    "                                   model_dir,\n",
    "                                   'epoch{}_step{:05d}_weights.png'.format(\n",
    "                                       epoch, step)))\n",
    "        plot_heldout_prediction(heldout_seq.images, probs,\n",
    "                                fname=os.path.join(\n",
    "                                    model_dir,\n",
    "                                    'epoch{}_step{}_pred.png'.format(\n",
    "                                        epoch, step)),\n",
    "                                title='mean heldout logprob {:.2f}'\n",
    "                                .format(heldout_log_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baiduenv",
   "language": "python",
   "name": "baiduenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
