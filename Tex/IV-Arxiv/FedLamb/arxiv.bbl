\begin{thebibliography}{10}

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{chen2021convergent}
Xiangyi Chen, Belhal Karimi, Weijie Zhao, and Ping Li.
\newblock Convergent adaptive gradient methods in decentralized optimization.
\newblock {\em arXiv preprint arXiv:2109.03194}, 2021.

\bibitem{chen2020toward}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In {\em ACM-IMS Foundations of Data Science Conference (FODS)},
  Seattle, WA, 2020.

\bibitem{D16}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock {\em ICLR (Workshop Track)}, 2016.

\bibitem{DHS11}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research (JMLR)}, 2011.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{haddadpour2020federated}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock {\em arXiv preprint arXiv:2007.01154}, 2020.

\bibitem{haddadpour2020fedsketch}
Farzin Haddadpour, Belhal Karimi, Ping Li, and Xiaoyun Li.
\newblock Fedsketch: Communication-efficient and private federated learning via
  sketching.
\newblock {\em arXiv preprint arXiv:2008.04975}, 2020.

\bibitem{Proc:He-resnet16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em 2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pages
  770--778. {IEEE} Computer Society, 2016.

\bibitem{horvath2019stochastic}
Samuel Horv{\'a}th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and
  Peter Richt{\'a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock {\em arXiv preprint arXiv:1904.05115}, 2019.

\bibitem{ivkin2019communication}
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica,
  and Raman Arora.
\newblock Communication-efficient distributed {SGD} with sketching.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 13144--13154, Vancouver, Canada, 2019.

\bibitem{karimireddy2019scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J Reddi,
  Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for on-device federated
  learning.
\newblock {\em arXiv preprint arXiv:1910.06378}, 2019.

\bibitem{KB15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em ICLR}, 2015.

\bibitem{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em 11th $\{$USENIX$\}$ Symposium on Operating Systems Design and
  Implementation ($\{$OSDI$\}$ 14)}, pages 583--598, 2014.

\bibitem{li2019privacy}
Tian Li, Zaoxing Liu, Vyas Sekar, and Virginia Smith.
\newblock Privacy for free: Communication-efficient learning with differential
  privacy using sketches.
\newblock {\em arXiv preprint arXiv:1911.00972}, 2019.

\bibitem{li2019federated}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock {\em {IEEE} Signal Process. Mag.}, 37(3):50--60, 2020.

\bibitem{liang2019variance}
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei
  Cheng.
\newblock Variance reduced local sgd with lower communication complexity.
\newblock {\em arXiv preprint arXiv:1912.12844}, 2019.

\bibitem{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock {\em arXiv preprint arXiv:1712.01887}, 2017.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{MS10}
H.~Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock {\em COLT}, 2010.

\bibitem{N04}
Yurii Nesterov.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock {\em Springer}, 2004.

\bibitem{P64}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em Mathematics and Mathematical Physics}, 1964.

\bibitem{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock {\em Advances in neural information processing systems}, 24:693--701,
  2011.

\bibitem{reddi2020adaptive}
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\`y}, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock {\em arXiv preprint arXiv:2003.00295}, 2020.

\bibitem{RKK18}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock {\em ICLR}, 2018.

\bibitem{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock {\em arXiv preprint arXiv:1805.09767}, 2018.

\bibitem{TH12}
T.~Tieleman and G.~Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock {\em COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1299--1309, 2018.

\bibitem{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock {\em arXiv preprint arXiv:1904.00962}, 2019.

\bibitem{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock {\em arXiv preprint arXiv:1905.03817}, 2019.

\bibitem{Z12}
Matthew~D. Zeiler.
\newblock Adadelta: An adaptive learning rate method.
\newblock {\em arXiv:1212.5701}, 2012.

\bibitem{zhao2020distributed}
Weijie Zhao, Deping Xie, Ronglai Jia, Yulei Qian, Ruiquan Ding, Mingming Sun,
  and Ping Li.
\newblock Distributed hierarchical gpu parameter server for massive scale deep
  learning ads systems.
\newblock {\em arXiv preprint arXiv:2003.05622}, 2020.

\bibitem{Arxiv:Zhou_18}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em CoRR}, abs/1808.05671, 2018.

\bibitem{zhou2017convergence}
Fan Zhou and Guojing Cong.
\newblock On the convergence properties of a $ k $-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1708.01012}, 2017.

\end{thebibliography}
