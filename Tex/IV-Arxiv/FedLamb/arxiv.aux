\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{konevcny2016federated}
\citation{mcmahan2017communication}
\citation{alistarh2017qsgd}
\citation{wangni2018gradient}
\citation{lin2017deep}
\citation{mcmahan2017communication}
\citation{chen2020toward}
\citation{you2019large}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:introduction}{{1}{2}{Introduction}{section.1}{}}
\newlabel{eq:opt}{{1}{2}{Introduction}{equation.1.1}{}}
\citation{RKK18}
\citation{KB15}
\citation{TH12}
\citation{Z12}
\citation{D16}
\citation{DHS11}
\citation{MS10}
\citation{N04}
\citation{P64}
\citation{DHS11}
\citation{KB15}
\citation{KB15}
\citation{RKK18}
\citation{you2019large}
\citation{konevcny2016federated}
\citation{recht2011hogwild}
\citation{li2014scaling}
\citation{zhao2020distributed}
\citation{mcmahan2017communication}
\@writefile{toc}{\contentsline {paragraph}{Adaptive gradient methods.}{3}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{Federated learning.}{3}{section*.2}}
\citation{mcmahan2017communication}
\citation{konevcny2016federated}
\citation{zhou2017convergence}
\citation{stich2018local}
\citation{yu2019linear}
\citation{chen2020toward}
\@writefile{toc}{\contentsline {section}{\numberline {2}Layerwise and Dimensionwise Adaptive Method}{4}{section.2}}
\newlabel{sec:main}{{2}{4}{Layerwise and Dimensionwise Adaptive Method}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}AMSGrad, Local AMSGrad and Periodic Averaging}{4}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Layerwise and Dimensionwise Learning with Periodic Averaging}{4}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of Fed-LAMB (Algorithm\nobreakspace  {}\ref  {alg:ldams}), with a three-layer network and $\phi (x)=x$ as an example. The depth of each network layer represents the norm of its weights. For device $i$ and each local iteration in round $r$, the adaptive ratio of $j$-th layer $p_{r,i}^j$ is normalized according to $\delimiter "026B30D \theta _{r,i}^j\delimiter "026B30D $, and then used for updating the local model. At the end of each round $r$, local worker $i$ sends $\theta _{r,i} = [\theta _{r,i}^{\ell }]_{\ell =1}^{\mathsf  {h}}$ and $v_{r,i}$ to the central server, which transmits back aggregated $\theta $ and $\mathaccentV {hat}05Ev$ to local devices to complete a round of training.\relax }}{5}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:illustrate}{{1}{5}{Illustration of Fed-LAMB (Algorithm~\ref {alg:ldams}), with a three-layer network and $\phi (x)=x$ as an example. The depth of each network layer represents the norm of its weights. For device $i$ and each local iteration in round $r$, the adaptive ratio of $j$-th layer $p_{r,i}^j$ is normalized according to $\Vert \theta _{r,i}^j\Vert $, and then used for updating the local model. At the end of each round $r$, local worker $i$ sends $\theta _{r,i} = [\theta _{r,i}^{\ell }]_{\ell =1}^{\tot }$ and $v_{r,i}$ to the central server, which transmits back aggregated $\theta $ and $\hat v$ to local devices to complete a round of training.\relax }{figure.caption.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {Fed-LAMB}\ for Federated Learning\relax }}{5}{algorithm.1}}
\newlabel{alg:ldams}{{1}{5}{\algo \ for Federated Learning\relax }{algorithm.1}{}}
\newlabel{line:new1}{{9}{5}{\algo \ for Federated Learning\relax }{algorithm.1}{}}
\newlabel{line:new2}{{10}{5}{\algo \ for Federated Learning\relax }{algorithm.1}{}}
\newlabel{line:scale}{{11}{5}{\algo \ for Federated Learning\relax }{algorithm.1}{}}
\newlabel{line:layer}{{12}{5}{\algo \ for Federated Learning\relax }{algorithm.1}{}}
\newlabel{line:final}{{16}{5}{\algo \ for Federated Learning\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}On The Convergence of \textsc  {Fed-LAMB}}{5}{section.3}}
\newlabel{sec:theory}{{3}{5}{On The Convergence of \algo }{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Finite Time Analysis of \textsc  {Fed-LAMB}}{6}{subsection.3.1}}
\newlabel{ass:smooth}{{1}{6}{}{assumption.1}{}}
\newlabel{ass:boundgrad}{{2}{6}{}{assumption.2}{}}
\newlabel{ass:var}{{3}{6}{}{assumption.3}{}}
\newlabel{ass:phi}{{4}{6}{}{assumption.4}{}}
\newlabel{lemma:iterates}{{1}{6}{}{Lemma.1}{}}
\citation{you2019large}
\citation{you2019large}
\citation{ghadimi2013stochastic}
\citation{you2019large}
\citation{chen2020toward}
\citation{chen2020toward}
\newlabel{lemma:ratio}{{2}{7}{}{Lemma.2}{}}
\newlabel{th:multiple update}{{1}{7}{}{Theorem.1}{}}
\newlabel{bound1multiple}{{4}{7}{}{equation.3.4}{}}
\newlabel{coro:main}{{1}{7}{}{Corollary.1}{}}
\newlabel{coro:rate}{{5}{7}{}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Comparisons and discussions}{7}{subsection.3.2}}
\citation{chen2020toward}
\citation{Arxiv:Zhou_18}
\citation{chen2020toward}
\citation{karimireddy2019scaffold}
\citation{reddi2020adaptive}
\citation{chen2020toward}
\citation{li2019federated}
\citation{liang2019variance}
\citation{haddadpour2020federated}
\citation{horvath2019stochastic}
\citation{karimireddy2019scaffold}
\citation{haddadpour2020fedsketch}
\citation{ivkin2019communication}
\citation{li2019privacy}
\citation{RKK18}
\citation{lecun1998mnist}
\citation{krizhevsky2009learning}
\citation{Proc:He-resnet16}
\citation{mcmahan2017communication}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{9}{section.4}}
\newlabel{sec:numerical}{{4}{9}{Numerical Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}MNIST with Multilayer Perceptron and Convolutional Neural Network}{9}{subsection.4.1}}
\citation{chen2020toward}
\citation{reddi2020adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Top Row}: Test accuracy on MNIST+MLP, with non-iid data distribution. \textbf  {Bottom Row}: Test accuracy on MNIST+CNN, with non-iid data distribution. \textbf  {Left panel:} 1 local epoch. \textbf  {Right panel:} 5 local epochs. 50 clients for each run.\relax }}{10}{figure.caption.4}}
\newlabel{fig:mnist-mlp-noniid}{{2}{10}{\textbf {Top Row}: Test accuracy on MNIST+MLP, with non-iid data distribution. \textbf {Bottom Row}: Test accuracy on MNIST+CNN, with non-iid data distribution. \textbf {Left panel:} 1 local epoch. \textbf {Right panel:} 5 local epochs. 50 clients for each run.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}CIFAR-10 with Convolutional Neural Network and Residual Neural Network}{10}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Top Row}: Test accuracy on CIFAR+ResNet, with iid data distribution for 10 clients 1 and 3 local epochs and 50 clients 1 local epoch respectively. \textbf  {Middle Row:} Test accuracy on CIFAR+ResNet with iid data distribution for 50 clients 3 local epochs and on CIFAR+CNN with iid data distribution for 50 clients 1 and 3 local epochs respectively. \textbf  {Bottom Row:} Test accuracy on CIFAR+CNN with non iid data distribution for 50 clients and 3 local epochs.\relax }}{11}{figure.caption.5}}
\newlabel{fig:cifar-cnn-iid}{{3}{11}{\textbf {Top Row}: Test accuracy on CIFAR+ResNet, with iid data distribution for 10 clients 1 and 3 local epochs and 50 clients 1 local epoch respectively. \textbf {Middle Row:} Test accuracy on CIFAR+ResNet with iid data distribution for 50 clients 3 local epochs and on CIFAR+CNN with iid data distribution for 50 clients 1 and 3 local epochs respectively. \textbf {Bottom Row:} Test accuracy on CIFAR+CNN with non iid data distribution for 50 clients and 3 local epochs.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{11}{section.5}}
\newlabel{sec:conclusion}{{5}{11}{Conclusion}{section.5}{}}
\bibstyle{plain}
\bibdata{ref}
\bibcite{alistarh2017qsgd}{1}
\bibcite{chen2020toward}{2}
\bibcite{D16}{3}
\bibcite{DHS11}{4}
\bibcite{ghadimi2013stochastic}{5}
\bibcite{haddadpour2020federated}{6}
\bibcite{haddadpour2020fedsketch}{7}
\bibcite{Proc:He-resnet16}{8}
\bibcite{horvath2019stochastic}{9}
\bibcite{ivkin2019communication}{10}
\bibcite{karimireddy2019scaffold}{11}
\bibcite{KB15}{12}
\bibcite{konevcny2016federated}{13}
\bibcite{krizhevsky2009learning}{14}
\bibcite{lecun1998mnist}{15}
\bibcite{li2014scaling}{16}
\bibcite{li2019privacy}{17}
\bibcite{li2019federated}{18}
\bibcite{liang2019variance}{19}
\bibcite{lin2017deep}{20}
\bibcite{mcmahan2017communication}{21}
\bibcite{MS10}{22}
\bibcite{N04}{23}
\bibcite{P64}{24}
\bibcite{recht2011hogwild}{25}
\bibcite{reddi2020adaptive}{26}
\bibcite{RKK18}{27}
\bibcite{stich2018local}{28}
\bibcite{TH12}{29}
\bibcite{wangni2018gradient}{30}
\bibcite{you2019large}{31}
\bibcite{yu2019linear}{32}
\bibcite{Z12}{33}
\bibcite{zhao2020distributed}{34}
\bibcite{Arxiv:Zhou_18}{35}
\bibcite{zhou2017convergence}{36}
\@writefile{toc}{\contentsline {section}{\numberline {A}Theoretical Analysis}{16}{appendix.A}}
\newlabel{app:proofs}{{A}{16}{Theoretical Analysis}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of notations used in the paper.\relax }}{16}{table.caption.7}}
\newlabel{tab:notationsapp}{{1}{16}{Summary of notations used in the paper.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Intermediary Lemmas}{16}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem\nobreakspace  {}\ref  {th:multiple update}}{17}{subsection.A.2}}
\newlabel{app:proofmain}{{A.2}{17}{Proof of Theorem~\ref {th:multiple update}}{subsection.A.2}{}}
\citation{RKK18}
\newlabel{eq:main}{{7}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.A.7}{}}
\newlabel{eq:defseq}{{8}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.A.8}{}}
\newlabel{eq:gap}{{9}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.A.9}{}}
\newlabel{eq:main2}{{12}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.A.12}{}}
\newlabel{eq:inner}{{13}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.A.13}{}}
\newlabel{eqn1}{{15}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.A.15}{}}
\newlabel{eqn:x1}{{16}{19}{Proof of Theorem~\ref {th:multiple update}}{equation.A.16}{}}
\newlabel{eq:inter}{{17}{19}{Proof of Theorem~\ref {th:multiple update}}{equation.A.17}{}}
\newlabel{eqn:B1}{{18}{19}{Proof of Theorem~\ref {th:multiple update}}{equation.A.18}{}}
\citation{Proc:He-resnet16}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Numerical Experiments}{22}{appendix.B}}
\newlabel{app:numericals}{{B}{22}{Additional Numerical Experiments}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {From Left to Right}: Test accuracy on CIFAR+ResNet, with iid data distribution. 10 clients and (Left) 1 local epoch, (Right) 3 local epochs.\relax }}{22}{figure.caption.8}}
\newlabel{fig:cifar-cnn-iid-bis}{{4}{22}{\textbf {From Left to Right}: Test accuracy on CIFAR+ResNet, with iid data distribution. 10 clients and (Left) 1 local epoch, (Right) 3 local epochs.\relax }{figure.caption.8}{}}
