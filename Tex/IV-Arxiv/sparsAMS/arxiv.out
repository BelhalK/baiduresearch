\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Our Contributions}{section.1}% 2
\BOOKMARK [1][-]{section.2}{Related Work}{}% 3
\BOOKMARK [2][-]{subsection.2.1}{Distributed SGD with Compressed Gradients}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.2}{Adaptive Optimization}{section.2}% 5
\BOOKMARK [1][-]{section.3}{Communication-Efficient Adaptive Optimization}{}% 6
\BOOKMARK [2][-]{subsection.3.1}{Gradient Compressors}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.2}{Comp-AMS for Distributed Optimization}{section.3}% 8
\BOOKMARK [1][-]{section.4}{Convergence Analysis}{}% 9
\BOOKMARK [1][-]{section.5}{Experiments}{}% 10
\BOOKMARK [2][-]{subsection.5.1}{Error Feedback Fixes the Convergence of Compressed AMSGrad}{section.5}% 11
\BOOKMARK [2][-]{subsection.5.2}{Linear Speedup of Comp-AMS}{section.5}% 12
\BOOKMARK [2][-]{subsection.5.3}{General Evaluation and Communication Efficiency}{section.5}% 13
\BOOKMARK [2][-]{subsection.5.4}{Discussion on Comp-AMS and QAdam}{section.5}% 14
\BOOKMARK [1][-]{section.6}{Conclusion}{}% 15
\BOOKMARK [1][-]{appendix.A}{Additional content}{}% 16
\BOOKMARK [2][-]{subsection.A.1}{Extension to the Single-Machine Setting}{appendix.A}% 17
\BOOKMARK [2][-]{subsection.A.2}{QAdam Method}{appendix.A}% 18
\BOOKMARK [1][-]{appendix.B}{Proof of the Convergence Results}{}% 19
\BOOKMARK [2][-]{subsection.B.1}{Proof of Theorem 1}{appendix.B}% 20
\BOOKMARK [2][-]{subsection.B.2}{Intermidiary Lemmas}{appendix.B}% 21
\BOOKMARK [1][-]{appendix.C}{Model Architecture of the Experiments}{}% 22
