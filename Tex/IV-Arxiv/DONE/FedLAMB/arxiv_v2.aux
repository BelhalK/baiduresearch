\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{konevcny2016federated,mcmahan2017communication}
\citation{alistarh2017qsgd,wangni2018gradient}
\citation{lin2017deep}
\citation{Proc:Rothchild_ICML20}
\citation{mcmahan2017communication}
\citation{yu2019linear}
\citation{chen2020toward}
\citation{chen2020toward}
\citation{li2019federated,liang2019variance,karimireddy2019scaffold}
\citation{you2019large}
\citation{chen2020toward}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:introduction}{{1}{2}{Introduction}{section.1}{}}
\newlabel{eq:opt}{{1}{2}{Introduction}{equation.1.1}{}}
\citation{TH12}
\citation{Z12}
\citation{KB15}
\citation{dozat2016incorporating}
\citation{reddi2019convergence}
\citation{DHS11,mcmahan2010adaptive}
\citation{N04}
\citation{P64}
\citation{DHS11}
\citation{KB15}
\citation{KB15}
\citation{reddi2019convergence}
\citation{zhou2020towards}
\citation{you2019large}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}}
\newlabel{sec:related}{{2}{3}{Related Work}{section.2}{}}
\citation{konevcny2016federated}
\citation{recht2011hogwild,li2014scaling,zhao2020distributed}
\citation{mcmahan2017communication}
\citation{mcmahan2017communication}
\citation{konevcny2016federated,zhou2017convergence}
\citation{stich2018local,yu2019linear}
\citation{chen2020toward}
\@writefile{toc}{\contentsline {section}{\numberline {3}Layer-wise and Dimension-wise Adaptive Optimization}{4}{section.3}}
\newlabel{sec:main}{{3}{4}{Layer-wise and Dimension-wise Adaptive Optimization}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}AMSGrad, Local AMSGrad and Periodic Averaging}{4}{subsection.3.1}}
\citation{you2019large}
\citation{you2019large}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Layer-wise and Dimension-wise Adaptive Local Update}{5}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of \textsc  {Fed-LAMB}\ (Algorithm\nobreakspace  {}\ref  {alg:ldams}), with a three-layer network and $\phi (x)=x$ as an example. For device $i$ and each local iteration in round $r$, the adaptive ratio of $j$-th layer $p_{r,i}^j$ is normalized according to $\delimiter "026B30D \theta _{r,i}^j\delimiter "026B30D $, and then used for updating the local model. At the end of each round $r$, local worker $i$ sends $\theta _{r,i} = [\theta _{r,i}^{\ell }]_{\ell =1}^{\mathsf  {h}}$ and $v_{r,i}$ to the central server, which transmits back aggregated $\theta $ and $\mathaccentV {hat}05Ev$ to local devices to complete a round of training.}}{5}{figure.1}}
\newlabel{fig:illustrate}{{1}{5}{Illustration of \algo \ (Algorithm~\ref {alg:ldams}), with a three-layer network and $\phi (x)=x$ as an example. For device $i$ and each local iteration in round $r$, the adaptive ratio of $j$-th layer $p_{r,i}^j$ is normalized according to $\Vert \theta _{r,i}^j\Vert $, and then used for updating the local model. At the end of each round $r$, local worker $i$ sends $\theta _{r,i} = [\theta _{r,i}^{\ell }]_{\ell =1}^{\tot }$ and $v_{r,i}$ to the central server, which transmits back aggregated $\theta $ and $\hat v$ to local devices to complete a round of training}{figure.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {Fed-LAMB}\ for federated learning}}{6}{algorithm.1}}
\newlabel{alg:ldams}{{1}{6}{Layer-wise and Dimension-wise Adaptive Local Update}{algorithm.1}{}}
\newlabel{line:new1}{{9}{6}{Layer-wise and Dimension-wise Adaptive Local Update}{algorithm.1}{}}
\newlabel{line:new2}{{10}{6}{Layer-wise and Dimension-wise Adaptive Local Update}{algorithm.1}{}}
\newlabel{line:scale}{{11}{6}{Layer-wise and Dimension-wise Adaptive Local Update}{algorithm.1}{}}
\newlabel{line:layer}{{12}{6}{Layer-wise and Dimension-wise Adaptive Local Update}{algorithm.1}{}}
\newlabel{eq:upadtelayer}{{2}{6}{Layer-wise and Dimension-wise Adaptive Local Update}{equation.3.2}{}}
\newlabel{line:final}{{16}{6}{Layer-wise and Dimension-wise Adaptive Local Update}{equation.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Convergence of \textsc  {Fed-LAMB}}{6}{section.4}}
\newlabel{sec:theory}{{4}{6}{Convergence of \algo }{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Finite Time Analysis of \textsc  {Fed-LAMB}}{7}{subsection.4.1}}
\newlabel{ass:smooth}{{1}{7}{}{assumption.1}{}}
\newlabel{ass:boundgrad}{{2}{7}{}{assumption.2}{}}
\newlabel{ass:var}{{3}{7}{}{assumption.3}{}}
\newlabel{ass:phi}{{4}{7}{}{assumption.4}{}}
\newlabel{lemma:iterates}{{1}{7}{}{Lemma.1}{}}
\newlabel{lemma:ratio}{{2}{7}{}{Lemma.2}{}}
\citation{you2019large}
\citation{you2019large}
\citation{ghadimi2013stochastic}
\citation{you2019large}
\citation{chen2020toward}
\citation{chen2020toward}
\citation{chen2020toward}
\newlabel{th:multiple update}{{1}{8}{}{Theorem.1}{}}
\newlabel{bound1multiple}{{3}{8}{}{equation.4.3}{}}
\newlabel{coro:main}{{1}{8}{}{Corollary.1}{}}
\newlabel{coro:rate}{{4}{8}{}{equation.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Comparisons}{8}{subsection.4.2}}
\citation{Arxiv:Zhou_18}
\citation{chen2020toward}
\citation{karimireddy2019scaffold}
\citation{reddi2020adaptive}
\citation{KB15}
\citation{reddi2020adaptive}
\citation{karimireddy2019scaffold}
\newlabel{thm:chen}{{2}{9}{Corollary 5.2 in~\citet {chen2020toward}}{Theorem.2}{}}
\newlabel{eqn:chen rate}{{5}{9}{Corollary 5.2 in~\citet {chen2020toward}}{equation.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}More Discussion}{9}{subsection.4.3}}
\citation{mcmahan2017communication}
\citation{chen2020toward}
\citation{reddi2020adaptive}
\citation{reddi2019convergence}
\citation{lecun1998mnist}
\citation{xiao2017fashion}
\citation{krizhevsky2009learning}
\citation{deng2009imagenet}
\citation{Proc:He-resnet16}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{10}{section.5}}
\newlabel{sec:numerical}{{5}{10}{Numerical Experiments}{section.5}{}}
\citation{reddi2020adaptive}
\citation{mcmahan2017communication}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Comparison under iid setting}{11}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {i.i.d. data setting}. Test accuracy on MNIST and FMNIST against the number of communication rounds with 50 clients. \textbf  {1st row:} 1 local epoch. \textbf  {2nd row:} 3 local epochs. We see that \textsc  {Fed-LAMB}\ converges faster to better solution (higher test accuracy) in all cases. }}{11}{figure.2}}
\newlabel{fig:iid}{{2}{11}{\textbf {i.i.d. data setting}. Test accuracy on MNIST and FMNIST against the number of communication rounds with 50 clients. \textbf {1st row:} 1 local epoch. \textbf {2nd row:} 3 local epochs. We see that \algo \ converges faster to better solution (higher test accuracy) in all cases}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {non-i.i.d. data setting.} Test accuracy on MNIST and FMNIST against the number of communication rounds, with 50 clients. \textbf  {1st row:} 1 local epoch. \textbf  {2nd row:} 3 local epochs. We see that \textsc  {Fed-LAMB}\ converges faster to better solution (higher test accuracy) in all cases.}}{12}{figure.3}}
\newlabel{fig:noniid}{{3}{12}{\textbf {non-i.i.d. data setting.} Test accuracy on MNIST and FMNIST against the number of communication rounds, with 50 clients. \textbf {1st row:} 1 local epoch. \textbf {2nd row:} 3 local epochs. We see that \algo \ converges faster to better solution (higher test accuracy) in all cases}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Comparison under non-iid setting}{12}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {non-i.i.d. data setting.} Test accuracy on CIFAR-10 + ResNet-18 and TinyImagenet + ResNet-18 with 50 clients. }}{13}{figure.4}}
\newlabel{fig:noniidresnet18}{{4}{13}{\textbf {non-i.i.d. data setting.} Test accuracy on CIFAR-10 + ResNet-18 and TinyImagenet + ResNet-18 with 50 clients}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test Accuracy on ResNet-18 Network.}}{13}{table.1}}
\newlabel{tab:acc}{{1}{13}{Test Accuracy on ResNet-18 Network}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{13}{section.6}}
\newlabel{sec:conclusion}{{6}{13}{Conclusion}{section.6}{}}
\citation{reddi2020adaptive}
\citation{chen2020toward}
\citation{reddi2020adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {A}Hyper-parameter Tuning and Algorithms}{14}{appendix.A}}
\newlabel{app:experiment}{{A}{14}{Hyper-parameter Tuning and Algorithms}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}The Adp-Fed Algorithm\nobreakspace  {}\citep  {reddi2020adaptive}}{14}{subsection.A.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Adaptive Federated Optimization\nobreakspace  {}\citep  {reddi2020adaptive}}}{14}{algorithm.2}}
\newlabel{alg:adp-fed}{{2}{14}{The Adp-Fed Algorithm~\citep {reddi2020adaptive}}{algorithm.2}{}}
\newlabel{adpfed line:local SGD}{{8}{14}{The Adp-Fed Algorithm~\citep {reddi2020adaptive}}{algorithm.2}{}}
\newlabel{adpfed line:global adam}{{15}{14}{The Adp-Fed Algorithm~\citep {reddi2020adaptive}}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Hyper-parameter Tuning}{14}{subsection.A.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Search grids of the learning rate.}}{14}{table.2}}
\newlabel{tab:tuning}{{2}{14}{Search grids of the learning rate}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Theoretical Analysis}{15}{appendix.B}}
\newlabel{app:proofs}{{B}{15}{Theoretical Analysis}{appendix.B}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Summary of notations used in the paper.}}{15}{table.3}}
\newlabel{tab:notationsapp}{{3}{15}{Summary of notations used in the paper}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Intermediary Lemmas}{15}{subsection.B.1}}
\citation{reddi2019convergence}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proof of Theorem\nobreakspace  {}\ref  {th:multiple update}}{16}{subsection.B.2}}
\newlabel{app:proofmain}{{B.2}{16}{Proof of Theorem~\ref {th:multiple update}}{subsection.B.2}{}}
\newlabel{eq:main}{{6}{16}{Proof of Theorem~\ref {th:multiple update}}{equation.B.6}{}}
\newlabel{eq:defseq}{{7}{17}{Proof of Theorem~\ref {th:multiple update}}{equation.B.7}{}}
\newlabel{eq:gap}{{8}{17}{Proof of Theorem~\ref {th:multiple update}}{equation.B.8}{}}
\newlabel{eq:main2}{{9}{17}{Proof of Theorem~\ref {th:multiple update}}{equation.B.9}{}}
\newlabel{eqn1}{{10}{17}{Proof of Theorem~\ref {th:multiple update}}{equation.B.10}{}}
\newlabel{eqn:x1}{{11}{17}{Proof of Theorem~\ref {th:multiple update}}{equation.B.11}{}}
\newlabel{eq:inter}{{12}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.B.12}{}}
\newlabel{eqn:B1}{{13}{18}{Proof of Theorem~\ref {th:multiple update}}{equation.B.13}{}}
\bibstyle{plainnat}
\bibdata{ref}
\bibcite{alistarh2017qsgd}{{1}{2017}{{Alistarh et~al.}}{{Alistarh, Grubic, Li, Tomioka, and Vojnovic}}}
\bibcite{chen2020toward}{{2}{2020}{{Chen et~al.}}{{Chen, Li, and Li}}}
\bibcite{deng2009imagenet}{{3}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei{-}Fei}}}
\bibcite{dozat2016incorporating}{{4}{2016}{{Dozat}}{{}}}
\bibcite{DHS11}{{5}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{ghadimi2013stochastic}{{6}{2013}{{Ghadimi and Lan}}{{}}}
\bibcite{Proc:He-resnet16}{{7}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{karimireddy2019scaffold}{{8}{2019}{{Karimireddy et~al.}}{{Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh}}}
\bibcite{KB15}{{9}{2015}{{Kingma and Ba}}{{}}}
\bibcite{konevcny2016federated}{{10}{2016}{{Kone{\v {c}}n{\`y} et~al.}}{{Kone{\v {c}}n{\`y}, McMahan, Yu, Richt{\'a}rik, Suresh, and Bacon}}}
\bibcite{krizhevsky2009learning}{{11}{2009}{{Krizhevsky}}{{}}}
\bibcite{lecun1998mnist}{{12}{1998}{{LeCun}}{{}}}
\bibcite{li2014scaling}{{13}{2014}{{Li et~al.}}{{Li, Andersen, Park, Smola, Ahmed, Josifovski, Long, Shekita, and Su}}}
\bibcite{li2019federated}{{14}{2020}{{Li et~al.}}{{Li, Sahu, Talwalkar, and Smith}}}
\bibcite{liang2019variance}{{15}{2019}{{Liang et~al.}}{{Liang, Shen, Liu, Pan, Chen, and Cheng}}}
\bibcite{lin2017deep}{{16}{2018}{{Lin et~al.}}{{Lin, Han, Mao, Wang, and Dally}}}
\bibcite{mcmahan2010adaptive}{{17}{2010}{{McMahan and Streeter}}{{}}}
\bibcite{mcmahan2017communication}{{18}{2017}{{McMahan et~al.}}{{McMahan, Moore, Ramage, Hampson, and y~Arcas}}}
\bibcite{N04}{{19}{2004}{{Nesterov}}{{}}}
\bibcite{P64}{{20}{1964}{{Polyak}}{{}}}
\bibcite{recht2011hogwild}{{21}{2011}{{Recht et~al.}}{{Recht, R{\'{e}}, Wright, and Niu}}}
\bibcite{reddi2019convergence}{{22}{2018}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{reddi2020adaptive}{{23}{2021}{{Reddi et~al.}}{{Reddi, Charles, Zaheer, Garrett, Rush, Kone{\v {c}}n{\'y}, Kumar, and McMahan}}}
\bibcite{Proc:Rothchild_ICML20}{{24}{}{{Rothchild et~al.}}{{Rothchild, Panda, Ullah, Ivkin, Stoica, Braverman, Gonzalez, and Arora}}}
\bibcite{stich2018local}{{25}{2019}{{Stich}}{{}}}
\bibcite{TH12}{{26}{2012}{{Tieleman and Hinton}}{{}}}
\bibcite{wangni2018gradient}{{27}{2018}{{Wangni et~al.}}{{Wangni, Wang, Liu, and Zhang}}}
\bibcite{xiao2017fashion}{{28}{2017}{{Xiao et~al.}}{{Xiao, Rasul, and Vollgraf}}}
\bibcite{you2019large}{{29}{2020}{{You et~al.}}{{You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song, Demmel, Keutzer, and Hsieh}}}
\bibcite{yu2019linear}{{30}{2019}{{Yu et~al.}}{{Yu, Jin, and Yang}}}
\bibcite{Z12}{{31}{2012}{{Zeiler}}{{}}}
\bibcite{zhao2020distributed}{{32}{2020}{{Zhao et~al.}}{{Zhao, Xie, Jia, Qian, Ding, Sun, and Li}}}
\bibcite{Arxiv:Zhou_18}{{33}{2018}{{Zhou et~al.}}{{Zhou, Chen, Cao, Tang, Yang, and Gu}}}
\bibcite{zhou2017convergence}{{34}{2018}{{Zhou and Cong}}{{}}}
\bibcite{zhou2020towards}{{35}{2020}{{Zhou et~al.}}{{Zhou, Karimi, Yu, Xu, and Li}}}
