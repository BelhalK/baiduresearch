%\subsection{Localized Federated Optimization}

\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%


\begin{algorithm}[H]
\caption{\texttt{CFL}($R$, $\tau, \eta, \gamma$): Communication-efficient Federated Learning with Sketching for homogeneous setting. }\label{Alg:ce-h}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $\boldsymbol{w}^{(0)}$ as an initial  model shared by all local devices, the number of communication rounds $R$, the the number of local updates $\tau$, and global and local learning rates $\gamma$ and $\eta$, respectively
%\State Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\State \textbf{for $r=0, \ldots, R-1$ do}
\State $\qquad$\textbf{parallel for device $j=1,\ldots,n$ do}:
\State $\qquad\quad$ Set $\boldsymbol{w}^{(r)}=\boldsymbol{w}^{(r-1)}-\gamma\underline{\mathbf{S}}^{(r)}$
\State $\qquad\quad$ Set $\boldsymbol{w}_j^{(0,r)}=\boldsymbol{w}^{(r)}$ 
\State $\qquad\quad $\textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}
\State $\qquad\quad\quad$ Sample a mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}\triangleq\nabla{f}_j(\boldsymbol{w}^{(\ell,r)}_j,\xi_j^{(\ell,r)})$
\State $\qquad\quad\quad$ $\boldsymbol{w}^{(\ell+1,r)}_{j}=\boldsymbol{w}^{(\ell,r)}_j-\eta~ \tilde{\mathbf{g}}_{j}^{(\ell,r)}$ \label{eq:update-rule-alg}
\State $\qquad\quad$\textbf{end for}
\State $\qquad\quad\quad$Device $j$ sends $\mathbf{S}_j^{(r)}=\mathbf{S}\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}\right)$ back to the server.
\State $\qquad$Server \textbf{computes} 
\State $\qquad\qquad {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}^n\mathbf{S}\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}\right)$ %and \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.
\State $\qquad\qquad$ Sever runs $\underline{\mathbf{S}}^{(r)}= \texttt{HEAVYMIX}(\mathbf{S}^{(r)})$.
%\State $\qquad\quad$ \textbf{end if}
\State $\qquad$\textbf{end parallel for}
\State \textbf{end}
\State \textbf{Output:} ${\boldsymbol{w}}^{(R-1)}$
\vspace{- 0.1cm}
%\State \todo{How about we call the algorithm local SGD with decoupled rates (LSDR) and specialize the analysis for IID and non-IID cases}
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{\texttt{CFL}($R$, $\tau, \eta, \gamma$): Communication-efficient Federated Learning with Sketching and gradient tracking. }\label{Alg:one-shot-using data samoples-b}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $\boldsymbol{w}^{(0)}$ as an initial  model shared by all local devices, the number of communication rounds $R$, the the number of local updates $\tau$, and global and local learning rates $\gamma$ and $\eta$, respectively
%\State Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\State \textbf{for $r=0, \ldots, R-1$ do}
\State $\qquad$\textbf{parallel for device $j=1,\ldots,n$ do}:
\State $\qquad\quad$ Set $\boldsymbol{w}_j^{(0,r)}=\boldsymbol{w}^{(r-1)}-\gamma\underline{\mathbf{S}}^{(r)}$ 
\State $\qquad\quad$ Set $\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left(\underline{\mathbf{S}}^{(r)}-\underline{\mathbf{S}}^{(r)}_j\right)$
\State $\qquad\quad $\textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}
\State $\qquad\quad\quad$ Sample a mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}\triangleq\nabla{f}_j(\boldsymbol{w}^{(\ell,r)}_j,\xi_j^{(\ell,r)})$
\State $\qquad\quad\quad$ $\boldsymbol{w}^{(\ell+1,r)}_{j}=\boldsymbol{w}^{(\ell,r)}_j-\eta~ \tilde{\mathbf{g}}_{j}^{(\ell,r)}$ \label{eq:update-rule-alg}
\State $\qquad\quad$\textbf{end for}
\State $\qquad\quad\quad$Device $j$ sends $\mathbf{S}_j^{(r)}=\mathbf{S}\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}\right)$ back to the server.
\State $\qquad$Server \textbf{runs} 
\State $\qquad\qquad\underline{\mathbf{S}}_j^{(r)}= \texttt{HEAVYMIX}(\mathbf{S}_j^{(r)})$ and returns $\underline{\mathbf{S}}_j^{(r)}$ to server $j$.
\State $\qquad\qquad \underline{\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}^n\underline{\mathbf{S}}_j^{(r)}$ %and \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.
\State $\qquad\qquad$ Sever broadcasts $\underline{\mathbf{S}}^{(r)}$.
%\State $\qquad\quad$ \textbf{end if}
\State $\qquad$\textbf{end parallel for}
\State \textbf{end}
\State \textbf{Output:} ${\boldsymbol{w}}^{(R-1)}$
\vspace{- 0.1cm}
%\State \todo{How about we call the algorithm local SGD with decoupled rates (LSDR) and specialize the analysis for IID and non-IID cases}
\end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{comment}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Assumptions%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\begin{assumption}[Contraction \cite{stich2018sparsified}]
A randomized function $\texttt{Comp}_k:\mathbb{R}^d\rightarrow\mathbb{R}^d$ is called a compression operator, if there exists a constant $c\in(0,1]$ (that may depend on $k$ and $d$), such that for every $\mathbf{x}\in\mathbb{R}^d$, we have $\mathbb{E}\left[\left\|\mathbf{x}-\texttt{Comp}_k(\mathbf{x})\right\|^2_2\right]\leq(1-c)\left\|\mathbf{x}\right\|^2_2$ where expectation is taken over $\texttt{Comp}_k$. 
\end{assumption}
We note that for the contraction operators (see~ \cite{stich2018sparsified} for more examples) we have $c=\frac{k}{d}$.
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{\texttt{FEDSKETCH-III}($R$, $\tau, \eta, \gamma$): Communication-efficient Federated Learning via Sketching with memory. }\label{Alg:ce-h-wm}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $\boldsymbol{w}^{(0)}$ as an initial  model shared by all local devices, the number of communication rounds $R$, the the number of local updates $\tau$, and global and local learning rates $\gamma$ and $\eta$, respectively
%\State Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\State \textbf{for $r=0, \ldots, R-1$ do}
\State $\qquad$\textbf{parallel for device $j=1,\ldots,n$ do}:
\State $\qquad\quad$ Set $\boldsymbol{w}^{(r)}=\boldsymbol{w}^{(r-1)}-\gamma\underline{\mathbf{S}}^{(r)}$ \belhal{This should be out of the local loops right?} \textcolor{blue}{FH: Good point, but in order to save communication cost each worker store both local and global models, so this should be here!}
\State $\qquad\quad$ Set $\boldsymbol{w}_j^{(0,r)}=\boldsymbol{w}^{(r)}$ 
\State $\qquad\quad $\textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}
\State $\qquad\quad\quad$ Sample a mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}\triangleq\nabla{f}_j(\boldsymbol{w}^{(\ell,r)}_j,\xi_j^{(\ell,r)})$
\State $\qquad\quad\quad$ $\boldsymbol{w}^{(\ell+1,r)}_{j}=\boldsymbol{w}^{(\ell,r)}_j-\eta~ \tilde{\mathbf{g}}_{j}^{(\ell,r)}$ \label{eq:update-rule-alg}
\State $\qquad\quad$\textbf{end for}
\State $\qquad\quad\quad$Device $j$ sends $\mathbf{S}_j^{(r)}=\mathbf{S}\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}+\mathbf{m}_j^{(r)}\right)$ back to the server.
\State $\qquad\quad\quad$Device $j$ computes $\mathbf{m}_j^{(r)}=\mathbf{m}_j^{(r-1)}+\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}\right)-\mathbf{S}_j^{(r)}$ back to the server.
\State $\qquad$Server \textbf{computes} 
\State $\qquad\qquad {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}^p\mathbf{S}^{(r)}_j$ %and \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.
\State $\qquad\qquad$ Sever runs $\underline{\mathbf{S}}^{(r)}= \texttt{HEAVYMIX}(\mathbf{S}^{(r)})$.
%\State $\qquad\quad$ \textbf{end if}
\State $\qquad$\textbf{end parallel for}
\State \textbf{end}
\State \textbf{Output:} ${\boldsymbol{w}}^{(R-1)}$
\vspace{- 0.1cm}
%\State \todo{How about we call the algorithm local SGD with decoupled rates (LSDR) and specialize the analysis for IID and non-IID cases}
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Strongly convex or \pl]
Given $0<k=O\left(\frac{e}{\mu^2}\right)\leq d$
and running Algorithm~\ref{Alg:PFLHom} with sketch of size $c=O\left(k\log \frac{d R}{\delta}\right)$,  under Assumptions~\ref{Assu:1} and \ref{Assu:1.5},and the choice of learning rate $\eta=\frac{1}{L\gamma (\frac{\mu^2d}{p}+1) \tau}$ with probability at least $1-\delta$, we have:
\begin{align}
                \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \exp{-\left(\frac{ R}{\kappa (\frac{\mu^2d}{p}+1)}\right)}\Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\left(\frac{1}{2\gamma^2 {(\frac{\mu^2d}{p}+1)}^2 }+\frac{1}{2p}\right)\frac{\sigma^2}{\mu\tau}
\end{align}
\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergence result for \texttt{FEDSKETCH-III}}
For this part we use standard perturbed iterate analysis. With short hand notation of 

\begin{align}
\Delta_j^{(r)}&\triangleq\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}\nonumber\\
\tilde{\mathbf{g}}^{(r)}_j&=\Delta_j^{(r)}+\mathbf{m}_j^{(r)}\nonumber\\
\mathbf{m}_j^{(r+1)}&=\mathbf{m}_j^{(r)}+\left[\Delta_j^{(r)}-\underline{\mathbf{S}}_j^{(r)}\right]\nonumber\\
&=\sum_{r=0}^{R-1}\left[\Delta_j^{(r)}-\underline{\mathbf{S}}_j^{(r)}\right]\nonumber\\
\tilde{\mathbf{g}}^{(r)}&\triangleq\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_j^{(r)}\nonumber\\
\underline{\mathbf{S}}^{(r)}&=\frac{1}{p}\sum_{j=1}^p\underline{\mathbf{S}}^{(r)}_j
\end{align}


Before proceeding to the proof we need a few short hand notation. We have:
\begin{align}
     \sum_{r=0}^{r-1}\left(\tilde{\mathbf{g}}^{(r)}-\underline{\mathbf{S}}^{(r)}\right)&{=} \frac{1}{p}\sum_{r=0}^{r-1}\sum_{j=1}^p\left(\Delta_j^{(r)}-\underline{\mathbf{S}}^{(r)}_j\right)\nonumber\\
    &= \frac{1}{p}\sum_{j=1}^p\left[\sum_{r=0}^{r-1}\left(\Delta_j^{(r)}-\underline{\mathbf{S}}^{(r)}_j\right)\right]\nonumber\\
    &= \frac{1}{p}\sum_{j=1}^p\mathbf{m}_j^{(r)}\nonumber\\
    &\triangleq\mathbf{m}^{(r)}
\end{align}

Based on Algorithm~\ref{} and the update rules:
\begin{align}
\mathbf{S}_j^{(r)}&=\mathbf{S}\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}+\mathbf{m}_j^{(r)}\right)\nonumber\\
\mathbf{m}_j^{(r)}&=\mathbf{m}_j^{(r-1)}+\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}\right)-\mathbf{S}_j^{(r)}
\end{align}

\todo{Continue from here....>}



\begin{property}[\cite{ivkin2019communication}]\label{prop:2}
Based on Lemma~\ref{} in \cite{ivkin2019communication}, for all $1\leq j\leq p$ we have
\begin{align}
    \mathbb{E}_{\mathbf{S}_j}\left[\left\|\mathbf{m}_j^{(r+1)}\right\|^2_2\right]\leq\left(1-\frac{k}{d}\right)\left\|\mathbf{m}_j^{(r)}+\mathbf{\Delta}_j^{(r)}\right\|_2^2\label{eq:sketch_var}
\end{align}
\end{property}
Based on Property~\ref{prop:2} we have tha following Lemma:
\begin{lemma}\label{lmm:bndng-m}
\begin{align}
        \mathbb{E}\left\|\mathbf{m}^{(r+1)}\right\|^2\leq\frac{2d}{k}\sum_{c=0}^{r+1}\left(1-\frac{k}{2d}\right)^{c}\left\|\mathbf{\Delta}_j^{(r+1-c)}\right\|_2^2
\end{align}
\end{lemma}
\begin{proof}
\begin{align}
   \mathbb{E}_{\mathbf{S}_j}\left[\left\|\mathbf{m}_j^{(r+1)}\right\|^2_2\right]&\leq\left(1-\frac{k}{d}\right)\left\|\mathbf{m}_j^{(r)}+\mathbf{\Delta}_j^{(r)}\right\|_2^2\nonumber\\
  &\stackrel{(a)}{\leq}\left(1-\frac{k}{d}\right)\left[\left(1+\frac{k}{2d}\right)\left\|\mathbf{m}_j^{(r)}\right\|_2^2+\left(1+\frac{2d}{k}\right)\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2\right]\nonumber\\
  &\leq \left(1-\frac{k}{2d}\right)\mathbb{E}\left[\left\|\mathbf{m}_j^{(r)}\right\|_2^2\right]+\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2\nonumber\\
  &\leq \left(1-\frac{k}{2d}\right)\left[\left(1-\frac{k}{2d}\right)\mathbb{E}\left[\left\|\mathbf{m}_j^{(r-1)}\right\|_2^2\right]+\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r-1)}\right\|_2^2\right]+\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2\nonumber\\
  &=\left(1-\frac{k}{2d}\right)^2\mathbb{E}\left[\left\|\mathbf{m}_j^{(r-1)}\right\|_2^2\right]+\left(1-\frac{k}{2d}\right)\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r-1)}\right\|_2^2+\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2\nonumber\\
  &\leq \left(1-\frac{k}{2d}\right)^3\mathbb{E}\left[\left\|\mathbf{m}_j^{(r-2)}\right\|_2^2\right]+\left(1-\frac{k}{2d}\right)^2\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r-2)}\right\|_2^2+\left(1-\frac{k}{2d}\right)\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r-1)}\right\|_2^2+\frac{2d}{k}\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2\nonumber\\
  &\leq \dots\leq\nonumber\\
  &\leq \left(1-\frac{k}{2d}\right)^{r+1}\mathbb{E}\left[\left\|\mathbf{m}_j^{(0)}\right\|_2^2\right]+\frac{2d}{k}\sum_{c=0}^{r+1}\left(1-\frac{k}{2d}\right)^{c}\left\|\mathbf{\Delta}_j^{(r+1-c)}\right\|_2^2\nonumber\\
  &\stackrel{(b)}{=}\frac{2d}{k}\sum_{c=0}^{r+1}\left(1-\frac{k}{2d}\right)^{c}\left\|\mathbf{\Delta}_j^{(r+1-c)}\right\|_2^2
\end{align}
\end{proof}

Next, taking expectation with respect to $\xi$ (random mini-batches) we obtain:
\begin{align}
    \mathbb{E}_{\xi}\left[\mathbb{E}_{\mathbf{S}_j}\left[\left\|\mathbf{m}_j^{(r+1)}\right\|^2_2\right]\right]\leq \frac{2d}{k}\sum_{c=0}^{r+1}\left(1-\frac{k}{2d}\right)^{c}\mathbb{E}_{\xi}\left[\left\|\mathbf{\Delta}_j^{(r+1-c)}\right\|_2^2 \right]
\end{align}
Next step is to bound the term $\mathbb{E}_{\xi}\left[\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2 \right]$:
\begin{lemma}\label{lmm:bndng-delta}
\begin{align}
    \mathbb{E}_{\xi}\left[\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2 \right]\leq\tau\sigma^2\eta^2+\tau\eta^2\sum_{\ell=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(r,\ell)}\right\|_2^2
\end{align}
\end{lemma}
\begin{proof}
\begin{align}
    \mathbb{E}_{\xi}\left[\left\|\mathbf{\Delta}_j^{(r)}\right\|_2^2 \right]&=\mathbb{E}_{\xi}\left[\left\|-\eta\sum_{\ell=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(r,\ell)}\right\|_2^2 \right]\nonumber\\
    &=\eta^2\mathbb{E}_{\xi}\left[\left\|\sum_{\ell=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(r,\ell)}\right\|_2^2 \right]\nonumber\\
    &=\eta^2 \text{Var}\left(\sum_{\ell=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(r,\ell)}\right)+\eta^2\left[\left\|\sum_{\ell=0}^{\tau-1}\mathbb{E}_{\xi}\left[\tilde{\mathbf{g}}_j^{(r,\ell)}\right]\right\|_2^2 \right]\nonumber\\
    &\stackrel{(a)}{=}\eta^2 \sum_{\ell=0}^{\tau-1}\text{Var}\left(\tilde{\mathbf{g}}_j^{(r,\ell)}\right)+\eta^2\left[\left\|\sum_{\ell=0}^{\tau-1}{\mathbf{g}}_j^{(r,\ell)}\right\|_2^2 \right]\nonumber\\
        &\stackrel{(b)}{\leq}\eta^2 \sum_{\ell=0}^{\tau-1}\sigma^2+\eta^2\left\|\sum_{\ell=0}^{\tau-1}{\mathbf{g}}_j^{(r,\ell)}\right\|_2^2 \nonumber\\
        &\stackrel{(c)}{\leq} \tau\sigma^2\eta^2+\tau\eta^2\sum_{\ell=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(r,\ell)}\right\|_2^2 
\end{align}
where (a) is due to independent mini-batch sampling, (b) comes from Assumption~\ref{} and (c) holds because of $\left\|\sum_{i=1}^s\mathbf{b}_i\right\|^2\leq s\sum_{i=1}^s\left\|\mathbf{b}_i\right\|^2$.
\end{proof}
Now, mixing Lemmas~\ref{lmm:bndng-delta} and \ref{lmm:bndng-m} we obtain:
\begin{align}
    \mathbb{E}_{\xi}\left[\mathbb{E}_{\mathbf{S}_j}\left[\left\|\mathbf{m}_j^{(r)}\right\|^2_2\right]\right]&\leq \frac{2d}{k}\sum_{c=0}^{r}\left(1-\frac{k}{2d}\right)^{c}\left(\tau\sigma^2\eta^2+\tau\eta^2\sum_{\ell=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(r-c,\ell)}\right\|_2^2\right)\nonumber\\
    &\leq \frac{2d}{k}\sum_{c=0}^{r}\left(\tau\sigma^2\eta^2+\tau\eta^2\sum_{\ell=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(r-c,\ell)}\right\|_2^2\right)\nonumber\\
    &=\frac{2d}{k}\left((r+1)\tau\sigma^2\eta^2+\tau\eta^2\sum_{c=0}^{r}\sum_{\ell=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(r-c,\ell)}\right\|_2^2\right)
\end{align}

From the $L$-smoothness gradient assumption on global objective, by using  $\underline{\mathbf{S}}^{(r)}$ we have:
\begin{align}
    f(&{\boldsymbol{x}}^{(r+1)})-f({\boldsymbol{x}}^{(r)})\nonumber\\ 
    &\leq -\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\underline{\mathbf{S}}^{(r)}\big\rangle+\frac{\gamma^2 L}{2}\|\underline{\mathbf{S}}^{(r)}\|^2\nonumber\\
    &=-\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle+\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}^{(r)}-\underline{\mathbf{S}}^{(r)}\big\rangle+\frac{\gamma^2 L}{2}\|\underline{\mathbf{S}}^{(r)}-\tilde{\mathbf{g}}^{(r)}+\tilde{\mathbf{g}}^{(r)}\|^2\nonumber\\
    &\leq -\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle+\gamma\alpha\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|^2+\frac{\gamma}{\alpha}\left\|\tilde{\mathbf{g}}^{(r)}-\underline{\mathbf{S}}^{(r)}\right\|^2+{\gamma^2 L}\left\|\underline{\mathbf{S}}^{(r)}-\tilde{\mathbf{g}}^{(r)}\right\|^2+{\gamma^2 L}\left\|\tilde{\mathbf{g}}^{(r)}\right\|^2\nonumber\\
    &=-\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle+\gamma\alpha\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|^2+\left(\frac{\gamma}{\alpha}+{\gamma^2 L}\right)\left\|\tilde{\mathbf{g}}^{(r)}-\underline{\mathbf{S}}^{(r)}\right\|^2+\gamma^2 L\left\|\tilde{\mathbf{g}}^{(r)}\right\|^2\label{eq:wit_exp}
\end{align}
Next, we take expectation from both side of Eq.~(\ref{eq:wit_exp}) as follows:
\begin{align}
    \mathbb{E}\left[f({\boldsymbol{x}}^{(r+1)})-f({\boldsymbol{x}}^{(r)})\right]\leq \underbrace{-\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\mathbb{E}\left[\tilde{\mathbf{g}}^{(r)}\right]\big\rangle}_{(\mathrm{I})}+\gamma\alpha\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|^2+\underbrace{\left(\frac{\gamma}{\alpha}+{\gamma^2 L}\right)\mathbb{E}\left[\left\|\tilde{\mathbf{g}}^{(r)}-\underline{\mathbf{S}}^{(r)}\right\|^2\right]}_{(\mathrm{II})}+\underbrace{\gamma^2 L\mathbb{E}\left[\left\|\tilde{\mathbf{g}}^{(r)}\right\|^2\right]}_{(\mathrm{III})}
\end{align}
In the rest of the proof we bound the terms $(\mathrm{I},\mathrm{II},\mathrm{III})$ as follows:
Now, we start bounding these terms:

\paragraph{Bounding $(\mathrm{I})$:}
\begin{lemma}
\begin{align}
    -\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\mathbb{E}\left[\tilde{\mathbf{g}}^{(r)}\right]\big\rangle\leq ...
\end{align}
\end{lemma}
\begin{proof}
\begin{align}
    -\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\mathbb{E}\left[\tilde{\mathbf{g}}^{(r)}\right]\big\rangle&=-\gamma \left\langle\nabla f({\boldsymbol{x}}^{(r)}),\mathbb{E}\left[\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_j^{(r)}\right]\right\rangle\nonumber\\
    &=-\gamma\frac{1}{p}\sum_{j=1}^p \left\langle\nabla f({\boldsymbol{x}}^{(r)}),\mathbb{E}\left[\tilde{\mathbf{g}}_j^{(r)}\right]\right\rangle\nonumber\\
    &=-\gamma\frac{1}{p}\sum_{j=1}^p \left\langle\nabla f({\boldsymbol{x}}^{(r)}),\mathbb{E}\left[\tilde{\mathbf{g}}_j^{(r)}\right]\right\rangle\nonumber\\
    &=-\gamma\frac{1}{p}\sum_{j=1}^p \left\langle\nabla f({\boldsymbol{x}}^{(r)}),\mathbb{E}\left[\tilde{\Delta}_j^{(r)}+\mathbf{m}_j^{(r)}\right]\right\rangle\nonumber\\
\end{align}
\end{proof}

\todo{To be continued from here...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deferentially private algorithm}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Communication-efficient algorithm}
Here we propose the communication-efficient algorithm:




\begin{algorithm}[H]
\caption{\texttt{FEDSKETCH-II}($R$, $\tau, \eta, \gamma$): Communication-efficient Federated Learning via Sketching.  
}\label{Alg:ce-h}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $\boldsymbol{w}^{(0)}$ as an initial  model shared by all local devices, the number of communication rounds $R$, the the number of local updates $\tau$, and global and local learning rates $\gamma$ and $\eta$, respectively
%\State Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\State \textbf{for $r=0, \ldots, R-1$ do}
\State $\qquad$\textbf{parallel for device $j=1,\ldots,n$ do}:
\State $\qquad\quad$ Set $\boldsymbol{w}^{(r)}=\boldsymbol{w}^{(r-1)}-\gamma{\mathbf{S}}^{(r)}$
\State $\qquad\quad$ Set $\boldsymbol{w}_j^{(0,r)}=\boldsymbol{w}^{(r)}$ 
\State $\qquad\quad $\textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}
\State $\qquad\quad\quad$ Sample a mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}\triangleq\nabla{f}_j(\boldsymbol{w}^{(\ell,r)}_j,\xi_j^{(\ell,r)})$
\State $\qquad\quad\quad$ $\boldsymbol{w}^{(\ell+1,r)}_{j}=\boldsymbol{w}^{(\ell,r)}_j-\eta~ \tilde{\mathbf{g}}_{j}^{(\ell,r)}$ \label{eq:update-rule-alg}
\State $\qquad\quad$\textbf{end for}
\State $\qquad\quad\quad$Device $j$ sends $\mathbf{S}_j^{(r)}=\texttt{HEAVYMIX}\left(\boldsymbol{w}_j^{(0,r)}-~{\boldsymbol{w}}_{j}^{(\tau,r)}\right)$ back to the server.
\State $\qquad$Server \textbf{computes} 
\State $\qquad\qquad {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}^n\mathbf{S}_j$ and \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.
%\State $\qquad\qquad$ Sever runs $\underline{\mathbf{S}}^{(r)}= (\mathbf{S}^{(r)})$.
%\State $\qquad\quad$ \textbf{end if}
\State $\qquad$\textbf{end parallel for}
\State \textbf{end}
\State \textbf{Output:} ${\boldsymbol{w}}^{(R-1)}$
\vspace{- 0.1cm}
%\State \todo{How about we call the algorithm local SGD with decoupled rates (LSDR) and specialize the analysis for IID and non-IID cases}
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Private and Communication-efficient algorithm}


\subsection{Communication-efficient and differential private algorithm based on induced compressor}

\begin{algorithm}[H]
\caption{\texttt{FEDSKETCH}($R$, $\tau, \eta, \gamma$): Private Federated Learning with Sketching. }\label{Alg:combined}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $\boldsymbol{x}^{(0)}$ as an initial  model shared by all local devices, the number of communication rounds $R$, the the number of local updates $\tau$, and global and local learning rates $\gamma$ and $\eta$, respectively
%\State Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\State \textbf{for $r=0, \ldots, R-1$ do}
\State $\qquad$\textbf{parallel for device $j=1,\ldots,n$ do}:
\State $\qquad\quad$ Set $\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma{\mathbf{S}}^{(r-1)}$
\State $\qquad\quad$ Set $\boldsymbol{x}_j^{(0,r)}=\boldsymbol{x}^{(r)}$ 
\State $\qquad\quad $\textbf{for} $c=0,\ldots,\tau-1$ \textbf{do}
\State $\qquad\quad\quad$ Sample a mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}\triangleq\nabla{f}_j(\boldsymbol{x}^{(\ell,r)}_j,\xi_j^{(c,r)})$
\State $\qquad\quad\quad$ $\boldsymbol{x}^{(\ell+1,r)}_{j}=\boldsymbol{x}^{(\ell,r)}_j-\eta~ \tilde{\mathbf{g}}_{j}^{(c,r)}$ \label{eq:update-rule-alg}
\State $\qquad\quad$\textbf{end for}
\State $\qquad\quad\quad$Device $j$ sends $$\mathbf{g}^{(\text{ind},r)}_j\triangleq \texttt{HEAVYMIX}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right)+\mathbf{S}\left[\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right)-\texttt{HEAVYMIX}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right)\right]$$ 
back to the server.
\State $\qquad$Server \textbf{computes} 
\State $\qquad\qquad {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}\mathbf{g}^{(\text{ind},r)}_j$ and \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.
%\State $\qquad$Sever chooses a set of devices  $\mathcal{P}_t$ with distribution $q_j$.
%\State $\qquad\quad$ \textbf{end if}
\State $\qquad$\textbf{end parallel for}
\State \textbf{end}
\State \textbf{Output:} ${\boldsymbol{x}}^{(R-1)}$
\vspace{- 0.1cm}
%\State \todo{How about we call the algorithm local SGD with decoupled rates (LSDR) and specialize the analysis for IID and non-IID cases}
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%removed proofs%%%%%%%%%%%%%%%%%%


\begin{theorem}[General non-convex]
Given $0<k=O\left(\frac{e}{\mu^2}\right)\leq d$
and running Algorithm~\ref{Alg:PFLHom} with sketch of size $c=O\left(k\log \frac{d R}{\delta}\right)$,  under Assumptions~\ref{Assu:1} and \ref{Assu:1.5}, if 
\begin{align}
   1\geq {\tau L^2\eta^2\tau}+(\frac{\mu^2 d}{p}+1)\eta\gamma L{\tau}\label{eq:cnd-lrs-h} 
\end{align}
with probability at least $1-\delta$, we have:
\begin{align}\label{eq:thm1-result}
    \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2\leq \frac{2\left(f(\boldsymbol{x}^{(0)})-f(\boldsymbol{x}^{*})\right)}{\eta\gamma\tau R}+\frac{L\eta\gamma(\frac{\mu^2 d}{p}+1)}{p}\sigma^2+{L^2\eta^2\tau }\sigma^2
\end{align}
\end{theorem}


\begin{corollary}[Linear speed up] 
In Eq.~(\ref{eq:thm1-result}) by letting $\eta\gamma=O\left(\frac{1}{L}\sqrt{\frac{p}{R\tau\left(\frac{\mu^2 d}{p}+1\right)}}\right)$, and for $\gamma\geq p$  convergence rate reduces to:
\begin{align}
    \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2&\leq O\left(\frac{L\sqrt{\left(\frac{\mu^2 d}{p}+1\right)}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{*})\right)}{\sqrt{pR\tau}}+\frac{\left(\sqrt{\left(\frac{\mu^2 d}{p}+1\right)}\right)\sigma^2}{\sqrt{pR\tau}}+\frac{p\sigma^2}{R\left(\frac{\mu^2 d}{p}+1\right)\gamma^2}\right)\label{eq:convg-error}
\end{align}
Note that according to Eq.~(\ref{eq:convg-error}), if we pick  a fixed constant value for  $\gamma$, in order to achieve an $\epsilon$-accurate solution, $R=O\left(\frac{1}{\epsilon}\right)$ communication cost and $\tau=O\left(\frac{\left(\frac{\mu^2 d}{p}+1\right)}{p\epsilon}\right)$ are necessary.

\end{corollary}




\begin{remark}\label{rmk:cnd-lr}

Condition in Eq.~(\ref{eq:cnd-lrs-h}) can be rewritten as 
\begin{align}
    \eta&\leq \frac{-\gamma L\tau\left(\frac{\mu^2 d}{p}+1\right)+\sqrt{\gamma^2 \left(L\tau\left(\frac{\mu^2 d}{p}+1\right)\right)^2+4L^2\tau^2}}{2L^2\tau^2}\nonumber\\
    &= \frac{-\gamma L\tau\left(\frac{\mu^2 d}{p}+1\right)+L\tau\sqrt{\left(\frac{\mu^2 d}{p}+1\right)^2\gamma^2 +4}}{2L^2\tau^2}\nonumber\\
    &=\frac{\sqrt{\left(\frac{\mu^2 d}{p}+1\right)^2\gamma^2 +4}-\left(\frac{\mu^2 d}{p}+1\right)\gamma}{2L\tau}\label{eq:lrcnd}
\end{align}
So based on Eq.~(\ref{eq:lrcnd}), if we set $\eta=O\left(\frac{1}{L\gamma}\sqrt{\frac{p}{R\tau\left(\frac{\mu^2 d}{p}+1\right)}}\right)$, this implies that:
\begin{align}
    R\geq \frac{\tau p}{\left(\frac{\mu^2 d}{p}+1\right)\gamma^2\left(\sqrt{\left(\frac{\mu^2 d}{p}+1\right)^2\gamma^2+4}-\left(\frac{\mu^2 d}{p}+1\right)\gamma\right)^2}\label{eq:iidexact}
\end{align}
We note that $\gamma^2\left(\sqrt{\left(\frac{\mu^2 d}{p}+1\right)^2\gamma^2+4}-\left(\frac{\mu^2 d}{p}+1\right)\gamma\right)^2=\Theta(1)\leq 5 $ therefore even for $\gamma\geq p$ we need to have 
\begin{align}
    R\geq \frac{\tau p}{5\left(\frac{\mu^2 d}{p}+1\right)}=O\left(\frac{\tau p}{\left(\frac{\mu^2 d}{p}+1\right)}\right)
\end{align}
\textbf{Therefore for the choice of $\tau=O\left(\frac{\frac{\mu^2 d}{p}+1}{p\epsilon}\right)$ we need to have $R=O\left(\frac{1}{\epsilon}\right)$.}
\end{remark}



\section{Convergence proofs of Algorithm~\ref{Alg:PFLHom}}

Before proceeding to the proof, we would like to highlight that 
\begin{align}
    \boldsymbol{x}^{(r)}- ~{\boldsymbol{x}}_{j}^{(\tau,r)}=\eta\sum_{\ell=0}^{\tau-1}\tilde{g}_j^{(\ell,r)}\label{eq:decent-smoothe}
\end{align}

From the updating rule of Algorithm~\ref{Alg:PFLHom} we have


\begin{align}
     {\boldsymbol{x}}^{(r+1)}=\boldsymbol{x}^{(r)}-\gamma\underline{\mathbf{S}}^{(r)}&=\boldsymbol{x}^{(r)}-\gamma\left[\frac{1}{p}\sum_{j=1}^n\mathbf{S}\left[\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right]\right]\nonumber\\
     &=\boldsymbol{x}^{(r)}-\gamma\left[\frac{1}{p}\sum_{j=1}^n\mathbf{S}\left[\eta\sum_{\ell=0}^{\tau-1}\tilde{g}_j^{(\ell,r)}\right]\right]\nonumber\\
     &\stackrel{(a)}{=}\boldsymbol{x}^{(r)}-\gamma\left[\eta\frac{1}{p}\sum_{j=1}^n\mathbf{S}\left[\sum_{\ell=0}^{\tau-1}\tilde{g}_j^{(\ell,r)}\right]\right]\label{eq:update-rule-dec}
\end{align}
where (a) comes from linearity of sketches.


In what follows, we use the following notation to denote the stochastic gradient used to update the global model at $r$th communication round $$\tilde{\mathbf{g}}_{\mathbf{S}}^{(r)}=\frac{\eta}{p}\sum_{j=1}^{p}\mathbf{S}\left[\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right].$$ 
and notice that $\boldsymbol{x}^{(r)} = \boldsymbol{x}^{(r-1)} - \gamma \tilde{\mathbf{g}}_{\mathbf{S}}^{(r)}$.


Then using the Assumption~\ref{} we have:
\begin{align}
  \mathbb{E}_{\mathbf{S}}\left[\tilde{\mathbf{g}}_{\mathbf{S}}^{(r)}\right]=\frac{1}{p}\sum_{j=1}\left[-\eta\mathbb{E}_{\mathbf{S}}\left[ \mathbf{S}\left[\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right]\right]\right]=\frac{1}{p}\sum_{j=1}\left(-\eta\left[\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right]\right)\triangleq \tilde{\mathbf{g}}^{(r)}\label{eq:unbiased_gd} 
\end{align}

The proof of theorem relies on the following key lemmas. For ease of exposition, we defer the proof of lemmas to latter section and only focus on proving the main theorem. 

\begin{lemma}\label{lemma:tasbih1-iid}
Under Assumption~\ref{Assu:2}, we have the following bound: 
\begin{align}
\mathbb{E}_{\mathbf{S},\xi}\Big[\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\Big]&=\mathbb{E}_{\xi}\mathbb{E}_{\mathbf{S}}\Big[\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\Big]\nonumber\\
&\leq \tau(\frac{\mu^2 d}{p}+1)\frac{1}{p}\sum_{j=1}^p\left[\sum_{c=0}^{\tau-1}\|\mathbf{g}_j^{(c,r)}\|^2+\sigma^2\right] \label{eq:lemma1}
\end{align}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}\label{lemma:cross-inner-bound-unbiased}
  Under Assumptions \ref{Assu:1}, and according to the \texttt{FLDL} Algorithm the expected inner product between stochastic gradient and full batch gradient can be bounded with:

\begin{align}
    - \mathbb{E}\left[\left\langle\nabla f({\boldsymbol{x}}^{(r)}),{{\tilde{\mathbf{g}}}^{(r)}}\right\rangle\right]&\leq \frac{1}{2}\eta\frac{1}{p}\sum_{j=1}^p\sum_{c=0}^{\tau-1}\left[-\|\nabla f({\boldsymbol{x}}^{(r)})\|_2^2-\|\nabla{f}(\boldsymbol{x}_j^{(c,r)})\|_2^2+L^2\|{\boldsymbol{x}}^{(r)}-\boldsymbol{x}_j^{(c,r)}\|_2^2\right]\label{eq:lemma3-thm2}
\end{align}
\todo{fix this!}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following lemmas bounds the distance of local solutions from global solution at $r$th communication round.
\begin{lemma}\label{lemma:dif-under-pl-sgd-iid}
Under Assumptions~\ref{Assu:2} we have:
\begin{align}
      \mathbb{E}\left[\|{\boldsymbol{x}}^{(r)}-\boldsymbol{x}_j^{(\ell,r)}\|_2^2\right]&\leq \eta^2\sum_{\ell=0}^{\tau-1}\left[\tau\left\|{\mathbf{g}}_j^{(\ell,r)}\right\|_2^2+\tau\sigma^2\right]\nonumber\\
      &=\eta^2\tau\sum_{\ell=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\eta^2\tau\sigma^2
\end{align}
\todo{fix this!}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}(of Theorem~\ref{thm:lsgwd-lr})
From the $L$-smoothness gradient assumption on global objective, by using  $\tilde{\mathbf{g}}^{(r)}$ in inequality (\ref{eq:decent-smoothe}) we have:
\begin{align}
    f({\boldsymbol{x}}^{(r+1)})-f({\boldsymbol{x}}^{(r)})\leq -\gamma \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle+\frac{\gamma^2 L}{2}\|\tilde{\mathbf{g}}^{(r)}\|^2\label{eq:Lipschitz-c1}
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By taking expectation on both sides of above inequality over sampling, we get:
\begin{align}
    \mathbb{E}\left[\mathbb{E}_\mathbf{S}\Big[f({\boldsymbol{x}}^{(r+1)})-f({\boldsymbol{x}}^{(r)})\Big]\right]&\leq -\gamma\mathbb{E}\left[\mathbb{E}_\mathbf{S}\left[ \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\big\rangle\right]\right]+\frac{\gamma^2 L}{2}\mathbb{E}\left[\mathbb{E}_\mathbf{S}\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\right]\nonumber\\
    &\stackrel{(a)}{=}-\gamma\underbrace{\mathbb{E}\left[\left[ \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle\right]\right]}_{(\mathrm{I})}+\frac{\gamma^2 L}{2}\underbrace{\mathbb{E}\left[\mathbb{E}_\mathbf{S}\Big[\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\Big]\right]}_{\mathrm{(II)}}\label{eq:Lipschitz-c-gd}
\end{align}
We proceed to use Lemma~\ref{lemma:tasbih1-iid}, Lemma~\ref{lemma:cross-inner-bound-unbiased}, and Lemma~\ref{lemma:dif-under-pl-sgd-iid}, to bound  terms $(\mathrm{I})$ and $(\mathrm{II})$ in right hand side of (\ref{eq:Lipschitz-c-gd}), which gives
\begin{align}
     \mathbb{E}\left[\mathbb{E}_\mathbf{S}\Big[f({\boldsymbol{x}}^{(r+1)})-f({\boldsymbol{x}}^{(r)})\Big]\right]&\leq \gamma\frac{1}{2}\eta\frac{1}{p}\sum_{j=1}^p\sum_{c=0}^{\tau-1}\left[-\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2-\left\|\mathbf{g}_j^{(\ell,r)}\right\|_2^2+L^2\eta^2\sum_{\ell=0}^{\tau-1}\left[\tau\left\|{\mathbf{g}}_j^{(\ell,r)}\right\|_2^2+\sigma^2\right]\right]\nonumber\\
     &\qquad+\frac{(\frac{\mu d^2}{p}+1)\gamma^2 L}{2}\left[\frac{\eta^2\tau}{p}\sum_{j=1}^p\sum_{\ell=0}^{\tau-1}\|\mathbf{g}^{(\ell,r)}_{j}\|^2+\frac{\tau\eta^2 \sigma^2}{p}\right]\nonumber\\
     &\stackrel{\text{\ding{192}}}{\leq}\frac{\gamma\eta}{2p}\sum_{j=1}^p\sum_{\ell=0}^{\tau-1}\left[-\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2-\left\|\mathbf{g}_j^{(\ell,r)}\right\|_2^2+\tau L^2\eta^2\left[\tau\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\sigma^2\right]\right]\nonumber\\
     &\qquad+\frac{\gamma^2 L(\frac{\mu ^2d}{p}+1)}{2}\left[\frac{\eta^2\tau}{p}\sum_{j=1}^p\sum_{\ell=0}^{\tau-1}\|\mathbf{g}^{(c,r)}_{j}\|^2+\frac{\tau\eta^2 \sigma^2}{p}\right]\nonumber\\
     &=-\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2\nonumber\\
     &-\left(1-{\tau L^2\eta^2\tau}-{(\frac{\mu^2 d}{p}+1)\eta\gamma L}{\tau}\right)\frac{\eta\gamma}{2p}\sum_{j=1}^p\sum_{\ell=0}^{\tau-1}\|\mathbf{g}^{(\ell,r)}_{j}\|^2+\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+\gamma(\frac{\mu^2d}{p}+1)\right)\sigma^2\nonumber\\
     &\stackrel{\text{\ding{193}}}{\leq} -\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2+\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+\gamma(\frac{\mu^2d}{p}+1)\right)\sigma^2\label{eq:finalll}
\end{align}

where in \ding{192} we incorporate outer summation $\sum_{c=0}^{\tau-1}$, \ding{193} follows from condition 
\begin{align}
   1\geq {\tau L^2\eta^2\tau}+(\frac{\mu^2 d}{p}+1)\eta\gamma L{\tau}. 
\end{align}
Summing up for all $R$ communication rounds and  rearranging the terms gives:
\begin{align}
    \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2\leq \frac{2\left(f(\boldsymbol{x}^{(0)})-f(\boldsymbol{x}^{*})\right)}{\eta\gamma\tau R}+\frac{L\eta\gamma{(\frac{\mu^2 d}{p}+1)}}{p}\sigma^2+{L^2\eta^2\tau }\sigma^2
\end{align}
From above inequality, is it easy to see that in order to achieve a linear speed up, we need to have $\eta\gamma=O\left(\frac{\sqrt{p}}{\sqrt{R \tau}}\right)=O\left(\frac{\sqrt{p}}{\sqrt{T}}\right)$

\todo{fix this!}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Lemmas}
\subsection{Proof of Lemma~\ref{}}
\begin{align}
\mathbb{E}_{{\color{blue}\xi^{(r)}|\boldsymbol{x}^{(r)}}}\mathbb{E}_{{\color{blue}\mathbf{S}}}\Big[\|\frac{1}{p}\sum_{j=1}^p \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]&{=} \mathbb{E}_{{\color{blue}\xi}}\left[\mathbb{E}_{{\color{blue}\mathbf{S}}}\Big[\|\frac{1}{p}\sum_{j=1}^p\underbrace{\mathbf{S}\left(\overbrace{\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j}^{\tilde{\mathbf{g}}_j^{(r)}}\right)}_{\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}}\|^2\Big]\right]\nonumber\\
&\stackrel{(a)}{=}\mathbb{E}_{{\color{blue}\xi}}\left[\mathbb{E}_{{\color{blue}\mathbf{S}}}\left[\left[\|\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}-\frac{1}{p}\sum_{j=1}^p\mathbb{E}_{{\color{blue}\mathbf{S}}}\left[\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]\|^2\right]+\|\mathbb{E}_{{\color{blue}\mathbf{S}}}\left[\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]\|^2\right]\right]\nonumber\\
&\stackrel{(b)}{=}\mathbb{E}_{{\color{blue}\xi}}\left[\mathbb{E}_{{\color{blue}\mathbf{S}}}\left[\frac{1}{p^2}\sum_{j=1}^p\left[\left\|\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}-\tilde{\mathbf{g}}^{(r)}_j\right\|^2\right]\right]+\left\|\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_j^{(r)}\right\|^2\right]\nonumber\\
&\stackrel{(c)}{\leq}\mathbb{E}_{{\color{blue}\xi}}\left[\frac{1}{p}\sum_{j=1}^p\left[\frac{\mu^2 d}{p}\left\|\tilde{\mathbf{g}}_j^{(r)}\right\|^2+\left\|\tilde{\mathbf{g}}_j^{(r)}\right\|^2\right]\right]\nonumber\\
&=(\frac{\mu^2 d}{p}+1)\frac{1}{p}\sum_{j=1}^p\mathbb{E}_{{\color{blue}\xi}}\left[\left\|\tilde{\mathbf{g}}_j^{(r)}\right\|^2\right]\nonumber\\
&=(\frac{\mu^2 d}{p}+1)\frac{1}{p}\sum_{j=1}^p\left[\mathbb{E}_{{\color{blue}\xi}}\left[\left\|\tilde{\mathbf{g}}_j^{(r)}-\mathbb{E}_{{\color{blue}\xi}}\left[\tilde{\mathbf{g}}_j^{(r)}\right]\right\|^2\right]+\left\|\mathbb{E}_{{\color{blue}\xi}}\left[\tilde{\mathbf{g}}_j^{(r)}\right]\right\|^2\right]\nonumber\\
&=(\frac{\mu^2 d}{p}+1)\frac{1}{p}\sum_{j=1}^p\left[\mathbb{E}_{{\color{blue}\xi}}\left[\left\|\tilde{\mathbf{g}}_j^{(r)}-{\mathbf{g}}_j^{(r)}\right\|^2\right]+\left\|{\mathbf{g}}_j^{(r)}\right\|^2\right]\label{eq:lemma1}
\end{align}
where (a) holds due to $\mathbb{E}\left[\left\|\mathbf{x}\right\|^2\right]=\text{Var}[\mathbf{x}]+\left\|\mathbb{E}[\mathbf{x}]\right\|^2$, (b) is due to $\mathbb{E}_{{\color{blue}\mathbf{S}}}\left[\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]=\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_{j}^{(r)}$ and (c) follows from Assumption~\ref{Assu:09}.



The following lemma is a middle step in proving Lemma~\ref{lemma:tasbih1-iid}.

\begin{lemma}\label{lemma:variance-bound-for-prrof1}
Under Assumptions \ref{Assu:2}, we have the following variance bound from the averaged stochastic gradient:
\begin{align}
    \mathbb{E}_{\xi}\left[\Big[\|{\tilde{\mathbf{g}}_j^{(r)}}-{\mathbf{g}_j^{(r)}}\|^2\Big]\right]\leq \tau \sigma^2
\end{align}
\end{lemma}
%%%%%%%

\begin {proof}
We have
\begin{align}
    \mathbb{E}\left[\left\|{\tilde{\mathbf{g}}_j^{(t)}}-{\mathbf{g}_j^{(t)}}\right\|^2\right]&\stackrel{(a)}{=}\mathbb{E}\left[\left\|\sum_{c=0}^{\tau-1}\left[\tilde{\mathbf{g}}_j^{(c,r)}-\mathbf{g}_j^{(c,r)}\right]\right\|^2\right]\nonumber\\
    &{=}\text{Var}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right)\nonumber\\
    &\stackrel{(b)}{=}\sum_{c=0}^{\tau-1}\text{Var}\left(\tilde{\mathbf{g}}_j^{(c,r)}\right)\nonumber\\
    &{=}\sum_{c=0}^{\tau-1}\mathbb{E}\left[\left\|\tilde{\mathbf{g}}_j^{(c,r)}-\mathbf{g}_j^{(c,r)}\right\|^2\right]\nonumber\\
    &\stackrel{(c)}{\leq}\tau\sigma^2\label{eq:var_b_mid}
    \end{align}
where in (a) we use the definition of ${\tilde{\mathbf{g}}}^t$ and ${{\mathbf{g}}}^t$, in (b) we use the fact that mini-batches are chosen in i.i.d. manner at each local machine, and (c) immediately follows from Assumptions~\ref{Assu:2}.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%
Equipped with Lemma~\ref{lemma:variance-bound-for-prrof1}, we now turn to proving Lemma~\ref{lemma:tasbih1-iid}. First we note that i.i.d. data distribution implies $\mathbb{E}[{\tilde{\mathbf{g}}}_j^{(c,r)}]=\mathbf{g}_j^{(c,r)}=\nabla{f}(\boldsymbol{x}_j^{(c,r)})$, from which we have
\begin{align}
\left\|{\mathbf{g}}_j^{(r)}\right\|^2&=\|\sum_{c=0}^{\tau-1}\mathbf{g}_j^{(c,r)}\|^2\nonumber\\
&\stackrel{(a)}{\leq} \tau\sum_{c=0}^{\tau-1}\|\mathbf{g}_j^{(c,r)}\|^2\label{eq:mid-bounding-absg}
\end{align} 
where (a) is due to $\left\|\sum_{j=1}^n\mathbf{a}_i\right\|^2\leq n\sum_{j=1}^n\left\|\mathbf{a}_i\right\|^2$, which leads to the following bound:
\begin{align}
    \mathbb{E}_{\xi^{(r)}|\boldsymbol{x}^{(r)}}\mathbb{E}_{\mathbf{S}}\Big[\|\frac{1}{p}\sum_{j=1}^p \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]\leq\tau(\frac{\mu d}{p}+1)\frac{1}{p}\sum_{j=1}^p\left[\sum_{c=0}^{\tau-1}\|\mathbf{g}_j^{(c,r)}\|^2+\sigma^2\right] 
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{}}
We have:

\begin{align}
    -\mathbb{E}_{\{{\xi}^{(t)}_{1}, \ldots, {\xi}^{(t)}_{p}|{\boldsymbol{x}}^{(t)}_{1},\ldots,  {\boldsymbol{x}}^{(t)}_{p}\}} &\mathbb{E}_\mathbf{S}\left[ \big\langle\nabla f({\boldsymbol{x}}^{(r)}),\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\big\rangle\right]\nonumber\\
    &=-\mathbb{E}_{\{{\xi}^{(t)}_{1}, \ldots, {\xi}^{(t)}_{p}|{\boldsymbol{x}}^{(t)}_{1},\ldots,  {\boldsymbol{x}}^{(t)}_{p}\}}\left[\left\langle \nabla f({\boldsymbol{x}}^{(r)}),\eta\frac{1}{p}\sum_{j=1}^p\sum_{\ell=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(\ell,r)}\right\rangle\right]\nonumber\\
    &=-\left\langle \nabla f({\boldsymbol{x}}^{(r)}),\eta\frac{1}{p}\sum_{j=1}^p\sum_{\ell=0}^{\tau-1}\mathbb{E}\left[\tilde{\mathbf{g}}_j^{(c,r)}\right]\right\rangle\nonumber\\
        &=-\eta\sum_{\ell=0}^{\tau-1}\frac{1}{p}\sum_{j=1}^p\left\langle \nabla f({\boldsymbol{x}}^{(r)}),{\mathbf{g}}_j^{(\ell,r)}\right\rangle\nonumber\\ 
     &\stackrel{\text{\ding{192}}}{=}\frac{1}{2}\eta\sum_{\ell=0}^{\tau-1}\frac{1}{p}\sum_{j=1}^p\left[-\|\nabla f({\boldsymbol{x}}^{(r)})\|_2^2-\|\nabla{f}_j(\boldsymbol{x}_j^{(\ell,r)})\|_2^2+\|\nabla f({\boldsymbol{x}}^{(r)})-\nabla{f}(\boldsymbol{x}_j^{(\ell,r)})\|_2^2\right]\nonumber\\
    &\stackrel{\text{\ding{193}}}{\leq}\frac{1}{2}\eta\sum_{\ell=0}^{\tau-1}\frac{1}{p}\sum_{j=1}^p\left[-\|\nabla f({\boldsymbol{x}}^{(r)})\|_2^2-\|\nabla{f}(\boldsymbol{x}_j^{(\ell,r)})\|_2^2+L^2\|{\boldsymbol{x}}^{(r)}-\boldsymbol{x}_j^{(\ell,r)}\|_2^2\right]
   \label{eq:bounding-cross-no-redundancy}
\end{align}

where \ding{192} is due to $2\langle \mathbf{a},\mathbf{b}\rangle=\|\mathbf{a}\|^2+\|\mathbf{b}\|^2-\|\mathbf{a}-\mathbf{b}\|^2$, and \ding{193} follows from Assumption \ref{Assu:1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{}}

\begin{align}
 \mathbb{E}\left[\left\|\boldsymbol{x}^{(r)}-\boldsymbol{x}_j^{(c,r)}\right\|_2^2\right]&=\mathbb{E}\left[\left\|\boldsymbol{x}^{(r)}-\left(\boldsymbol{x}^{(r)}-\eta\sum_{k=0}^{c}\tilde{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]\nonumber\\
 &=\mathbb{E}\left[\left\|\eta\sum_{k=0}^{c}\tilde{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &\stackrel{\text{\ding{192}}}{=}\mathbb{E}\left[\left\|\eta\sum_{k=0}^{c}\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\left[\left\|\eta\sum_{k=0}^{c}{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &\stackrel{\text{\ding{193}}}{\leq}\eta^2c\sum_{k=0}^{c}\mathbb{E}\left[\left\|\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\left(c+1\right)\eta^2\sum_{k=0}^{c}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
  &{\leq}\eta^2\tau\sum_{k=0}^{\tau-1}\mathbb{E}\left[\left\|\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\tau\eta^2\sum_{k=0}^{\tau-1}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
  &\stackrel{\text{\ding{194}}}{\leq}\tau\eta^2\sum_{k=0}^{\tau-1}\sigma^2+\tau\eta^2\sum_{k=0}^{\tau-1}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &{=}\eta^2\sum_{k=0}^{\tau-1}\left[\tau\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2+\tau\sigma^2\right]
\end{align}

where \ding{192} comes from $\mathbb{E}\left[\mathbf{x}^2\right]=\text{Var}\left[\mathbf{x}\right]+\left[\mathbb{E}\left[\mathbf{x}\right]\right]^2$ and \ding{193} holds because $\text{Var}\left(\sum_{j=1}^n\mathbf{x}_j\right)=\sum_{j=1}^n\text{Var}\left(\mathbf{x}_j\right)$ for i.i.d. vectors $\mathbf{x}_i$ (and i.i.d. assumption comes from i.i.d. sampling), and finally \ding{194} follows from Assumption~\ref{Assu:2}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{thm:pl-iid}}
From Eq.~(\ref{eq:finalll}) under condition:
\begin{align}
       1\geq {\tau L^2\eta^2\tau}+{(\frac{\mu^2d}{p}+1)\eta\gamma L}{\tau} 
\end{align}
we obtain:
\begin{align}
         \mathbb{E}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\Big]&\leq -\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2+\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+\gamma(\frac{\mu^2d}{p}+1)\right)\sigma^2\nonumber\\
         &\leq -\eta\mu\gamma{\tau} \left(f({\boldsymbol{w}}^{(r)})-f({\boldsymbol{w}}^{(r)})\right)+\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+\gamma(\frac{\mu^2d}{p}+1)\right)\sigma^2 
\end{align}
which leads to the following bound:
\begin{align}
            \mathbb{E}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \left(1-\eta\mu\gamma{\tau}\right) \Big[f({\boldsymbol{w}}^{(r)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+(\frac{\mu^2d}{p}+1)\gamma\right)\sigma^2
\end{align}
which leads to the following bound by setting $\Delta=1-\eta\mu\gamma{\tau}$:
\begin{align}
            \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \Delta^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1-\Delta^R}{1-\Delta}\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+(\frac{\mu^2d}{p}+1)\gamma\right)\sigma^2\nonumber\\
            &\leq \Delta^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1}{1-\Delta}\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+(\frac{\mu^2d}{p}+1)\gamma\right)\sigma^2\nonumber\\
            &={\left(1-\eta\mu\gamma{\tau}\right)}^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1}{\eta\mu\gamma{\tau}}\frac{L\tau\gamma\eta^2 }{2p}\left(pL\tau\eta+(\frac{\mu^2d}{p}+1)\gamma\right)\sigma^2\nonumber\\
            &\leq \exp{-\left(\eta\mu\gamma{\tau} R\right)}\Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\left(\frac{L\kappa\eta^2 \tau}{2}+\frac{\kappa\eta }{2p}(\frac{\mu^2d}{p}+1)\gamma\right)\sigma^2
\end{align}
Then for the choice of $\eta=\frac{1}{L\gamma (\frac{\mu^2d}{p}+1) \tau}$ we obtain:
\begin{align}
                \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \exp{-\left(\frac{ R}{\kappa (\frac{\mu^2d}{p}+1)}\right)}\Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\left(\frac{1}{2\gamma^2 {(\frac{\mu^2d}{p}+1)}^2 }+\frac{1}{2p}\right)\frac{\sigma^2}{\mu\tau}
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following Lemma will be useful in our proof. 
\begin{lemma}
If you define $Q\triangleq3\sqrt[3]{\frac{ad^2}{2}}\sqrt{1+\sqrt{1+\frac{256 a}{27e^3d^4}}}$ and $$x\leq -\frac{1}{2}\sqrt{\frac{1}{3a}\left(Q-\frac{12a}{eQ}\right)}+\frac{1}{2}\sqrt{\frac{4}{eQ}-\frac{Q}{3a}+\frac{2d\sqrt{3}}{\sqrt{a}\sqrt{Q-\frac{12a}{e Q}}}}$$ then for positive constants $a,d,c\geq 0$, we have:
\begin{align}
    ax^{4}+dx-\frac{1}{e}\leq 0
\end{align}
\end{lemma}
\begin{proof}
We use the results in \cite{wiki:xxx} with $b=c=0$ and $p=0, q=\frac{d}{a}$. In this case, the only real valued root is 
\begin{align}
x&=-S+\frac{1}{2}\sqrt{-4S^2+\frac{d}{aS}},\:\text{and}\:S=\frac{1}{2}\sqrt{\frac{1}{3a}\left(Q-\frac{12a}{eQ}\right)}\nonumber\\
\implies x&=-\frac{1}{2}\sqrt{\frac{1}{3a}\left(Q-\frac{12a}{eQ}\right)}+\frac{1}{2}\sqrt{\frac{4}{eQ}-\frac{Q}{3a}+\frac{2d\sqrt{3}}{\sqrt{a}\sqrt{Q-\frac{12a}{e Q}}}} \nonumber\\
Q&=\sqrt[3]{\frac{{27a}{d^2}+\sqrt{\left({27a}{d^2}\right)^2+4\left(\frac{12a}{e}\right)^3}}{2}}=3\sqrt[3]{\frac{ad^2}{2}}\sqrt{1+\sqrt{1+\frac{256 a}{27e^3d^4}}}
\end{align} 
Therefore, we have with $Q=3\sqrt[3]{\frac{ad^2}{2}}\sqrt{1+\sqrt{1+\frac{256 a}{27e^3d^4}}}$:
\begin{align}
    x\leq -\frac{1}{2}\sqrt{\frac{1}{3a}\left(Q-\frac{12a}{eQ}\right)}+\frac{1}{2}\sqrt{\frac{4}{eQ}-\frac{Q}{3a}+\frac{2d\sqrt{3}}{\sqrt{a}\sqrt{Q-\frac{12a}{e Q}}}}
\end{align}
\end{proof}

Next, consider the following condition in~\cite{haddadpour2020federated}: 
\begin{align}
    \left(10\gamma^2L^4\tau^4\right)\eta^4+\left(L\gamma\tau\right)\eta-\frac{1}{q+1}\leq 0
\end{align}
where $a=10\gamma^2L^4\tau^4, d=L\gamma\tau$ and $e=q+1$, in this case 
\begin{align}
    Q=3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2\underbrace{\sqrt{1+\sqrt{1+\frac{2560}{27\gamma^2(q+1)^3}}}}_{\triangleq C}=3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2 C
\end{align}

Then we have:
\begin{align}
    x&\leq -\frac{1}{2}\sqrt{\frac{1}{3a}\left(Q-\frac{12a}{eQ}\right)}+\frac{1}{2}\sqrt{\frac{4}{eQ}-\frac{Q}{3a}+\frac{2d\sqrt{3}}{\sqrt{a}\sqrt{Q-\frac{12a}{e Q}}}}\nonumber\\
    &=-\frac{1}{2}\sqrt{\frac{3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2 C}{30\gamma^2L^4\tau^4}-\frac{4}{(q+1)3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2 C}}\nonumber\\
    &\quad+\frac{1}{2}\sqrt{\frac{4}{(q+1)3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2 C}-\frac{3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2 C}{30\gamma^2L^4\tau^4}+\frac{2L\gamma\tau\sqrt{3}}{\sqrt{10\gamma^2L^4\tau^4}\sqrt{3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2 C-\frac{120 \gamma^2L^4\tau^4}{(q+1) 3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{4}{3}} L^2\tau^2 C}}}}\nonumber\\
    &=\frac{1}{2L\tau\gamma^{\frac{1}{3}}}\Big(\sqrt{\frac{4}{(q+1)3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{2}{3}} C}-\frac{\sqrt[3]{\frac{10}{2}} C}{10}+\frac{2\sqrt{3}}{\sqrt{10}\sqrt{3\sqrt[3]{\frac{10}{2}} C-\frac{40 }{(q+1)\gamma^{\frac{2}{3}} \sqrt[3]{\frac{10}{2}} C}}}}-\sqrt{\frac{\sqrt[3]{\frac{10}{2}} C}{10}-\frac{4}{(q+1)3\sqrt[3]{\frac{10}{2}}\gamma^{\frac{2}{3}} C}}\Big)\nonumber\\
\end{align}