\begin{thebibliography}{1}

\bibitem{alistarh2017qsgd}
{\sc D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic}, {\em Qsgd:
  Communication-efficient sgd via gradient quantization and encoding}, in
  Advances in Neural Information Processing Systems, 2017, pp.~1709--1720.

\bibitem{bottou2008tradeoffs}
{\sc L.~Bottou and O.~Bousquet}, {\em The tradeoffs of large scale learning},
  in Advances in neural information processing systems, 2008, pp.~161--168.

\bibitem{haddadpour2020federated}
{\sc F.~Haddadpour, M.~M. Kamani, A.~Mokhtari, and M.~Mahdavi}, {\em Federated
  learning with compression: Unified analysis and sharp guarantees}, arXiv
  preprint arXiv:2007.01154,  (2020).

\bibitem{horvath2020better}
{\sc S.~Horv{\'a}th and P.~Richt{\'a}rik}, {\em A better alternative to error
  feedback for communication-efficient distributed learning}, arXiv preprint
  arXiv:2006.11077,  (2020).

\bibitem{ivkin2019communication}
{\sc N.~Ivkin, D.~Rothchild, E.~Ullah, I.~Stoica, R.~Arora, et~al.}, {\em
  Communication-efficient distributed sgd with sketching}, in Advances in
  Neural Information Processing Systems, 2019, pp.~13144--13154.

\bibitem{li2019privacy}
{\sc T.~Li, Z.~Liu, V.~Sekar, and V.~Smith}, {\em Privacy for free:
  Communication-efficient learning with differential privacy using sketches},
  arXiv preprint arXiv:1911.00972,  (2019).

\bibitem{lin2017deep}
{\sc Y.~Lin, S.~Han, H.~Mao, Y.~Wang, and W.~J. Dally}, {\em Deep gradient
  compression: Reducing the communication bandwidth for distributed training},
  arXiv preprint arXiv:1712.01887,  (2017).

\bibitem{robbins1951stochastic}
{\sc H.~Robbins and S.~Monro}, {\em A stochastic approximation method}, The
  annals of mathematical statistics,  (1951), pp.~400--407.

\bibitem{stich2018sparsified}
{\sc S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi}, {\em Sparsified sgd with
  memory}, in Advances in Neural Information Processing Systems, 2018,
  pp.~4447--4458.

\end{thebibliography}
