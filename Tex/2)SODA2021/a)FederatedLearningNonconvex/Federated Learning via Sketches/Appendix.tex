\section{Appendix}

\paragraph{Notation.} Here we indicate the count sketch of the vector $\boldsymbol{x}$ with $\mathbf{S}(\boldsymbol{x})$ and with abuse of notation we indicate the expectation over the randomness of count sketch with $\mathbb{E}_{\mathbf{S}}[.]$. We illustrate the the random subset of the devices selected by server with $\mathcal{K}$ with size $|\mathcal{K}|=k\leq p$, and we represent the expectation over the device sampling with $\mathbb{E}_{\mathcal{K}}[.]$. 

We will use the following fact (which is also used in \cite{li2019convergence,haddadpour2019convergence}) in proving results.
\begin{fact}[\cite{li2019convergence,haddadpour2019convergence}]\label{fact:1}
Let
$\{x_i\}_{i=1}^p$ denote any fixed deterministic sequence. We sample a multiset $\mathcal{P}$ (with size $K$) uniformly at random where $x_j$ is sampled  with probability $q_j$ for $1\leq j\leq p$ with replacement.  Let $\mathcal{P} = \{i_1,\ldots, i_K\} \subset[p]$ (some $i_j$â€™s may have the same value). Then
\begin{align}
    \mathbb{E}_{\mathcal{P}}\left[\sum_{i\in \mathcal{P}}x_i\right]=\mathbb{E}_{\mathcal{P}}\left[\sum_{k=1}^Kx_{i_k}\right]=K\mathbb{E}_{\mathcal{P}}\left[x_{i_k}\right]=K\left[\sum_{j=1}^pq_jx_j\right]
\end{align}
\end{fact}



\section{Results for the Homogeneous Setting}
\label{sec:app:sgd:undrr-pl}



In this section, we study the convergence properties of our  \texttt{FedSKETCH} method presented in Algorithm~\ref{Alg:PFLHom}. Before stating the proofs for \texttt{FedSKETCH} in the homogeneous setting, we first mention the following intermediate lemmas. 



\begin{lemma}\label{lemma:tasbih1-iid}
Using unbiased compression and under Assumption~\ref{Assu:1.5}, we have the following bound: 
\begin{align}
\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{{\mathbf{S},\xi^{(r)}}}\Big[\|\tilde{\mathbf{g}}_{\mathbf{S}}^{(r)}\|^2\Big]\right]&=\mathbb{E}_{{\xi}^{(r)}}\mathbb{E}_{\mathbf{S}}\Big[\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\Big]\leq \tau(\frac{\omega}{k}+1)\sum_{j=1}^mq_j\left[\sum_{c=0}^{\tau-1}\|\mathbf{g}_j^{(c,r)}\|^2+\sigma^2\right] \label{eq:lemma1}
\end{align}
\end{lemma}

\begin{proof}
\begin{align}
&\mathbb{E}_{{\xi^{(r)}|\boldsymbol{w}^{(r)}}}\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{{\mathbf{S}}}\Big[\|\frac{1}{k}\sum_{j\in \mathcal{K}} \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]\right]\nonumber\\
&=\mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{\mathbf{S}}\Big[\|\frac{1}{k}\sum_{j\in\mathcal{K}}\underbrace{\mathbf{S}\left(\overbrace{\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j}^{\tilde{\mathbf{g}}_j^{(r)}}\right)}_{\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}}\|^2\Big]\right]\right]\nonumber\\
&\stackrel{\text{\ding{192}}}{=}\mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\left[\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}-\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbb{E}_{\mathbf{S}}\left[\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]\|^2\right]+\|\mathbb{E}_{\mathbf{S}}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S},j}^{(r)}\right]\|^2\right]\right]\nonumber\\
&\stackrel{\text{\ding{193}}}{=}\mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{\mathbf{S}}\left[\|\frac{1}{k}\left[\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}-\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\right]\|^2\right]+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
&\stackrel{}{=} \mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\left[\text{Var}_{\mathbf{S}}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]\right]+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
&\stackrel{}{=} \mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k^2}\sum_{j\in\mathcal{K}}\text{Var}_{\mathbf{S}_j}\left[\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
&\stackrel{}{\leq} \mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k^2}\sum_{j\in\mathcal{K}}\omega\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
&\stackrel{}{=} \left[\mathbb{E}_{\xi}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\omega\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\mathbb{E}_{{\xi}^{(r)}}\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
&\stackrel{}{=} \left[\mathbb{E}_{\xi}\left[\frac{\omega}{k}\sum_{j=1}^pq_j\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\left[\text{Var}\left(\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\right)+\|\frac{1}{k}\sum_{j\in\mathcal{K}}{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\right]\nonumber\\
&\stackrel{}{=} \frac{\omega}{k}\sum_{j=1}^pq_j\mathbb{E}_{\xi}\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k^2}\sum_{j\in\mathcal{K}}\text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)+\|\frac{1}{k}\sum_{j\in\mathcal{K}}{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
&\stackrel{\ding{195}}{\leq}\frac{\omega}{k}\sum_{j=1}^pq_j\mathbb{E}_{\xi}\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k^2}\sum_{j\in\mathcal{K}}\tau\sigma^2+\frac{1}{k}\sum_{j\in\mathcal{K}}\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
&=\frac{\omega}{k}\sum_{j=1}^pq_j\left[\text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)+\left\|\mathbf{g}_{j}^{(r)}\right\|^2\right]+\left[\frac{\tau\sigma^2}{k}+\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
&\stackrel{\ding{196}}{\leq}\frac{\omega}{k}\sum_{j=1}^pq_j\left[\tau\sigma^2+\left\|\mathbf{g}_{j}^{(r)}\right\|^2\right]+\left[\frac{\tau\sigma^2}{k}+\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
&=(\omega+1)\frac{\tau\sigma^2}{k}+(\frac{\omega}{k}+1)\left[\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\label{eq:lemma111}%\nonumber\\
%&....\nonumber\\
%&\stackrel{}{\leq} \mathbb{E}_{{\xi}^{(r)}}\left[\left[\frac{1}{k}\sum_{j=1}^pq_j\omega\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\sum_{j=1}q_j\|\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
%&=\left(\frac{\omega}{k}+1\right)\sum_{j=1}^pq_j\mathbb{E}_{{\xi}^{(r)}}\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2\nonumber\\
%&=\left(\frac{\omega}{k}+1\right)\sum_{j=1}^pq_j\left[\text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)+\left\|{\mathbf{g}}_{j}^{(r)}\right\|^2\right]
\end{align}
where \text{\ding{192}} holds due to $\mathbb{E}\left[\left\|\mathbf{x}\right\|^2\right]=\text{Var}[\mathbf{x}]+\left\|\mathbb{E}[\mathbf{x}]\right\|^2$, \text{\ding{193}} is due to $\mathbb{E}_{\mathbf{S}}\left[\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]=\frac{1}{p}\sum_{j=1}^m\tilde{\mathbf{g}}_{j}^{(r)}$.


Next we show that from Assumptions~\ref{Assu:2}, we have 
\begin{align}\label{eq:100000}
    \mathbb{E}_{\xi^{(r)}}\left[\Big[\|{\tilde{\mathbf{g}}_j^{(r)}}-{\mathbf{g}_j^{(r)}}\|^2\Big]\right]\leq \tau \sigma^2
\end{align}
To do so, note that 
\begin{align}
    \text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)&=\mathbb{E}_{\xi^{(r)}}\left[\left\|{\tilde{\mathbf{g}}_j^{(r)}}-{\mathbf{g}_j^{(r)}}\right\|^2\right]\nonumber\\
    &\stackrel{\text{\ding{192}}}{=}\mathbb{E}_{\xi^{(r)}}\left[\left\|\sum_{c=0}^{\tau-1}\left[\tilde{\mathbf{g}}_j^{(c,r)}-\mathbf{g}_j^{(c,r)}\right]\right\|^2\right]\nonumber\\
    &{=}\text{Var}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right)\nonumber\\
    &\stackrel{\text{\ding{193}}}{=}\sum_{c=0}^{\tau-1}\text{Var}\left(\tilde{\mathbf{g}}_j^{(c,r)}\right)\nonumber\\
    &{=}\sum_{c=0}^{\tau-1}\mathbb{E}\left[\left\|\tilde{\mathbf{g}}_j^{(c,r)}-\mathbf{g}_j^{(c,r)}\right\|^2\right]\nonumber\\
    &\stackrel{\text{\ding{194}}}{\leq}\tau\sigma^2\label{eq:var_b_mid}
    \end{align}
where in \text{\ding{192}} we use the definition of ${\tilde{\mathbf{g}}}_j^{(r)}$ and ${{\mathbf{g}}}_j^{(r)}$, in \text{\ding{193}} we use the fact that mini-batches are chosen in i.i.d. manner at each local machine, and \text{\ding{194}} immediately follows from Assumptions~\ref{Assu:1.5}.

Replacing $\mathbb{E}_{\xi^{(r)}}\left[\|{\tilde{\mathbf{g}}_j^{(r)}}-{\mathbf{g}_j^{(r)}}\|^2\right]$ in \eqref{eq:lemma111} by its upper bound in \eqref{eq:100000} implies that 
\begin{align}
\mathbb{E}_{{\xi^{(r)}|\boldsymbol{w}^{(r)}}}\mathbb{E}_{\mathbf{S},\mathcal{K}}\Big[\|\frac{1}{k}\sum_{j\in\mathcal{K}} \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]
\leq (\omega+1)\frac{\tau\sigma^2}{k}+(\frac{\omega}{k}+1)\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\label{eq:lemma112}
\end{align}

Further note that we have 
\begin{align}
\left\|{\mathbf{g}}_j^{(r)}\right\|^2&=\|\sum_{c=0}^{\tau-1}\mathbf{g}_j^{(c,r)}\|^2\stackrel{}{\leq} \tau\sum_{c=0}^{\tau-1}\|\mathbf{g}_j^{(c,r)}\|^2\label{eq:mid-bounding-absg}
\end{align} 
where the last inequality is due to $\left\|\sum_{j=1}^n\mathbf{a}_i\right\|^2\leq n\sum_{j=1}^n\left\|\mathbf{a}_i\right\|^2$, which together with \eqref{eq:lemma112} leads to the following bound:
\begin{align}
    \mathbb{E}_{{\xi^{(r)}|\boldsymbol{w}^{(r)}}}\mathbb{E}_{\mathbf{S}}\Big[\|\frac{1}{k}\sum_{j\in\mathcal{K}} \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]\leq(\omega+1)\frac{\tau\sigma^2}{k}+\tau(\frac{\omega}{k}+1)\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(c,r)}\|^2,
\end{align}
and the proof is complete.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}\label{lemma:cross-inner-bound-unbiased}
  Under Assumption~\ref{Assu:1}, and according to the \texttt{FedCOM} algorithm the expected inner product between stochastic gradient and full batch gradient can be bounded with:
\begin{align}
    - \mathbb{E}_{\xi,\mathbf{S},\mathcal{K}}\left[\left\langle\nabla f({\boldsymbol{w}}^{(r)}),{{\tilde{\mathbf{g}}}^{(r)}}\right\rangle\right]&\leq \frac{1}{2}\eta\frac{1}{m}\sum_{j=1}^m\sum_{c=0}^{\tau-1}\left[-\|\nabla f({\boldsymbol{w}}^{(r)})\|_2^2-\|\nabla{f}(\boldsymbol{w}_j^{(c,r)})\|_2^2+L^2\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\|_2^2\right]\label{eq:lemma3-thm2}
\end{align}

\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
We have:
\begin{align}
    &-\mathbb{E}_{\{{\xi}^{(t)}_{1}, \ldots, {\xi}^{(t)}_{m}|{\boldsymbol{w}}^{(t)}_{1},\ldots,  {\boldsymbol{w}}^{(t)}_{m}\}} \mathbb{E}_{\mathbf{S},\mathcal{K}}\left[ \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}_{\mathbf{S},\mathcal{K}}^{(r)}\big\rangle\right]\nonumber\\
    &=-\mathbb{E}_{\{{\xi}^{(t)}_{1}, \ldots, {\xi}^{(t)}_{m}|{\boldsymbol{w}}^{(t)}_{1},\ldots,  {\boldsymbol{w}}^{(t)}_{m}\}}\left[\left\langle \nabla f({\boldsymbol{w}}^{(r)}),\eta\sum_{j\in\mathcal{K}}q_j\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right\rangle\right]\nonumber\\
    &=-\left\langle \nabla f({\boldsymbol{w}}^{(r)}),\eta\sum_{j=1}^mq_j\sum_{c=0}^{\tau-1}\mathbb{E}_{\xi,\mathbf{S}}\left[\tilde{\mathbf{g}}_{j,\mathbf{S}}^{(c,r)}\right]\right\rangle\nonumber\\
        &=-\eta\sum_{c=0}^{\tau-1}\sum_{j=1}^mq_j\left\langle \nabla f({\boldsymbol{w}}^{(r)}),{\mathbf{g}}_j^{(c,r)}\right\rangle\nonumber\\ 
     &\stackrel{\text{\ding{192}}}{=}\frac{1}{2}\eta\sum_{c=0}^{\tau-1}\sum_{j=1}^mq_j\left[-\|\nabla f({\boldsymbol{w}}^{(r)})\|_2^2-\|{{\nabla{f}}}(\boldsymbol{w}_j^{(c,r)})\|_2^2+\|\nabla f({\boldsymbol{w}}^{(r)})-\nabla{f}(\boldsymbol{w}_j^{(c,r)})\|_2^2\right]\nonumber\\
    &\stackrel{\text{\ding{193}}}{\leq}\frac{1}{2}\eta\sum_{c=0}^{\tau-1}\sum_{j=1}^mq_j\left[-\|\nabla f({\boldsymbol{w}}^{(r)})\|_2^2-\|\nabla{f}(\boldsymbol{w}_j^{(c,r)})\|_2^2+L^2\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\|_2^2\right]
   \label{eq:bounding-cross-no-redundancy}
\end{align}

where \ding{192} is due to $2\langle \mathbf{a},\mathbf{b}\rangle=\|\mathbf{a}\|^2+\|\mathbf{b}\|^2-\|\mathbf{a}-\mathbf{b}\|^2$, and \ding{193} follows from Assumption \ref{Assu:1}.
\end{proof}



The following lemma bounds the distance of local solutions from global solution at $r$th communication round.
\begin{lemma}\label{lemma:dif-under-pl-sgd-iid}
Under Assumptions~\ref{Assu:1.5} we have:
\begin{align}
      \mathbb{E}\left[\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\|_2^2\right]&\leq\eta^2\tau\sum_{c=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\eta^2\tau\sigma^2
\end{align}

\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}
Note that
\begin{align}
 \mathbb{E}\left[\left\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\right\|_2^2\right]&=\mathbb{E}\left[\left\|{\boldsymbol{w}}^{(r)}-\left({\boldsymbol{w}}^{(r)}-\eta\sum_{k=0}^{c}\tilde{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]\nonumber\\
 &=\mathbb{E}\left[\left\|\eta\sum_{k=0}^{c}\tilde{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &\stackrel{\text{\ding{192}}}{=}\mathbb{E}\left[\left\|\eta\sum_{k=0}^{c}\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\left[\left\|\eta\sum_{k=0}^{c}{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &\stackrel{\text{\ding{193}}}{=}\eta^2\sum_{k=0}^{c}\mathbb{E}\left[\left\|\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\left(c+1\right)\eta^2\sum_{k=0}^{c}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
  &{\leq}\eta^2\sum_{k=0}^{\tau-1}\mathbb{E}\left[\left\|\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\tau\eta^2\sum_{k=0}^{\tau-1}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
  &\stackrel{\text{\ding{194}}}{\leq}\eta^2\sum_{k=0}^{\tau-1}\sigma^2+\tau\eta^2\sum_{k=0}^{\tau-1}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &{=}\eta^2\tau\sigma^2+\eta^2\sum_{k=0}^{\tau-1}\tau\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2
\end{align}

where \ding{192} comes from $\mathbb{E}\left[\mathbf{x}^2\right]=\text{Var}\left[\mathbf{x}\right]+\left[\mathbb{E}\left[\mathbf{x}\right]\right]^2$ and \ding{193} holds because $\text{Var}\left(\sum_{j=1}^n\mathbf{x}_j\right)=\sum_{j=1}^n\text{Var}\left(\mathbf{x}_j\right)$ for i.i.d. vectors $\mathbf{x}_i$ (and i.i.d. assumption comes from i.i.d. sampling), and finally \ding{194} follows from Assumption~\ref{Assu:1.5}.
\end{proof}

\subsection{Main result for the non-convex setting}
Now we are ready to present our result for the homogeneous setting. We first state and prove the result for the general nonconvex objectives.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Non-convex]\label{thm:lsgwd-lr} For \texttt{FedSKETCH}$(\tau, \eta, \gamma)$, for all $0\leq t\leq R\tau-1$,  under Assumptions \ref{Assu:1} to \ref{Assu:1.5}, if the learning rate satisfies \begin{align}
   1\geq {\tau^2 L^2\eta^2}+\left(\frac{\omega}{k}+1\right){\eta\gamma L}{\tau}
\label{eq:cnd-thm4.3}
\end{align}
and all local model parameters are initialized at the same point ${\boldsymbol{w}}^{(0)}$, then the average-squared gradient after $\tau$ iterations is bounded as follows:
\begin{align}
        \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq \frac{2\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)}{\eta\gamma\tau R}+\frac{L\eta\gamma{\left(\omega+1\right)}}{k}\sigma^2+{L^2\eta^2\tau }\sigma^2\label{eq:thm1-result} 
\end{align}
where $\boldsymbol{w}^{(*)}$ is the global optimal solution with  function value $f(\boldsymbol{w}^{(*)})$.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
Before proceeding to the proof of Theorem~\ref{thm:lsgwd-lr}, we would like to highlight that 
\begin{align}
    \boldsymbol{w}^{(r)}- ~{\boldsymbol{w}}_{j}^{(\tau,r)}=\eta\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}.\label{eq:decent-smoothe}
\end{align}

From the updating rule of Algorithm~\ref{Alg:PFLHom} we have

{
\begin{align}
     {\boldsymbol{w}}^{(r+1)}=\boldsymbol{w}^{(r)}-\gamma\eta\left(\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbf{S}\Big(\sum_{c=0,r}^{\tau-1}\tilde{\mathbf{g}}_{j}^{(c,r)}\Big)\right)=\boldsymbol{w}^{(r)}-\gamma\left[\frac{\eta}{k}\sum_{j\in\mathcal{K}}\mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_{j}^{(c,r)}\right)\right]\label{eq:update-rule-dec}
\end{align}
}
In what follows, we use the following notation to denote the stochastic gradient used to update the global model at $r$th communication round $$\tilde{\mathbf{g}}_{\mathbf{S},\mathcal{K}}^{(r)}\triangleq\frac{\eta}{p}\sum_{j=1}^{p}\mathbf{S}\left(\frac{\boldsymbol{w}^{(r)}- ~{\boldsymbol{w}}_{j}^{(\tau,r)}}{\eta}\right)=\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right).$$ 
and notice that $\boldsymbol{w}^{(r)} = \boldsymbol{w}^{(r-1)} - \gamma \tilde{\mathbf{g}}^{(r)}$.


Then using the Assumption~\ref{Assu:09} we have:
\begin{align}
  \mathbb{E}_\mathbf{S}\left[\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\right]=\frac{1}{k}\sum_{j\in\mathcal{K}}\left[-\eta\mathbb{E}_\mathbf{S}\left[ \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right)\right]\right]=\frac{1}{k}\sum_{j\in\mathcal{K}}\left[-\eta\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right)\right]\triangleq \tilde{\mathbf{g}}_{\mathbf{S},\mathcal{K}}^{(r)}\label{eq:unbiased_gd1} 
\end{align}



%%%%%%%%%%%%%


From the $L$-smoothness gradient assumption on global objective, by using  $\tilde{\mathbf{g}}^{(r)}$ in inequality (\ref{eq:decent-smoothe}) we have:
\begin{align}
    f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\leq -\gamma \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle+\frac{\gamma^2 L}{2}\|\tilde{\mathbf{g}}^{(r)}\|^2\label{eq:Lipschitz-c1}
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By taking expectation on both sides of above inequality over sampling, we get:
\begin{align}
    \mathbb{E}\left[\mathbb{E}_\mathbf{S}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\Big]\right]&\leq -\gamma\mathbb{E}\left[\mathbb{E}_\mathbf{S}\left[ \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\big\rangle\right]\right]+\frac{\gamma^2 L}{2}\mathbb{E}\left[\mathbb{E}_\mathbf{S}\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\right]\nonumber\\
    &\stackrel{(a)}{=}-\gamma\underbrace{\mathbb{E}\left[\left[ \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle\right]\right]}_{(\mathrm{I})}+\frac{\gamma^2 L}{2}\underbrace{\mathbb{E}\left[\mathbb{E}_\mathbf{S}\Big[\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\Big]\right]}_{\mathrm{(II)}}\label{eq:Lipschitz-c-gd}
\end{align}
We proceed to use Lemma~\ref{lemma:tasbih1-iid}, Lemma~\ref{lemma:cross-inner-bound-unbiased}, and Lemma~\ref{lemma:dif-under-pl-sgd-iid}, to bound  terms $(\mathrm{I})$ and $(\mathrm{II})$ in right hand side of (\ref{eq:Lipschitz-c-gd}), which gives
\begin{align}
     &\mathbb{E}\left[\mathbb{E}_{\mathbf{S}}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\Big]\right]\nonumber\\
     &\leq \gamma\frac{1}{2}\eta\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\left[-\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2-\left\|\mathbf{g}_j^{(c,r)}\right\|_2^2+L^2\eta^2\sum_{c=0}^{\tau-1}\left[\tau\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\sigma^2\right]\right]\nonumber\\
     &\quad+\frac{\gamma^2 L(\frac{\omega}{k}+1)}{2}\left[{\eta^2\tau}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\|\mathbf{g}^{(c,r)}_{j}\|^2\right]+\frac{\gamma^2\eta^2 L(\omega+1)}{2}\frac{\tau \sigma^2}{k}\nonumber\\
     &\stackrel{\text{\ding{192}}}{\leq}\frac{\gamma\eta}{2}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\left[-\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2-\left\|\mathbf{g}_j^{(c,r)}\right\|_2^2+\tau L^2\eta^2\left[\tau\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\sigma^2\right]\right]\nonumber\\
     &\quad+\frac{\gamma^2 L(\frac{\omega}{k}+1)}{2}\left[{\eta^2\tau}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\|\mathbf{g}^{(c,r)}_{j}\|^2\right]+\frac{\gamma^2\eta^2 L(\omega+1)}{2}\frac{\tau \sigma^2}{k}\nonumber\\
     &=-\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\nonumber\\
     &\quad-\left(1-{\tau L^2\eta^2\tau}-{(\frac{\omega}{k}+1)\eta\gamma L}{\tau}\right)\frac{\eta\gamma}{2}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\|\mathbf{g}^{(c,r)}_{j}\|^2+\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+\gamma(\omega+1)\right)\sigma^2\nonumber\\
     &\stackrel{\text{\ding{193}}}{\leq} -\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2+\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+\gamma(\omega+1)\right)\sigma^2\label{eq:finalll}
\end{align}
where in \ding{192} we incorporate outer summation $\sum_{c=0}^{\tau-1}$, and  \ding{193} follows from condition 
\begin{align}
   1\geq {\tau L^2\eta^2\tau}+(\frac{\omega}{k}+1)\eta\gamma L{\tau}. 
\end{align}
Summing up for all $R$ communication rounds and  rearranging the terms gives:
\begin{align}
    \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq \frac{2\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)}{\eta\gamma\tau R}+\frac{L\eta\gamma{(\omega+1)}}{k}\sigma^2+{L^2\eta^2\tau }\sigma^2
\end{align}
From above inequality, is it easy to see that in order to achieve a linear speed up, we need to have $\eta\gamma=O\left(\frac{\sqrt{k}}{\sqrt{R \tau}}\right)$.
\end{proof}


\begin{corollary}[Linear speed up] 
In Eq.~(\ref{eq:thm1-result}) for the choice of  $\eta\gamma=O\left(\frac{1}{L}\sqrt{\frac{k}{R\tau\left(\omega+1\right)}}\right)$, and $\gamma\geq k$  the  convergence rate reduces to:
\begin{align}
    \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2&\leq O\left(\frac{L\sqrt{\left(\omega+1\right)}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{*})\right)}{\sqrt{kR\tau}}+\frac{\left(\sqrt{\left(\omega+1\right)}\right)\sigma^2}{\sqrt{kR\tau}}+\frac{k\sigma^2}{R\gamma^2}\right).\label{eq:convg-error}
\end{align}
Note that according to Eq.~(\ref{eq:convg-error}), if we pick  a fixed constant value for  $\gamma$, in order to achieve an $\epsilon$-accurate solution, $R=O\left(\frac{1}{\epsilon}\right)$ communication rounds and $\tau=O\left(\frac{\omega+1}{k\epsilon}\right)$ local updates are necessary. We also highlight  that Eq.~(\ref{eq:convg-error}) also allows us to choose $R=O\left(\frac{\omega+1}{\epsilon}\right)$ and $\tau=O\left(\frac{1}{k\epsilon}\right)$ to get the  same convergence rate.
\end{corollary}

\begin{remark}\label{rmk:cnd-lr}

Condition in Eq.~(\ref{eq:cnd-thm4.3}) can be rewritten as 
\begin{align}
    \eta&\leq \frac{-\gamma L\tau\left(\frac{\omega}{k}+1\right)+\sqrt{\gamma^2 \left(L\tau\left(\frac{\omega}{k}+1\right)\right)^2+4L^2\tau^2}}{2L^2\tau^2}\nonumber\\
    &= \frac{-\gamma L\tau\left(\frac{\omega}{k}+1\right)+L\tau\sqrt{\left(\frac{\omega}{k}+1\right)^2\gamma^2 +4}}{2L^2\tau^2}\nonumber\\
    &=\frac{\sqrt{\left(\frac{\omega}{k}+1\right)^2\gamma^2 +4}-\left(\frac{\omega}{k}+1\right)\gamma}{2L\tau}\label{eq:lrcnd}
\end{align}

So based on Eq.~(\ref{eq:lrcnd}), if we set $\eta=O\left(\frac{1}{L\gamma}\sqrt{\frac{p}{R\tau\left(\omega+1\right)}}\right)$, it implies that:
\begin{align}
    R\geq \frac{\tau k}{\left(\omega+1\right)\gamma^2\left(\sqrt{\left(\frac{\omega}{k}+1\right)^2\gamma^2+4}-\left(\frac{\omega}{k}+1\right)\gamma\right)^2}\label{eq:iidexact}
\end{align}
We note that $\gamma^2\left(\sqrt{\left(\frac{\omega}{k}+1\right)^2\gamma^2+4}-\left(\frac{\omega}{k}+1\right)\gamma\right)^2=\Theta(1)\leq 5 $ therefore even for $\gamma\geq m$ we need to have 
\begin{align}
    R\geq \frac{\tau k}{5\left(\omega+1\right)}=O\left(\frac{\tau k}{\left(\omega+1\right)}\right)\label{eq:lrbnd-homog}
\end{align}

{Therefore, for the choice of $\tau=O\left(\frac{\omega+1}{k\epsilon}\right)$, due to condition in Eq.~(\ref{eq:lrbnd-homog}), we need to have $R=O\left(\frac{1}{\epsilon}\right)$. Similarly, we can have $R=O\left(\frac{\omega+1}{\epsilon}\right)$ and $\tau=O\left(\frac{1}{k\epsilon}\right)$.}


\end{remark}

\begin{corollary}[Special case, $\gamma=1$]
By letting $\gamma=1$, $\omega=0$ and $k=p$ the convergence rate in Eq.~(\ref{eq:thm1-result}) reduces to 
\begin{align}
     \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2&\leq \frac{2\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)}{\eta R\tau}+\frac{L\eta }{p}\sigma^2+{L^2\eta^2\tau }\sigma^2
\end{align}
which matches the rate  obtained in~\cite{wang2018cooperative}. In this case the communication complexity and the number of local updates become \begin{align}
    {R}=O\left(\frac{p}{\epsilon}\right), \:\:\: \tau=O\left(\frac{1}{\epsilon}\right).
\end{align}
This simply implies  that in this special case the convergence rate of our algorithm reduces to the  rate obtained in~\cite{wang2018cooperative}, which indicates the tightness of  our analysis.
\end{corollary}



\subsection{Main result for the PL/Strongly convex setting}
 
We now turn to stating the convergence rate for the homogeneous setting under PL condition which naturally leads to the same rate for strongly convex functions.
\begin{theorem}[PL or strongly convex]\label{thm:pl-iid}
For \texttt{FedSKETCH}$(\tau, \eta, \gamma)$, for all $0\leq t\leq R\tau-1$,  under Assumptions \ref{Assu:1} to \ref{Assu:1.5} and \ref{assum:pl},if the learning rate satisfies \begin{align}
   1\geq {\tau^2 L^2\eta^2}+\left(\frac{\omega}{k}+1\right){\eta\gamma L}{\tau} 
%\label{eq:cnd-thm4.3}
\end{align}

and if the all the models are initialized with $\boldsymbol{w}^{(0)}$ we obtain:
\begin{align}
        \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \left(1-\eta\gamma{\mu\tau}\right)^R\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\mu}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(1+\omega\right)\frac{\gamma\eta L\sigma^2}{2k}\right]
\end{align}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
From Eq.~(\ref{eq:finalll}) under condition:
\begin{align}
       1\geq {\tau L^2\eta^2\tau}+{{(\frac{\omega}{k}+1)}\eta\gamma L}{\tau} 
\end{align}
we obtain:
\begin{align}
         \mathbb{E}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\Big]&\leq -\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2+\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+\gamma(\omega+1)\right)\sigma^2\nonumber\\
         &\leq -\eta\mu\gamma{\tau} \left(f({\boldsymbol{w}}^{(r)})-f({\boldsymbol{w}}^{(r)})\right)+\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+\gamma(\omega+1)\right)\sigma^2 
\end{align}
which leads to the following bound:
\begin{align}
            \mathbb{E}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \left(1-\eta\mu\gamma{\tau}\right) \Big[f({\boldsymbol{w}}^{(r)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+{(\omega+1)}\gamma\right)\sigma^2
\end{align}
By setting $\Delta=1-\eta\mu\gamma{\tau}$ we obtain  the following bound:
\begin{align}
            &\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\nonumber\\
            &\leq \Delta^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1-\Delta^R}{1-\Delta}\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+{(\omega+1)}\gamma\right)\sigma^2\nonumber\\
            &\leq \Delta^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1}{1-\Delta}\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+{(\omega+1)}\gamma\right)\sigma^2\nonumber\\
            &={\left(1-\eta\mu\gamma{\tau}\right)}^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1}{\eta\mu\gamma{\tau}}\frac{L\tau\gamma\eta^2 }{2k}\left(kL\tau\eta+{(\omega+1)}\gamma\right)\sigma^2
\end{align}
\end{proof}


\begin{corollary}
If we  let $\eta\gamma\mu\tau\leq\frac{1}{2}$, $\eta=\frac{1}{2L\left(\frac{\omega}{k}+1\right)\tau\gamma }$ and $\kappa=\frac{L}{\mu}$ the convergence error in Theorem~\ref{thm:pl-iid}, with $\gamma\geq k$ results in:

\begin{align}
&\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\nonumber\\
&\leq e^{-\eta\gamma{\mu\tau}R}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\mu}}\left[\frac{1}{2} \tau L^2\eta^2\sigma^2+\left(1+\omega\right)\frac{\gamma\eta L\sigma^2}{2k}\right]\nonumber\\
&\leq e^{-\frac{R}{2\left(\frac{\omega}{k}+1\right)\kappa}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\mu}}\left[\frac{1}{2} L^2\frac{\tau\sigma^2}{L^2\left(\frac{\omega}{k}+1\right)^2\gamma^2\tau^2}+\frac{\left(1+\omega\right) L\sigma^2}{2\left(\frac{\omega}{k}+1\right)L\tau k}\right]\nonumber\\
&=O\left(e^{-\frac{R}{2\left(\frac{\omega}{k}+1\right)\kappa}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{\sigma^2}{\left(\frac{\omega}{k}+1\right)^2\gamma^2\mu\tau}+\frac{\left(\omega+1\right)\sigma^2}{\mu\left(\frac{\omega}{k}+1\right) \tau k}\right)
\nonumber\\
&=O\left(e^{-\frac{R}{2\left(\frac{\omega}{k}+1\right)\kappa}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{\sigma^2}{\gamma^2\mu\tau}+\frac{\left(\omega+1\right)\sigma^2}{\mu\left(\frac{\omega}{k}+1\right) \tau k}\right)
\label{eq:pliid}
\end{align}
which indicates  that to achieve an error of $\epsilon$, we need to have $R=O\left(\left(\frac{\omega}{k}+1\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $\tau=\frac{\left(\omega+1\right)}{k\left(\frac{\omega}{k}+1\right)\epsilon}$. {Additionally, we note that if $\gamma\rightarrow\infty$, yet $R=O\left(\left(\frac{\omega}{k}+1\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $\tau=\frac{\left(\omega+1\right)}{k\left(\frac{\omega}{k}+1\right)\epsilon}$ will be necessary.}
\end{corollary}

\subsection{Main result for the general convex setting}

\begin{theorem}[Convex]\label{thm:cvx-iid}
 For a general convex function $f(\boldsymbol{w})$ with optimal solution $\boldsymbol{w}^{(*)}$, using  \texttt{FedSKETCH}$(\tau, \eta, \gamma)$ to optimize $\tilde{f}(\boldsymbol{w},\phi)=f(\mathbf{\boldsymbol{w}})+\frac{\phi}{2}\left\|\boldsymbol{w}\right\|^2$,  for all $0\leq t\leq R\tau-1$,  under Assumptions \ref{Assu:1} to \ref{Assu:1.5}, if the learning rate satisfies \begin{align}
   1\geq {\tau^2 L^2\eta^2}+\left(\frac{\omega}{k}+1\right){\eta\gamma L}{\tau} 
%\label{eq:cnd-thm4.3}
\end{align}
and if the all the models initiate with $\boldsymbol{w}^{(0)}$, with $\phi=\frac{1}{\sqrt{k\tau}}$ and $\eta=\frac{1}{2L\gamma\tau\left(1+\frac{\omega}{k}\right)}$ we obtain:
\begin{align}
        \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq e^{-\frac{ R}{2L\left(1+\frac{\omega}{k}\right) \sqrt{m\tau}}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)\nonumber\\
        &\qquad +\left[\frac{\sqrt{k}\sigma^2}{8\sqrt{\tau}\gamma^2\left(1+\frac{\omega}{k}\right)^2} +\frac{\left(\omega+1\right)\sigma^2}{4\left(\frac{\omega}{k}+1\right)\sqrt{k\tau}} \right] +\frac{1}{2\sqrt{k\tau}}\left\|\boldsymbol{w}^{(*)}\right\|^2\label{eq:cvx-iid}
\end{align}{{}}
\end{theorem}
We note that above theorem implies that to achieve a convergence error of $\epsilon$ we need to have $R=O\left(L\left(1+\frac{\omega}{k}\right)\frac{1}{\epsilon}\log\left(\frac{1}{\epsilon}\right)\right)$ and $\tau=O\left(\frac{\left(\omega+1\right)^2}{k\left(\frac{\omega}{k}+1\right)^2\epsilon}\right)$.


\begin{proof}
Since $\tilde{f}(\boldsymbol{w}^{(r)},\phi)=f(\boldsymbol{w}^{(r)})+\frac{\phi}{2}\left\|\boldsymbol{w}^{(r)}\right\|^2$ is $\phi$-PL, according to Theorem~\ref{thm:pl-iid}, we have:
\begin{align}
   & \tilde{f}(\boldsymbol{w}^{(R)},\phi)-\tilde{f}(\boldsymbol{w}^{(*)},\phi)\nonumber\\
   &={f}(\boldsymbol{w}^{(r)})+\frac{\phi}{2}\left\|\boldsymbol{w}^{(r)}\right\|^2-\left({f}(\boldsymbol{w}^{(*)})+\frac{\phi}{2}\left\|\boldsymbol{w}^{(*)}\right\|^2\right)\nonumber\\
    &\leq \left(1-\eta\gamma{\phi\tau}\right)^R\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\phi}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(1+\omega\right)\frac{\gamma\eta L\sigma^2}{2k}\right]\label{eq:mid-cvx}
\end{align}
Next rearranging Eq.~(\ref{eq:mid-cvx}) and replacing $\mu$ with $\phi$ leads to the following error bound:
\begin{align}
  &  {f}(\boldsymbol{w}^{(R)})-f^*\nonumber\\
  &\leq \left(1-\eta\gamma{\phi\tau}\right)^R\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\phi}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(1+\omega\right)\frac{\gamma\eta L\sigma^2}{2k}\right] \nonumber\\
  &\qquad +\frac{\phi}{2}\left(\left\|\boldsymbol{w}^*\right\|^2-\left\|\boldsymbol{w}^{(r)}\right\|^2\right)\nonumber\\
    &\leq e^{-\left(\eta\gamma{\phi\tau}\right)R}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\phi}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(1+\omega\right)\frac{\gamma\eta L\sigma^2}{2k}\right] +\frac{\phi}{2}\left\|\boldsymbol{w}^{(*)}\right\|^2 
\end{align}
Next, if we set $\phi=\frac{1}{\sqrt{k\tau}}$ and $\eta=\frac{1}{2\left(1+\frac{\omega}{k}\right)L\gamma \tau}$, we obtain that
\begin{align}
        &{f}(\boldsymbol{w}^{(R)})-f^*\nonumber\\
        &\leq e^{-\frac{R}{2\left(1+\frac{\omega}{k}\right)L \sqrt{m\tau}}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\sqrt{k\tau}\left[\frac{\sigma^2}{8\tau\gamma^2\left(1+\frac{\omega}{k}\right)^2} +\frac{\left(\omega+1\right)\sigma^2}{4\left(\frac{\omega}{k}+1\right)\tau k}\right] +\frac{1}{2\sqrt{k\tau}}\left\|\boldsymbol{w}^{(*)}\right\|^2 ,
\end{align}
thus the proof is complete. 
\end{proof}

\newpage




\newpage
\section{Proof of main Theorems}
The proof of Theorem~\ref{thm:homog_case} follows directly from the results in~\cite{haddadpour2020federated}. For the sake of the completeness we review an assumptions from this reference for the quantiziation with their notation.

\begin{assumption}[\cite{haddadpour2020federated}]\label{Assu:quant}
The output of the compression operator $Q(\mathbf{x})$ is an unbiased estimator of its input $\mathbf{x}$, and its variance grows with the squared of the squared of $\ell_2$-norm of its argument, i.e., $\mathbb{E}\left[Q(\mathbf{x})\right]=\mathbf{x}$ and $\mathbb{E}\left[\left\|Q(\mathbf{x})-\mathbf{x}\right\|^2\right]\leq \omega\left\|\mathbf{x}\right\|^2$ .
\end{assumption}


\subsection{Proof of Theorem~\ref{thm:homog_case}}
Based on Assumption~\ref{Assu:quant} we have:
\begin{theorem}[\cite{haddadpour2020federated}]\label{thm:fromhaddad}
 Consider \texttt{FedCOM} in \cite{haddadpour2020federated}. Suppose that the conditions in Assumptions~\ref{Assu:1}, \ref{Assu:1.5} and \ref{Assu:quant} hold. If the local data distributions of all users are identical (homogeneous setting), then we have  
 \begin{itemize}
     \item \textbf{Nonconvex:}  By choosing stepsizes as $\eta=\frac{1}{L\gamma}\sqrt{\frac{p}{R\tau\left(\frac{\omega}{p}+1\right)}}$ and $\gamma\geq p$, the sequence of iterates satisfies  $\frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq {\epsilon}$ if we set
     $R=O\left(\frac{1}{\epsilon}\right)$ and $ \tau=O\left(\frac{\frac{\omega}{p}+1}{{p}\epsilon}\right)$.
     \item \textbf{Strongly convex or PL:}
      By choosing stepsizes as $\eta=\frac{1}{2L\left(\frac{\omega}{p}+1\right)\tau\gamma}$ and $\gamma\geq m$, we obtain that the iterates satisfy $\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if  we set
     $R=O\left(\left(\frac{\omega}{p}+1\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon}\right)$.
     \item \textbf{Convex:} By choosing stepsizes as $\eta=\frac{1}{2L\left(\frac{\omega}{p}+1\right)\tau\gamma}$ and $\gamma\geq p$, we obtain that the iterates satisfy $ \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if we set
     $R=O\left(\frac{L\left(1+\frac{\omega}{p}\right)}{\epsilon}\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon^2}\right)$.
 \end{itemize}
\end{theorem}

\begin{proof}
Since the sketching \texttt{PRIVIX} and \texttt{HEAPRIX}, satisfy the Assumption~\ref{Assu:quant} with $\omega=\mu^2d$ and $\omega=\mu^2d-1$ respectively with probablity $1-\delta$.  Therefore, all the results in Theorem~\ref{thm:homog_case}, conclude from Theorem~\ref{thm:fromhaddad} with probability $1-\delta$ and plugging $\omega=\mu^2d$ and $\omega=\mu^2d-1$ respectively into the corresponding convergence bounds.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:hetreg_case}}
For the heterogeneous setting, the results in~\cite{haddadpour2020federated} requires the following extra assumption that naturally holds for the sketching: 

\begin{assumption}[\cite{haddadpour2020federated}]\label{assum:009}
The compression scheme $Q$ for the heterogeneous data distribution setting satisfies the following condition $
    \mathbb{E}_Q[\|\frac{1}{m}\sum_{j=1}^m Q(\boldsymbol{x}_j)\|^2-\|Q(\frac{1}{m}\sum_{j=1}^m \boldsymbol{x}_j)\|^2]\leq G_q$.
\end{assumption}
We note that since sketching is a linear compressor, in the case of our algorithms for heterogeneous setting we have $G_q=0$. 

Next, we restate the Theorem in \cite{haddadpour2020federated} here as follows:

\begin{theorem}\label{thm:fromhaddad-het}
 Consider \texttt{FedCOMGATE} in \cite{haddadpour2020federated}. If Assumptions~\ref{Assu:1}, \ref{Assu:2}, \ref{Assu:quant}  and \ref{assum:009} hold, then even for the case the local data distribution of users are different  (heterogeneous setting) we have
 \begin{itemize}
     \item \textbf{Non-convex:} By choosing stepsizes as $\eta=\frac{1}{L\gamma}\sqrt{\frac{p}{R\tau\left(\omega+1\right)}}$ and $\gamma\geq p$, we obtain that the iterates satsify  $\frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq \epsilon$ if we set
     $R=O\left(\frac{\omega+1}{\epsilon}\right)$ and $ \tau=O\left(\frac{1}{p\epsilon}\right)$.
     \item \textbf{Strongly convex or PL:}
      By choosing stepsizes as $\eta=\frac{1}{2L\left(\frac{\omega}{p}+1\right)\tau\gamma}$ and ${\gamma\geq \sqrt{p\tau}}$, we obtain that the iterates satisfy $\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if we set
      $R=O\left(\left(\omega+1\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon}\right)$.
     \item \textbf{Convex:}  By choosing stepsizes as $\eta=\frac{1}{2L\left(\omega+1\right)\tau\gamma}$ and ${\gamma\geq \sqrt{p\tau}}$, we obtain that the iterates satisfy $\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if we set
     $R=O\left(\frac{L\left(1+\omega\right)}{\epsilon}\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon^2}\right)$.
 \end{itemize}
 
\end{theorem}
\begin{proof}
Since the sketching \texttt{PRIVIX} and \texttt{HEAPRIX}, satisfy the Assumption~\ref{Assu:quant} with $\omega=\mu^2d$ and $\omega=\mu^2d-1$ respectively with probablity $1-\delta$.  Therefore, all the results in Theorem~\ref{thm:hetreg_case}, conclude from Theorem~\ref{thm:fromhaddad-het} with probability $1-\delta$ and plugging $\omega=\mu^2d$ and $\omega=\mu^2d-1$ respectively into the convergence bounds.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

