\begin{thebibliography}{10}

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit
  Khirirat, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5973--5983, 2018.

\bibitem{basu2019qsparse}
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi.
\newblock Qsparse-local-sgd: Distributed sgd with quantization, sparsification
  and local computations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14695--14706, 2019.

\bibitem{bayoumi2020tighter}
Ahmed Khaled~Ragab Bayoumi, Konstantin Mishchenko, and Peter Richtarik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529, 2020.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569, 2018.

\bibitem{bonawitz2017practical}
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H~Brendan
  McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In {\em Proceedings of the 2017 ACM SIGSAC Conference on Computer and
  Communications Security}, pages 1175--1191, 2017.

\bibitem{bottou-bousquet-2008}
L\'{e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  161--168, 2008.

\bibitem{carlini2019secret}
Nicholas Carlini, Chang Liu, {\'U}lfar Erlingsson, Jernej Kos, and Dawn Song.
\newblock The secret sharer: Evaluating and testing unintended memorization in
  neural networks.
\newblock In {\em 28th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 19)}, pages 267--284, 2019.

\bibitem{DBLP:journals/tcs/CharikarCF04}
Moses Charikar, Kevin~C. Chen, and Martin Farach{-}Colton.
\newblock Finding frequent items in data streams.
\newblock {\em Theor. Comput. Sci.}, 312(1):3--15, 2004.

\bibitem{chen2020toward}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock {\em ACM-IMS Foundations of Data Science Conference}, 2020.

\bibitem{cormode2005improved}
Graham Cormode and Shan Muthukrishnan.
\newblock An improved data stream summary: the count-min sketch and its
  applications.
\newblock {\em Journal of Algorithms}, 55(1):58--75, 2005.

\bibitem{geyer2017differentially}
Robin~C Geyer, Tassilo Klein, and Moin Nabi.
\newblock Differentially private federated learning: A client level
  perspective.
\newblock {\em arXiv preprint arXiv:1712.07557}, 2017.

\bibitem{haddadpour2019local}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe.
\newblock Local sgd with periodic averaging: Tighter analysis and adaptive
  synchronization.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{haddadpour2019trading}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe.
\newblock Trading redundancy for communication: Speeding up distributed sgd for
  non-convex optimization.
\newblock In {\em ICML}, pages 2545--2554, 2019.

\bibitem{haddadpour2020federated}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock {\em arXiv preprint arXiv:2007.01154}, 2020.

\bibitem{haddadpour2019convergence}
Farzin Haddadpour and Mehrdad Mahdavi.
\newblock On the convergence of local descent methods in federated learning.
\newblock {\em arXiv preprint arXiv:1910.14425}, 2019.

\bibitem{hardy2017private}
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
  Guillaume Smith, and Brian Thorne.
\newblock Private federated learning on vertically partitioned data via entity
  resolution and additively homomorphic encryption.
\newblock {\em arXiv preprint arXiv:1711.10677}, 2017.

\bibitem{horvath2019stochastic}
Samuel Horv{\'a}th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and
  Peter Richt{\'a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock {\em arXiv preprint arXiv:1904.05115}, 2019.

\bibitem{horvath2020better}
Samuel Horv{\'a}th and Peter Richt{\'a}rik.
\newblock A better alternative to error feedback for communication-efficient
  distributed learning.
\newblock {\em arXiv preprint arXiv:2006.11077}, 2020.

\bibitem{ivkin2019communication}
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Ion Stoica, Raman Arora, et~al.
\newblock Communication-efficient distributed sgd with sketching.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13144--13154, 2019.

\bibitem{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{karimireddy2019scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J Reddi,
  Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for on-device federated
  learning.
\newblock {\em arXiv preprint arXiv:1910.06378}, 2019.

\bibitem{kleinberg2003bursty}
Jon Kleinberg.
\newblock Bursty and hierarchical structure in streams.
\newblock {\em Data Mining and Knowledge Discovery}, 7(4):373--397, 2003.

\bibitem{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{li2019privacy}
Tian Li, Zaoxing Liu, Vyas Sekar, and Virginia Smith.
\newblock Privacy for free: Communication-efficient learning with differential
  privacy using sketches.
\newblock {\em arXiv preprint arXiv:1911.00972}, 2019.

\bibitem{li2019federated}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock {\em arXiv preprint arXiv:1908.07873}, 2019.

\bibitem{li2018federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock {\em arXiv preprint arXiv:1812.06127}, 2018.

\bibitem{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock {\em arXiv preprint arXiv:1907.02189}, 2019.

\bibitem{liang2019variance}
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei
  Cheng.
\newblock Variance reduced local sgd with lower communication complexity.
\newblock {\em arXiv preprint arXiv:1912.12844}, 2019.

\bibitem{lin2019don}
Tao Lin, Sebastian~U Stich, Kumar~Kshitij Patel, and Martin Jaggi.
\newblock Don't use large mini-batches, use local sgd.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock {\em arXiv preprint arXiv:1712.01887}, 2017.

\bibitem{liu2019enhancing}
Zaoxing Liu, Tian Li, Virginia Smith, and Vyas Sekar.
\newblock Enhancing the privacy of federated learning with sketching.
\newblock {\em arXiv preprint arXiv:1911.01812}, 2019.

\bibitem{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock {\em arXiv preprint arXiv:1602.05629}, 2016.

\bibitem{mcmahan2017learning}
H~Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li~Zhang.
\newblock Learning differentially private recurrent language models.
\newblock {\em arXiv preprint arXiv:1710.06963}, 2017.

\bibitem{philippenko2020artemis}
Constantin Philippenko and Aymeric Dieuleveut.
\newblock Artemis: tight convergence guarantees for bidirectional compression
  in federated learning.
\newblock {\em arXiv preprint arXiv:2006.14591}, 2020.

\bibitem{reddi2020adaptive}
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\`y}, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock {\em arXiv preprint arXiv:2003.00295}, 2020.

\bibitem{reisizadeh2020fedpaq}
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and
  Ramtin Pedarsani.
\newblock Fedpaq: A communication-efficient federated learning method with
  periodic averaging and quantization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2021--2031, 2020.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{rothchild2020fetchsgd}
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica,
  Vladimir Braverman, Joseph Gonzalez, and Raman Arora.
\newblock Fetchsgd: Communication-efficient federated learning with sketching,
  2020.

\bibitem{sahu2018convergence}
Anit~Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
  Virginia Smith.
\newblock On the convergence of federated optimization in heterogeneous
  networks.
\newblock {\em arXiv preprint arXiv:1812.06127}, 2018.

\bibitem{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem{stich2019error}
Sebastian~U Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed communication.
\newblock {\em arXiv preprint arXiv:1909.05350}, 2019.

\bibitem{stich2019local}
Sebastian~Urban Stich.
\newblock Local sgd converges fast and communicates little.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations ({ICLR})}, 2019.

\bibitem{tang2018communication}
Hanlin Tang, Shaoduo Gan, Ce~Zhang, Tong Zhang, and Ji~Liu.
\newblock Communication compression for decentralized training.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7652--7662, 2018.

\bibitem{wang2018cooperative}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative sgd: A unified framework for the design and analysis of
  communication-efficient sgd algorithms.
\newblock {\em arXiv preprint arXiv:1808.07576}, 2018.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1509--1519, 2017.

\bibitem{wu2018error}
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.
\newblock Error compensated quantized sgd and its applications to large-scale
  distributed optimization.
\newblock {\em arXiv preprint arXiv:1806.08054}, 2018.

\bibitem{yu2019linear}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  7184--7193, 2019.

\bibitem{yu2019parallel}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5693--5700, 2019.

\bibitem{zhang2016parallel}
Jian Zhang, Christopher De~Sa, Ioannis Mitliagkas, and Christopher R{\'e}.
\newblock Parallel sgd: When does averaging help?
\newblock {\em arXiv preprint arXiv:1606.07365}, 2016.

\bibitem{zhou2018convergence}
Fan Zhou and Guojing Cong.
\newblock On the convergence properties of a k-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock In {\em IJCAI}, 2018.

\end{thebibliography}
