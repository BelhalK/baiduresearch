\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{RKK18}
\citation{LFDA17}
\citation{Rnet16,goodfellow2014generative}
\citation{Atari13}
\citation{GMH13}
\citation{RKK18}
\citation{KB15}
\citation{TH12}
\citation{Z12}
\citation{D16}
\citation{DHS11,MS10}
\citation{N04}
\citation{P64}
\citation{P64}
\citation{RKK18}
\citation{KB15}
\citation{CJ12,RS13,SALS15,ALLW18}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{KB15,RKK18}
\citation{CJ12,RS13,SALS15,ALLW18}
\citation{SALS15}
\citation{H14}
\citation{SALS15}
\citation{KB15}
\citation{P64}
\citation{DHS11}
\citation{DHS11}
\citation{KB15}
\citation{KB15}
\citation{KB15}
\citation{RKK18}
\citation{RKK18}
\citation{RKK18}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Optimistic Online learning}{2}{subsection.2.1}}
\newlabel{optFTRL}{{1}{2}{Optimistic Online learning}{equation.2.1}{}}
\newlabel{optFTRL@cref}{{[equation][1][]1}{[1][2][]2}}
\newlabel{optFTRL}{{2}{2}{Optimistic Online learning}{equation.2.2}{}}
\newlabel{optFTRL@cref}{{[equation][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Adaptive optimization methods}{2}{subsection.2.2}}
\citation{RKK18}
\citation{DHS11}
\citation{N04}
\citation{P64}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {AMSGrad} \cite  {RKK18}}}{3}{algorithm.1}}
\newlabel{amsgrad}{{1}{3}{Adaptive optimization methods}{algorithm.1}{}}
\newlabel{amsgrad@cref}{{[algorithm][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\textsc  {Optimistic-AMSGrad}}{3}{section.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textsc  {Optimistic-AMSGrad}}}{3}{algorithm.2}}
\newlabel{optadam}{{2}{3}{\textsc {Optimistic-AMSGrad}}{algorithm.2}{}}
\newlabel{optadam@cref}{{[algorithm][2][]2}{[1][3][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Scheme of \textsc  {OPTIMISTIC-AMSGRAD}.}}{3}{figure.1}}
\newlabel{scheme}{{1}{3}{Scheme of \textsc {OPTIMISTIC-AMSGRAD}}{figure.1}{}}
\newlabel{scheme@cref}{{[figure][1][]1}{[1][3][]3}}
\citation{CJ12,RS13,SALS15}
\citation{RKK18,KB15}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Theoretical analysis}{4}{subsection.3.1}}
\newlabel{thm:main}{{1}{4}{}{Theorem.1}{}}
\newlabel{thm:main@cref}{{[Theorem][1][]1}{[1][4][]4}}
\newlabel{bound:optada}{{3}{4}{}{equation.3.3}{}}
\newlabel{bound:optada@cref}{{[equation][3][]3}{[1][4][]4}}
\citation{RKK18}
\citation{ZRSKK18,CLSH19,WWB18,ZTYCG18,ZS18,LO18}
\citation{CLSH19}
\newlabel{bc}{{4}{5}{}{equation.3.4}{}}
\newlabel{bc@cref}{{[equation][4][]4}{[1][4][]5}}
\newlabel{boundAMS}{{5}{5}{Theoretical analysis}{equation.3.5}{}}
\newlabel{boundAMS@cref}{{[equation][5][]5}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Comparison to some related methods}{5}{subsection.3.2}}
\newlabel{app:related}{{3.2}{5}{Comparison to some related methods}{subsection.3.2}{}}
\newlabel{app:related@cref}{{[subsection][2][3]3.2}{[1][5][]5}}
\citation{Princeton18}
\citation{CYYZC19}
\citation{MY16}
\citation{MY16}
\citation{MY16}
\citation{DISZ18}
\citation{DISZ18}
\citation{DISZ18}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{CJ12,RS13,SALS15}
\citation{DISZ18}
\citation{DISZ18}
\citation{RS13,SALS15,DISZ18}
\newlabel{OPT-DISZ}{{3}{6}{Comparison to some related methods}{algorithm.3}{}}
\newlabel{OPT-DISZ@cref}{{[algorithm][3][]3}{[1][6][]6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \textsc  {Optimistic-Adam\nobreakspace  {}\cite  {DISZ18}+$\mathaccentV {hat}05E{v}_t$}. }}{6}{algorithm.3}}
\citation{SAB16}
\citation{BZ13}
\citation{WN11}
\citation{CJ76}
\citation{E79}
\citation{SAB16}
\citation{SAB16}
\citation{SAB16}
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient Prediction}{7}{section.4}}
\newlabel{sec:predict_m}{{4}{7}{Gradient Prediction}{section.4}{}}
\newlabel{sec:predict_m@cref}{{[section][4][]4}{[1][6][]7}}
\newlabel{vvv}{{6}{7}{Gradient Prediction}{equation.4.6}{}}
\newlabel{vvv@cref}{{[equation][6][]6}{[1][7][]7}}
\newlabel{nox}{{7}{7}{Gradient Prediction}{equation.4.7}{}}
\newlabel{nox@cref}{{[equation][7][]7}{[1][7][]7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces \textsc  {Regularized Approximate Minimal Polynomial Extrapolation} (RMPE) \cite  {SAB16} }}{7}{algorithm.4}}
\newlabel{algex}{{4}{7}{Gradient Prediction}{algorithm.4}{}}
\newlabel{algex@cref}{{[algorithm][4][]4}{[1][7][]7}}
\newlabel{key_to_comp}{{8}{7}{Gradient Prediction}{equation.4.8}{}}
\newlabel{key_to_comp@cref}{{[equation][8][]8}{[1][7][]7}}
\citation{RKK18}
\citation{RKK18}
\citation{KB15}
\citation{RKK18}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. }}{8}{figure.2}}
\newlabel{simu}{{2}{8}{\small (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better}{figure.2}{}}
\newlabel{simu@cref}{{[figure][2][]2}{[1][7][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{8}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training loss vs. Number of iterations. The first row are results with fully-connected neural network.}}{9}{figure.3}}
\newlabel{train_loss}{{3}{9}{Training loss vs. Number of iterations. The first row are results with fully-connected neural network}{figure.3}{}}
\newlabel{train_loss@cref}{{[figure][3][]3}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textit  {MNIST-back-image} + convolutional neural network.}}{9}{figure.4}}
\newlabel{figs:M_image_new3}{{4}{9}{\textit {MNIST-back-image} + convolutional neural network}{figure.4}{}}
\newlabel{figs:M_image_new3@cref}{{[figure][4][]4}{[1][9][]9}}
\citation{RKK18}
\citation{KB15}
\citation{MNIST07}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textit  {CIFAR10} + Res-18. We compare three methods in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy. We observe that \textsc  {Optimistic-AMSGrad} consistently improves the two baselines.}}{10}{figure.5}}
\newlabel{figs:CIFAR10_new3}{{5}{10}{\textit {CIFAR10} + Res-18. We compare three methods in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy. We observe that \textsc {Optimistic-AMSGrad} consistently improves the two baselines}{figure.5}{}}
\newlabel{figs:CIFAR10_new3@cref}{{[figure][5][]5}{[1][9][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textit  {CIFAR100} + Res-50. We compare three methods in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy.}}{10}{figure.6}}
\newlabel{figs:CIFAR100_new3}{{6}{10}{\textit {CIFAR100} + Res-50. We compare three methods in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy}{figure.6}{}}
\newlabel{figs:CIFAR100_new3@cref}{{[figure][6][]6}{[1][9][]10}}
\citation{CNN15}
\citation{Rnet16}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The training loss of \textsc  {OPTIMISTIC-AMSGrad} with different $r$.}}{11}{figure.7}}
\newlabel{fig:compare-r}{{7}{11}{The training loss of \textsc {OPTIMISTIC-AMSGrad} with different $r$}{figure.7}{}}
\newlabel{fig:compare-r@cref}{{[figure][7][]7}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Choice of parameter $r$}{11}{subsection.5.1}}
\citation{MG15}
\@writefile{toc}{\contentsline {section}{\numberline {6}Concluding Remarks}{12}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Discussion on the iteration cost}{12}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Conclusion}{12}{subsection.6.2}}
\bibstyle{abbrvnat}
\bibdata{reference}
\bibcite{ALLW18}{{1}{2018}{{Abernethy et~al.}}{{Abernethy, Lai, Levy, and Wang}}}
\bibcite{Princeton18}{{2}{2019}{{Agarwal et~al.}}{{Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and Zhang}}}
\bibcite{BZ13}{{3}{2013}{{Brezinski and Zaglia}}{{}}}
\bibcite{CJ76}{{4}{1976}{{Cabay and Jackson}}{{}}}
\bibcite{CLSH19}{{5}{2019{a}}{{Chen et~al.}}{{Chen, Liu, Sun, and Hong}}}
\bibcite{CYYZC19}{{6}{2019{b}}{{Chen et~al.}}{{Chen, Yuan, Yi, Zhou, Chen, and Yang}}}
\bibcite{CJ12}{{7}{2012}{{Chiang et~al.}}{{Chiang, Yang, Lee, Mahdavi, Lu, Jin, and Zhu}}}
\bibcite{DISZ18}{{8}{2018}{{Daskalakis et~al.}}{{Daskalakis, Ilyas, Syrgkanis, and Zeng}}}
\bibcite{D16}{{9}{2016}{{Dozat}}{{}}}
\bibcite{DHS11}{{10}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{E79}{{11}{1979}{{Eddy}}{{}}}
\bibcite{goodfellow2014generative}{{12}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{GMH13}{{13}{2013}{{Graves et~al.}}{{Graves, rahman Mohamed, and Hinton}}}
\bibcite{H14}{{14}{2016}{{Hazan}}{{}}}
\bibcite{Rnet16}{{15}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{KB15}{{16}{2015}{{Kingma and Ba}}{{}}}
\bibcite{MNIST07}{{17}{2007}{{Larochelle et~al.}}{{Larochelle, Erhan, Courville, Bergstra, and Bengio}}}
\bibcite{LFDA17}{{18}{2017}{{Levine et~al.}}{{Levine, Finn, Darrell, and Abbeel}}}
\bibcite{LO18}{{19}{2019}{{Li and Orabona.}}{{}}}
\bibcite{MG15}{{20}{2015}{{Martens and Grosse}}{{}}}
\bibcite{MS10}{{21}{2010}{{McMahan and Streeter}}{{}}}
\bibcite{Atari13}{{22}{2013}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller}}}
\bibcite{MY16}{{23}{2016}{{Mohri and Yang}}{{}}}
\bibcite{N04}{{24}{2004}{{Nesterov}}{{}}}
\bibcite{P64}{{25}{1964}{{Polyak}}{{}}}
\bibcite{RS13}{{26}{2013}{{Rakhlin and Sridharan}}{{}}}
\bibcite{RKK18}{{27}{2018}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{SAB16}{{28}{2016}{{Scieur et~al.}}{{Scieur, d'Aspremont, and Bach}}}
\bibcite{CNN15}{{29}{2015}{{Springenberg et~al.}}{{Springenberg, Dosovitskiy, Brox, and Riedmiller}}}
\bibcite{SALS15}{{30}{2015}{{Syrgkanis et~al.}}{{Syrgkanis, Agarwal, Luo, and Schapire}}}
\bibcite{TH12}{{31}{2012}{{Tieleman and Hinton}}{{}}}
\bibcite{T08}{{32}{2008}{{Tseng}}{{}}}
\bibcite{WN11}{{33}{2011}{{Walker and Ni.}}{{}}}
\bibcite{WWB18}{{34}{2019}{{Ward et~al.}}{{Ward, Wu, and Bottou.}}}
\bibcite{ZRSKK18}{{35}{2018}{{Zaheer et~al.}}{{Zaheer, Reddi, Sachan, Kale, and Kumar}}}
\bibcite{Z12}{{36}{2012}{{Zeiler}}{{}}}
\bibcite{ZTYCG18}{{37}{2018}{{Zhou et~al.}}{{Zhou, Tang, Yang, Cao, and Gu}}}
\bibcite{ZS18}{{38}{2018}{{Zou and Shen}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Theorem\nobreakspace  {}\ref  {thm:main}}{15}{appendix.A}}
\newlabel{app:thm}{{A}{15}{Proof of Theorem~\ref {thm:main}}{appendix.A}{}}
\newlabel{app:thm@cref}{{[appendix][1][2147483647]A}{[1][15][]15}}
