\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2018)Abernethy, Lai, Levy, and Wang]{ALLW18}
J.~Abernethy, K.~A. Lai, K.~Y. Levy, and J.-K. Wang.
\newblock Faster rates for convex-concave games.
\newblock \emph{COLT}, 2018.

\bibitem[Brezinski and Zaglia(2013)]{BZ13}
C.~Brezinski and M.~R. Zaglia.
\newblock Extrapolation methods: theory and practice.
\newblock \emph{Elsevier}, 2013.

\bibitem[Cabay and Jackson(1976)]{CJ76}
S.~Cabay and L.~Jackson.
\newblock A polynomial extrapolation method for finding limits and antilimits
  of vector sequences.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1976.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and Zhu]{CJ12}
C.-K. Chiang, T.~Yang, C.-J. Lee, M.~Mahdavi, C.-J. Lu, R.~Jin, and S.~Zhu.
\newblock Online optimization with gradual variations.
\newblock \emph{COLT}, 2012.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and Zeng]{DISZ18}
C.~Daskalakis, A.~Ilyas, V.~Syrgkanis, and H.~Zeng.
\newblock Training gans with optimism.
\newblock \emph{ICLR}, 2018.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020convergence}
A.~D{\'e}fossez, L.~Bottou, F.~Bach, and N.~Usunier.
\newblock On the convergence of adam and adagrad.
\newblock \emph{arXiv preprint arXiv:2003.02395}, 2020.

\bibitem[Dozat(2016)]{D16}
T.~Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock \emph{ICLR (Workshop Track)}, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{DHS11}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2011.

\bibitem[Eddy(1979)]{E79}
R.~Eddy.
\newblock Extrapolating to the limit of a vector sequence.
\newblock \emph{Information linkage between applied mathematics and industry,
  Elsevier}, 1979.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock \emph{NIPS}, 2014.

\bibitem[Graves et~al.(2013)Graves, rahman Mohamed, and Hinton]{GMH13}
A.~Graves, A.~rahman Mohamed, and G.~Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock \emph{ICASSP}, 2013.

\bibitem[Hazan(2016)]{H14}
E.~Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Rnet16}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{CVPR}, 2016.

\bibitem[Kingma and Ba(2015)]{KB15}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Larochelle et~al.(2007)Larochelle, Erhan, Courville, Bergstra, and
  Bengio]{MNIST07}
H.~Larochelle, D.~Erhan, A.~Courville, J.~Bergstra, and Y.~Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock \emph{ICML}, 2007.

\bibitem[Levine et~al.(2017)Levine, Finn, Darrell, and Abbeel]{LFDA17}
S.~Levine, C.~Finn, T.~Darrell, and P.~Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{NIPS}, 2017.

\bibitem[McMahan and Streeter(2010)]{MS10}
H.~B. McMahan and M.~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{COLT}, 2010.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{Atari13}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{NIPS (Deep Learning Workshop)}, 2013.

\bibitem[Nesterov(2004)]{N04}
Y.~Nesterov.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock \emph{Springer}, 2004.

\bibitem[Polyak(1964)]{P64}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Mathematics and Mathematical Physics}, 1964.

\bibitem[Rakhlin and Sridharan(2013)]{RS13}
A.~Rakhlin and K.~Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock \emph{NIPS}, 2013.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{RKK18}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{ICLR}, 2018.

\bibitem[Scieur et~al.(2016)Scieur, d'Aspremont, and Bach]{SAB16}
D.~Scieur, A.~d'Aspremont, and F.~Bach.
\newblock Regularized nonlinear acceleration.
\newblock \emph{NIPS}, 2016.

\bibitem[Springenberg et~al.(2015)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{CNN15}
J.~Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{ICLR}, 2015.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and Schapire]{SALS15}
V.~Syrgkanis, A.~Agarwal, H.~Luo, and R.~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock \emph{NIPS}, 2015.

\bibitem[Tieleman and Hinton(2012)]{TH12}
T.~Tieleman and G.~Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Walker and Ni.(2011)]{WN11}
H.~F. Walker and P.~Ni.
\newblock Anderson acceleration for fixed-point iterations.
\newblock \emph{SIAM Journal on Numerical Analysis}, 2011.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Y.~Yan, T.~Yang, Z.~Li, Q.~Lin, and Y.~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock \emph{arXiv preprint arXiv:1808.10396}, 2018.

\bibitem[Zeiler(2012)]{Z12}
M.~D. Zeiler.
\newblock Adadelta: An adaptive learning rate method.
\newblock \emph{arXiv:1212.5701}, 2012.

\bibitem[Zhou et~al.(2018)Zhou, Tang, Yang, Cao, and Gu]{zhou2018convergence}
D.~Zhou, Y.~Tang, Z.~Yang, Y.~Cao, and Q.~Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018.

\end{thebibliography}
