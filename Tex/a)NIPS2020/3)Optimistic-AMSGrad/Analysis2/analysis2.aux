\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ghadimi2013stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {1}Nonconvex Analysis}{1}{section.1}}
\newlabel{eq:minproblem}{{1}{1}{Nonconvex Analysis}{equation.1.1}{}}
\newlabel{eq:minproblem@cref}{{[equation][1][]1}{[1][1][]1}}
\newlabel{eq:random}{{2}{1}{Nonconvex Analysis}{equation.1.2}{}}
\newlabel{eq:random@cref}{{[equation][2][]2}{[1][1][]1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces OPTIMISTIC-AMSGRAD}}{1}{algorithm.1}}
\newlabel{alg:opt-ams}{{1}{1}{Nonconvex Analysis}{algorithm.1}{}}
\newlabel{alg:opt-ams@cref}{{[algorithm][1][]1}{[1][1][]1}}
\newlabel{eq:finalupdate}{{3}{1}{Nonconvex Analysis}{equation.1.3}{}}
\newlabel{eq:finalupdate@cref}{{[equation][3][]3}{[1][1][]1}}
\citation{ghadimi2013stochastic}
\citation{yan2018unified}
\newlabel{ass:nonconv}{{1}{2}{}{assumption.1}{}}
\newlabel{ass:nonconv@cref}{{[assumption][1][]1}{[1][1][]2}}
\newlabel{ass:boundedparam}{{2}{2}{}{assumption.2}{}}
\newlabel{ass:boundedparam@cref}{{[assumption][2][]2}{[1][2][]2}}
\newlabel{ass:smooth}{{3}{2}{}{assumption.3}{}}
\newlabel{ass:smooth@cref}{{[assumption][3][]3}{[1][2][]2}}
\newlabel{ass:guessbound}{{4}{2}{}{assumption.4}{}}
\newlabel{ass:guessbound@cref}{{[assumption][4][]4}{[1][2][]2}}
\newlabel{ass:bounded}{{5}{2}{}{assumption.5}{}}
\newlabel{ass:bounded@cref}{{[assumption][5][]5}{[1][2][]2}}
\newlabel{lem:bound}{{1}{2}{}{Lemma.1}{}}
\newlabel{lem:bound@cref}{{[Lemma][1][]1}{[1][2][]2}}
\newlabel{eq:deftilde}{{8}{2}{Nonconvex Analysis}{equation.1.8}{}}
\newlabel{eq:deftilde@cref}{{[equation][8][]8}{[1][2][]2}}
\newlabel{lem:momentum}{{2}{2}{}{Lemma.2}{}}
\newlabel{lem:momentum@cref}{{[Lemma][2][]2}{[1][2][]2}}
\citation{ghadimi2013stochastic}
\newlabel{lem:squarev}{{3}{3}{}{Lemma.3}{}}
\newlabel{lem:squarev@cref}{{[Lemma][3][]3}{[1][3][]3}}
\newlabel{eq:smoothness}{{17}{4}{Nonconvex Analysis}{equation.1.17}{}}
\newlabel{eq:smoothness@cref}{{[equation][17][]17}{[1][3][]4}}
\newlabel{eq:termA1}{{19}{4}{Nonconvex Analysis}{equation.1.19}{}}
\newlabel{eq:termA1@cref}{{[equation][19][]19}{[1][4][]4}}
\newlabel{eq:termA2}{{20}{4}{Nonconvex Analysis}{equation.1.20}{}}
\newlabel{eq:termA2@cref}{{[equation][20][]20}{[1][4][]4}}
\newlabel{eq:termA}{{21}{4}{Nonconvex Analysis}{equation.1.21}{}}
\newlabel{eq:termA@cref}{{[equation][21][]21}{[1][4][]4}}
\newlabel{eq:termB1}{{22}{4}{Nonconvex Analysis}{equation.1.22}{}}
\newlabel{eq:termB1@cref}{{[equation][22][]22}{[1][4][]4}}
\newlabel{eq:termB2}{{23}{4}{Nonconvex Analysis}{equation.1.23}{}}
\newlabel{eq:termB2@cref}{{[equation][23][]23}{[1][4][]4}}
\newlabel{eq:termB3}{{25}{5}{Nonconvex Analysis}{equation.1.25}{}}
\newlabel{eq:termB3@cref}{{[equation][25][]25}{[1][4][]5}}
\newlabel{eq:termB}{{27}{5}{Nonconvex Analysis}{equation.1.27}{}}
\newlabel{eq:termB@cref}{{[equation][27][]27}{[1][5][]5}}
\newlabel{eq:term3}{{28}{5}{Nonconvex Analysis}{equation.1.28}{}}
\newlabel{eq:term3@cref}{{[equation][28][]28}{[1][5][]5}}
\newlabel{eq:expectationtildegrad}{{30}{5}{Nonconvex Analysis}{equation.1.30}{}}
\newlabel{eq:expectationtildegrad@cref}{{[equation][30][]30}{[1][5][]5}}
\citation{defossez2020convergence}
\@writefile{toc}{\contentsline {section}{\numberline {2}Containment of the iterates for a Deep Neural Network}{6}{section.2}}
\newlabel{eq:lossmln}{{32}{6}{Containment of the iterates for a Deep Neural Network}{equation.2.32}{}}
\newlabel{eq:lossmln@cref}{{[equation][32][]32}{[1][6][]6}}
\newlabel{eq:mildassumptions}{{33}{6}{Containment of the iterates for a Deep Neural Network}{equation.2.33}{}}
\newlabel{eq:mildassumptions@cref}{{[equation][33][]33}{[1][6][]6}}
\newlabel{eq:boundderivativeloss}{{34}{6}{Containment of the iterates for a Deep Neural Network}{equation.2.34}{}}
\newlabel{eq:boundderivativeloss@cref}{{[equation][34][]34}{[1][6][]6}}
\newlabel{eq:decrease}{{37}{7}{Containment of the iterates for a Deep Neural Network}{equation.2.37}{}}
\newlabel{eq:decrease@cref}{{[equation][37][]37}{[1][7][]7}}
\newlabel{eq:gradientatell}{{38}{7}{Containment of the iterates for a Deep Neural Network}{equation.2.38}{}}
\newlabel{eq:gradientatell@cref}{{[equation][38][]38}{[1][7][]7}}
\bibstyle{abbrvnat}
\bibdata{referencesB}
\bibcite{defossez2020convergence}{{1}{2020}{{D{\'e}fossez et~al.}}{{D{\'e}fossez, Bottou, Bach, and Usunier}}}
\bibcite{ghadimi2013stochastic}{{2}{2013}{{Ghadimi and Lan}}{{}}}
\bibcite{yan2018unified}{{3}{2018}{{Yan et~al.}}{{Yan, Yang, Li, Lin, and Yang}}}
