\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2018)Abernethy, Lai, Levy, and Wang]{ALLW18}
J.~Abernethy, K.~A. Lai, K.~Y. Levy, and J.-K. Wang.
\newblock Faster rates for convex-concave games.
\newblock \emph{COLT}, 2018.

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{Princeton18}
N.~Agarwal, B.~Bullins, X.~Chen, E.~Hazan, K.~Singh, C.~Zhang, and Y.~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock \emph{ICML}, 2019.

\bibitem[Brezinski and Zaglia(2013)]{BZ13}
C.~Brezinski and M.~R. Zaglia.
\newblock Extrapolation methods: theory and practice.
\newblock \emph{Elsevier}, 2013.

\bibitem[Cabay and Jackson(1976)]{CJ76}
S.~Cabay and L.~Jackson.
\newblock A polynomial extrapolation method for finding limits and antilimits
  of vector sequences.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1976.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Liu, Sun, and Hong]{CLSH19}
X.~Chen, S.~Liu, R.~Sun, and M.~Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Yuan, Yi, Zhou, Chen, and
  Yang]{CYYZC19}
Z.~Chen, Z.~Yuan, J.~Yi, B.~Zhou, E.~Chen, and T.~Yang.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock \emph{ICLR}, 2019{\natexlab{b}}.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and Zhu]{CJ12}
C.-K. Chiang, T.~Yang, C.-J. Lee, M.~Mahdavi, C.-J. Lu, R.~Jin, and S.~Zhu.
\newblock Online optimization with gradual variations.
\newblock \emph{COLT}, 2012.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and Zeng]{DISZ18}
C.~Daskalakis, A.~Ilyas, V.~Syrgkanis, and H.~Zeng.
\newblock Training gans with optimism.
\newblock \emph{ICLR}, 2018.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020convergence}
A.~D{\'e}fossez, L.~Bottou, F.~Bach, and N.~Usunier.
\newblock On the convergence of adam and adagrad.
\newblock \emph{arXiv preprint arXiv:2003.02395}, 2020.

\bibitem[Dozat(2016)]{D16}
T.~Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock \emph{ICLR (Workshop Track)}, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{DHS11}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2011.

\bibitem[Eddy(1979)]{E79}
R.~Eddy.
\newblock Extrapolating to the limit of a vector sequence.
\newblock \emph{Information linkage between applied mathematics and industry,
  Elsevier}, 1979.

\bibitem[Gers et~al.(1999)Gers, Schmidhuber, and Cummins]{gers1999learning}
F.~A. Gers, J.~Schmidhuber, and F.~Cummins.
\newblock Learning to forget: Continual prediction with lstm.
\newblock 1999.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock \emph{NIPS}, 2014.

\bibitem[Graves et~al.(2013)Graves, rahman Mohamed, and Hinton]{GMH13}
A.~Graves, A.~rahman Mohamed, and G.~Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock \emph{ICASSP}, 2013.

\bibitem[Hazan(2016)]{H14}
E.~Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Rnet16}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{CVPR}, 2016.

\bibitem[Kingma and Ba(2015)]{KB15}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Larochelle et~al.(2007)Larochelle, Erhan, Courville, Bergstra, and
  Bengio]{MNIST07}
H.~Larochelle, D.~Erhan, A.~Courville, J.~Bergstra, and Y.~Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock \emph{ICML}, 2007.

\bibitem[Levine et~al.(2017)Levine, Finn, Darrell, and Abbeel]{LFDA17}
S.~Levine, C.~Finn, T.~Darrell, and P.~Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{NIPS}, 2017.

\bibitem[Li and Orabona.(2019)]{LO18}
X.~Li and F.~Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock \emph{AISTAT}, 2019.

\bibitem[McMahan and Streeter(2010)]{MS10}
H.~B. McMahan and M.~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{COLT}, 2010.

\bibitem[Mertikopoulos et~al.(2018)Mertikopoulos, Lecouat, Zenati, Foo,
  Chandrasekhar, and Piliouras]{mertikopoulos2018optimistic}
P.~Mertikopoulos, B.~Lecouat, H.~Zenati, C.-S. Foo, V.~Chandrasekhar, and
  G.~Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock \emph{arXiv preprint arXiv:1807.02629}, 2018.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{Atari13}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{NIPS (Deep Learning Workshop)}, 2013.

\bibitem[Mohri and Yang(2016)]{MY16}
M.~Mohri and S.~Yang.
\newblock Accelerating optimization via adaptive prediction.
\newblock \emph{AISTATS}, 2016.

\bibitem[Nesterov(2004)]{N04}
Y.~Nesterov.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock \emph{Springer}, 2004.

\bibitem[Polyak(1964)]{P64}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Mathematics and Mathematical Physics}, 1964.

\bibitem[Rakhlin and Sridharan(2013{\natexlab{a}})]{rakhlin2013online}
A.~Rakhlin and K.~Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock \emph{NIPS}, 2013{\natexlab{a}}.

\bibitem[Rakhlin and Sridharan(2013{\natexlab{b}})]{RS13b}
S.~Rakhlin and K.~Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3066--3074, 2013{\natexlab{b}}.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{RKK18}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{ICLR}, 2018.

\bibitem[Scieur et~al.(2016)Scieur, d'Aspremont, and Bach]{SAB16}
D.~Scieur, A.~d'Aspremont, and F.~Bach.
\newblock Regularized nonlinear acceleration.
\newblock \emph{NIPS}, 2016.

\bibitem[Springenberg et~al.(2015)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{CNN15}
J.~Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{ICLR}, 2015.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and Schapire]{SALS15}
V.~Syrgkanis, A.~Agarwal, H.~Luo, and R.~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock \emph{NIPS}, 2015.

\bibitem[Tieleman and Hinton(2012)]{TH12}
T.~Tieleman and G.~Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Tseng(2008)]{T08}
P.~Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock 2008.

\bibitem[Walker and Ni.(2011)]{WN11}
H.~F. Walker and P.~Ni.
\newblock Anderson acceleration for fixed-point iterations.
\newblock \emph{SIAM Journal on Numerical Analysis}, 2011.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou.]{WWB18}
R.~Ward, X.~Wu, and L.~Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from
  any initialization.
\newblock \emph{ICML}, 2019.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Y.~Yan, T.~Yang, Z.~Li, Q.~Lin, and Y.~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock \emph{arXiv preprint arXiv:1808.10396}, 2018.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and Kumar]{ZRSKK18}
M.~Zaheer, S.~Reddi, D.~Sachan, S.~Kale, and S.~Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Zeiler(2012)]{Z12}
M.~D. Zeiler.
\newblock Adadelta: An adaptive learning rate method.
\newblock \emph{arXiv:1212.5701}, 2012.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Tang, Yang, Cao, and Gu]{ZTYCG18}
D.~Zhou, Y.~Tang, Z.~Yang, Y.~Cao, and Q.~Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv:1808.05671}, 2018{\natexlab{a}}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Tang, Yang, Cao, and
  Gu]{zhou2018convergence}
D.~Zhou, Y.~Tang, Z.~Yang, Y.~Cao, and Q.~Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018{\natexlab{b}}.

\bibitem[Zou and Shen(2018)]{ZS18}
F.~Zou and L.~Shen.
\newblock On the convergence of adagrad with momentum for training deep neural
  networks.
\newblock \emph{arXiv:1808.03408}, 2018.

\end{thebibliography}
