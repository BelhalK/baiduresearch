@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{chilimbi2014project,
	title={Project adam: Building an efficient and scalable deep learning training system},
	author={Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
	booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
	pages={571--582},
	year={2014}
}

@article{mcmahan2016communication,
	title={Communication-efficient learning of deep networks from decentralized data},
	author={McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and others},
	journal={arXiv preprint arXiv:1602.05629},
	year={2016}
}


@inproceedings{alistarh2017qsgd,
	title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
	author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1709--1720},
	year={2017}
}

@article{lin2017deep,
	title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
	author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
	journal={arXiv preprint arXiv:1712.01887},
	year={2017}
}

@inproceedings{wangni2018gradient,
	title={Gradient sparsification for communication-efficient distributed optimization},
	author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1299--1309},
	year={2018}
}
@inproceedings{stich2018sparsified,
	title={Sparsified SGD with memory},
	author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={4447--4458},
	year={2018}
}

@inproceedings{wang2018atomo,
	title={Atomo: Communication-efficient learning via atomic sparsification},
	author={Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
	booktitle={Advances in Neural Information Processing Systems},
	pages={9850--9861},
	year={2018}
}

@article{tang2019doublesqueeze,
	title={DoubleSqueeze: Parallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression},
	author={Tang, Hanlin and Lian, Xiangru and Zhang, Tong and Liu, Ji},
	journal={arXiv preprint arXiv:1905.05957},
	year={2019}
}

@inproceedings{lian2017can,
	title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
	author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5330--5340},
	year={2017}
}

@article{duchi2011dual,
	title={Dual averaging for distributed optimization: Convergence analysis and network scaling},
	author={Duchi, John C and Agarwal, Alekh and Wainwright, Martin J},
	journal={IEEE Transactions on Automatic control},
	volume={57},
	number={3},
	pages={592--606},
	year={2011},
	publisher={IEEE}
}

@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Jul},
	pages={2121--2159},
	year={2011}
}

@article{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann},
  journal={http://yann. lecun. com/exdb/mnist/},
  year={1998}
}



@article{kingma2014adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{reddi2019convergence,
	title={On the convergence of adam and beyond},
	author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
	journal={arXiv preprint arXiv:1904.09237},
	year={2019}
}

@article{nazari2019dadam,
	title={DADAM: A consensus-based distributed adaptive gradient method for online optimization},
	author={Nazari, Parvin and Tarzanagh, Davoud Ataee and Michailidis, George},
	journal={arXiv preprint arXiv:1901.09109},
	year={2019}
}

@article{yuan2016convergence,
	title={On the convergence of decentralized gradient descent},
	author={Yuan, Kun and Ling, Qing and Yin, Wotao},
	journal={SIAM Journal on Optimization},
	volume={26},
	number={3},
	pages={1835--1854},
	year={2016},
	publisher={SIAM}
}

@article{nedic2009distributed,
	title={Distributed subgradient methods for multi-agent optimization},
	author={Nedic, Angelia and Ozdaglar, Asuman},
	journal={IEEE Transactions on Automatic Control},
	volume={54},
	number={1},
	pages={48},
	year={2009}
}

@article{reddi2020adaptive,
  title={Adaptive Federated Optimization},
  author={Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
  journal={arXiv preprint arXiv:2003.00295},
  year={2020}
}

@article{aji2017sparse,
  title={Sparse communication for distributed gradient descent},
  author={Aji, Alham Fikri and Heafield, Kenneth},
  journal={arXiv preprint arXiv:1704.05021},
  year={2017}
}


@article{jegou2010product,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}

@inproceedings{ge2013optimized,
  title={Optimized product quantization for approximate nearest neighbor search},
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2946--2953},
  year={2013}
}

@article{luo2019adaptive,
	title={Adaptive gradient methods with dynamic bound of learning rate},
	author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
	journal={arXiv preprint arXiv:1902.09843},
	year={2019}
}

@article{chen2010approximate,
  title={Approximate nearest neighbor search by residual vector quantization},
  author={Chen, Yongjian and Guan, Tao and Wang, Cheng},
  journal={Sensors},
  volume={10},
  number={12},
  pages={11259--11273},
  year={2010},
  publisher={Molecular Diversity Preservation International}
}

@article{chen2018convergence,
	title={On the convergence of a class of adam-type algorithms for non-convex optimization},
	author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
	journal={arXiv preprint arXiv:1808.02941},
	year={2018}
}

@article{ward2018adagrad,
	title={Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization},
	author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
	journal={arXiv preprint arXiv:1806.01811},
	year={2018}
}

@article{yan2018unified,
	title={A unified analysis of stochastic momentum methods for deep learning},
	author={Yan, Yan and Yang, Tianbao and Li, Zhe and Lin, Qihang and Yang, Yi},
	journal={arXiv preprint arXiv:1808.10396},
	year={2018}
}

@article{li2018convergence,
	title={On the convergence of stochastic gradient descent with adaptive stepsizes},
	author={Li, Xiaoyu and Orabona, Francesco},
	journal={arXiv preprint arXiv:1805.08114},
	year={2018}
}

@article{zou2018convergence,
	title={On the convergence of weighted AdaGrad with momentum for training deep neural networks},
	author={Zou, Fangyu and Shen, Li},
	journal={arXiv preprint arXiv:1808.03408},
	year={2018}
}

@article{agarwal2018case,
	title={The case for full-matrix adaptive regularization},
	author={Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
	journal={arXiv preprint arXiv:1806.02958},
	year={2018}
}

@inproceedings{zaheer2018adaptive,
	title={Adaptive methods for nonconvex optimization},
	author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
	booktitle={Advances in Neural Information Processing Systems},
	pages={9793--9803},
	year={2018}
}

@article{boyd2011distributed,
	title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
	author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
	journal={Foundations and Trends{\textregistered} in Machine learning},
	volume={3},
	number={1},
	pages={1--122},
	year={2011},
	publisher={Now Publishers, Inc.}
}

@article{shi2015extra,
	title={Extra: An exact first-order algorithm for decentralized consensus optimization},
	author={Shi, Wei and Ling, Qing and Wu, Gang and Yin, Wotao},
	journal={SIAM Journal on Optimization},
	volume={25},
	number={2},
	pages={944--966},
	year={2015},
	publisher={SIAM}
}

@article{di2016next,
	title={Next: In-network nonconvex optimization},
	author={Di Lorenzo, Paolo and Scutari, Gesualdo},
	journal={IEEE Transactions on Signal and Information Processing over Networks},
	volume={2},
	number={2},
	pages={120--136},
	year={2016},
	publisher={IEEE}
}

@inproceedings{hong2017prox,
	title={Prox-PDA: The proximal primal-dual algorithm for fast distributed nonconvex optimization and learning over networks},
	author={Hong, Mingyi and Hajinezhad, Davood and Zhao, Ming-Min},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={1529--1538},
	year={2017},
	organization={JMLR. org}
}

@article{tang2018d,
	title={$\text{D}^ 2$: Decentralized Training over Decentralized Data},
	author={Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
	journal={arXiv preprint arXiv:1803.07068},
	year={2018}
}

@article{assran2018stochastic,
	title={Stochastic gradient push for distributed deep learning},
	author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Michael},
	journal={arXiv preprint arXiv:1811.10792},
	year={2018}
}


@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}