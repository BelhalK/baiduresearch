\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2018)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2018case}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock The case for full-matrix adaptive regularization.
\newblock \emph{arXiv preprint arXiv:1806.02958}, 2018.

\bibitem[Aji and Heafield(2017)]{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock \emph{arXiv preprint arXiv:1704.05021}, 2017.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem[Assran et~al.(2018)Assran, Loizou, Ballas, and
  Rabbat]{assran2018stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock \emph{arXiv preprint arXiv:1811.10792}, 2018.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, Eckstein,
  et~al.]{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Chen et~al.(2018)Chen, Liu, Sun, and Hong]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.02941}, 2018.

\bibitem[Chen et~al.(2010)Chen, Guan, and Wang]{chen2010approximate}
Yongjian Chen, Tao Guan, and Cheng Wang.
\newblock Approximate nearest neighbor search by residual vector quantization.
\newblock \emph{Sensors}, 10\penalty0 (12):\penalty0 11259--11273, 2010.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{11th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 14)}, pages 571--582, 2014.

\bibitem[Di~Lorenzo and Scutari(2016)]{di2016next}
Paolo Di~Lorenzo and Gesualdo Scutari.
\newblock Next: In-network nonconvex optimization.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 2\penalty0 (2):\penalty0 120--136, 2016.

\bibitem[Duchi et~al.(2011{\natexlab{a}})Duchi, Hazan, and
  Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011{\natexlab{a}}.

\bibitem[Duchi et~al.(2011{\natexlab{b}})Duchi, Agarwal, and
  Wainwright]{duchi2011dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic control}, 57\penalty0
  (3):\penalty0 592--606, 2011{\natexlab{b}}.

\bibitem[Ge et~al.(2013)Ge, He, Ke, and Sun]{ge2013optimized}
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun.
\newblock Optimized product quantization for approximate nearest neighbor
  search.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2946--2953, 2013.

\bibitem[Hong et~al.(2017)Hong, Hajinezhad, and Zhao]{hong2017prox}
Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1529--1538. JMLR. org, 2017.

\bibitem[Jegou et~al.(2010)Jegou, Douze, and Schmid]{jegou2010product}
Herve Jegou, Matthijs Douze, and Cordelia Schmid.
\newblock Product quantization for nearest neighbor search.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 33\penalty0 (1):\penalty0 117--128, 2010.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Li and Orabona(2018)]{li2018convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock \emph{arXiv preprint arXiv:1805.08114}, 2018.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5330--5340, 2017.

\bibitem[Lin et~al.(2017)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock \emph{arXiv preprint arXiv:1712.01887}, 2017.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adaptive}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{arXiv preprint arXiv:1902.09843}, 2019.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson,
  et~al.]{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{arXiv preprint arXiv:1901.09109}, 2019.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48, 2009.

\bibitem[Reddi et~al.(2020)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\`y}, Kumar, and McMahan]{reddi2020adaptive}
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\`y}, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock \emph{arXiv preprint arXiv:2003.00295}, 2020.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock $\text{D}^ 2$: Decentralized training over decentralized data.
\newblock \emph{arXiv preprint arXiv:1803.07068}, 2018.

\bibitem[Tang et~al.(2019)Tang, Lian, Zhang, and Liu]{tang2019doublesqueeze}
Hanlin Tang, Xiangru Lian, Tong Zhang, and Ji~Liu.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock \emph{arXiv preprint arXiv:1905.05957}, 2019.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{wang2018atomo}
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris
  Papailiopoulos, and Stephen Wright.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9850--9861, 2018.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1299--1309, 2018.

\bibitem[Ward et~al.(2018)Ward, Wu, and Bottou]{ward2018adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from
  any initialization.
\newblock \emph{arXiv preprint arXiv:1806.01811}, 2018.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock \emph{arXiv preprint arXiv:1808.10396}, 2018.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
Kun Yuan, Qing Ling, and Wotao Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (3):\penalty0
  1835--1854, 2016.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9793--9803, 2018.

\bibitem[Zou and Shen(2018)]{zou2018convergence}
Fangyu Zou and Li~Shen.
\newblock On the convergence of weighted adagrad with momentum for training
  deep neural networks.
\newblock \emph{arXiv preprint arXiv:1808.03408}, 2018.

\end{thebibliography}
