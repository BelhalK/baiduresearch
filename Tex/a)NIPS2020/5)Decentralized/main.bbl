\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2018)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2018case}
N.~Agarwal, B.~Bullins, X.~Chen, E.~Hazan, K.~Singh, C.~Zhang, and Y.~Zhang.
\newblock The case for full-matrix adaptive regularization.
\newblock \emph{arXiv preprint arXiv:1806.02958}, 2018.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem[Assran et~al.(2018)Assran, Loizou, Ballas, and
  Rabbat]{assran2018stochastic}
M.~Assran, N.~Loizou, N.~Ballas, and M.~Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock \emph{arXiv preprint arXiv:1811.10792}, 2018.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, Eckstein,
  et~al.]{boyd2011distributed}
S.~Boyd, N.~Parikh, E.~Chu, B.~Peleato, J.~Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Chen et~al.(2018)Chen, Liu, Sun, and Hong]{chen2018convergence}
X.~Chen, S.~Liu, R.~Sun, and M.~Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.02941}, 2018.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
T.~Chilimbi, Y.~Suzue, J.~Apacible, and K.~Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{11th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 14)}, pages 571--582, 2014.

\bibitem[Di~Lorenzo and Scutari(2016)]{di2016next}
P.~Di~Lorenzo and G.~Scutari.
\newblock Next: In-network nonconvex optimization.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 2\penalty0 (2):\penalty0 120--136, 2016.

\bibitem[Duchi et~al.(2011{\natexlab{a}})Duchi, Hazan, and
  Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011{\natexlab{a}}.

\bibitem[Duchi et~al.(2011{\natexlab{b}})Duchi, Agarwal, and
  Wainwright]{duchi2011dual}
J.~C. Duchi, A.~Agarwal, and M.~J. Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic control}, 57\penalty0
  (3):\penalty0 592--606, 2011{\natexlab{b}}.

\bibitem[Hong et~al.(2017)Hong, Hajinezhad, and Zhao]{hong2017prox}
M.~Hong, D.~Hajinezhad, and M.-M. Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1529--1538. JMLR. org, 2017.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li and Orabona(2018)]{li2018convergence}
X.~Li and F.~Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock \emph{arXiv preprint arXiv:1805.08114}, 2018.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
X.~Lian, C.~Zhang, H.~Zhang, C.-J. Hsieh, W.~Zhang, and J.~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5330--5340, 2017.

\bibitem[Lin et~al.(2017)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Y.~Lin, S.~Han, H.~Mao, Y.~Wang, and W.~J. Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock \emph{arXiv preprint arXiv:1712.01887}, 2017.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adaptive}
L.~Luo, Y.~Xiong, Y.~Liu, and X.~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{arXiv preprint arXiv:1902.09843}, 2019.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson,
  et~al.]{mcmahan2016communication}
H.~B. McMahan, E.~Moore, D.~Ramage, S.~Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
P.~Nazari, D.~A. Tarzanagh, and G.~Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{arXiv preprint arXiv:1901.09109}, 2019.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
A.~Nedic and A.~Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48, 2009.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2019convergence}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
W.~Shi, Q.~Ling, G.~Wu, and W.~Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d}
H.~Tang, X.~Lian, M.~Yan, C.~Zhang, and J.~Liu.
\newblock $\text{D}^ 2$: Decentralized training over decentralized data.
\newblock \emph{arXiv preprint arXiv:1803.07068}, 2018.

\bibitem[Tang et~al.(2019)Tang, Lian, Zhang, and Liu]{tang2019doublesqueeze}
H.~Tang, X.~Lian, T.~Zhang, and J.~Liu.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock \emph{arXiv preprint arXiv:1905.05957}, 2019.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{wang2018atomo}
H.~Wang, S.~Sievert, S.~Liu, Z.~Charles, D.~Papailiopoulos, and S.~Wright.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9850--9861, 2018.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1299--1309, 2018.

\bibitem[Ward et~al.(2018)Ward, Wu, and Bottou]{ward2018adagrad}
R.~Ward, X.~Wu, and L.~Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from
  any initialization.
\newblock \emph{arXiv preprint arXiv:1806.01811}, 2018.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
K.~Yuan, Q.~Ling, and W.~Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (3):\penalty0
  1835--1854, 2016.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
M.~Zaheer, S.~Reddi, D.~Sachan, S.~Kale, and S.~Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9793--9803, 2018.

\bibitem[Zou and Shen(2018)]{zou2018convergence}
F.~Zou and L.~Shen.
\newblock On the convergence of weighted adagrad with momentum for training
  deep neural networks.
\newblock \emph{arXiv preprint arXiv:1808.03408}, 2018.

\end{thebibliography}
