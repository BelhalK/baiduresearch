\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Related work}{}% 2
\BOOKMARK [1][-]{section.3}{Decentralized training and divergence of DADAM}{}% 3
\BOOKMARK [2][-]{subsection.3.1}{Decentralized optimization }{section.3}% 4
\BOOKMARK [2][-]{subsection.3.2}{Divergence of DADAM}{section.3}% 5
\BOOKMARK [1][-]{section.4}{Convergent decentralized adaptive gradient methods}{}% 6
\BOOKMARK [2][-]{subsection.4.1}{Importance and difficulties of consensus on adaptive learning rates}{section.4}% 7
\BOOKMARK [2][-]{subsection.4.2}{On decentralized adaptive gradient methods}{section.4}% 8
\BOOKMARK [1][-]{section.5}{Experiments}{}% 9
\BOOKMARK [1][-]{section.6}{Broader Impact Statement}{}% 10
\BOOKMARK [1][-]{appendix.A}{Appendix}{}% 11
\BOOKMARK [2][-]{subsection.A.1}{Proof of Theorem 2}{appendix.A}% 12
\BOOKMARK [2][-]{subsection.A.2}{Proof of Theorem 3}{appendix.A}% 13
\BOOKMARK [2][-]{subsection.A.3}{Proof of Lemmas}{appendix.A}% 14
\BOOKMARK [2][-]{subsection.A.4}{Additional experiments and details}{appendix.A}% 15
