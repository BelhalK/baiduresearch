\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Decentralized Adaptive Training and Divergence of DADAM}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Related Work}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Decentralized Optimization }{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Divergence of DADAM}{section.2}% 5
\BOOKMARK [1][-]{section.3}{Decentralized Adaptive Gradient Methods and their Convergence}{}% 6
\BOOKMARK [2][-]{subsection.3.1}{Importance and Difficulties of Consensus on Adaptive Learning Rates}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.2}{Decentralized Adaptive Gradient Unifying Framework}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.3}{Convergence Analysis}{section.3}% 9
\BOOKMARK [2][-]{subsection.3.4}{Illustrative Numerical Experiments}{section.3}% 10
\BOOKMARK [1][-]{section.4}{Conclusion}{}% 11
\BOOKMARK [1][-]{section.5}{Broader Impact of Our Work}{}% 12
\BOOKMARK [1][-]{appendix.A}{Appendix}{}% 13
\BOOKMARK [2][-]{subsection.A.1}{Proof of Theorem 2}{appendix.A}% 14
\BOOKMARK [2][-]{subsection.A.2}{Proof of Theorem 3}{appendix.A}% 15
\BOOKMARK [2][-]{subsection.A.3}{Proof of Lemmas}{appendix.A}% 16
\BOOKMARK [2][-]{subsection.A.4}{Additional experiments and details}{appendix.A}% 17
