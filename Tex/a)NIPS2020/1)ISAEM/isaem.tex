\documentclass[11pt]{article}
%\usepackage{fullpage,graphicx,algorithm,algorithmic,bm,amsmath,amsthm,amssymb,color,hyperref,cite,natbib}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers}{natbib}
\usepackage{natbib,fullpage}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{color,wrapfig}
\usepackage[textwidth=1cm,textsize=footnotesize]{todonotes}
% ready for submission
\usepackage{neurips_2020}

\usepackage{lipsum}
\usepackage[colorlinks=true,
linkcolor=red,
urlcolor=blue,
citecolor=blue]{hyperref}

\usepackage{xargs}
\usepackage{stmaryrd}

\setlength{\parskip}{.2cm}

\newtheorem{Fact}{Fact}
\newtheorem{Lemma}{Lemma}
\newtheorem{Prop}{Proposition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Conjecture}{Conjecture}
\newtheorem{Property}{Property}
\newtheorem{Observation}{Observation}
%\theorembodyfont{\rmfamily}
\newtheorem{Exa}{Example}
\newtheorem{assumption}{H\!\!}
\newtheorem{assumptionA}{G\!\!}
\newtheorem{Remark}{Remark}
\newtheorem*{Lemma*}{Lemma}
\newtheorem*{Theorem*}{Theorem}
 \makeatletter
\renewenvironment{proof}[1][\proofname]{%
   \par\pushQED{\qed}\normalfont%
   \topsep6\p@\@plus6\p@\relax
   \trivlist\item[\hskip\labelsep\bfseries#1]%
   \ignorespaces
}{%
   \popQED\endtrivlist\@endpefalse
}
\makeatother
\usepackage{shortcuts_OPT}

%\renewcommand{\textwidth}{5.5in}

% Here's the definition of Sb, stolen from amstex
    \makeatletter
    \def\multilimits@{\bgroup
  \Let@
  \restore@math@cr
  \default@tag
 \baselineskip\fontdimen10 \scriptfont\tw@
 \advance\baselineskip\fontdimen12 \scriptfont\tw@
 \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
 \lineskiplimit\lineskip
 \vbox\bgroup\ialign\bgroup\hfil$\m@th\scriptstyle{##}$\hfil\crcr}
    \def\Sb{_\multilimits@}
    \def\endSb{\crcr\egroup\egroup\egroup}
\makeatother

\newtheoremstyle{t}         %name
    {\baselineskip}{2\topsep}      %space above and below
    {\rm}                   %Body font
    {0pt}{\bfseries}  %Heading indent and font
    {}                      %after heading
    { }                      %head after space
    {\thmname{#1}\thmnumber{#2}.}

\theoremstyle{t}
\newtheorem{q}{Q}
\parindent=0pt

\makeatletter
\DeclareRobustCommand*\cal{\@fontswitch\relax\mathcal}
\makeatother

\newcommand{\tcr}[1]{\textcolor{red}{#1}}
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}

\begin{document}
\title{Fast Two-Time-Scale Noisy EM Algorithms}
\author{
  Belhal Karimi \\
  Cognitive And Computing Lab\\
  Baidu Research\\
  Beijing, China \\
  \texttt{belhal.karimi@baidu.com} 
   \And
  Ping Li \\
  Cognitive And Computing Lab\\
  Baidu Research\\
  Beijing, China \\
  \texttt{liping@baidu.com} }
\date{\today}

\maketitle

\begin{abstract}
Training latent data models using the EM algorithm is the most popular choice for current learning tasks. 
For today's modern and complex tasks, variants of the EM have been initially introduced by \citep{neal1998view}, using incremental updates to scale to large dataset, and by \citep{wei1990monte, delyon1999}, using Monte-Carlo (MC) approximations to bypass the impossible conditional expectation of the latent data for most nonconvex models.
In this paper, we propose to combine both techniques in a single class of methods called Two-Time-Scale EM Methods. 
We motivate the choice of a double dynamics by invoking the variance reduction virtue of each stage of the method on both noises: the incremental update and the MC approximation.
We establish finite-time and independent of the initialization convergence bounds for nonconvex objective function.
Numerical applications are also presented in this article to illustrate our findings.
\end{abstract}


\section{Introduction}
Learning latent data models is critical for modern machine learning problems, see \citep{mclachlan2007algorithm} for references.
We formulate the training of such model as the following empirical risk minimization problem:
\beq \label{eq:em_motivate}
\min_{ \param \in \Param }~ \overline{\calL} ( \param ) \eqdef \Pen (\param) + \calL ( \param )~~\text{with}~~\calL ( \param ) = \frac{1}{n} \sum_{i=1}^n \calL_i( \param) \eqdef  \frac{1}{n} \sum_{i=1}^n \big\{ - \log g( y_i ; \param ) \big\}\eqs,
\eeq
We denote the observations by $\{y_i\}_{i=1}^n$, $\Param \subset \rset^d$ is the convex parameters space.
We consider a regularized model where $\Pen : \Param \rightarrow \rset$ is a smooth convex regularization function and for $\param \in \Param$, $g(y;\param)$ is the (incomplete) likelihood of each individual  observation.
The objective function $ \overline{\calL} ( \param )$ is possibly \emph{nonconvex} and is assumed to be lower bounded $ \overline{\calL} ( \param ) > - \infty$ for all $\param \in \Param$.

%We assume that $ \overline{\calL} ( \param ) > - \infty$ for all $\param \in \Param$.
In the latent variable model,  $g(y_i ; \param)$, is the marginal of the
complete data likelihood defined as $f(z_i,y_i; \param)$, i.e. $g(y_i; \param) = \int_{\Zset} f (z_i,y_i;\param) \mu(\rmd z_i)$, where $\{ z_i \}_{i=1}^n$ are the (unobserved)
latent variables.   
In this papaer, we make the assumption of a complete model belonging to the curved exponential family, \ie
\beq \label{eq:exp}
f(z_i,y_i; \param) = h  (z_i,y_i) \exp \big( \pscal{S(z_i,y_i)}{\phi(\param)} - \psi(\param) \big)\eqs,
\eeq
where $\psi(\param)$, $h(z_i,y_i)$ are scalar functions, $\phi(\param) \in \rset^k$ is a vector function, and $S(z_i,y_i) \in \rset^k$ is the complete data sufficient statistics.

Full batch EM \citep{dempster1977Maximum} is the method of reference for that kind of task and is a two steps procedure. The {\sf E-step} amounts to computing the conditional expectation of the complete data sufficient statistics, 
\begin{equation}
\label{eq:definition-overline-bss}
\overline{\bss}(\param)= \frac{1}{n} \sum_{i=1}^n \overline{\bss}_i(\param) \quad  \text{where}  \quad \overline{\bss}_i(\param)= \int_{\Zset} S(z_i,y_i) p(z_i|y_i;\param) \mu(\rmd z_i) \,.
\end{equation}
The {\sf M-step} is given by
\beq \label{eq:mstep}
\textsf{M-step:}~~\hat{\param} = \overline{\param}( \overline{\bss}(\param) ) \eqdef \argmin_{ \vartheta \in \Param } ~\big\{ \Pen( \vartheta ) + \psi( \vartheta) - \pscal{ \overline{\bss}(\param)}{ \phi ( \vartheta) } \big\},
\eeq

Two caveats of this method are the following: (a) with the explosion of data, the first step of the EM is computationally inefficient as it requires a full pass over the dataset at each iteration and (b) the complexity of modern models makes the expectation intractable. So far, both challenges have been addressed separately, to the best of our knowledge, and we give an overview of current solutions in the sequel.

\paragraph{Prior Work} Inspired by stochastic optimization procedures, \citep{neal1998view} and \citep{cappe2009line} developed respectively an incremental and an online variant of the \textsf{E-step} in models where the expectation is computable then extensively used and studied in \citep{nguyen2020mini, liang2009online,cappe2011online}.
Some improvements of that methods have been provided and analyzed, globally and in finite-time, in \citep{karimi2019global} where variance reduction techniques taken from the optimization literature have been efficiently applied to scale the EM algorithm to large datasets.

Regarding the computation of the expectation under the posterior distribution, the first method was the Monte-Carlo EM (MCEM) introduced in the seminal paper \citep{wei1990monte} where a MC approximation fo this expectation is computed. A variant of that method is the Stochastic Approximation of the EM (SAEM) in \citep{delyon1999} leveraging the power of Robbins-Monro type of update \citep{robbins1951stochastic} to ensure pointwise convergence of the vector of estimated parameters rather using a decreasing stepsize than increasing the number of MC samples.
The MCEM and the SAEM have been successfully applied in mixed effects models \citep{mcculloch1997maximum,hughes1999mixed,baey2016nonlinear} or to do inference for joint modelling of time to event data coming from clinical trials in \citep{das2010Inferences}, among other applications.

Recently, an incremental variant of the SAEM was proposed in \citep{kuhn2019properties} showing positive empirical results but its analysis is limited to asymptotic consideration. 
Gradient-based methods have been developed and analyzed in \citep{zhu2017high} but they remain out of the scope of this paper as they tackle the high-dimensionality issue.


\paragraph{Contributions} This paper \textit{introduces} and \textit{analyzes} a new class of methods which purpose is to combine both solutions proposed in the past years in a two-time-scale manner in order to optimize \eqref{eq:em_motivate} for current modern examples and settings. The main contributions of the paper are:
\begin{itemize}

\item We propose a two-time-scale method based on Stochastic Approximation (SA), to alleviate the problem of MC computation, and on Incremental updates, to scale to large datasets. We describe in details the edges of each level of our method based on variance reduction arguments. The derivation of such class of algorithms has two advantages. First, it combines two powerful ideas, commonly used separately, to tackle large scale and highly nonlinear learning tasks. Then, it gives a simple formulation as a \textit{scaled-gradient method}, as introduced in \citep{karimi2019global}, which makes the global analysis accessible.

\item We also establish global (independent of the initialization) and finite-time (true at each iteration) upper bounds on a classical suboptimality condition in the nonconvex literature, \ie\ the second order moment of the gradient of the objective function. 


\end{itemize}

In Section~\ref{sec:tts} we give rigorous mathematical definitions of the various updates used for both incremental and Monte-Carlo EMs and we introduce the main class of new algorithms, based on two different dynamics, we are proposing to analyze and compare to baselines algorithms. Section~\ref{sec:mainanalysis} presents the main theoretical guarantees of this newly introduced two-time-scale class of algorithms. Results are given both in finite-time and in the nonconvex setting.
Finally, we illustrate the advantages of our method in Section~\ref{sec:numerical} on two numerical experiments.



\section{Two-Time-Scale Stochastic EM Algorithms}\label{sec:tts}
We recall and formalize in this section the different methods found in the literature that aim to solving the large scale problem and the intractable expectation. 
We then provide the general framework of our method to efficiently tackle the optimization problem \eqref{eq:em_motivate}.
\subsection{Monte Carlo Integration and Stochastic Approximation} \label{sec:sEM}
As mentioned in the introduction, for complex and possibly nonlinear models, the expectation under the posterior distribution defined in \eqref{eq:definition-overline-bss} is not tractable. In that case, the first solution involves computing a Monte Carlo integration of that latter term. 
For all $ i \in \inter$, draw for $m \in \llbracket 1, M \rrbracket$, samples $z_{i,m} \sim p(z_i|y_i;\theta)$ and compute the MC integration $\tilde{\bss}$ of the deterministic quantity $\overline{\bss}(\param)$:
\beq\label{eq:mcstep}
\textsf{MC-step}:~ \tilde{\bss} = \frac{1}{n} \sum_{i=1}^n\frac{1}{M} \sum_{m=1}^M S(z_{i,m}, y_i)
\eeq
and then update the parameter $\hat{\param} = \overline{\param}( \hat{\bss} ) $.
This algorithm bypasses the intractable expectation issue but is rather computationally expensive in order to reach point wise convergence ($M$ needs to be large).
An alternative to that stochastic algorithm is to use a Robbins-Monro (RM) type of update.
We denote, at iteration $k$, the following quantity
\beq\label{eq:stats}
\tilde{S}^{(k+1)} = \frac{1}{n} \sum_{i=1}^n \tilde{S}^{(k+1)}_i = \frac{1}{n} \sum_{i=1}^n\frac{1}{M} \sum_{m=1}^M S(z_{i,m}^{(k)}, y_i) \quad \textrm{where} \quad z_{i,m}^{(k)} \sim p(z_i|y_i;\theta^{(k)})
\eeq
Then, the RM updated of the sufficient statistics $\hat{\bss}^{(k+1)}$ reads:
\beq\label{eq:rmstep}
\textsf{SA-step}:~ \hat{\bss}^{(k+1)} =  \hat{\bss}^{(k)}  + \gamma_{k+1}(\tilde{S}^{(k+1)} - \hat{\bss}^{(k)} )
\eeq
where $\{ \gamma_{k} \}_{k>1} \in (0,1)$ is a sequence of decreasing step sizes to ensure asymptotic convergence.
This is called the Stochastic Approximation of the EM (SAEM) and has been shown theoretically to converge to a maximum of the likelihood of the observations under very general conditions \citep{delyon1999}.
In the simulation step \eqref{eq:stats}, since the relation between the observed data $y_i$ and the latent variable $z_i$ can be non linear, sampling from the posterior distribution $p(z_i|y_i;\theta)$, under the current model $\theta$, could require using an inference algorithm. 
\citep{kuhn2004coupling} proved almost sure convergence of the sequence of parameters obtained by this algorithm coupled with an MCMC procedure during the simulation step. 
In simple scenarios, the samples $\{z_{i,m}\}_{m=0}^{M-1}$ are conditionally independent and identically distributed with distribution $p(z_i,\theta)$.
Nevertheless, in most cases, sampling exactly from this distribution is not an option and the Monte Carlo batch is sampled by Monte Carlo Markov Chains (MCMC) algorithm.
In the \textsf{SA-step}, the sequence of decreasing positive integers $\{ \gamma_{k} \}_{k>1}$ controls the convergence of the algorithm.
 In practice, $\gamma_k$ is set equal to $1$ during the first few iterations to let the algorithm explore the parameter space without memory and converge quickly to a neighbourhood of the target estimate. 
 The Stochastic Approximation is performed during the remaining iterations where $\gamma_k = 1/k^\alpha$, where $\alpha \in (0,1)$, ensuring the almost sure convergence of the estimate.
 It is inappropriate to start with small values for step size $\gamma_k$ and large values for the number of simulations $M_k$. Rather, it is recommended that one decrease $\gamma_k$ and keep a constant and small numer of MC samples $M_k$ which shows a great advantage over the \textsf{MC-step} \eqref{eq:mcstep}, which requires large $M_k$ to converge.



This Robbins-Monro type of update represents the \textit{first level} of our algorithm, needed to temper the variance and noise implied by MC integration.
In the next section, we derive variants of this algorithm to adapt to the sheer size of data of today's applications and formalize the \textit{second level} of our class of Two-Time-Scale EM methods.

\subsection{Incremental and Bi-Level Inexact EM Methods} \label{sec:sEM}
Strategies to scale to large datasets include classical incremental and variance reduced variants.
We will explicit a general update that will cover those variants and that represents the \textit{second level} of our algorithm, namely the incremental update of the noisy statistics $\hat{S}^{(k)}$ inside the RM type of update.

\beq \label{eq:sestep}
\textsf{Incremental-step}:~\tilde{S}^{(k+1)} = \tilde{S}^{(k)} + \rho_{k+1} \big( \StocEstep^{(k+1)}- \tilde{S}^{(k)}  \big),
\eeq

Note $\{ \rho_{k} \}_{k>1} \in (0,1)$ is a sequence of step sizes, $\StocEstep^{(k)}$ is a proxy for $\tilde{S}^{(k)}$,
If the stepsize is equal to one and the proxy $\StocEstep^{(k)} = \hat{S}^{(k)}$, i.e., computed in a full batch manner as in \eqref{eq:stats}, then we recover the SAEM algorithm.
Also if $\rho_{k}=1$, $\gamma_{k}=1$ and $\StocEstep^{(k)} = \tilde{S}^{(k)}$, then we recover the Monte Carlo EM algorithm.

We now introduce three variants of the SAEM update depending on different definitions of the proxy $\StocEstep^{(k)}$ and the choice of the stepsize $\rho_k$.
Let $i_k \in \inter$ be a random index drawn at iteration $k$ and $\tau_i^k = \max \{ k' : i_{k'} = i,~k' < k \}$ be the iteration index where $i \in \inter$ is last drawn prior to iteration $k$.
For iteration $k \geq 0$, the \FISAEM\ method draws \emph{two} indices \emph{independently} and uniformly as $i_k, j_k \in \inter$. In addition to $\tau_i^k$ which was defined \wrt $i_k$, we define $t_j^k = \{ k' : j_{k'} = j , k' < k \}$ to be the iteration index where the sample $j \in \inter$ is last drawn as $j_k$ prior to iteration $k$. With the initialization $\overline{\StocEstep}^{(0)} = \overline{\bss}^{(0)}$, we use a slightly different update rule from SAGA inspired by \citep{reddi2016fast}. Then, we obtain:
\begin{align}
&\emph{(\ISAEM\ \citep{karimi2019non, kuhn2019properties})} & \StocEstep^{(k+1)} &= \StocEstep^{(k)} + {\textstyle \frac{1}{n}}\big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(\tau_{i_k}^k)} \big) \label{eq:isaem} \\
&\emph{(\SAEMVR\ This paper )} &\StocEstep^{(k+1)} &= \tilde{S}^{(\ell(k))} +  \big( \tilde{S}_{i_k}^{(k)}  -\tilde{S}_{i_k}^{(\ell(k))}   \big) \label{eq:vrsaem}\\
&\emph{(\FISAEM\ This paper )} &\StocEstep^{(k+1)} &= \overline{\StocEstep}^{(k)} + \big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(t_{i_k}^k)} \big) \label{eq:fisaem}\\
&    &\overline{\StocEstep}^{(k+1)} &= \overline{\StocEstep}^{(k)} + n^{-1} \big( \tilde{S}_{j_k}^{(k)}  - \tilde{S}_{j_k}^{(t_{j_k}^k)} \big).
%\emph{(SAGA-EM)} & ~~~~\StocEstep^{(k+1)} = s & \gamma_{k+1} =
\end{align}
The stepsize is set to $\rho_{k+1} = 1$ for the \ISAEM\ method; $\rho_{k+1} = \gamma$ is  constant for the \SAEMVR\ and \FISAEM\ methods.
Moreover, for \ISAEM\ we initialize with $\StocEstep^{(0)} = \tilde{S}^{(0)}$; for \SAEMVR\, we set an epoch size of $m$ and define $\ell(k) \eqdef m \lfloor k/m \rfloor$ as the first iteration number in the epoch that iteration $k$ is in. \vspace{-.2cm}


\subsection{Two-Time-Scale Noisy EM methods}
We now introduce the general method derived using the two variance reduction techniques described above.
Algorithm~\ref{alg:ttsem} leverages both levels \eqref{eq:rmstep} and \eqref{eq:sestep} in order to output a vector of fitted parameters $\hat{\param}^{(K)}$ where $K$ is some randomly chosen termination point.

The update in \eqref{eq:twolevels} is said to have two timescales as the step sizes satisfy $\lim \limits_{k \to \infty} \gamma_k/\rho_k < 1$ such that $ \tilde{S}^{(k+1)} $  is updated at a faster timescale than $\hat{\bss}^{(k+1)}$.

\begin{algorithm}[H]
\caption{Two-Time-Scale Noisy EM methods.}\label{alg:ttsem}
  \begin{algorithmic}[1]
  \STATE \textbf{Input:} initializations $\hat{\param}^{(0)} \leftarrow 0$, $\hat{\bss}^{(0)} \leftarrow \hat{S}^{(0)}$, $K_{\sf max}$ $\leftarrow$ max.~iteration number. \STATE Set the terminating iteration number, $K \in \{0,\dots,K_{\sf max}-1\}$, as a discrete r.v.~with:\vspace{-.1cm}
  \beq \label{eq:random}
   P( K = k ) = \frac{ \gamma_{k} }{\sum_{\ell=0}^{K_{\sf max}-1} \gamma_\ell}.\vspace{-.2cm}
  \eeq
  \FOR {$k=0,1,2,\dots, K$}
  \STATE Draw index $i_k \in \inter$ uniformly (and $j_k \in \inter$ for \FISAEM).
     \STATE Compute $\hat{S}_{i_k}^{(k)}$ using the {\sf MC-step} \eqref{eq:mcstep},  for the drawn indices.
   \STATE Compute the surrogate sufficient statistics $\StocEstep^{(k+1)}$ using \eqref{eq:isaem} or \eqref{eq:vrsaem} or \eqref{eq:fisaem}.
   \STATE Compute $\hat{S}^{(k+1)}$ and $\hat{\bss}^{(k+1)}$ using respectively \eqref{eq:sestep} and \eqref{eq:rmstep}:
\beq \label{eq:twolevels}
\begin{split}
& \tilde{S}^{(k+1)} = \tilde{S}^{(k)} + \rho_{k+1} \big( \StocEstep^{(k+1)}- \tilde{S}^{(k)}  \big)\\
&  \hat{\bss}^{(k+1)} =  \hat{\bss}^{(k)}  + \gamma_{k+1}(\tilde{S}^{(k+1)} - \hat{\bss}^{(k)} )
\end{split}
\eeq
   \STATE Compute $\hat{\param}^{(k+1)}$ via the {\sf M-step} \eqref{eq:mstep}.
\ENDFOR
\STATE \textbf{Return}: $\hat{\param}^{(K)}$.
%\STATE \textbf{Return:} $\prm_t$.
  \end{algorithmic}
\end{algorithm}


The next section presents the main results of this paper and establishes global and finite-time bounds for the three different updates of our two-time-scale scheme.. 

\section{Finite Time Analysis of the Two-Time-Scale Scheme} \label{sec:mainanalysis}
Following \citep{cappe2009line}, it can be shown that stationary points of the objective function \eqref{eq:em_motivate} corresponds to the stationary points of the following \textit{nonconvex} Lyapunov function:
\beq\label{eq:em_sspace}
\min_{ {\bss} \in \Sset }~  V ( {\bss} ) \eqdef \overline\calL( \op(\bss) ) = \Pen (  \op(\bss) ) + \frac{1}{n} \sum_{i=1}^n {\cal L}_i (  \op(\bss) )
\eeq
We thus propose to study the latter minimization problem in the sequel.
%\begin{assumption}\label{ass:convset}
%$\Theta$ is an open set of $\rset^d$ and the sets $\Zset, \Sset$ are measurable open sets such that:
%\beq
%\Sset \supset \left\{  n^{-1} \sum_{i=1}^n u_i, u_i \in {\rm conv}(\overline{\bss}_i(\param))  \right\}
%\eeq
%where $\overline{\bss}_i(\param)$ is defined in \eqref{eq:definition-overline-bss} and ${\rm conv} \{ A \}$ denotes the closed convex hull of the set $A$.
%\end{assumption}

An important assumption in order to derive convergence guarantees reads as follows:
\begin{assumption}\label{ass:compact}
The sets $\Zset, \Sset$ are compact. There exists constants $C_{\Sset}, C_{\Zset}$ such that:
\beq \textstyle \label{eq:compact}
C_{\Sset} \eqdef \max_{ \bss, \bss' \in \Sset } \| \bss - \bss' \| < \infty,~~~~C_{\Zset} \eqdef \max_{i \in \inter} \int_{\Zset} | S(z,y_i) | \mu( \rmd z ) < \infty.
\eeq
\end{assumption}

\begin{assumption}\label{ass:expected}
The conditional distribution is smooth on ${\rm int}(\Param)$. For any $i \in \inter$, $z \in \Zset$, $\param, \param' \in {\rm int} (\Param)^2$, we have
$\big| p( z | y_i; \param ) - p( z | y_i; \param' ) \big| \leq  \Lip{p} \| \param - \param' \|$.
%For any $i \in \inter$, the map $\param \to \overline{\bss}_i(\param)$ is continuously differentiable in $\param$.
\end{assumption}

We also recall from the introduction that we consider curved exponential family models. besides:
\begin{assumption} \label{ass:reg}
For any $\bm{s} \in \Sset$, the function $\param \mapsto L(s,\param) \eqdef \Pen( \param ) + \psi( \param) - \pscal{ \bss}{ \phi ( \param) }$ admits a unique global minimum $\mstep{\bss} \in {\rm int}(\Param)$.
In addition, $\jacob{\phi}{\param}{\overline{\param}(\bss )}$ is full rank, $\Lip{\phi}$-Lipschitz and $\overline{\param}( \bss )$ is $\Lip{\theta}$-Lipschitz.
\end{assumption}
We denote by $\hess{L}{\param}(\bss,\param)$ the Hessian (w.r.t to $\param$ for a given value of $\bss$) of the function $\param \mapsto L(\bss,\param)= \Pen(\param) + \psi(\param) -\pscal{\bss}{\phi(\param)}$, and define
\beq\label{eq:Bss}
\operatorname{B}( \bss ) \eqdef\jacob{ \phi }{ \param }{ \mstep{\bss} } \Big( \hess{L}{\param}( {\bss},  \mstep{\bss} )  \Big)^{-1} \jacob{ \phi }{ \param }{ \mstep{\bss} }^\top.
\eeq
\begin{assumption}\label{ass:eigen}
It holds that $ \upsilon_{\max} \eqdef \sup_{\bss \in \Sset} \| \operatorname{B}( \bss ) \| < \infty$ and $0 < \upsilon_{\min}  \eqdef \inf_{\bss \in \Sset} \lambda_{\rm min} ( \operatorname{B}( \bss ) )$.
There exists a constant $\Lip{B}$ such that for all $\bss, \bss' \in \Sset^2$, we have $ \| \operatorname{B}( \bss ) - \operatorname{B}( \bss' )  \| \leq \Lip{B} \| {\bss} - {\bss}' \|$.
\end{assumption}

We now formulate the main difference with the work done in \citep{karimi2019global}. 
The class of algorithms we develop in this paper are two time-scale where the first stage corresponds to the variance reduction trick used in \citep{karimi2019global} in order to accelerate incremental methods and reduce the variance induced by the index sampling. 
The second stage is the Robbins-Monro type of update that aims to reduce the variance induced by the MC approximations

Indeed the expectations \eqref{eq:definition-overline-bss} are never available and requires Monte Carlo approximation.
Thus, at iteration $k+1$, we introduce the errors when approximating the quantity $ \overline{\bss}_i(\hat{\param}(\hat{\bss}^{(k-1)}))$.
For all $i \in \inter$, $r > 0$ and $\vartheta \in \Theta$, define:
\beq\label{eq:mcerror}
\eta_{i}^{(r)} \eqdef \tilde{S}_{i}^{(r)} -  \overline{\bss}_i(\vartheta^{(r)})
\eeq
For instance, we consider that the MC approximation is unbiased if for all $ i \in \inter$ and $m \in \llbracket 1, M \rrbracket$, the samples $z_{i,m} \sim p(z_i|y_i;\theta)$ are i.i.d. under the posterior distribution, \ie $\EE[\eta_{i}^{(r)}|{\cal F}_r] = 0$ where  ${\cal F}_r$ is the filtration up to iteration $r$.
The following results are derived under the assumption of control of the fluctuations implied by the approximation stated as follows:
\begin{assumption}\label{ass:mcerror}
There exist a positive sequence of MC batch size $\{M_r\}_{r > 0}$ and constants $(C, C_{\eta})$ such that for all $k >0$, $i \in \inter$ and $\vartheta \in \Theta$:
\beq\label{eq:boundederror}
\EE\left[\norm{\eta_{i}^{(r)}}^2 \right] \leq \frac{C_{\eta}}{M_r} \quad \textrm{and} \quad \EE\left[\norm{\EE[\eta_{i}^{(r)}|{\cal F}_r]}^2\right] \leq \frac{C}{M_r}
\eeq
\end{assumption}
In that setting, we can prove two important results on the Lyapunov function. The first one suggests smoothness:
\begin{Lemma} \label{lem:smooth}
\citep{karimi2019global} Assume H\ref{ass:compact}-H\ref{ass:eigen}.  
For all $\bss,\bss' \in \Sset$ and $i \in \inter$, we have
\beq \label{eq:smooth}
\| \overline{\bss}_i ( \overline{\param} ({\bss})) - \overline{\bss}_i ( \overline{\param} ({\bss}' )) \| \leq \Lip{{\bss}} \| {\bss} - {\bss}' \|,~~\| \grd  V ( {\bss} ) - \grd  V ( {\bss}' ) \| \leq \Lip{V} \| {\bss} - {\bss}' \|,
\eeq
where $\Lip{\bss} \eqdef C_{\Zset} \Lip{p} \Lip{\theta}$ and $\Lip{V}  \eqdef \upsilon_{\max} \big( 1 + \Lip{{\bss}} \big) + \Lip{B} C_{\Sset}$.
\end{Lemma}
and the second one suggests a growth condition on the gradient of $V$ depending on the mean field of the algorithm:
\begin{Lemma}\label{lem:growth}
Assume H\ref{ass:reg},H\ref{ass:eigen}. For all $\bss \in \Sset$,
\beq \label{eq:semigrad}
\upsilon_{\min}^{-1} \pscal{\grd V ( {\bss} ) }{ {\bss} - \os( \op ({\bss})) }
\geq \big\| {\bss} - \os( \op ({\bss})) \big\|^2 \geq \upsilon_{\max}^{-2} \| \grd V ( {\bss} ) \|^2,
\eeq
\end{Lemma}
See proofs of this Lemma in Appendix~\ref{app:growth}.

\subsection{Global Convergence of Incremental Noisy EM Algorithms}
We present in this section a finite-time analysis of the incremental variant of the Stochastic Approximation of the EM algorithm. We want to draw the attention of the readers that the word "global" here does not mean for a global optimum of the nonconvex function, but of the independence of our analysis on the initialization and the iteration $k$ (finite time).

The first intermediate result is the computation of the quantity $\hat{S}^{(k+1)} - \hat{\bss}^{(k)}$, which corresponds to the dirft term of \eqref{eq:rmstep} and reads as follows:
\begin{Lemma} \label{lem:meanfield_isaem}
The update \eqref{eq:isaem} is equivalent to the following update on the resulting statistics 
\beq
\hat{\bss}^{(k+1)} =  \hat{\bss}^{(k)}  + \gamma_{k+1} \big( \tilde{S}^{(k+1)} - \hat{\bss}^{(k)} \big) \quad \textrm{where} \quad  \tilde{S}^{(k+1)} = \frac{1}{n}\sum_{i=1}^n \tilde{S}_{i}^{(\tau_i^{k+1})}
\eeq 
Also:
\beq
\EE\left[\tilde{S}^{(k+1)} - \hat{\bss}^{(k)}\right] = \EE\left[\overline{\bss}^{(k)} - \hat{\bss}^{(k)}\right] + \left(1 - \frac{1}{n} \right) \EE\left[\frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}- \overline{\bss}^{(k)}\right]  +\frac{1}{n}\EE\left[\eta_{i_k}^{(k+1)}\right]
\eeq
where $\overline{\bss}^{(k)}$ is defined by \eqref{eq:definition-overline-bss} and $\tau_i^k = \max \{ k' : i_{k'} = i,~k' < k \}$.
\end{Lemma}
See proofs of this Lemma in Appendix~\ref{app:prooflemmainc}.

The following main result for the \ISAEM\ algorithm is derived under a control of the Monte Carlo fluctuations as described by assumption H~\ref{ass:mcerror}.
Typically, the controls exhibited below are of interest when the number of MC samples $M_k$ increase with the iteration index $f$.

\begin{Theorem}\label{thm:isaem}
Assume H\ref{ass:compact}-H\ref{ass:mcerror}.
Let $K_{\max }$ be a positive integer. 
Let $\left\{\gamma_{k}, k \in \mathbb{N}\right\}$ be a sequence of positive step sizes and consider the \ISAEM\ sequence $\left\{\hat{\bss}^{(k)}, k \in \mathbb{N}\right\}$ obtained with $\rho_{k+1}=1$ for any $k>0$. We also set $c_1 = \upsilon_{\min}^{-1}$, $\alpha = \max\{8, 1+6\upsilon_{\min}\}$, $\overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}$, $\gamma_{k+1} = \frac{1}{k \alpha c_1 \overline{L}}$, $\beta = \frac{c_1 \overline{L}}{n}$. 
Assume that $ \hat{\bss}^{(k)} \in \mathcal{S}$ for any $k \leq K_{\max }$.
\beq
\upsilon_{\max}^{-2}\sum_{k=0}^{K_{\max}} \tilde{\alpha}_k \EE \left[\norm{\grd V( \hs{k} )}^2 \right]  \leq   \EE \left[ V( \hs{0} ) - V( \hs{K} ) \right]+ \sum_{k=0}^{K_{\max}-1} \tilde{\Gamma}_k         \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] 
\eeq
\end{Theorem} 
See proof in Appendix~\ref{app:theoremisaem}.

\subsection{Global Convergence of Two-Time-Scale Noisy EM Algorithms}
We now proceed by giving our main result regarding the global convergence of the \FISAEM\ algorithm.
Two important auxiliary Lemmas, which proofs are given in Appendix~\ref{app:bothauxvrsaem}, are need in order to derive our finite-time bound.
The first one derives an identity for the quantity $\EE[ \| \hs{k} - \tilde{S}^{(k+1)}   \|^2 ]$ using the \SAEMVR\ update:
\begin{Lemma}\label{lem:auxvrsaem}
For any $k \geq 0$ and consider the \SAEMVR\ update in \eqref{eq:vrsaem} with $\rho_k = \rho$, it holds for all $k>0$ 
\beq
\begin{split}
  \EE\left[\norm{ \hs{k} - \tilde{S}^{(k+1)}}^2 \right] \leq& 2\rho^2 \EE[ \| \hs{k} - \os^{(k)} \|^2] +  2\rho^2\Lip{\bss}^2 \EE[ \| \hs{k} - \hs{\ell(k)} \|^2 ]\\
  &+2(1-\rho)^2 \EE[ \| \hs{(k)} - \tilde{S}^{(k)} \|^2 ]+ 2\rho^2\EE[\|\eta_{i_k}^{(k+1)} \|^2]
\end{split}
\eeq
where we recall that $\ell(k)$ is the first iteration number in the epoch that iteration $k$ is in.
\end{Lemma}

The second one derives an identity for the quantity $\EE[ \| \hs{k} - \tilde{S}^{(k+1)}   \|^2 ]$ using the \FISAEM\ update:
\begin{Lemma}\label{lem:aux1}
For any $k \geq 0$ and consider the \FISAEM\ update in \eqref{eq:fisaem} with $\rho_k = \rho$, it holds for all $k>0$ 
\beq
\begin{split}
  \EE\left[\norm{ \hs{k} - \tilde{S}^{(k+1)}}^2 \right] \leq& 2\rho^2 \EE[ \| \hs{k} - \os^{(k)} \|^2] +  2\rho^2\frac{\Lip{\bss}^2}{n}
\sum_{i=1}^n \EE[ \| \hs{k} - \hs{t_i^k} \|^2 ]\\
  &+2(1-\rho)^2 \EE[ \| \hs{(k)} - \tilde{S}^{(k)} \|^2 ]+ 2\rho^2\EE[\|\eta_{i_k}^{(k+1)} \|^2]
\end{split}
\eeq
\end{Lemma}

We now state the main result regarding the \SAEMVR\ method.
\begin{Theorem}\label{thm:vrsaem}
Assume H\ref{ass:compact}-H\ref{ass:mcerror}.
Let $K_{\max }$ be a positive integer. 
Let $\left\{\gamma_{k}, k \in \mathbb{N}\right\}$ be a sequence of positive step sizes and consider the \SAEMVR\ sequence $\left\{\hat{\bss}^{(k)}, k \in \mathbb{N}\right\}$ obtained with $\rho_{k+1}=\rho$ for any $k>0$.

Assume that $ \hat{\bss}^{(k)} \in \mathcal{S}$ for any $k \leq K_{\max }$.
By setting $\overline{L} = \max \{\Lip{\bss}, \Lip{V} \}$, $\rho = \frac{\mu}{ c_1 \overline{L}  n^{2/3}}$, $m = \frac{n c_1^2}{2 \mu^2+\mu c_1^2}$ and a constant $\mu \in (0,1)$, we have the following bound:
\beq
\begin{split}
 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ] \leq & \frac{2 n^{2/3} \overline{L}}{\mu \upsilon_{\min}^2\upsilon_{\max}^2}\EE[ V( \hs{0} ) - V( \hs{K_{\sf max}}) ] \\
 &+ \frac{2 n^{2/3} \overline{L}}{\mu \upsilon_{\min}^2\upsilon_{\max}^2} \sum_{k=0}^{K_{\sf max}-1} \left[  \tilde{\eta}^{(k+1)} + \chi^{(k+1)} \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2\right]\right]
 \end{split}
\eeq
\end{Theorem} 
See proof in Appendix~\ref{app:theoremvrsaem}.
We now state the main result regarding the \FISAEM\ method.
\begin{Theorem}\label{thm:fisaem}
Assume H\ref{ass:compact}-H\ref{ass:mcerror}.
Let $K_{\max }$ be a positive integer. 
Let $\left\{\gamma_{k}, k \in \mathbb{N}\right\}$ be a sequence of positive step sizes and consider the \FISAEM\ sequence $\left\{\hat{\bss}^{(k)}, k \in \mathbb{N}\right\}$ obtained with $\rho_{k+1}=\rho$ for any $k>0$.

Assume that $ \hat{\bss}^{(k)} \in \mathcal{S}$ for any $k \leq K_{\max }$. By setting $\alpha =\max\{2, 1+2\upsilon_{\min}\}$, $\overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}$, $\beta = \frac{c_1 \overline{L}}{n}$, $\rho = \frac{1}{n^{2/3}}$, $c_1(k\alpha-1) \geq c_1(\alpha-1) \geq 2$, $\alpha \geq 2$, we have the following bound:
\beq
\begin{split}
 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ]  \leq &\frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}\upsilon_{\max}^2}  \big[ V(\hat{\bss}^{(0)} )  - V(\hat{\bss}^{(K_{\sf max})}) \big]\\
 &   + \frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}\upsilon_{\max}^2} \sum_{k=0}^{K_{\sf max}-1} \left[ \Xi^{(k+1)}  +\Gamma_{k+1} \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2\right]\right]
\end{split}
\eeq

\end{Theorem} 
See proof in Appendix~\ref{app:theoremfisaem}.
Note that in those two bounds, the quantities $\tilde{\eta}^{(k+1)} $ and $ \Xi^{(k+1)} $ are abstraction that depends only on the MC fluctuations $\EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] $ and some constants.


\textbf{Remarks:} The following remarks are worth noting on the quantity $\EE\big[\norm{ \hs{k} -  \tilde{S}^{(k)} }^2\big] $:
\begin{itemize}
\item This term is the price we pay for the two time scale dynamics and corresponds to the gap between the two asynchronous updates (one is on  $\hs{k}$ and the other on $ \tilde{S}^{(k)}$).
\item It is trivial to see that if $\rho = 1$, \ie\ there is no variance reduction, then 
$$
\EE\big[\norm{ \hs{k}  - \tilde{S}^{(k)}   }^2\big] = \EE\big[\norm{ \StocEstep^{(k+1)} - \tilde{S}^{(k+1)} }^2\big]= 0 \quad \textrm{with} \quad \hs{0} = \tilde{S}^{(0)}=0
$$
which strengthen the fact that this quantity characterizes the impact of the variance reduction technique introduced in our two stages class of methods. 
\end{itemize}
The following lemma, which proof can be found in Appendix~\ref{app:gap_dynamics}, can be derived to characterize this gap:
\begin{Lemma} \label{lem:gap_dynamics}
Consider a decreasing stepsize $\gamma_k \in (0,1)$ and a constant $\rho \in (0,1)$, then the following inequality holds:
\beq
\begin{split}
\EE\big[\norm{ \hs{k} - \tilde{S}^{(k)}   }^2\big]  \leq \frac{\rho}{1-\rho}\sum_{\ell = 0}^k (1-\gamma_{\ell} )^2 (   \StocEstep^{(\ell)} - \tilde{S}^{(\ell)})
\end{split}
\eeq
where $\StocEstep^{(k)}  $ is defined either by \eqref{eq:vrsaem} (\SAEMVR\ ) or \eqref{eq:fisaem} (\FISAEM\ ).
\end{Lemma}

In the next section, we illustrate the benefits of our two-time-scale class of methods on several numerical applications.
\section{Numerical Examples}\label{sec:numerical}
\subsection{Gaussian Mixture Models}
We begin by a simple and illustrative example.
The authors acknowledge that the following model can be trained using deterministic EM-type of algorithms but propose to apply stochastic methods, including theirs, and to compare their performances.
Given $n$ observations $\{y_i\}_{i=1}^n$, we want to fit a Gaussian Mixture Model (GMM) whose distribution is modeled as a Gaussian mixture of $M$ components, each with a unit variance. 
Let $z_i \in \inter[M]$ be the latent labels of each component, the complete log-likelihood is defined as:
\beq \label{eq:comp_like} \textstyle
\log f( z_i, y_i; \param) =
% \sum_{m=1}^M \indiacc{m}(z_i) \left[\log(\omega_m) - (y_i-\mu_m)^2/2 \right] =
\sum_{m=1}^{M} \indiacc{m}(z_i) \left[ \log(\omega_m) - \mu_m^2/2 \right] + \sum_{m=1}^M \indiacc{m}(z_i) \mu_m y_i + {\rm constant} \eqsp.
\eeq
where $\param \eqdef (\bomega, \bmu)$ with $\bomega= \{\omega_{m}\}_{m=1}^{M-1}$ are the mixing weights with the convention $\omega_M= 1 - \sum_{m=1}^{M-1} \omega_m$  and $\bmu= \{\mu_m \}_{m =1}^M$ are the means.  We use the penalization 
%$\Pen( \param ) \eqdef  \epsilon \sum_{m=1}^M \big\{ \mu_m^2 / 2 - \log ( \omega_m ) \big\} - \epsilon \log \big( 1 - \sum_{m=1}^{M-1} \omega_m \big)$ where $\epsilon > 0$ is a small regularization parameter. 
$\Pen(\param)= \frac{\delta}{2}\sum_{m=1}^M \mu_m^2 - \log \Dir(\bomega; M, \epsilon)$ where $\delta > 0$ and $\Dir(\cdot; M,\epsilon)$ is the $M$ dimensional symmetric Dirichlet distribution with concentration parameter $\epsilon > 0$.
The constraint set on $\param$ is given by
\beq \textstyle
\Param = \{ \omega_m,~m=1,...,M-1 : \omega_m \geq 0,~\sum_{m=1}^{M-1} \omega_m \leq 1\} \times \{ \mu_m \in \rset ,~m=1,...,M \}.
\eeq
Exact two time scale updates are given in Appendix~\ref{app:gmm_update}.

In the following experiments on synthetic data, we generate samples from a GMM model with $M=2$ components with two mixtures with means $\mu_1 = - \mu_2 = 0.5$.
We use $n = 10^5$ synthetic samples and run the bEM method until convergence (to double precision) to obtain the ML estimate $\mu^\star$ averaged on $50$ datasets. We compare the bEM, iEM (incremental EM), SAEM, \ISAEM, \SAEMVR\ and \FISAEM\ methods in terms of their precision measured by $| \mu - \mu^\star |^2$. 
We set the stepsize of the \textsf{SA-step} of all method as $\gamma_k = 1/k^{\alpha}$ with $\alpha = 0.5$, and the stepsizes of the \textsf{Incremental-step} for \SAEMVR\ and the \FISAEM\ to a constant stepsize equal to $1/n^{2/3}$. 
\begin{wrapfigure}[10]{r}{.5\textwidth}
\begin{minipage}{0.5\textwidth}\vspace{-.9cm}


\begin{figure}[H]
\includegraphics[width=\textwidth]{pic_paper/tts_gmm_n100k.png}\vspace{-.2cm}
\caption{TO COMPLETE}\vspace{-.2cm}
\label{fig:gmm_tts}
\end{figure}

\end{minipage}
\end{wrapfigure}

The number of MC samples is fixed to $M=10$ chains.
Figure \ref{fig:gmm_tts} shows the convergence of the precision $|\mu - \mu^*|^2$ for the different methods against the epoch(s) elapsed (one epoch equals $n$ iterations). We observe that the \SAEMVR\ and \FISAEM\ methods outperform the other stochastic methods, supporting the benefits of our newly introduced scheme.



\vspace{.2cm}
\subsection{Deformable Template Model for Image Analysis}
Let $(y_i, i \in \inter)$ be observed gray level images defined on a grid of pixels.
Let $u \in \mathcal{U} \subset \rset^2$ denotes the pixel index on the image and $x_u \in \mathcal{D} \subset \rset^2$ its location.
The model used in this experiment suggests that each image $y_i$ is a deformation of a template, noted $I: \mathcal{D} \to \rset$, common to all images of the dataset:
\beq\label{eq:deformablemodel}
y_{i}(u)=I\left(x_{u}-\Phi_{i}\left(x_{u}, z_i\right)\right)+\varepsilon_{i}(u)
\eeq
where $\phi_i: \rset^2 \to \rset^2$ is a deformation function, $z_i$ some latent variable parametrizing this deformation and $\varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)$ is an observation error.

The template model, given $(p_k, k \in \llbracket 1, k_p \rrbracket)$ landmarks on the template, a fixed known kernel $\mathbf{K}_{\mathbf{p}}$ and a vector of parameters $\beta \in \rset^{k_p}$ is defined as follows:
\beq
I_{\xi}=\mathbf{K}_{\mathbf{p}} \beta, \quad \textrm{where} \quad \left(\mathbf{K}_{\mathbf{p}} \beta \right)(x)=\sum_{k=1}^{k_{p}} \mathbf{K}_{\mathbf{p}}\left(x, p_{k}\right) \beta_k
\eeq

Besides, we parameterize the deformation model given some landmarks $(g_k, k \in \llbracket 1, k_g \rrbracket)$ and a fixed kernel $\mathbf{K}_{\mathbf{g}}$ as:
\beq
\Phi_{i}=\mathbf{K}_{\mathbf{g}} z_{i} \quad \textrm{where} \quad \left(\mathbf{K}_{\mathbf{g}} z_{i}\right)(x)=\sum_{k=1}^{k_{s}} \mathbf{K}_{\mathbf{g}}\left(x, g_{k}\right)\left(z_{i}^{(1)}(k), z_{i}^{(2)}(k)\right)
\eeq
where we put a Gaussian prior on the latent variables, $z_i \sim \mathcal{N}(0,\Gamma)$ and $z_i \in \left( \rset^{k_g}\right)^2$.
The vector of parameters we ought to estimate is thus $\param = \big( \beta, \Gamma, \sigma  \big)$.
The complete model belongs to the curved exponential family, see \citep{allassonniere2007towards}, which vector of sufficient statistics $S = \big(S_1(z),S_2(z),S_3(z) \big)$ read:
\beq \label{eq:suffstat_deformable}
\begin{split}
& S_1(z) = \frac{1}{n} \sum_{i=1}^nS_1(y_i, z_i)  = \frac{1}{n} \sum_{i=1}^n \left(\mathbf{K}_{p}^{z_{i}}\right)^\top y_{i}\\
& S_2(z) =\frac{1}{n} \sum_{i=1}^n S_2(y_i, z_i) = \frac{1}{n} \sum_{i=1}^n \left(\mathbf{K}_{p}^{z_{i}}\right)^\top\left(\mathbf{K}_{p}^{z_{i}}\right)\\
& S_3(z) =\frac{1}{n} \sum_{i=1}^n S_3(y_i, z_i)  = \frac{1}{n}  \sum_{i=1}^n  z_{i}^{t} z_{i} 
\end{split}
\eeq
where for any pixel $u \in \rset^2$ and $j \in \llbracket 1, k_g \rrbracket$ we noted:
\beq
\mathbf{K}_{p}^{z_{i}}(x_u,j) = \mathbf{K}_{p}^{z_{i}}(x_u - \phi_i(x_u,z_i), p_j)
\eeq
Finally, the Two-Time-Scale \textsf{M-step} yields the following parameter updates:
\beq
\bar{\param}(\hat{s}) 
= \left(
\begin{array}{c}
\beta(\hat{s}) =   \hat{s}_2^{-1}(z) \hat{s}_1(z)    \\
\Gamma(\hat{s}) = \frac{1}{n} \hat{s}_3(z)   \\
 \sigma(\hat{s}) =\beta(\hat{s})^\top  \hat{s}_2(z) \beta(\hat{s}) - 2\beta(\hat{s}) \hat{s}_1(z)
\end{array}
\right)
\eeq
where $\hat{s} = (\hat{s}_1(z),\hat{s}_2(z),\hat{s}_3(z))$ is the vector of statistics obtained via the \textsf{SA-step} \eqref{eq:rmstep} and using the MC approximation of the sufficient statistics $\big(S_1(z),S_2(z),S_3(z) \big)$ defined in \eqref{eq:suffstat_deformable}.

\paragraph{Comparison using epochs credit}


\paragraph{Comparison using number of training samples credit}


\subsection{PK Model with Absorption Lag Time}
This numerical example was conducted in order to characterize the pharmacokinetics of orally administered drug to simulated patients, using a population pharmacokinetic approach. $M = 50$ synthetic datasets were generated for $n = 500$ patients with $10$ observations (concentration measures) per patient.

\paragraph{The model:}
We consider a one-compartment pharmacokinetics (PK) model for oral administration with an absorption lag-time ($T^{\textrm{lag}}$), assuming first-order absorption and linear elimination processes.
The final model includes $ka$ is the absorption rate constant, $V$ the volume of distribution, $k$ the elimination rate constant.
We also add several covariates to our model such as $D$ the dose of drug administered, $t$ the time at which measures are taken and the weight such as $V$ is function of it. More precisely, the log-volume $\log(V)$ is a linear function of the log-weight $lw70= \log(wt/70)$.
The final reads:
\begin{equation} \label{eq:pkmodel}
f(t,ka, V, k) = \frac{D\,ka}{V(ka - k)}(\exponential^{-ka\,(t - T^{\textrm{lag}})}-\exponential^{-k\,(t - T^{\textrm{lag}})})\eqs,
\end{equation}
Here, $T^{\textrm{lag}}$, $ka$, $V$ and $k$ are  PK parameters that can change from one individual to another.


Let $ z_i=(T_i^{\textrm{lag}}, ka_i, V_i, k_i)$ be the vector of individual PK parameters for individual $i$.
The model for the $j$-th measured concentration, noted $y_{ij}$, for individual $i$ writes:
\begin{equation}
y_{ij} = f(t_{ij},z_i)+ \varepsilon_{ij}\eqs.
\end{equation}

We assume in this example that the residual errors are independent and normally distributed with mean 0 and variance $\sigma^2$.
Lognormal distributions are used for the three PK parameters:
\begin{align}
& \log(T_i^{\textrm{lag}}) \sim \mathcal{N}(\log(T^{\textrm{lag}}_{\rm pop}), \omega^2_{T^{\textrm{lag}}} ) \sim \mathcal{N}(\log(ka_{\rm pop}), \omega^2_{ka})\eqs,\\
&\log(V_i) \sim \mathcal{N}(\log(V_{\rm pop}), \omega^2_{V})\eqs,
 \log(k_i) \sim \mathcal{N}(\log(k_{\rm pop}), \omega^2_{k})\eqs.
\end{align}

The complete model belongs to the curved exponential family, which vector of sufficient statistics $S = \big(S_1(z),S_2(z),S_3(z) \big)$ read:
\beq \label{eq:suffstat_deformable}
\begin{split}
S_1(z)  = \frac{1}{n} \sum_{i=1}^n z_i  ,  \quad S_2(z) =\frac{1}{n} \sum_{i=1}^n z_i^\top z_i  , \quad S_3(z)  = \frac{1}{n}  \sum_{i=1}^n  \left(y_i - f(t_{i},z_i)\right)^2
\end{split}
\eeq
where we have noted $y_i$ and $t_i$ the vector of observations and time for each patient $i$.

\begin{wrapfigure}[10]{r}{.5\textwidth}
\begin{minipage}{0.5\textwidth}\vspace{-.9cm}


\begin{figure}[H]
\includegraphics[width=\textwidth]{pic_paper/pk500.png}\vspace{-.2cm}
\caption{TO COMPLETE}\vspace{-.2cm}
\label{fig:pk_tts}
\end{figure}

\end{minipage}
\end{wrapfigure}
\paragraph{Monte Carlo study:}
We conduct a Monte Carlo study to showcase the benefits of our scheme.

$M=50$ datasets have been simulated using the following PK parameters values:
$T^{\textrm{lag}}_{\rm pop} =1$, $ka_{\rm pop} =1$, $V_{\rm pop}= 8$, $k_{\rm pop}=0.1$, $ \omega_{T^{\textrm{lag}}}=0.4$, $\omega_{ka}=0.5$, $\omega_{V}=0.2$, $\omega_{k}=0.3$ and $\sigma^2=0.5$.
We define the mean square distance over the $M$ replicates $E_k(\ell) = \frac{1}{M}\sum_{m=1}^{M}{\left(\theta_k^{(m)}(\ell) - \theta^* \right)^2}$ and plot it against the epochs (passes overt the data) Figure~\ref{fig:pk_tts}.

Note that the {\sf MC-step} \eqref{eq:mcstep} is performed using a Metropolis Hastings procedure since the posterior distribution under the model $\theta$ noted $p(z_i | y_i, \theta)$ is intractable due to the nonlinearity of the model \eqref{eq:pkmodel}.
Figure~\ref{fig:pk_tts} shows



\section{Conclusion}


\newpage
\linespread{1.1}
\normalsize

\bibliographystyle{abbrvnat}
\bibliography{references}

\linespread{1}
\newpage

\appendix


\section{Proof of Lemma~\ref{lem:growth}}\label{app:growth}
\begin{Lemma*} 
Assume H\ref{ass:reg},H\ref{ass:eigen}. For all $\bss \in \Sset$,
\beq \label{eq:semigrad}
\upsilon_{\min}^{-1} \pscal{\grd V ( {\bss} ) }{ {\bss} - \os( \op ({\bss})) }
\geq \big\| {\bss} - \os( \op ({\bss})) \big\|^2 \geq \upsilon_{\max}^{-2} \| \grd V ( {\bss} ) \|^2,
\eeq
\end{Lemma*}
\begin{proof}
Using H\ref{ass:reg} and the fact that we can exchange integration with differentiation and the Fisher's identity,   we obtain
\beq \label{eq:grd_v}
\begin{split}
\grd_{ \bss} V( {\bss} ) & = \jacob{ \overline{\param} }{ \bss }{\bss}^\top
\Big( \grd_\param \Pen( \mstep{\bss} )  + \grd_\param \calL( \overline\param( {\bss} ) )  \Big) \\
& =  \jacob{ \overline{\param} }{ \bss }{\bss}^\top \Big( \grd_\param \psi( \mstep{\bss}) + \grd_\param \Pen( \mstep{\bss} ) - \jacob{\phi}{\param}{\mstep{\bss} }^\top  \os( \op ({\bss})) \Big)\\
& =   \jacob{ \overline{\param} }{ \bss }{\bss}^\top \jacob{\phi}{\param}{ \mstep{\bss} }^\top \!~ ({\bss} - \os( \op ({\bss})) ) \eqsp,
\end{split}
\eeq
Consider the following vector map:
\beq
{\bss} \to \grd_{\param} L(\bss, \param) \vert_{\param= \mstep{\bss}}= \grd_\param \psi ( \mstep{\bss} ) + \grd_{ \param} \Pen(\mstep{\bss}  ) - \jacob{ \phi }{ \param }{\mstep{\bss}  }^\top \!~{\bss} \eqsp.
\eeq
Taking the gradient of the above map \wrt ${\bss}$ and using assumption H\ref{ass:reg}, we show that:
\beq
{\bm 0} = - \jacob{\phi}{\param}{\mstep{\bss} } + \Big( \underbrace{ \grd_{\param}^2 \big( \psi( \param ) + \Pen( \param ) - \pscal{ \phi( \param ) }{ {\bss} } \big)}_{= \hess{{L}}{\param} ( {\bss}; \param )} \big|_{\param = \mstep{\bss}  } \Big) \jacob{ \overline{\param} }{\bss}{\bss} \eqsp.
\eeq
The above yields
\beq
\grd_{ \bss} V( {\bss} )  = \operatorname{B}(\bss) ({\bss} - \os( \op ({\bss})) )
\eeq
where we recall $\operatorname{B}(\bss) = \jacob{ \phi }{ \param }{ \mstep{\bss} } \Big( \hess{{L}}{\param}( {\bss}; \mstep{\bss} )  \Big)^{-1} \jacob{ \phi }{ \param }{\mstep{\bss} }^\top$. The proof of \eqref{eq:semigrad} follows directly from the assumption~H\ref{ass:eigen}.
\end{proof}


\section{Proof of Lemma~\ref{lem:meanfield_isaem}}\label{app:prooflemmainc}
\begin{Lemma*}
 Assume H\ref{ass:convset}. The update \eqref{eq:isaem} is equivalent to the following update on the resulting statistics 
\beq
\hat{\bss}^{(k+1)} =  \hat{\bss}^{(k)}  + \gamma_{k+1} \big( \tilde{S}^{(k+1)} - \hat{\bss}^{(k)} \big)
\eeq 
Also:
\beq
\EE\left[\tilde{S}^{(k+1)} - \hat{\bss}^{(k)}\right] = \EE\left[\overline{\bss}^{(k)} - \hat{\bss}^{(k)}\right] + \left(1 - \frac{1}{n} \right) \EE\left[\frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}- \overline{\bss}^{(k)}\right]  +\frac{1}{n}\EE\left[\eta_{i_k}^{(k+1)}\right]
\eeq
where $\overline{\bss}^{(k)}$ is defined by \eqref{eq:definition-overline-bss} and $\tau_i^k = \max \{ k' : i_{k'} = i,~k' < k \}$.
\end{Lemma*}
\begin{proof}
From update \eqref{eq:isaem}, we have:
\beq
\begin{split}
\tilde{S}^{(k+1)} - \hat{\bss}^{(k)} & = \tilde{S}^{(k)} - \hat{\bss}^{(k)} +\frac{1}{n}\left( \tilde{S}_{i_k}^{(k+1)} - \tilde{S}_{i_k}^{(\tau_i^k)}  \right)\\
& = \overline{\bss}^{(k)} - \hat{\bss}^{(k)} + \tilde{S}^{(k)}- \overline{\bss}^{(k)}  - \frac{1}{n}\left( \tilde{S}_{i_k}^{(\tau_i^k)} - \tilde{S}_{i_k}^{(k+1)}   \right)
\end{split}
\eeq
Since $\tilde{S}_{i_k}^{(k+1)} = \overline{\bss}_{i_k}(\param^{(k)}) + \eta_{i_k}^{(k+1)}$ we have 
\beq
\begin{split}
\tilde{S}^{(k+1)} - \hat{\bss}^{(k)} = \overline{\bss}^{(k)} - \hat{\bss}^{(k)} + \tilde{S}^{(k)}- \overline{\bss}^{(k)}  - \frac{1}{n}\left( \tilde{S}_{i_k}^{(\tau_i^k)} -  \overline{\bss}_{i_k}(\param^{(k)})   \right) + \frac{1}{n}\eta_{i_k}^{(k+1)}
\end{split}
\eeq
Taking the full expectation of both side of the equation leads to:
\beq
\begin{split}
\EE\left[\tilde{S}^{(k+1)} - \hat{\bss}^{(k)}\right] = \EE\left[\overline{\bss}^{(k)} - \hat{\bss}^{(k)}\right] & + \EE\left[\frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}\right] \\
& -\frac{1}{n} \EE\left[\EE\left[ \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}_{i_k}(\param^{(k)})  | \mathcal{F}_{k} \right]\right] + \frac{1}{n} \EE\left[\eta_{i_k}^{(k+1)}\right]
\end{split}
\eeq
The following equalities:
\beq
\EE\left[ \tilde{S}_i^{(\tau_i^k)} | \mathcal{F}_{k} \right] =\frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)} \quad \textrm{and} \quad \EE\left[  \overline{\bss}_{i_k}(\param^{(k)})  | \mathcal{F}_{k} \right]= \overline{\bss}^{(k)}
\eeq 
concludes the proof of the Lemma.
\end{proof}
 

\section{Proof of Theorem~\ref{thm:isaem}}\label{app:theoremisaem}
\begin{Theorem*}
Assume H\ref{ass:compact}-H\ref{ass:mcerror}.
Let $K_{\max }$ be a positive integer. 
Let $\left\{\gamma_{k}, k \in \mathbb{N}\right\}$ be a sequence of positive step sizes and consider the \ISAEM\ sequence $\left\{\hat{\bss}^{(k)}, k \in \mathbb{N}\right\}$ obtained with $\rho_{k+1}=1$ for any $k>0$. We also set $c_1 = \upsilon_{\min}^{-1}$, $\alpha = \max\{8, 1+6\upsilon_{\min}\}$, $\overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}$, $\gamma_{k+1} = \frac{1}{k \alpha c_1 \overline{L}}$, $\beta = \frac{c_1 \overline{L}}{n}$. 
Assume that $ \hat{\bss}^{(k)} \in \mathcal{S}$ for any $k \leq K_{\max }$.
\beq
\upsilon_{\max}^{-2}\sum_{k=0}^{K_{\max}} \tilde{\alpha}_k \EE \left[\norm{\grd V( \hs{k} )}^2 \right]  \leq   \EE \left[ V( \hs{0} ) - V( \hs{K} ) \right]+ \sum_{k=0}^{K_{\max}-1} \tilde{\Gamma}_k         \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] 
\eeq
\end{Theorem*} 

\begin{proof}

We begin our proof by giving this auxiliary Lemma setting an upper bound for the quantity $\EE \left[ \|  \tilde{S}^{(k+1)} - \hs{k}   \|^2 \right]$
\begin{Lemma}\label{lem:aux2}
For any $k \geq 0$ and consider the \ISAEM\ update in \eqref{eq:isaem}, it holds that
\beq
\begin{split}
\EE \left[ \|  \tilde{S}^{(k+1)} - \hs{k}   \|^2 \right] \leq &4 \EE\left[ \|  \os^{(k)} - \hs{k} \|^2 \right] 
+ \frac{2\Lip{\bss}^2}{n^3} \sum_{i=1}^n \EE\left[ \| \hs{k} - \hs{t_i^k} \|^2 \right]\\
&+ 2\frac{C_{\eta}}{M_k} + 4 \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right] 
\end{split}
\eeq
\end{Lemma}

\begin{proof}
Applying the \ISAEM\ update yields:
\beq
\begin{split}
 \EE[ \|  \tilde{S}^{(k+1)} - \hs{k} \|^2 ] &=  \EE[ \| \tilde{S}^{(k)} - \hs{k}  -\frac{1}{n}\big(\tilde{S}^{(\tau_i^k)}_{i_k} - \tilde{S}^{(k)}_{i_k}  \big)  \|^2 ]\\
 & \leq  4 \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right] + 4 \EE\left[\norm{   \overline{\bss}^{(k)} - \hs{k} }^2\right] \\
 &+  \frac{2}{n^2} \EE\big[ \norm{ \os_{i_k}^{(k)} - \os_{i_k}^{(t_{i_k}^k)} }^2\big] + 2\frac{C_{\eta}}{M_k}
\end{split}
\eeq

The last expectation can be further bounded by
\beq
\begin{split}
&
\frac{2}{n^2}\EE[ \| \os_{i_k}^{(k)} - \os_{i_k}^{(t_{i_k}^k)} \|^2 ] = \frac{2}{n^3} \sum_{i=1}^n \EE[ \| \os_i^{(k)} - \os_i^{(t_i^k)} \|^2 ] \overset{(a)}{\leq} \frac{2\Lip{\bss}^2}{n^3}
\sum_{i=1}^n \EE[ \| \hs{k} - \hs{t_i^k} \|^2 ],
\end{split}
\eeq
where (a) is due to Lemma~\ref{lem:smooth} and which concludes the proof of the Lemma.

\end{proof}


Under the smoothness of the Lyapunov function $V$ (cf. Lemma~\ref{lem:smooth}), we can write:
\beq
\begin{split}
V( \hs{k+1} ) & \leq V( \hs{k} ) + \gamma_{k+1} \pscal{  \tilde{S}^{(k+1)}  - \hs{k}}{ \grd V( \hs{k} ) } + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \|\tilde{S}^{(k+1)} -  \hs{k}  \|^2 \\
\end{split}
\eeq

Taking the expectation on both sidesyields:
\beq
\EE \left[V( \hs{k+1} ) \right]  \leq \EE \left[ V( \hs{k} ) \right] + \gamma_{k+1} \EE \left[\pscal{  \tilde{S}^{(k+1)}  - \hs{k}}{ \grd V( \hs{k} ) }  \right]+ \frac{\gamma_{k+1}^2 \Lip{V}}{2} \EE \left[\|\tilde{S}^{(k+1)} -  \hs{k}  \|^2  \right]
\eeq

Using Lemma~\ref{lem:meanfield_isaem}, we obtain:
\beq
\begin{split}
& \EE \left[\pscal{  \tilde{S}^{(k+1)}  - \hs{k}}{ \grd V( \hs{k} ) }  \right] =\\
&  \EE \left[\pscal{  \overline{\bss}^{(k)}  - \hs{k}}{ \grd V( \hs{k} ) }  \right]  + \left(1 - \frac{1}{n}\right)\EE\left[\pscal{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}{ \grd V( \hs{k} ) }\right]  +  \frac{1}{n} \EE \left[\pscal{ \eta_{i_k}^{(k)}}{ \grd V( \hs{k} ) }  \right]\\
& \overset{(a)}{\leq} -\upsilon_{\min}\EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right] + \left(1 - \frac{1}{n}\right)\EE\left[\pscal{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}{ \grd V( \hs{k} ) }\right] +  \frac{1}{n} \EE \left[\pscal{ \eta_{i_k}^{(k)}}{ \grd V( \hs{k} ) }  \right]\\
& \overset{(b)}{\leq} -\upsilon_{\min}\EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right] + \frac{1 - \frac{1}{n}}{2\beta}\EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]\\
& + \frac{\beta(n-1) + 1}{2n}\EE\left[ \norm{\grd V( \hs{k} )}^2\right]  +  \frac{1}{2 n} \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] \\
& \overset{(a)}{\leq} \left(\upsilon^2_{\max}\frac{\beta(n-1) + 1}{2n}-\upsilon_{\min}\right) \EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right] + \frac{1 - \frac{1}{n}}{2\beta}\EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]+  \frac{1}{2 n} \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] 
\end{split}
\eeq
where (a) is due to the growth condition \eqref{lem:growth} and (b) is due to Young's inequality (with $\beta \to 1$).
Note $a_k = \gamma_{k+1}\left(\upsilon_{\min} - \upsilon^2_{\max}\frac{\beta(n-1) + 1}{2n}\right) $ and
\beq\label{eq:final1}
\begin{split}
a_k \EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right]  \leq & \EE \left[ V( \hs{k} ) - V( \hs{k+1} ) \right] + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \EE \left[\|\tilde{S}^{(k+1)} -  \hs{k}  \|^2  \right]\\
&+ \frac{\gamma_{k+1}(1 - \frac{1}{n})}{2\beta}\EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]+  \frac{\gamma_{k+1}}{2 n} \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] 
\end{split}
\eeq

We now give an upper bound of $\EE \left[\|\tilde{S}^{(k+1)} -  \hs{k}  \|^2  \right]$ using Lemma~\ref{lem:aux2} and plug it into \eqref{eq:final1}:

\beq\label{eq:final2}
\begin{split}
\left( a_k - 2\gamma_{k+1}^2 \Lip{V} \right) \EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right]  \leq &  \EE \left[ V( \hs{k} ) - V( \hs{k+1} ) \right] \\
&  +   \gamma_{k+1} \left(\frac{1}{2 \beta}(1 - \frac{1}{n} ) + 2 \gamma_{k+1}\Lip{V} \right)            \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]\\
& + \gamma_{k+1} \left(\gamma_{k+1} \Lip{V} +    \frac{1}{2 n}\right)           \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] \\
& + \frac{\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2}{n^3} \sum_{i=1}^n \EE[ \| \hs{k} - \hs{\tau_i^k} \|^2 ]
\end{split}
\eeq


Next, we observe that
\beq
\frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k+1} - \hs{t_i^{k+1}} \|^2 ] = \frac{1}{n} \sum_{i=1}^n
\Big( \frac{1}{n} \EE[ \| \hs{k+1} - \hs{k} \|^2 ] + \frac{n-1}{n} \EE[ \| \hs{k+1} - \hs{\tau_i^k} \|^2 ]  \Big)
\eeq
where the equality holds as $i_k$ and $j_k$ are drawn independently. For any $\beta > 0$, it holds
\beq
\begin{split}
& \EE[ \| \hs{k+1} - \hs{t_i^k} \|^2 ] \\
& = \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + \| \hs{k} - \hs{\tau_i^k} \|^2 + 2 \pscal{\hs{k+1} - \hs{k}}{\hs{k}- \hs{\tau_i^k}} \Big] \\
& = \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + \| \hs{k} - \hs{\tau_i^k} \|^2 - 2 \gamma_{k+1} \pscal{ \hs{k} - \tilde{S}^{(k+1)} }{\hs{k}- \hs{\tau_i^k}} \Big] \\
& \leq  \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + \| \hs{k} - \hs{\tau_i^k} \|^2 +  \frac{\gamma_{k+1}}{\beta} \| \hs{k} - \tilde{S}^{(k+1)}\|^2 + \gamma_{k+1} \beta \| \hs{k}- \hs{\tau_i^k} \|^2 \Big]
\end{split}
\eeq
where the last inequality is due to the Young's inequality. Subsequently, we have
\beq
\begin{split}
& \frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k+1} - \hs{\tau_i^{k+1}} \|^2 ] \\
& \leq \EE[  \| \hs{k+1} - \hs{k} \|^2 ] + \frac{n-1}{n^2} \sum_{i=1}^n \EE \Big[ (1+\gamma_{k+1} \beta) \|  \hs{k} - \hs{\tau_i^k} \|^2 + \frac{\gamma_{k+1}}{\beta} \|  \hs{k} - \tilde{S}^{(k+1)} \|^2 \Big]
\end{split}
\eeq
Observe that $\hs{k+1} - \hs{k} = - \gamma_{k+1} ( \hs{k} - \tilde{S}^{(k+1)} )$. Applying Lemma~\ref{lem:aux2} yields
\beq
\begin{split}
& \frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k+1} - \hs{\tau_i^{k+1}} \|^2 ] \\
& \leq \big(\gamma_{k+1}^2 +\frac{n-1}{n}\frac{\gamma_{k+1}}{\beta}  \big)\EE \Big[  \|   \tilde{S}^{(k+1)} -  \hs{k} \|^2  \Big] + \sum_{i=1}^n \EE \Big[  \frac{1 - \frac{1}{n} + \gamma_{k+1} \beta}{n} \|  \hs{k} - \hs{\tau_i^k} \|^2  \Big] \\
& \leq 4\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)\EE \Big[  \|   \os^{(k)} - \hs{k}  \|^2  \Big] + 2\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)\EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right]\\
&+  4 \big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)\EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right] \\
&+  \sum_{i=1}^n \EE \Big[ \frac{1 - \frac{1}{n} + \gamma_{k+1} \beta + \frac{2\gamma_{k+1} \Lip{\bss}^2}{n^2}(\gamma_{k+1} +\frac{1}{\beta})}{n} \|  \hs{k} - \hs{t_i^k} \|^2  \Big]  
\end{split}
\eeq
Let us define
\beq
\Delta^{(k)} \eqdef \frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k} - \hs{\tau_i^{k}} \|^2 ]
\eeq
From the above, we get
\beq
\begin{split}
 \Delta^{(k+1)} \leq & \big(1 - \frac{1}{n} + \gamma_{k+1} \beta + \frac{2\gamma_{k+1} \Lip{\bss}^2}{n^2}(\gamma_{k+1} +\frac{1}{\beta})  \big) \Delta^{(k)} +4 \big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big) \EE \Big[  \|   \os^{(k)} - \hs{k}  \|^2  \Big]\\
 &  + 2\big(\gamma_{k+1}^2  +\frac{\gamma_{k+1}}{\beta}  \big)\EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right]+  4 \big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big) \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]
\end{split}
\eeq

Setting $c_1 = \upsilon_{\min}^{-1}$, $\alpha =\max\{8, 1+6\upsilon_{\min}\}$, $\overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}$, $\gamma_{k+1} = \frac{1}{k \alpha c_1 \overline{L}}$, $\beta = \frac{c_1 \overline{L}}{n}$, $c_1(k\alpha-1) \geq c_1(\alpha-1) \geq 6$, $\alpha \geq 8$, we observe that
\beq
1 - \frac{1}{n} + \gamma_{k+1} \beta + \frac{2\gamma_{k+1} \Lip{\bss}^2}{n^2}(\gamma_{k+1} +\frac{1}{\beta}) 
 \leq 1 - \frac{c_1(k\alpha  - 1) - 4}{k\alpha n c_1 } \leq 1 - \frac{2}{k\alpha n c_1 }
\eeq
which shows that $1 - \frac{1}{n} + \gamma_{k+1} \beta + \frac{2\gamma_{k+1} \Lip{\bss}^2}{n^2}(\gamma_{k+1} +\frac{1}{\beta})  \in (0,1)$ for any $k >0$.
Denote $ \Lambda_{(k+1)} =\frac{1}{n} - \gamma_{k+1} \beta - \frac{2\gamma_{k+1} \Lip{\bss}^2}{n^2}(\gamma_{k+1} +\frac{1}{\beta}) $ and note that $\Delta^{(0)} = 0$, thus the telescoping sum yields:
\beq
\begin{split}
\Delta^{(k+1)} \leq & 4 \sum_{ \ell = 0 }^k \prod_{j = \ell +1}^k \Big( 1 -  \Lambda_{(j)} \Big) \big(\gamma_{\ell+1}^2 +\frac{\gamma_{\ell+1}}{\beta}  \big)  \EE[  \|  \os^{(\ell)} - \hs{\ell}  \|^2 ] + 2\sum_{ \ell = 0 }^k \prod_{j = \ell +1}^k \Big( 1 -  \Lambda_{(j)} \Big) \big(\gamma_{\ell+1}^2  +\frac{\gamma_{\ell+1}}{\beta}  \big) \EE \left[\norm{ \eta_{i_\ell}^{(\ell)}}^2 \right]\\
& +  4 \sum_{ \ell = 0 }^k   \prod_{j = \ell +1}^k \Big( 1 -  \Lambda_{(j)} \Big)  \big(\gamma_{\ell+1}^2 +\frac{\gamma_{\ell+1}}{\beta}  \big)  \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^\ell)}-  \overline{\bss}^{(\ell)}}^2\right]
\end{split}
\eeq
Note $\omega_{k,\ell} = \prod_{j = \ell +1}^k \Big( 1 -  \Lambda_{(j)} \Big)$
Summing on both sides over $k=0$ to $k = K_{\max}-1$ yields:

\beq\label{eq:Delta}
\begin{split}
& \sum_{k=0}^{K_{\max}-1} \Delta^{(k+1)}\\
&=  4 \sum_{k=0}^{K_{\max}-1} \big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big) \omega_{k,1} \EE[  \|  \os^{(k)} - \hs{k}  \|^2 ] + 2 \sum_{k=0}^{K_{\max}-1} \big(\gamma_{k+1}^2  +\frac{\gamma_{k+1}}{\beta}  \big)\omega_{k,1}\EE \left[\norm{ \eta_{i_\ell}^{(k)}}^2 \right]\\
& +  \sum_{k=0}^{K_{\max}-1} 4 \big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big) \omega_{k,1}  \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]\\
& \leq  \sum_{k=0}^{K_{\max}-1}\frac{4\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)}{ \Lambda_{(k+1)}}   \EE[  \|  \os^{(k)} - \hs{k}  \|^2 ] + \sum_{k=0}^{K_{\max}-1}\frac{2\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)}{ \Lambda_{(k+1)}}  \EE \left[\norm{ \eta_{i_\ell}^{(k)}}^2 \right]\\
& +  \sum_{k=0}^{K_{\max}-1}\frac{4\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)}{ \Lambda_{(k+1)}}  \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]
\end{split}
\eeq

We recall \eqref{eq:final2} where we have summed on both sides from $k=0$ to $k = K_{\max}-1$:
\beq\label{eq:final3}
\begin{split}
\sum_{k=0}^{K_{\max}-1}  \left( a_k - 2\gamma_{k+1}^2 \Lip{V} \right) \EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right]  \leq &  \EE \left[ V( \hs{0} ) - V( \hs{K} ) \right] \\
&  +   \sum_{k=0}^{K_{\max}-1} \gamma_{k+1} \left(\frac{1}{2 \beta}(1 - \frac{1}{n} ) + 2 \gamma_{k+1}\Lip{V} \right)            \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right]\\
& + \sum_{k=0}^{K_{\max}-1} \gamma_{k+1} \left(\gamma_{k+1} \Lip{V} +    \frac{1}{2 n}\right)           \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] \\
& +\sum_{k=0}^{K_{\max}-1} \frac{\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2}{n^2} \Delta^{(k)}
\end{split}
\eeq

Plugging \eqref{eq:Delta} into \eqref{eq:final3} results in:

\beq
\begin{split}
\sum_{k=0}^{K_{\max}-1}  \tilde{\alpha}_k \EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right] + \sum_{k=0}^{K_{\max}-1}  \tilde{\beta}_k \EE\left[\norm{ \frac{1}{n} \sum_{i=1}^n \tilde{S}_i^{(\tau_i^k)}-  \overline{\bss}^{(k)}}^2\right] \leq   & \EE \left[ V( \hs{0} ) - V( \hs{K} ) \right]\\
&+ \sum_{k=0}^{K_{\max}-1} \tilde{\Gamma}_k         \EE \left[\norm{ \eta_{i_k}^{(k)}}^2 \right] 
\end{split}
\eeq
where:
\begin{align*}
&  \tilde{\alpha}_k = a_k - 2\gamma_{k+1}^2 \Lip{V} -  \frac{\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2}{n^2}\frac{4\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)}{ \Lambda_{(k+1)}}   \\
&  \tilde{\beta}_k =  \gamma_{k+1} \left(\frac{1}{2 \beta}(1 - \frac{1}{n} ) + 2 \gamma_{k+1}\Lip{V} \right) -  \frac{\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2}{n^2}\frac{4\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)}{ \Lambda_{(k+1)}} \\
&  \tilde{\Gamma}_k = \gamma_{k+1} \left(\gamma_{k+1} \Lip{V} +    \frac{1}{2 n}\right)  +  \frac{\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2}{n^2} \frac{2\big(\gamma_{k+1}^2 +\frac{\gamma_{k+1}}{\beta}  \big)}{ \Lambda_{(k+1)}}
\end{align*}
and
\begin{align*}
&  a_k  = \gamma_{k+1}\left(\upsilon_{\min} - \upsilon^2_{\max}\frac{\beta(n-1) + 1}{2n}\right)\\
& \Lambda_{(k+1)} =\frac{1}{n} - \gamma_{k+1} \beta - \frac{2\gamma_{k+1} \Lip{\bss}^2}{n^2}(\gamma_{k+1} +\frac{1}{\beta})\\
& c_1 = \upsilon_{\min}^{-1}, \alpha = \max\{8, 1+6\upsilon_{\min}\}, \overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}, \gamma_{k+1} = \frac{1}{k \alpha c_1 \overline{L}}, \beta = \frac{c_1 \overline{L}}{n}
\end{align*}
When, for any $k >0$, $\tilde{\alpha}_k \geq 0$, we have by Lemma~\ref{lem:growth} that:
\beq
\sum_{k=0}^{K_{\max}} \tilde{\alpha}_k \EE \left[\norm{\grd V( \hs{k} )}^2 \right] \leq \upsilon_{\max}^2\sum_{k=0}^{K_{\max}} \tilde{\alpha}_k \EE \left[\norm{  \overline{\bss}^{(k)}  - \hs{k}}^2  \right] 
\eeq
which yields an upper bound of the gradient of the Lyapunov function $V$ along the path of the \ISAEM\ update and concludes the proof of the Theorem.
\end{proof}

\clearpage
\section{Proofs of Auxiliary Lemmas}
\subsection{Proof of Lemma~\ref{lem:auxvrsaem} and Lemma~\ref{lem:aux1}} \label{app:bothauxvrsaem}
\begin{Lemma*}
For any $k \geq 0$ and consider the \SAEMVR\ update in \eqref{eq:vrsaem} with $\rho_k = \rho$, it holds for all $k>0$ 
\beq
\begin{split}
  \EE\left[\norm{ \hs{k} - \tilde{S}^{(k+1)}}^2 \right] \leq& 2\rho^2 \EE[ \| \hs{k} - \os^{(k)} \|^2] +  2\rho^2\Lip{\bss}^2 \EE[ \| \hs{k} - \hs{\ell(k)} \|^2 ]\\
  &+2(1-\rho)^2 \EE[ \| \hs{(k)} - \tilde{S}^{(k)} \|^2 ]+ 2\rho^2\EE[\|\eta_{i_k}^{(k+1)} \|^2]
\end{split}
\eeq
where we recall that $\ell(k)$ is the first iteration number in the epoch that iteration $k$ is in.
\end{Lemma*}
\begin{proof}
Beforehand, we provide a rewiriting of the quantity $ \hs{k+1} - \hs{k} $ that will be useful throughout this proof:
\beq\label{eq:vrsaem_drift}
\begin{split}
\hs{k+1} - \hs{k}  = -\gamma_{k+1}  ( \hs{k} - \tilde{S}^{(k+1)}) &=-\gamma_{k+1}  ( \hs{k} - (1-\rho)\tilde{S}^{(k)} - \rho\StocEstep^{(k+1)})\\
& = -\gamma_{k+1} \left((1-\rho)\left[\hs{k} - \tilde{S}^{(k)} \right] +\rho\left[\hs{k} - \StocEstep^{(k+1)}\right] \right)
\end{split}
\eeq
We observe, using the identity \eqref{eq:vrsaem_drift}, that
\beq \label{eq:auxlemvrsaem}
\EE[ \| \hs{k} -\tilde{S}^{(k+1)} \|^2 ] \leq 2\rho^2 \EE[ \| \hs{k} - \os^{(k)} \|^2] + 2\rho^2 \EE[ \| \os^{(k)} - \StocEstep^{(k+1)} \|^2 ]+ 2(1-\rho)^2 \EE[ \| \hs{(k)} - \tilde{S}^{(k)} \|^2 ]
\eeq
For the latter term, we obtain its upper bound as % note $\EE[\StocEstep^{(k+1)}] = \os^{(k)}$
\beq
\begin{split}
\EE[ \| \os^{(k)} - \StocEstep^{(k+1)} \|^2 ] & = \EE\Big[ \Big\| \frac{1}{n} \sum_{i=1}^n \big( \os_i^{(k)} - \tilde{S}_i^{\ell(k)} \big) - \big( \os_{i_k}^{(k)} - \tilde{S}_{i_k}^{(\ell(k))} \big) \Big\|^2 \Big] \\
& \overset{(a)}{\leq} \EE[ \| \os_{i_k}^{(k)} - \os_{i_k}^{(\ell(k))} \|^2 ] + \EE[\|\eta_{i_k}^{(k+1)} \|^2] \overset{(b)}{\leq}  \Lip{\bss}^2 \EE[ \| \hs{k} - \hs{\ell(k)} \|^2 ]+ \EE[\|\eta_{i_k}^{(k+1)} \|^2]
\end{split}
\eeq
where $(a)$ uses the variance inequality and $(b)$ uses Lemma~\ref{lem:smooth}. 
Substituting into \eqref{eq:auxlemvrsaem} proves the lemma.
\end{proof}
\begin{Lemma*}
For any $k \geq 0$ and consider the \FISAEM\ update in \eqref{eq:fisaem} with $\rho_k = \rho$, it holds for all $k>0$ 
\beq
\begin{split}
  \EE\left[\norm{ \hs{k} - \tilde{S}^{(k+1)}}^2 \right] \leq& 2\rho^2 \EE[ \| \hs{k} - \os^{(k)} \|^2] +  2\rho^2\frac{\Lip{\bss}^2}{n}
\sum_{i=1}^n \EE[ \| \hs{k} - \hs{t_i^k} \|^2 ]\\
  &+2(1-\rho)^2 \EE[ \| \hs{(k)} - \tilde{S}^{(k)} \|^2 ]+ 2\rho^2\EE[\|\eta_{i_k}^{(k+1)} \|^2]
\end{split}
\eeq
\end{Lemma*}

\begin{proof}
Beforehand, we provide a rewiriting of the quantity $ \hs{k+1} - \hs{k} $ that will be useful throughout this proof:
\beq\label{eq:fisaem_drift}
\begin{split}
\hs{k+1} - \hs{k}  & = -\gamma_{k+1}  ( \hs{k} - \tilde{S}^{(k+1)}) \\
& =-\gamma_{k+1}  ( \hs{k} - (1-\rho)\tilde{S}^{(k)} - \rho\StocEstep^{(k+1)})\\
& = -\gamma_{k+1} \left((1-\rho)\left[\hs{k} - \tilde{S}^{(k)} \right] +\rho\left[\hs{k} - \StocEstep^{(k+1)}\right] \right)\\
& =  -\gamma_{k+1} \left((1-\rho)\left[\hs{k} - \tilde{S}^{(k)} \right] +\rho\left[\hs{k} - \overline{\StocEstep}^{(k)} - \big( \tilde{S}_{i_k}^{(k)}  -  \tilde{S}_{i_k}^{(t_{i_k}^k)}  \big)\right] \right) 
\end{split}
\eeq

We observe, using the identity \eqref{eq:fisaem_drift}, that
\beq \label{eq:auxlemfisaem}
\EE[ \| \hs{k} -\tilde{S}^{(k+1)} \|^2 ] \leq 2\rho^2 \EE[ \| \hs{k} - \os^{(k)} \|^2] + 2\rho^2 \EE[ \| \os^{(k)} - \StocEstep^{(k+1)} \|^2 ]+ 2(1-\rho)^2 \EE[ \| \hs{(k)} - \tilde{S}^{(k)} \|^2 ]
\eeq
For the latter term, we obtain its upper bound as % note $\EE[\StocEstep^{(k+1)}] = \os^{(k)}$
\beq
\begin{split}
\EE[ \| \os^{(k)} - \StocEstep^{(k+1)} \|^2 ] & = \EE\Big[ \Big\| \frac{1}{n} \sum_{i=1}^n \big( \os_i^{(k)} -\overline{\StocEstep}_i^{(k)} \big) - \big( \tilde{S}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)} \big) \Big\|^2 \Big] \\
& \overset{(a)}{\leq} \EE[ \| \os_{i_k}^{(k)} - \os_{i_k}^{(\ell(k))} \|^2 ] + \EE[\|\eta_{i_k}^{(k+1)} \|^2] 
\end{split}
\eeq
where $(a)$ uses the variance inequality.
We can further bound the last expectation using Lemma~\ref{lem:smooth}:
\beq
\EE[ \| \os_{i_k}^{(k)} - \os_{i_k}^{(t_{i_k}^k)} \|^2 ] = \frac{1}{n} \sum_{i=1}^n \EE[ \| \os_i^{(k)} - \os_i^{(t_i^k)} \|^2 ] \overset{(a)}{\leq} \frac{\Lip{\bss}^2}{n}
\sum_{i=1}^n \EE[ \| \hs{k} - \hs{t_i^k} \|^2 ]
\eeq
Substituting into \eqref{eq:auxlemfisaem} proves the lemma.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:gap_dynamics}}\label{app:gap_dynamics}
\begin{Lemma*} 
Consider a decreasing stepsize $\gamma_k \in (0,1)$ and a constant $\rho$, then the following inequality holds:
\beq
\begin{split}
\EE\big[\norm{ \hs{k} - \tilde{S}^{(k)}   }^2\big]  \leq \frac{\rho}{1-\rho}\sum_{\ell = 0}^k (1-\gamma_{\ell} )^2 (   \StocEstep^{(\ell)} - \tilde{S}^{(\ell)})
\end{split}
\eeq
where $\StocEstep^{(k)}  $ is defined either by \eqref{eq:fisaem} (\FISAEM\ ) or \eqref{eq:vrsaem} (\SAEMVR\ )
\end{Lemma*}


\begin{proof}
We begin by writing the two-time-scale update:
\beq\label{eq:updatetwo}
\begin{split}
& \tilde{S}^{(k+1)} = \tilde{S}^{(k)} + \rho \big( \StocEstep^{(k+1)}- \tilde{S}^{(k)}  \big)\\
&  \hat{\bss}^{(k+1)} =  \hat{\bss}^{(k)}  + \gamma_{k+1}(\tilde{S}^{(k+1)} - \hat{\bss}^{(k)} )
\end{split}
\eeq
where $\StocEstep^{(k+1)} = \frac{1}{n}\sum_{i=1}^n \tilde{S}_i^{(t_i^k)} + \big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(t_{i_k}^k)} \big) $ according to \eqref{eq:fisaem}.
Denote $\delta^{(k+1)} =  \hs{k+1} - \tilde{S}^{(k+1)} $. 
Then from \eqref{eq:updatetwo}, doing the subtraction of both equations yields:
\beq
\delta^{(k+1)} = (1-\gamma_{k+1} ) \delta^{(k)} + \frac{\rho}{1-\rho}(1-\gamma_{k+1} )(  \StocEstep^{(k+1)} -  \tilde{S}^{(k+1)})
\eeq
Using the telescoping sum and noting that $\delta^{(0)} = 0$, we have
\beq
\delta^{(k+1)} \leq \frac{\rho}{1-\rho}\sum_{\ell = 0}^k (1-\gamma_{\ell+1} )^2 (   \StocEstep^{(\ell+1)} - \tilde{S}^{(\ell+1)} )
\eeq 
\end{proof}


\subsection{Additional Intermediary Result}
\begin{Lemma} \label{lem:drift_fisaem}
 At iteration $k+1$,the drift term of update \eqref{eq:fisaem}, with $\rho_{k+1} = \rho$, is equivalent to the following :
\beq
\begin{split}
 \hs{k} -  \tilde{S}^{(k+1)}= & \rho (\hs{k} - \overline{\bss}^{(k)})  + \rho \eta_{i_k}^{(k+1)}+ \rho \left[\big(\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}\big) - \EE[\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}] \right] \\
 &+ (1-\rho)\left( \hs{k} - \tilde{S}^{(k)}\right)
\end{split}
\eeq
where we recall that $\eta_{i_k}^{(k+1)}$, defined in \eqref{eq:boundederror}, which is the gap between the MC approximation and the expected statistics.
\end{Lemma}
\begin{proof}
Using the \FISAEM\ update $ \tilde{S}^{(k+1)} = (1 - \rho)\tilde{S}^{(k)} + \rho \StocEstep^{(k+1)}$ where $\StocEstep^{(k+1)} = \overline{\StocEstep}^{(k)} + \big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(t_{i_k}^k)} \big)$ leads to the following decomposition:
\beq\notag
\begin{split}
 & \tilde{S}^{(k+1)} - \hs{k} \\
 &= (1 - \rho)\tilde{S}^{(k)} + \rho \left( \overline{\StocEstep}^{(k)} + \big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(t_{i_k}^k)} \big) \right) - \hs{k}+\rho \overline{\bss}^{(k)} - \rho \overline{\bss}^{(k)} \\
 & = \rho (\overline{\bss}^{(k)} - \hs{k}) + \rho(\tilde{S}_{i_k}^{(k)} - \overline{\bss}^{(k)}_{i_k}) + (1-\rho)\left(\tilde{S}^{(k)} - \hs{k}\right) + \rho \left( \overline{\StocEstep}^{(k)} - \overline{\bss}^{(k)}+ \big( \overline{\bss}_{i_k}^{(k)}   - \tilde{S}_{i_k}^{(t_{i_k}^k)} \big) \right)\\ 
&= \rho (\overline{\bss}^{(k)}-\hs{k}) + \rho \eta_{i_k}^{(k+1)} - \rho \left[\big(\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}\big) - \EE[\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}] \right] \\
 &+ (1-\rho)\left(\tilde{S}^{(k)} - \hs{k}\right)
\end{split}
\eeq
where we observe that $\EE[\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}] =\overline{\bss}^{(k)} -   \overline{\StocEstep}^{(k)} $ and which concludes the proof.

\textit{Important Note:} Note that $\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}$ is not equal to $\eta_{i_k}^{(k+1)}$, defined in \eqref{eq:boundederror}, which is the gap between the MC approximation and the expected statistics. Indeed $\tilde{S}_{i_k}^{(t_{i_k}^k)}$ is not computed under the same model as $\overline{\bss}_{i_k}^{(k)}$.
\end{proof}


\clearpage

\section{Proof of Theorem~\ref{thm:vrsaem}}\label{app:theoremvrsaem}
\begin{Theorem*}
Assume H\ref{ass:compact}-H\ref{ass:mcerror}.
Let $K_{\max }$ be a positive integer. 
Let $\left\{\gamma_{k}, k \in \mathbb{N}\right\}$ be a sequence of positive step sizes and consider the \SAEMVR\ sequence $\left\{\hat{\bss}^{(k)}, k \in \mathbb{N}\right\}$ obtained with $\rho_{k+1}=\rho$ for any $k>0$.

Assume that $ \hat{\bss}^{(k)} \in \mathcal{S}$ for any $k \leq K_{\max }$.
By setting $\overline{L} = \max \{\Lip{\bss}, \Lip{V} \}$, $\rho = \frac{\mu}{ c_1 \overline{L}  n^{2/3}}$, $m = \frac{n c_1^2}{2 \mu^2+\mu c_1^2}$ and a constant $\mu \in (0,1)$, we have the following bound:
\beq
\begin{split}
 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ] \leq & \frac{2 n^{2/3} \overline{L}}{\mu \upsilon_{\min}^2\upsilon_{\max}^2}\EE[ V( \hs{0} ) - V( \hs{K_{\sf max}}) ] \\
 &+ \frac{2 n^{2/3} \overline{L}}{\mu \upsilon_{\min}^2\upsilon_{\max}^2} \sum_{k=0}^{K_{\sf max}-1} \left[  \tilde{\eta}^{(k+1)} + \chi^{(k+1)} \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2\right]\right]
 \end{split}
\eeq
\end{Theorem*} 

\begin{proof}

Using the smoothness of $V$ and update \eqref{eq:vrsaem}, we obtain:
\beq\label{eq:smoothvrsaem}
\begin{split}
V( \hs{k+1} ) & \leq V( \hs{k} ) + \pscal{  \hs{k+1} - \hs{k}  }{ \grd V( \hs{k} ) } + \frac{ \Lip{V}}{2} \| \hs{k+1} - \hs{k} \|^2\\
& \leq V( \hs{k} ) - \gamma_{k+1} \pscal{  \hs{k}-  \tilde{S}^{(k+1)} }{ \grd V( \hs{k} ) } + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \|  \hs{k}  - \tilde{S}^{(k+1)} \|^2
\end{split}
\eeq
Denote $\Hdrift_{k+1} \eqdef  \hs{k} -  \tilde{S}^{(k+1)} $ the drift term of the \FISAEM\ update in \eqref{eq:rmstep} and  $\hmean_{k} =\hs{k} - \overline{\bss}^{(k)}$. Taking expectations on both sides show that
\beq \label{eq:lips_con}
\begin{split}
& \EE[ V( \hs{k+1} ) ] \\
& \overset{(a)}{\leq} \EE[ V( \hs{k} ) ] - \gamma_{k+1}(1-\rho) \EE \Big[ \pscal{ \hs{k} - \tilde{S}^{(k)} }{\grd V( \hs{k} ) } \Big]- \gamma_{k+1} \rho \EE \Big[ \pscal{ \hs{k} - \StocEstep^{(k+1)}  }{\grd V( \hs{k} ) } \Big] \\
 & + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \EE[ \| \Hdrift_{k+1} \|^2 ] \\
& \overset{(b)}{\leq} \EE[ V( \hs{k} ) ] - \gamma_{k+1} \rho \EE \Big[ \pscal{ \hmean_{k}  }{\grd V( \hs{k} ) } \Big]- \gamma_{k+1}(1-\rho) \EE \Big[ \pscal{ \hs{k} - \tilde{S}^{(k)} }{\grd V( \hs{k} ) } \Big] \\
 & - \gamma_{k+1}\rho \EE \Big[ \pscal{ \eta_{i_k}^{(k+1)} }{\grd V( \hs{k} ) } \Big] + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \EE[ \| \Hdrift_{k+1} \|^2 ] \\
 & \overset{(c)}{\leq} \EE[ V( \hs{k} ) ] - \left(\gamma_{k+1} \rho \upsilon_{\min} + \gamma_{k+1}  \upsilon_{\max}^2 \right)  \EE \Big[ \norm{\hmean_{k}}^2 \Big]+ \frac{\gamma_{k+1}^2 \Lip{V}}{2} \EE[ \| \Hdrift_{k+1} \|^2 ]\\
 &- \gamma_{k+1} \rho \EE\left[\norm{\eta_{i_k}^{(k+1)}}^2 \right] - \gamma_{k+1}(1-\rho) \EE \left[ \norm{ \hs{k} - \tilde{S}^{(k)}}^2 \right]  \\
\end{split}
\eeq
where we have used \eqref{eq:vrsaem_drift} in $(a)$ and $\EE \left[ \StocEstep^{(k+1)} \right] = \overline{\bss}^{(k)} + \EE[\eta_{i_k}^{(k+1)}]$ in $(b)$, the growth condition in Lemma~\ref{lem:growth} and the Young's inequality with the constant equal to $1$ in $(c)$.

Furthermore, for $k+1 \leq \ell(k) + m$ (\ie $k+1$ is in the same epoch as $k$), we have
\beq
\begin{split}
& \EE[ \| \hs{k+1} -  \hs{\ell(k)} \|^2 ] = \EE[ \| \hs{k+1} - \hs{k} + \hs{k} - \hs{\ell(k)} \|^2 ] \\
& = \EE \Big[  \| \hs{k} -  \hs{\ell(k)} \|^2 + \| \hs{k+1} - \hs{k}  \|^2 + 2 \pscal{\hs{k} -  \hs{\ell(k)} }{\hs{k+1} - \hs{k} } \Big] \\
& = \EE \Big[ \| \hs{k} -  \hs{\ell(k)} \|^2 + \gamma_{k+1}^2 \| \Hdrift_{k+1} \|^2 \\
&-2\gamma_{k+1} \pscal{\hs{k} -  \hs{\ell(k)} }{ \rho(\hmean_{k} - \eta_{i_k}^{(k+1)}) + (1-\rho)( \hs{k} - \tilde{S}^{(k)} )  } \Big] \\
& \leq \EE \Big[ (1 + \gamma_{k+1} \beta) \| \hs{k} -  \hs{\ell(k)} \|^2 + \gamma_{k+1}^2 \| \Hdrift_{k+1} \|^2 + \frac{\gamma_{k+1}\rho}{\beta} \| \hmean_{k} \|^2\\
& +  \frac{\gamma_{k+1}\rho}{\beta} \|\eta_{i_k}^{(k+1)} \|^2 + \frac{\gamma_{k+1}(1-\rho)}{\beta} \| \hs{k} - \tilde{S}^{(k)} \|^2 \Big],
\end{split}
\eeq
where we first used \eqref{eq:vrsaem_drift} and the last inequality is due to the Young's inequality.

Consider the following sequence
\beq
R_k \eqdef \EE[ V( \hs{k} ) + b_{{k}} \| \hs{k} - \hs{\ell(k)} \|^2 ]
\eeq
where $b_k \eqdef \overline{b}_{k~{\rm mod}~m}$ is a periodic sequence where:
\beq
\overline{b}_i = \overline{b}_{i+1} (1 + \gamma_{k+1} \beta + 2 \gamma_{k+1}^2\rho^2 \Lip{\bss}^2 ) + \gamma_{k+1}^2\rho^2 \Lip{V} \Lip{\bss}^2,~~i=0,1,\dots,m-1~~\text{with}~~\overline{b}_m = 0.
\eeq
Note that $\overline{b}_i$ is decreasing with $i$ and this implies
\beq
\overline{b}_i \leq \overline{b}_0 = \gamma_{k+1}^2\rho^2 \Lip{V} \Lip{\bss}^2 \frac{ (1 + \gamma_{k+1} \beta + 2 \gamma_{k+1}^2 \rho^2\Lip{\bss}^2 )^m - 1 }{ \gamma_{k+1} \beta + 2 \gamma_{k+1}^2 \rho^2\Lip{\bss}^2 },~i=1,2,\dots,m.
\eeq
For $k+1 \leq \ell(k) + m$, we have the following inequality
\beq\begin{split}
R_{k+1 } & \leq  \EE \Big[ V( \hs{k} )  - \left(\gamma_{k+1} \rho \upsilon_{\min} + \gamma_{k+1}  \upsilon_{\max}^2 \right)  \| \hmean_{k} \|^2 + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \| \Hdrift_{k+1}  \|^2 \Big] \\
& + \gamma_{k+1} \EE\left[\rho \norm{\eta_{i_k}^{(k+1)}}^2 -(1-\rho)\norm{ \hs{k} - \tilde{S}^{(k)}}^2 \right]\\
& + b_{k+1} \EE \left[ (1 + \gamma_{k+1} \beta) \| \hs{k} -  \hs{\ell(k)} \|^2 + \gamma_{k+1}^2 \| \Hdrift_{k+1} \|^2 + \frac{\gamma_{k+1}\rho}{\beta} \| \hmean_{k} \|^2 \right]\\
& + b_{k+1} \EE \left[ \frac{\gamma_{k+1}\rho}{\beta} \|\eta_{i_k}^{(k+1)} \|^2 + \frac{\gamma_{k+1}(1-\rho)}{\beta} \| \hs{k} - \tilde{S}^{(k)} \|^2 \right]
\end{split}
\eeq
And using Lemma~\ref{lem:auxvrsaem} we obtain:
\beq\begin{split}
R_{k+1 } & \leq  \EE \Big[ V( \hs{k} )  - \left(\gamma_{k+1} \rho \upsilon_{\min} + \gamma_{k+1}  \upsilon_{\max}^2  - \gamma_{k+1}^2\rho^2 \Lip{V}\right)  \| \hmean_{k} \|^2  + \gamma_{k+1}^2\rho^2 \Lip{V} \Lip{\bss}^2 \| \hs{k} - \hs{\ell(k)} \|^2 \Big] \\
& + b_{k+1} \EE \left[ (1 + \gamma_{k+1} \beta + 2\gamma_{k+1}^2 \rho^2 \Lip{\bss}^2) \| \hs{k} -  \hs{\ell(k)} \|^2  + (\frac{\gamma_{k+1}\rho}{\beta}+ 2\gamma_{k+1}^2 \rho^2) \| \hmean_{k} \|^2 \right]\\
& + \gamma_{k+1} \EE\left[(\rho+\rho^2\gamma_{k+1}\Lip{V}) \norm{\eta_{i_k}^{(k+1)}}^2 -(1-\rho - (1-\rho)^2\gamma_{k+1}\Lip{V})\norm{ \hs{k} - \tilde{S}^{(k)}}^2 \right]\\
& + b_{k+1} \EE \left[ (\frac{\gamma_{k+1}\rho}{\beta}+ 2\gamma_{k+1}^2 \rho^2) \|\eta_{i_k}^{(k+1)} \|^2 + (\frac{\gamma_{k+1}(1-\rho)}{\beta}+ 2\gamma_{k+1}^2 (1-\rho)^2) \| \hs{k} - \tilde{S}^{(k)} \|^2 \right]
\end{split}
\eeq
Rearranging the terms yields:
\beq
\begin{split}
R_{k+1 } & \leq  
\EE [ V( \hs{k} ) ] - \gamma_{k+1}\big(  \rho \upsilon_{\min} +   \upsilon_{\max}^2  - \gamma_{k+1}\rho^2 \Lip{V} - b_{k+1}(\frac{\rho}{\beta}+ 2\gamma_{k+1} \rho^2) \big) \EE[ \|  \hmean_{k} \|^2 ] \\
& + \Big(  \underbrace{b_{k+1} (1 + \gamma \beta + 2 \gamma^2\rho^2 \Lip{\bss}^2 ) + \gamma^2\rho^2 \Lip{V} \Lip{\bss}^2}_{=b_k~~\text{since $k+1 \leq \ell(k)+m$}} \Big) \EE\Big[  \| \hs{k} - \hs{\ell(k)} \|^2 \Big]+ \tilde{\eta}^{(k+1)} + \tilde{\chi}^{(k+1)}
\end{split}
\eeq
where
\beq
\begin{split}
&  \tilde{\eta}^{(k+1)}  = \left( \gamma_{k+1}(\rho+\rho^2\gamma_{k+1}\Lip{V}) + b_{k+1} (\frac{\gamma_{k+1}\rho}{\beta}+ 2\gamma_{k+1}^2 \rho^2) \right) \EE\left[ \norm{\eta_{i_k}^{(k+1)}}^2 \right]\\
& \chi^{(k+1)} = \left( b_{k+1} (\frac{\gamma_{k+1}(1-\rho)}{\beta}+ 2\gamma_{k+1}^2 (1-\rho)^2) - \gamma_{k+1}(1-\rho - (1-\rho)^2\gamma_{k+1}\Lip{V}) \right) \\
& \tilde{\chi}^{(k+1)} = \chi^{(k+1)} \EE\left[\| \hs{k} - \tilde{S}^{(k)} \|^2 \right]
\end{split}
\eeq
This leads, using Lemma~\ref{lem:growth}, that for any $\gamma_{k+1}$, $\rho$ and $\beta$ such that $  \rho \upsilon_{\min} +   \upsilon_{\max}^2  - \gamma_{k+1}\rho^2 \Lip{V} - b_{k+1}(\frac{\rho}{\beta}+ 2\gamma_{k+1} \rho^2)  >0$,
\beq
\begin{split}
\upsilon_{\max}^2 \EE[ \| \grd V( \hs{k} ) \|^2 ]  \leq \EE[ \| \hs{k} - \os^{(k)} \|^2 ] \leq & \frac{  R_{k} - R_{k+1} }{ \gamma_{k+1}\big(  \rho \upsilon_{\min} +   \upsilon_{\max}^2  - \gamma_{k+1}\rho^2 \Lip{V} - b_{k+1}(\frac{\rho}{\beta}+ 2\gamma_{k+1} \rho^2) \big)}\\
& +\frac{ \tilde{\eta}^{(k+1)} + \tilde{\chi}^{(k+1)} }{ \gamma_{k+1}\big(  \rho \upsilon_{\min} +   \upsilon_{\max}^2  - \gamma_{k+1}\rho^2 \Lip{V} - b_{k+1}(\frac{\rho}{\beta}+ 2\gamma_{k+1} \rho^2) \big)}
\end{split}
\eeq

We first remark that 
\beq
\begin{split}
&\gamma_{k+1}\big(  \rho \upsilon_{\min} +   \upsilon_{\max}^2  - \gamma_{k+1}\rho^2 \Lip{V} - b_{k+1}(\frac{\rho}{\beta}+ 2\gamma_{k+1} \rho^2) \big)\\
& \geq  \frac{\gamma_{k+1} \rho}{c_1}\big(1  - \gamma_{k+1}c_1\rho \Lip{V} - b_{k+1}(\frac{c_1}{\beta}+ 2\gamma_{k+1} \rho c_1) \big)
\end{split}
\eeq
where $c_1 = \upsilon_{\min}^{-1}$.
By setting $\overline{L} = \max \{\Lip{\bss}, \Lip{V} \}$, $\beta = \frac{c_1 \overline{L}}{n^{1/3}}$, $\rho = \frac{\mu}{ c_1 \overline{L}  n^{2/3}}$, $m = \frac{n c_1^2}{2 \mu^2+\mu c_1^2}$ and $\{ \gamma_{k+1}\}$ any sequence of decreasing stepsizes in $(0,1)$, it can be shown that there exists $\mu \in (0,1)$, such that the following lower bound holds
\beq
\begin{split}
& 1  - \gamma_{k+1}c_1\rho \Lip{V} - b_{k+1}(\frac{c_1}{\beta}+ 2\gamma_{k+1} \rho c_1)
 \geq 1 - \frac{\mu}{n^{\frac{2}{3}}} - \overline{b}_0 \big( \frac{n^{\frac{1}{3}}}{\overline{L}} + \frac{2 \mu}{\overline{L} n^{\frac{2}{3}}} \big) \\
 & \geq 1 - \frac{\mu }{n^{\frac{2}{3}}} - \frac{ \Lip{V} \mu^2 }{c_1^2 n^{\frac{4}{3}}} \frac{ (1 + \gamma \beta + 2 \gamma^2 \Lip{\bss}^2 )^m - 1 }{ \gamma \beta + 2 \gamma^2 \Lip{\bss}^2 } \big( \frac{n^{\frac{1}{3}}}{\overline{L}} + \frac{2 \mu}{\overline{L} n^{\frac{2}{3}}} \big) \\
 & \overset{(a)}{\geq} 1 - \frac{\mu}{ n^{\frac{2}{3}}} - \frac{ \mu }{c_1^2 } (\rme-1) \big( 1 + \frac{2 \mu}{n} \big)
 \geq 1 - \mu - \mu(1+2 \mu) \frac{\rme-1}{c_1^2} \overset{(b)}{ \geq} \frac{1}{2}
 \end{split}
\eeq
where the simplification in (a) is due to
\beq
\frac{\mu}{n} \leq \gamma \beta + 2 \gamma^2 \Lip{\bss}^2 \leq \frac{\mu}{n} + \frac{2 \mu^2}{c_1^2 n^{\frac{4}{3}}} \leq \frac{\mu c_1^2 + 2 \mu^2}{c_1^2} \frac{1}{n}~~\text{and}~~(1 + \gamma \beta + 2 \gamma^2 \Lip{\bss}^2 )^m \leq \rme-1.
\eeq
and the required $\mu$ in (b) can be found by solving the quadratic equation.

Finally, these results yield:
\beq
\begin{split}
\upsilon_{\max}^2 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ]  \leq  \frac{2(R_0 - R_{K_{\sf max}})}{ \upsilon_{\min} \rho} + 2\sum_{k=0}^{K_{\sf max}-1}  \frac{ \tilde{\eta}^{(k+1)} + \tilde{\chi}^{(k+1)}}{ \upsilon_{\min} \rho}
 \end{split}
\eeq

Note that $R_0 = \EE[ V( \hs{0} ) ]$ and if $K_{\sf max}$ is a multiple of $m$, then $R_{\sf max} = \EE[ V( \hs{K_{\sf max}}) ]$. Under the latter condition, we have
\beq
 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ] \leq \frac{2 n^{2/3} \overline{L}}{\mu \upsilon_{\min}^2\upsilon_{\max}^2}\EE[ V( \hs{0} ) - V( \hs{K_{\sf max}}) ] + \frac{2 n^{2/3} \overline{L}}{\mu \upsilon_{\min}^2\upsilon_{\max}^2} \sum_{k=0}^{K_{\sf max}-1} \left[  \tilde{\eta}^{(k+1)} + \tilde{\chi}^{(k+1)}\right]
\eeq
This concludes our proof.

\end{proof}

\clearpage

\section{Proof of Theorem~\ref{thm:fisaem}}\label{app:theoremfisaem}
\begin{Theorem*}
Assume H\ref{ass:compact}-H\ref{ass:mcerror}.
Let $K_{\max }$ be a positive integer. 
Let $\left\{\gamma_{k}, k \in \mathbb{N}\right\}$ be a sequence of positive step sizes and consider the \FISAEM\ sequence $\left\{\hat{\bss}^{(k)}, k \in \mathbb{N}\right\}$ obtained with $\rho_{k+1}=\rho$ for any $k>0$.

Assume that $ \hat{\bss}^{(k)} \in \mathcal{S}$ for any $k \leq K_{\max }$. By setting $\alpha =\max\{2, 1+2\upsilon_{\min}\}$, $\overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}$, $\beta = \frac{c_1 \overline{L}}{n}$, $\rho = \frac{1}{n^{2/3}}$, $c_1(k\alpha-1) \geq c_1(\alpha-1) \geq 2$, $\alpha \geq 2$, we have the following bound:
\beq
\begin{split}
 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ]  \leq &\frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}\upsilon_{\max}^2}  \big[ V(\hat{\bss}^{(0)} )  - V(\hat{\bss}^{(K_{\sf max})}) \big]\\
 &   + \frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}\upsilon_{\max}^2} \sum_{k=0}^{K_{\sf max}-1} \left[ \Xi^{(k+1)}  +\Gamma_{k+1} \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2\right]\right]
\end{split}
\eeq

\end{Theorem*} 

\begin{proof}
Using the smoothness of $V$ and update \eqref{eq:fisaem}, we obtain:
\beq\label{eq:smoothfisaem}
\begin{split}
V( \hs{k+1} ) & \leq V( \hs{k} ) + \pscal{  \hs{k+1} - \hs{k}  }{ \grd V( \hs{k} ) } + \frac{ \Lip{V}}{2} \| \hs{k+1} - \hs{k} \|^2\\
& \leq V( \hs{k} ) - \gamma_{k+1} \pscal{  \hs{k} - \tilde{S}^{(k+1)} }{ \grd V( \hs{k} ) } + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \| \hs{k}  -  \tilde{S}^{(k+1)}\|^2
\end{split}
\eeq
Denote $\Hdrift_{k+1} \eqdef   \hs{k} - \tilde{S}^{(k+1)} $ the drift term of the \FISAEM\ update in \eqref{eq:rmstep} and  $\hmean_{k} = \hs{k} - \overline{\bss}^{(k)}$. Using Lemma~\ref{lem:drift_fisaem} and the additional following identity:
\beq
\EE\left[\big(\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}\big) - \EE[\overline{\bss}_{i_k}^{(k)} - \tilde{S}_{i_k}^{(t_{i_k}^k)}] \right] = 0
\eeq
 we have: 
 
 \beq
\begin{split}
& \EE[V( \hs{k+1} )]  \\
& \leq \EE[ V( \hs{k} )] - \gamma_{k+1}\rho \EE[\pscal{ \hmean_{k}  }{ \grd V( \hs{k} ) } -\gamma_{k+1}  \EE\left[\pscal{ \rho \EE[\eta_{i_k}^{(k+1)} |{\cal F}_k] + (1-\rho) \EE[ \hs{k} - \tilde{S}^{(k)}]}{ \grd V( \hs{k} ) }\right]\\
& + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \| \Hdrift_{k+1}\|^2\\
& \overset{(a)}{\leq}  -\upsilon_{\min}\gamma_{k+1}\rho \EE\left[\norm{\hmean_{k}}^2 \right]  -\gamma_{k+1}\EE\left[\norm{\grd V( \hs{k} ) }^2 \right] -\frac{\gamma_{k+1}\rho^2}{2} \xi^{(k+1)} - \frac{\gamma_{k+1}(1-\rho)^2}{2} \EE[\norm{ \hs{k} - \tilde{S}^{(k)}}^2]\\
& + \frac{\gamma_{k+1}^2 \Lip{V}}{2} \|\Hdrift_{k+1}\|^2\\
& \overset{(b)}{\leq}  -(\upsilon_{\min}\gamma_{k+1}\rho+\gamma_{k+1} \upsilon_{\max}^2) \EE\left[\norm{\hmean_{k}}^2 \right] -\frac{\gamma_{k+1}\rho^2}{2} \xi^{(k+1)} - \frac{\gamma_{k+1}(1-\rho)^2}{2} \EE[\norm{ \hs{k} - \tilde{S}^{(k)}}^2]\\
&+ \frac{\gamma_{k+1}^2 \Lip{V}}{2} \| \Hdrift_{k+1}\|^2
\end{split}
\eeq
where $\xi^{(k+1)}  =\EE\left[\norm{\EE[\eta_{i_k}^{(k+1)}|{\cal F}_k]  }^2 \right]$.
\paragraph{ Bounding $\EE\left[\|  \Hdrift_{k+1}  \|^2\right]$} 
Using Lemma~\ref{lem:aux1}, we obtain:
\beq\label{eq:finalfisaem}
\begin{split}
& \gamma_{k+1}(\upsilon_{\min}\rho+\upsilon_{\max}^2 - \gamma_{k+1}\rho^2 \Lip{V})  \EE\left[\norm{\hmean_{k}}^2 \right]\\
& \leq  \EE\left[V( \hs{k} ) - V( \hs{k+1} ) \right] +\tilde{\xi}^{(k+1)} + \left( (1-\rho)^2 \gamma_{k+1}^2 \Lip{V} - \frac{\gamma_{k+1}(1-\rho)^2}{2} \right)  \EE[\norm{ \hs{k} - \tilde{S}^{(k)}}^2]\\
& \frac{ \gamma_{k+1}^2\Lip{V}\rho^2\Lip{\bss}^2}{n} \sum_{i=1}^n \EE[ \| \hs{k} - \hs{t_i^k} \|^2 ]
\end{split}
\eeq
where $ \tilde{\xi}^{(k+1)} =  \gamma_{k+1}^2 \rho^2 \Lip{V}\EE[\|\eta_{i_k}^{(k+1)} \|^2] - \frac{\gamma_{k+1}\rho^2}{2} \xi^{(k+1)}$.
Next, we observe that
\beq\label{eq:auxdelta}
\frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k+1} - \hs{t_i^{k+1}} \|^2 ] = \frac{1}{n} \sum_{i=1}^n
\Big( \frac{1}{n} \EE[ \| \hs{k+1} - \hs{k} \|^2 ] + \frac{n-1}{n} \EE[ \| \hs{k+1} - \hs{t_i^k} \|^2 ]  \Big)
\eeq
where the equality holds as $i_k$ and $j_k$ are drawn independently. Next,
\beq
\begin{split}
& \EE[ \| \hs{k+1} - \hs{t_i^k} \|^2 ] \\
& = \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + \| \hs{k} - \hs{t_i^k} \|^2 + 2 \pscal{\hs{k+1} - \hs{k}}{\hs{k}- \hs{t_i^k}} \Big]
\end{split}
\eeq
Note that $\hs{k+1} - \hs{k} = -\gamma_{k+1} ( \hs{k} - \tilde{S}^{(k+1)}) = -\gamma_{k+1} \Hdrift_{k+1}$ and that in expectation we recall that $\EE[\Hdrift_{k+1}|{\cal F}_k] =  \rho \hmean_{k} + \rho\EE[\eta_{i_k}^{(k+1)}|{\cal F}_k] + (1-\rho) \EE[\tilde{S}^{(k)} - \hs{k}]$ where $\hmean_{k} = \hs{k} - \overline{\bss}^{(k)}$.
Thus, for any $\beta > 0$, it holds
\beq
\begin{split}
& \EE[ \| \hs{k+1} - \hs{t_i^k} \|^2 ] \\
& = \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + \| \hs{k} - \hs{t_i^k} \|^2 + 2 \pscal{\hs{k+1} - \hs{k}}{\hs{k}- \hs{t_i^k}} \Big]\\
& \leq  \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + (1+ \gamma_{k+1} \beta) \| \hs{k} - \hs{t_i^k} \|^2 +  \frac{\gamma_{k+1} \rho^2}{\beta} \| \hmean_{k} \|^2 +  \frac{\gamma_{k+1} \rho^2}{\beta} \EE[\norm{\eta_{i_k}^{(k+1)}}^2 ]\\
&  + \frac{\gamma_{k+1}(1- \rho)^2}{\beta}  \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2 \right]\Big]
\end{split}
\eeq
where the last inequality is due to the Young's inequality. 
Plugging this into \eqref{eq:auxdelta} yields:
\beq
\begin{split}
& \EE[ \| \hs{k+1} - \hs{t_i^k} \|^2 ] \\
& = \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + \| \hs{k} - \hs{t_i^k} \|^2 + 2 \pscal{\hs{k+1} - \hs{k}}{\hs{k}- \hs{t_i^k}} \Big]\\
& \leq  \EE \Big[ \| \hs{k+1} - \hs{k} \|^2 + (1+ \gamma_{k+1} \beta) \| \hs{k} - \hs{t_i^k} \|^2 +  \frac{\gamma_{k+1} \rho^2}{\beta} \| \hmean_{k} \|^2 +  \frac{\gamma_{k+1} \rho^2}{\beta} \EE[\norm{\eta_{i_k}^{(k+1)}}^2 ]\\
&  + \frac{\gamma_{k+1}(1- \rho)^2}{\beta}  \EE\left[\norm{\hs{k} - \tilde{S}^{(k)}}^2 \right]\Big]
\end{split}
\eeq

Subsequently, we have
\beq
\begin{split}
& \frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k+1} - \hs{t_i^{k+1}} \|^2 ] \\
& \leq \EE[  \| \hs{k+1} - \hs{k} \|^2 ] + \frac{n-1}{n^2} \sum_{i=1}^n \EE \Big[(1+ \gamma_{k+1} \beta) \| \hs{k} - \hs{t_i^k} \|^2 +  \frac{\gamma_{k+1} \rho^2}{\beta} \| \hmean_{k} \|^2 \\
& +  \frac{\gamma_{k+1} \rho^2}{\beta} \EE[\norm{\eta_{i_k}^{(k+1)}}^2 ]
  + \frac{\gamma_{k+1}(1- \rho)^2}{\beta}  \EE\left[\norm{\hs{k} - \tilde{S}^{(k)}}^2 \right]\Big]\Big]
\end{split}
\eeq
We now use Lemma~\ref{lem:aux1} on $\Big\| \hs{k+1} - \hs{k} \Big\|^2 = \gamma_{k+1}^2\Big\|  \hs{k} - \tilde{S}^{(k+1)} \Big\|^2$ and obtain:
\beq
\begin{split}
&  \frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k+1} - \hs{t_i^{k+1}} \|^2 ]\\
& \leq  \left(2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}\right) \EE\left[\norm{\overline{\bss}^{(k)}-\hs{k}}^2 \right]  + \sum_{i=1}^n \left( \frac{\gamma_{k+1}^2\rho^2 \Lip{\bss}^2}{n} + \frac{(n-1) (1+ \gamma_{k+1} \beta)}{n^2}  \right) \EE \left[ \| \hs{k} - \hs{t_i^k} \|^2 \right]\\
& + \gamma_{k+1} (1-\rho)^2 \left( 2\gamma_{k+1} + \frac{1}{\beta} \right)\EE\left[ \norm{\hs{k} - \tilde{S}^{(k)}}^2\right] + \left(2 \gamma_{k+1}^2 + \frac{\gamma_{k+1} \rho^2}{\beta} \right)\EE[\norm{\eta_{i_k}^{(k+1)}}^2 ]\\
& \leq  \left(2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}\right) \EE\left[\norm{\overline{\bss}^{(k)}-\hs{k}}^2 \right]  + \sum_{i=1}^n \left( \frac{ 1 - \frac{1}{n} +\gamma_{k+1}\beta+\gamma_{k+1}^2\rho^2 \Lip{\bss}^2 }{n}   \right) \EE \left[ \| \hs{k} - \hs{t_i^k} \|^2 \right]\\
& + \gamma_{k+1} (1-\rho)^2 \left( 2\gamma_{k+1} + \frac{1}{\beta} \right)\EE\left[ \norm{\hs{k} - \tilde{S}^{(k)}}^2\right] + \left(2 \gamma_{k+1}^2 + \frac{\gamma_{k+1} \rho^2}{\beta} \right)\EE[\norm{\eta_{i_k}^{(k+1)}}^2 ]
\end{split}
\eeq
Let us define
\beq
\Delta^{(k)} \eqdef \frac{1}{n} \sum_{i=1}^n \EE[ \| \hs{k} - \hs{t_i^{k}} \|^2 ]
\eeq
From the above, we get
\beq
\begin{split}
 \Delta^{(k+1)} \leq & \left( 1 - \frac{1}{n} +\gamma_{k+1}\beta+\gamma_{k+1}^2\rho^2 \Lip{\bss}^2\right) \Delta^{(k)} + \left(2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}\right) \EE\left[\norm{\overline{\bss}^{(k)}-\hs{k}}^2 \right]\\
& + \gamma_{k+1} (1-\rho)^2 \left( 2\gamma_{k+1} + \frac{1}{\beta} \right)\EE\left[ \norm{\hs{k} - \tilde{S}^{(k)}}^2\right] + \gamma_{k+1}\left(2 \gamma_{k+1} + \frac{ \rho^2}{\beta} \right)\EE[\norm{\eta_{i_k}^{(k+1)}}^2 ]
 \end{split}
\eeq

Setting $c_1 = \upsilon_{\min}^{-1}$, $\alpha =\max\{2, 1+2\upsilon_{\min}\}$, $\overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}$, $\gamma_{k+1} = \frac{1}{k \alpha c_1 \overline{L}}$, $\beta = \frac{c_1 \overline{L}}{n}$, $\rho = \frac{1}{n^{2/3}}$, $c_1(k\alpha-1) \geq c_1(\alpha-1) \geq 2$, $\alpha \geq 2$, we observe that
\beq
1 - \frac{1}{n} +\gamma_{k+1}\beta+\gamma_{k+1}^2\rho^2 \Lip{\bss}^2
 \leq 1 - \frac{1}{n} + \frac{1}{\alpha kn} + \frac{ 1 }{ \alpha^2 c_1^2 k^2 n^{\frac{4}{3}} } \leq 1 - \frac{c_1(k\alpha  - 1) - 1}{k\alpha n c_1 } \leq 1 - \frac{1}{k\alpha n c_1 }
\eeq
which shows that $1 - \frac{1}{n} +\gamma_{k+1}\beta+\gamma_{k+1}^2\rho^2 \Lip{\bss}^2  \in (0,1)$ for any $k >0$.
Denote $ \Lambda_{(k+1)} =\frac{1}{n} -\gamma_{k+1}\beta-\gamma_{k+1}^2\rho^2 \Lip{\bss}^2 $ and note that $\Delta^{(0)} = 0$, thus the telescoping sum yields:
\beq
\begin{split}
\Delta^{(k+1)} \leq & \sum_{ \ell = 0 }^k \omega_{k, \ell} \left(2 \gamma_{\ell+1}^2 \rho^2 + \frac{\gamma_{\ell+1}^2 \rho^2}{\beta}\right)  \EE\left[\norm{\overline{\bss}^{(\ell)}-\hs{\ell}}^2 \right]\\
& +\sum_{ \ell = 0 }^k \omega_{k, \ell} \gamma_{\ell+1} (1-\rho)^2 \left( 2\gamma_{\ell+1} +\frac{1}{\beta} \right)\EE\left[ \norm{\tilde{S}^{(\ell)} - \hs{\ell}}^2\right] + \sum_{ \ell = 0 }^k \omega_{k, \ell}\gamma_{\ell+1} \tilde{\epsilon}^{(\ell+1)}  
\end{split}
\eeq
where $ \omega_{k, \ell} =  \prod_{j = \ell +1}^k \Big( 1 -  \Lambda_{(j)} \Big)$ and $\tilde{\epsilon}^{(\ell+1)}   = \left(2 \gamma_{k+1} + \frac{ \rho^2}{\beta} \right)\EE[\norm{\eta_{i_k}^{(k+1)}}^2 ]$.

Summing on both sides over $k=0$ to $k = K_{\max}-1$ yields:
\beq
\begin{split}
\sum_{k=0}^{K_{\sf max}-1} \Delta^{(k+1)} & \leq \sum_{k=0}^{K_{\sf max}-1}  \frac{2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}}{\Lambda_{(k+1)}}  \EE\left[\norm{\overline{\bss}^{(k)}-\hs{k}}^2 \right]\\
& +\sum_{k=0}^{K_{\sf max}-1} \frac{\gamma_{k+1} (1-\rho)^2 \left( 2\gamma_{k+1} +\frac{1}{\beta} \right)}{\Lambda_{(k+1)}}\EE\left[ \norm{\hs{k} - \tilde{S}^{(k)}}^2\right] + \sum_{k=0}^{K_{\sf max}-1} \frac{\gamma_{k+1}}{\Lambda_{(k+1)}} \tilde{\epsilon}^{(k+1)}  
\end{split}
\eeq

We recall \eqref{eq:finalfisaem} where we have summed on both sides from $k=0$ to $k = K_{\max}-1$:
\beq\label{eq:finalboundfi}
\begin{split}
& \EE \big[ V(\hat{\bss}^{(K_{\sf max})}) - V(\hat{\bss}^{(0)} ) \big] \\
& \leq  \sum_{k=0}^{K_{\sf max}-1} \Big\{ \gamma_{k+1}( -(\upsilon_{\min}\rho+\upsilon_{\max}^2) + \gamma_{k+1}\rho^2 \Lip{V})  \EE\left[\norm{\hmean_{k}}^2 \right]   + \gamma^2 \Lip{V}\rho^2 \Lip{\bss}^2 \Delta^{(k)}\Big\}\\
& +  \sum_{k=0}^{K_{\sf max}-1} \Big\{ \tilde{\xi}^{(k+1)} + \left( (1-\rho)^2 \gamma_{k+1}^2 \Lip{V} - \frac{\gamma_{k+1}(1-\rho)^2}{2} \right)  \EE[\norm{ \hs{k} - \tilde{S}^{(k)}}^2]\Big\}\\
& \leq  \sum_{k=0}^{K_{\sf max}-1} \Big\{ \left[  -\gamma_{k+1}(\upsilon_{\min}\rho+\upsilon_{\max}^2) + \gamma_{k+1}^2\rho^2 \Lip{V} + \frac{\rho^2\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2\left(2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}\right)}{\Lambda_{(k+1)}} \right] \EE\left[\norm{\hmean_{k}}^2 \right]\Big\}\\
&   +  \sum_{k=0}^{K_{\sf max}-1} \Xi^{(k+1)}  +  \sum_{k=0}^{K_{\sf max}-1}\Gamma_{k+1} \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2\right]
\end{split}
\eeq

where 
$$
\Xi^{(k+1)} =\tilde{\xi}^{(k+1)} +\frac{\gamma_{k+1}^3\Lip{V}\rho^2\Lip{\bss}^2}{\Lambda_{(k+1)}} \tilde{\epsilon}^{(k+1)} 
$$ 
and 
$$
\Gamma_{k+1} =  \left( (1-\rho)^2 \gamma_{k+1}^2 \Lip{V} - \frac{\gamma_{k+1}(1-\rho)^2}{2} \right)  +\frac{\gamma_{k+1}^3\Lip{V}\rho^2\Lip{\bss}^2 (1-\rho)^2 \left( 2\gamma_{k+1} +\frac{1}{\beta} \right)}{\Lambda_{(k+1)}}  
$$
We now analyse the following quantity
\beq
\begin{split}
& -\gamma_{k+1}(\upsilon_{\min}\rho+\upsilon_{\max}^2) + \gamma_{k+1}^2\rho^2 \Lip{V} + \frac{\rho^2\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2\left(2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}\right)}{\Lambda_{(k+1)}}\\
& = - \gamma_{k+1}\left[ (\upsilon_{\min}\rho+\upsilon_{\max}^2) + \gamma_{k+1}\rho^2 \Lip{V} + \frac{\rho^2\gamma_{k+1} \Lip{V}\Lip{\bss}^2\left(2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}\right)}{\Lambda_{(k+1)}} \right]
\end{split}
\eeq

Furthermore, we recall that  $c_1 = \upsilon_{\min}^{-1}$, $\alpha =\max\{2, 1+2\upsilon_{\min}\}$, $\overline{L} = \max\{ \Lip{\bss} , \Lip{V} \}$, $\gamma_{k+1} = \frac{1}{k \alpha c_1 \overline{L}}$, $\beta = \frac{c_1 \overline{L}}{n}$, $\rho = \frac{1}{n^{2/3}}$, $c_1(k\alpha-1) \geq c_1(\alpha-1) \geq 2$, $\alpha \geq 2$. Then,
\beq\label{eq:stepsizeineq}
\begin{split}
& \gamma_{k+1}^2\rho^2 \Lip{V} + \frac{\rho^2\gamma_{k+1}^2 \Lip{V}\Lip{\bss}^2\left(2 \gamma_{k+1}^2 \rho^2 + \frac{\gamma_{k+1} \rho^2}{\beta}\right)}{\frac{1}{n} -\gamma_{k+1}\beta-\gamma_{k+1}^2\rho^2 \Lip{\bss}^2} \\
& \leq \frac{1}{k^2 \alpha^2 c_1^2 \overline{L} n^{4/3}} + \frac{\overline{L} (k^2\alpha^2 c_1^2  n^{4/3})^{-1} \big( \frac{2}{k^2 \alpha^2 c_1^2 \overline{L}^2 n^{4/3}} + \frac{1}{k \alpha c_1^2 \overline{L}^2 n^{1/3}} \big) }{\frac{1}{n} - \frac{1}{k \alpha n} - \frac{1}{k^2 \alpha^2 c_1^2 n^{4/3}} }\\
& = \frac{1}{k^2 \alpha^2 c_1^2 \overline{L} n^{4/3}} + \frac{  \overline{L}\big( \frac{2}{k^2 \alpha^2 c_1^2 \overline{L}^2 n^{4/3}} + \frac{1}{k \alpha c_1^2 \overline{L}^2 n^{1/3}} \big) }{ (k\alpha c_1  n^{1/3}) (k\alpha-1) c_1 - 1 } \\
& \overset{(a)}{\leq} \frac{1}{k^2\alpha^2 c_1^2 \overline{L} n^{4/3}} + \frac{ \frac{1}{k \alpha c_1^2 \overline{L} n^{1/3}} \big(\frac{2}{k\alpha n}  +1 \big) }{ 2(\alpha c_1  n^{1/3}) - 1 }\\
& \leq \frac{1}{k^2\alpha^2 c_1^2 \overline{L} n^{4/3} } + \frac{1}{k \alpha^2 c_1^3\overline{L} n^{2/3} }\\
& \leq \frac{1}{\alpha c_1 \overline{L} n^{2/3} }
\end{split}
\eeq
where $(a)$ is due to $c_1(k\alpha-1) \geq c_1(\alpha-1) \geq 2$ and $k\alpha c_1 n^{1/3} \geq 1$.
Also, since $ -\gamma_{k+1}(\upsilon_{\min}\rho+\upsilon_{\max}^2) \leq  -\gamma_{k+1}\rho \upsilon_{\min} = -$

Using the Lemma~\ref{lem:growth}, we know that $\upsilon_{\max}^2 \| \grd V( \hs{k} ) \|^2 \leq \| \hs{k} - \os^{(k)} \|^2$ and using \eqref{eq:stepsizeineq} on \eqref{eq:finalboundfi} yields:

\beq
\begin{split}
\upsilon_{\max}^2 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ]  \leq &  \frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}} \big[ V(\hat{\bss}^{(0)} )  - V(\hat{\bss}^{(K_{\sf max})}) \big]\\
&   + \frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}} \sum_{k=0}^{K_{\sf max}-1} \Xi^{(k+1)}  +  \sum_{k=0}^{K_{\sf max}-1}\Gamma_{k+1} \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2\right]
\end{split}
\eeq
proving the final bound on the gradient of the Lyapunov function:
\beq
\begin{split}
 \sum_{k=0}^{K_{\sf max}-1}\gamma_{k+1} \EE[ \| \grd V( \hs{k} ) \|^2 ]  \leq &\frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}\upsilon_{\max}^2}  \big[ V(\hat{\bss}^{(0)} )  - V(\hat{\bss}^{(K_{\sf max})}) \big]\\
 &   + \frac{\alpha  \overline{L} n^{2/3}}{\upsilon_{\min}\upsilon_{\max}^2} \sum_{k=0}^{K_{\sf max}-1} \Xi^{(k+1)}  +  \sum_{k=0}^{K_{\sf max}-1}\Gamma_{k+1} \EE\left[\norm{ \hs{k} - \tilde{S}^{(k)}}^2\right]
\end{split}
\eeq


\paragraph{ Bounding $ \EE\big[\norm{ \hs{k} -  \tilde{S}^{(k)} }^2\big] $} Remark that this term is the price we pay for the two time scale dynamics and corresponds to the gap between the two asynchronous updates (one is on  $\hs{k}$ and the other on $ \tilde{S}^{(k)}$).


\textcolor{red}{FIND AN UPPER BOUND TO THAT GAP}

%
%\beq\label{eq:rmstep}
%\textsf{SA-step}:~ \hat{\bss}^{(k+1)} =  \hat{\bss}^{(k)}  + \gamma_{k+1}(\tilde{S}^{(k+1)} - \hat{\bss}^{(k)} )
%\eeq
%\beq \label{eq:sestep}
%\textsf{Incremental-step}:~\tilde{S}^{(k+1)} = \tilde{S}^{(k)} + \rho_{k+1} \big( \StocEstep^{(k+1)}- \tilde{S}^{(k)}  \big),
%\eeq
%\begin{align}
%&\emph{(\ISAEM\ \citep{karimi2019non, kuhn2019properties})} & \StocEstep^{(k+1)} &= \StocEstep^{(k)} + {\textstyle \frac{1}{n}}\big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(\tau_{i_k}^k)} \big) \label{eq:isaem} \\
%&\emph{(\SAEMVR\ This paper )} &\StocEstep^{(k+1)} &= \tilde{S}^{(\ell(k))} +  \big( \tilde{S}_{i_k}^{(k)}  -\tilde{S}_{i_k}^{(\ell(k))}   \big) \label{eq:vrsaem}\\
%&\emph{(\FISAEM\ This paper )} &\StocEstep^{(k+1)} &= \overline{\StocEstep}^{(k)} + \big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(t_{i_k}^k)} \big) \label{eq:fisaem}\\
%&    &\overline{\StocEstep}^{(k+1)} &= \overline{\StocEstep}^{(k)} + n^{-1} \big( \tilde{S}_{j_k}^{(k)}  - \tilde{S}_{j_k}^{(t_{j_k}^k)} \big).
%%\emph{(SAGA-EM)} & ~~~~\StocEstep^{(k+1)} = s & \gamma_{k+1} =
%\end{align}

\end{proof}

\clearpage


\section{Practical Implementations of Two-Time-Scale EM Methods}
\subsection{Application on GMM}\label{app:gmm_update}

We first recognize that the constraint set for $\param$ is given by
\beq \textstyle
\Param = \Delta^M \times \rset^M.
\eeq
Using the partition of the sufficient statistics as
$S( y_i,z_i ) = ( S^{(1)}( y_i,z_i)^\top , S^{(2)}( y_i,z_i )^\top, S^{(3)}(y_i,z_i) )^\top  \in \rset^{M-1} \times \rset^{M-1} \times \rset$, the partition $\phi( \param ) = ( \phi^{(1)}( \param )^\top ,\phi^{(2)}( \param )^\top,\phi^{(3)}( \param ) )^\top \in \rset^{M-1} \times \rset^{M-1} \times \rset$ and the fact that $\indiacc{M}(z_i)= 1 - \sum_{m=1}^{M-1} \indiacc{m}(z_i)$, the complete data log-likelihood can be expressed as in \eqref{eq:exp} with
\beq \label{eq:gmm_exp}
\begin{split}
& s_{i,m}^{(1)} = \indiacc{m}(z_i), \quad \phi_m^{(1)}(\param) =   \left\{\log(\omega_m) -\frac{\mu_m^2}{2}\right\} - \left\{\log(1 - {\textstyle  \sum_{j=1}^{M-1}} \omega_j) - \frac{\mu_M^2}{2}\right\} \eqsp,\\
& s_{i,m}^{(2)} =   \indiacc{m}(z_i) y_i, \quad \phi^{(2)}_m(\param) =  {\mu_m} \eqsp, \quad s_i^{(3)} = y_i, \quad \phi^{(3)}(\param) = \mu_M \eqsp,
\end{split}
\eeq
and $\psi(\param) =   - \left\{\log(1 - \sum_{m=1}^{M-1} \omega_m) - \frac{\mu_M^2}{2 \sigma^2}\right\}$.
We also define for each $m \in \llbracket 1, M\rrbracket$,  $j \in \llbracket 1, 3 \rrbracket$, $s_{m}^{(j)} = n^{-1}\sum_{i=1}^n s_{i,m}^{(j)}$. 
Consider the following latent sample used to compute an approximation of the conditional expected value $\EE_{\param}[ \mathbbm{1}_{\{z_i=m\}} | y= y_{i} ]$:
\beq \label{eq:cexp}
z_{i,m} \sim \prob \left(z_i = m |y_i; \param\right)
\eeq
where $m \in \llbracket1,M\rrbracket$, $i \in \inter$ and $\param = ({\bm w}, {\bm{\mu}}) \in \Theta$.

In particular, given iteration $k+1$, the computation of the approximated quantity $ \tilde{S}_{i_k}^{(k)}$ during {\sf Incremental-step} updates, see \eqref{eq:sestep} can be written as
\beq\label{eq:stat_gmm}
 \tilde{S}_{i_k}^{(k)} = \big( \underbrace{ \indiacc{1}(z_{i_k,1}) , ..., \indiacc{M-1}(z_{i_k,M-1})}_{\eqdef \tilde{s}_{i_k}^{(1)}} , \underbrace{ \indiacc{1}(z_{i_k,1})y_{i_k} , ..., \indiacc{M-1}(z_{i_k,M-1})y_{i_k}}_{\eqdef \tilde{s}_{i_k}^{(2)}}, \underbrace{y_{i_k}}_{\eqdef \overline{\bss}_{i_k}^{(3)}( \param^{(k)} )} \big)^\top.
\eeq


%For the {\sf M}-step, let $\epsilon > 0$ be the regularization parameter, we define the regularizer, which is necessary to avoid any division by zero, as follows:
Recall that we have used the following regularizer:
\beq \textstyle \label{eq:regu}
\Pen( \param ) = \frac{\delta}{2} \sum_{m=1}^M \mu_m^2 - \epsilon \sum_{m=1}^M  \log ( \omega_m )  - \epsilon \log \big( 1 - \sum_{m=1}^{M-1} \omega_m \big) \eqsp,
\eeq
It can be shown that the regularized {\sf M-step} in \eqref{eq:mstep} evaluates to
\beq \label{eq:mstep_gmm}
\overline{\param} ( {\bm s} )
= \left(
\begin{array}{c}
( 1+\epsilon M )^{-1} \big( {s}_1^{(1)} + \epsilon, \dots,  {s}_{M-1}^{(1)} + \epsilon \big)^\top \vspace{.2cm}\\
 \big( ({s}_1^{(1)} + \delta )^{-1} {s}_1^{(2)}  , \dots, ({s}_{M-1}^{(1)} + \delta )^{-1} {s}_{M-1}^{(2)}  \big)^\top \vspace{.2cm} \\
  \big(1 - \sum_{m=1}^{M-1}s_m^{(1)} +  \delta\big)^{-1} \big( s^{(3)} - \sum_{m=1}^{M-1} s_m^{(2)} \big)
\end{array}
\right)
= \left(
\begin{array}{c}
\overline{\bm{\omega}} ( {\bm s}) \\
\overline{\bm{\mu}} ( {\bm s}) \\
\overline{\mu}_M ( {\bm s})
\end{array}
\right) \eqsp.
\eeq
where we have defined for all $m \in \llbracket1,M\rrbracket$ and $j \in \llbracket1,3\rrbracket$ , $ {s}_m^{(j)}  = n^{-1} \sum\nolimits_{i=1}^n s_{i,m}^{(j)}$.
%\toi{We need to define $\Sset$ here... and it is creating some trouble for us}
%It can be verified that with $\epsilon > 0$, the stochastic EM methods have $\hat{\bm{\omega}}^{(k)} \geq \delta {\bf 1}$ for some $\delta > 0$ 


\subsection{Model Assumptions (GMM example)}\label{app:gmm_assumptions}
We use the GMM example to illustrate the required assumptions.

Many practical models can satisfy the compactness of the sets as in Assumption~H\ref{ass:compact}
For instance, the GMM example satisfies \eqref{eq:compact} as the sufficient statistics are composed of indicator functions and observations as defined Section~\ref{app:gmm_update} Equation~\eqref{eq:gmm_exp}.

Assumptions H\ref{ass:expected} and H\ref{ass:reg} are standard for the curved exponential family models.
For GMM, the following (strongly convex) regularization $\Pen( \param )$ ensures H\ref{ass:reg}:
$$
\Pen( \param ) = \frac{\delta}{2} \sum_{m=1}^M \mu_m^2 - \epsilon \sum_{m=1}^M  \log ( \omega_m )  - \epsilon \log \big( 1 - \sum_{m=1}^{M-1} \omega_m \big) 
$$
since it ensures $\param^{(k)}$ is unique and lies in ${\rm int}( \Delta^M ) \times \rset^M$.
We remark that for H\ref{ass:expected}, it is possible to define the Lipschitz constant $\Lip{p}$ independently for each data $y_i$ to yield a refined characterization. 

Again, H\ref{ass:eigen} is satisfied by practical models. For GMM, it can be verified by deriving the closed form expression for $\operatorname{B}( \bss )$ and using H\ref{ass:compact}.

Under H\ref{ass:compact} and H\ref{ass:reg}, we have $\| \hat{\bm s}^{(k)} \| < \infty$ since $\Sset$ is compact and $\hat{\param}^{(k)} \in {\rm int}( \Param )$ for any $k \geq 0$ which thus ensure that the EM methods operate in a closed set throughout the optimization process.


\subsection{Algorithms updates}
In the sequel, recall that, for all $i \in \inter[n]$ and iteration $k$, the computed statistic $ \tilde{S}_{i_k}^{(k)}$ is defined by \eqref{eq:stat_gmm}.
At iteration $k$, the several E-steps defined by \eqref{eq:isaem} or \eqref{eq:vrsaem} and \eqref{eq:fisaem} leads to the definition of the quantity $\hat{\bss}^{(k+1)} $. For the GMM example, after the initialization of the quantity $\hat{\bss}^{(0)} = n^{-1} \sum\nolimits_{i=1}^n \overline{\bss}_i^{(0)} $, those E-steps break down as follows:

\textbf{Batch EM (EM):} for all $i \in \inter$, compute $\overline{\bss}_{i}^{(k)}$ and set 
\beq
\hat{\bss}^{(k+1)} = n^{-1} \sum\nolimits_{i=1}^n \overline{\bss}_i^{(k)} \eqsp.
\eeq
where $\overline{\bss}_i^{(k)} $ are computed using the exact conditional expected balue $\EE_{\param}[ \mathbbm{1}_{\{z_i=m\}} | y= y_{i} ]$:
\beq 
\widetilde{\omega}_m ( y_{i} ; \param ) \eqdef \EE_{\param}[ \mathbbm{1}_{\{z_i=m\}} | y= y_{i} ]
= \frac{ {\omega}_{m} \!~ {\rm exp}(-\frac{1}{2}( y_{i} - {\mu}_{i} )^2) }{  \sum_{j=1}^{M}{ {\omega}_{j} \!~ \exp(-\frac{1}{2}( y_{i} - {\mu}_{j} )^2)} } \eqsp,
\eeq

\textbf{Incremental EM (iEM):} draw an index $i_k$ uniformly at random on $\inter[n]$, compute $\overline{\bss}_{i_k}^{(k)}$ and set 
\beq
\hat{\bss}^{(k+1)} = \hat{\bss}^{(k)}  +\frac{1}{n} \big(  \overline{\bss}_{i_k}^{(k)} -  \overline{\bss}_{i_k}^{(\tau_i^k)}\big) =   n^{-1} \sum\nolimits_{i=1}^n \overline{\bss}_i^{(\tau_i^k)} \eqsp.
\eeq


\textbf{batch SAEM (SAEM):} draw an index $i_k$ uniformly at random on $\inter[n]$, compute $\overline{\bss}_{i_k}^{(k)}$ and set 
\beq
\hat{\bss}^{(k+1)} = \hat{\bss}^{(k)}(1 - \gamma_{k+1}) + \gamma_{k+1}\tilde{S}^{(k)} \eqsp.
\eeq
where $ = \frac{1}{n} \sum_{i=1}^n  \tilde{S}_{i}^{(k)}$ with $ \tilde{S}_{i}^{(k)}$ defined in \eqref{eq:stat_gmm}.

\textbf{Incremental SAEM (\ISAEM):} draw an index $i_k$ uniformly at random on $\inter[n]$, compute $\overline{\bss}_{i_k}^{(k)}$ and set 
\beq
\hat{\bss}^{(k+1)} = \hat{\bss}^{(k)}(1 - \gamma_{k+1})+ \gamma_{k+1} \big(\tilde{S}^{(k)} +\frac{1}{n}(\tilde{S}_{i_k}^{(k)}-\tilde{S}_{i_k}^{(\tau_i^k)})  \big) \eqsp.
\eeq

\textbf{Variance Reduced Two-Time-Scale EM (\SAEMVR):} draw an index $i_k$ uniformly at random on $\inter[n]$, compute $\overline{\bss}_{i_k}^{(k)}$ and set 
\beq
\hat{\bss}^{(k+1)} = \hat{\bss}^{(k)}(1 - \gamma_{k+1})+ \gamma_{k+1} \big(\tilde{S}^{(k)} (1 - \rho) + \rho (\tilde{S}^{(\ell(k))} +  \big( \tilde{S}_{i_k}^{(k)}  -\tilde{S}_{i_k}^{(\ell(k))}   \big)) \big) \eqsp.
\eeq

\textbf{Fast Incremental Two-Time-Scale EM (\FISAEM):} draw an index $i_k$ uniformly at random on $\inter[n]$, compute $\overline{\bss}_{i_k}^{(k)}$ and set 
\beq
\hat{\bss}^{(k+1)} = \hat{\bss}^{(k)}(1 - \gamma_{k+1})+ \gamma_{k+1} \big(\tilde{S}^{(k)} (1 - \rho) + \rho (\overline{\StocEstep}^{(k)} + \big( \tilde{S}_{i_k}^{(k)}  - \tilde{S}_{i_k}^{(t_{i_k}^k)}) \big) \eqsp.
\eeq


Finally, the $k$-th update reads $\hp{k+1} = \overline{\param} (\hat{\bss}^{(k+1)})$ where the function ${\bm s} \to \overline{\param}({\bm s})$ is defined by \eqref{eq:mstep_gmm}.



\end{document}
