\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bassily et~al.(2014)Bassily, Smith, and Thakurta]{basm2014}
R.~Bassily, A.~Smith, and A.~Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In \emph{2014 IEEE 55th Annual Symposium on Foundations of Computer
  Science}, pages 464--473. IEEE, 2014.

\bibitem[Bousquet and Elisseeff(2002)]{boel02}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Chaudhuri et~al.(2011)Chaudhuri, Monteleoni, and Sarwate]{chmo2011}
K.~Chaudhuri, C.~Monteleoni, and A.~D. Sarwate.
\newblock Differentially private empirical risk minimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Mar):\penalty0 1069--1109, 2011.

\bibitem[Chen and Gu(2018)]{chgu2018}
J.~Chen and Q.~Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock \emph{arXiv preprint arXiv:1806.06763}, 2018.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{cheli2019}
X.~Chen, S.~Liu, R.~Sun, and M.~Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duha11}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Dwork et~al.(2014)Dwork, Roth, et~al.]{dwro2014}
C.~Dwork, A.~Roth, et~al.
\newblock The algorithmic foundations of differential privacy.
\newblock \emph{Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 9\penalty0 (3--4):\penalty0 211--407, 2014.

\bibitem[Dwork et~al.(2015{\natexlab{a}})Dwork, Feldman, Hardt, Pitassi,
  Reingold, and Roth]{dwfe15}
C.~Dwork, V.~Feldman, M.~Hardt, T.~Pitassi, O.~Reingold, and A.~Roth.
\newblock Generalization in adaptive data analysis and holdout reuse.
\newblock \emph{arXiv preprint arXiv:1506.02629}, 2015{\natexlab{a}}.

\bibitem[Dwork et~al.(2015{\natexlab{b}})Dwork, Feldman, Hardt, Pitassi,
  Reingold, and Roth]{dwfe2015a}
C.~Dwork, V.~Feldman, M.~Hardt, T.~Pitassi, O.~Reingold, and A.~Roth.
\newblock Generalization in adaptive data analysis and holdout reuse.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2350--2358, 2015{\natexlab{b}}.

\bibitem[Dwork et~al.(2015{\natexlab{c}})Dwork, Feldman, Hardt, Pitassi,
  Reingold, and Roth]{dwfe2015b}
C.~Dwork, V.~Feldman, M.~Hardt, T.~Pitassi, O.~Reingold, and A.~Roth.
\newblock The reusable holdout: Preserving validity in adaptive data analysis.
\newblock \emph{Science}, 349\penalty0 (6248):\penalty0 636--638,
  2015{\natexlab{c}}.

\bibitem[Dwork et~al.(2015{\natexlab{d}})Dwork, Feldman, Hardt, Pitassi,
  Reingold, and Roth]{dwfe2015c}
C.~Dwork, V.~Feldman, M.~Hardt, T.~Pitassi, O.~Reingold, and A.~L. Roth.
\newblock Preserving statistical validity in adaptive data analysis.
\newblock In \emph{Proceedings of the forty-seventh annual ACM symposium on
  Theory of computing}, pages 117--126. ACM, 2015{\natexlab{d}}.

\bibitem[Ghadimi and Lan(2013)]{ghla2013}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hare2016}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  1225--1234, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{hezh2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Keskar and Socher(2017)]{keso2017}
N.~S. Keskar and R.~Socher.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock \emph{arXiv preprint arXiv:1712.07628}, 2017.

\bibitem[Kingma and Ba(2015)]{kiba15}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{In Proceedings of the 3rd International Conference on
  Learning Representations (ICLR)}, 2015.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krhi2009}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Kuzborskij and Lampert(2018)]{kula2018}
I.~Kuzborskij and C.~Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  2820--2829, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, Haffner, et~al.]{lebo1998}
Y.~LeCun, L.~Bottou, Y.~Bengio, P.~Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2019)Li, Luo, and Qiao]{lilu2019}
J.~Li, X.~Luo, and M.~Qiao.
\newblock On generalization error bounds of noisy gradient methods for
  non-convex learning.
\newblock \emph{arXiv preprint arXiv:1902.00621}, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, and Liu]{luxi2019}
L.~Luo, Y.~Xiong, and Y.~Liu.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Marcus and Marcinkiewicz(1993)]{mama1993}
B.~S. Marcus, Mitchell and M.~A. Marcinkiewicz.
\newblock Building a large annotated corpus of english: the penn treebank.
\newblock \emph{Computational linguistics-Association for Computational
  Linguistics}, 19\penalty0 (2):\penalty0 313--330, 1993.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{stni2018}
S.~Merity, N.~S. Keskar, and R.~Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mou et~al.(2018)Mou, Wang, Zhai, and Zheng]{mowa2018}
W.~Mou, L.~Wang, X.~Zhai, and K.~Zheng.
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock In \emph{Conference On Learning Theory}, pages 605--638, 2018.

\bibitem[Pensia et~al.(2018)Pensia, Jog, and Loh]{pejo2018}
A.~Pensia, V.~Jog, and P.-L. Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In \emph{2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 546--550. IEEE, 2018.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and Telgarsky]{rara2017}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703, 2017.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reka2018}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Robbins and Monro(1951)]{romo51}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shbe14}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Simonyan and Zisserman(2014)]{sizi2014}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Tieleman and Hinton(2012)]{tige12}
T.~Tieleman and G.~Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 2012.

\bibitem[Wang and Xu(2019)]{waxu2019}
D.~Wang and J.~Xu.
\newblock Differentially private empirical risk minimization with smooth
  non-convex loss functions: A non-stationary view.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 1182--1189, 2019.

\bibitem[Wang et~al.(2017)Wang, Ye, and Xu]{waye2017}
D.~Wang, M.~Ye, and J.~Xu.
\newblock Differentially private empirical risk minimization revisited: Faster
  and more general.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2722--2731, 2017.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{wawu19}
R.~Ward, X.~Wu, and L.~Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock In \emph{International Conference on Machine Learning}, pages
  6677--6686, 2019.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and Recht]{wiro17}
A.~C. Wilson, R.~Roelofs, M.~Stern, N.~Srebro, and B.~Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4148--4158, 2017.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and Kumar]{zare18}
M.~Zaheer, S.~Reddi, D.~Sachan, S.~Kale, and S.~Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9793--9803, 2018.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Tang, Yang, Cao, and Gu]{zhta18}
D.~Zhou, Y.~Tang, Z.~Yang, Y.~Cao, and Q.~Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018{\natexlab{a}}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Chen, and Banerjee]{zhch2018}
Y.~Zhou, S.~Chen, and A.~Banerjee.
\newblock Stable gradient descent.
\newblock In \emph{UAI}, pages 766--775, 2018{\natexlab{b}}.

\bibitem[Zou et~al.(2019)Zou, Shen, Jie, Zhang, and Liu]{zosh2019}
F.~Zou, L.~Shen, Z.~Jie, W.~Zhang, and W.~Liu.
\newblock A sufficient condition for convergences of adam and rmsprop.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 11127--11135, 2019.

\end{thebibliography}
