
In this section, we present the proof of Theorem \ref{thm: main_rmsprop}, \ref{thm: main_rmsprop_sparse}
\label{sec: thm2_proof}, \ref{thm: main_rmsprop_mini}.

\subsection{Proof of Theorem \ref{thm: main_rmsprop} and Theorem \ref{thm: main_rmsprop_sparse}}

\theomainrmsprop*

The proof of Theorem \ref{thm: main_rmsprop} consists of two parts: We first prove that the convergence rate of a gradient-based iterative algorithm is related to the gradient concentration error $\alpha$ and its iteration 
time $T$. Then we combine the concentration error $\alpha$ achieved by SAGD with DPG-Lap in Theorem \ref{thm: acc_basic} with the first part to complete the proof of Theorem \ref{thm: main_rmsprop}. 
To simplify the analysis, we first use $\alpha$ and $\xi$ to denote the generalization error $\sqrt{d} \sigma(1+\mu)$ and probability $d \beta+d \exp (-\mu)$ in Theorem \ref{thm: acc_basic} in the following analysis. The details are presented in the following theorem.
\begin{theo} \label{thm: simp_gen}
Let $\tilde \g_1,...,\tilde \g_T$ be the noisy gradients generated in Algorithm 1 through DPG oracle over $T$ iterations.
Then, for every $t \in [T]$, $\tilde \g_t$ satisfies
%Let $\g_t = \nabla f(\w_t)$ be the population gradient at parameter $w_t$. For $t \in [T]$, the noisy gradient $\tilde \g_t$ from DPG with oracle satisfies
\begin{equation} \nr
    \mathbb{P}\{\|\tilde \g_t - \g_t\| \geq \alpha \} \leq \xi
\end{equation}
where the values of $\alpha$ and $\xi$ are given in Section \ref{sec: pri}. 
\end{theo}
With the guarantee of Theorem \ref{thm: simp_gen}, we have the following theorem showing the convergence of SAGD.
\begin{theo} \label{thm: opt_rmsprop}
 let $\eta_t = \eta$. Further more assume that $\nu$, $\beta$ and $\eta$ are chosen such that the following conditions satisfied: $\eta \leq \frac{\nu}{2L}$. 
 Under the Assumption A1 and A2, the Algorithm 1 with $T$ iterations, $\phi_t(\tilde \g_1,...,\tilde \g_t) = \tilde \g_t$ and $ \v_t = \left(1-\beta_{2}\right) \sum_{i=1}^{t} \beta_{2}^{t-i} \tilde \g_{i}^{2}$ achieves:
\begin{equation}\label{eq: opt_rmsprop}
 \min_{t = 1,..., T}\|\nabla f(x_t)\|^2 \leq
    \left(G+\nu \right) \times \left(   \frac{ f(\w_1) - f^\star}{\eta T} + \frac{3 \alpha^2}{4\nu}\right)
\end{equation}
with probability at least $1-T\xi$.
\end{theo}
We can now tackle the proof of our result stated in Theorem \ref{thm: opt_rmsprop}.
\begin{proof}
Using the update rule of RMSprop, we have $\phi_t(\tilde \g_1,...,\tilde \g_t) = \tilde \g_t$ and $ \psi_t(\tilde \g_1,...,\tilde \g_t) = \left(1-\beta_{2}\right) \sum_{i=1}^{t} \beta_{2}^{t-i} \tilde \g_{i}^{2}$.
Thus, we can rewrite the update of Algorithm \ref{algo: StAda} as:
\begin{equation}
\begin{split} \nr
    \w_{t+1} = \w_t - \eta_t \tilde  \g_t /(\sqrt{\v_t} + \nu) \ \ \text{and} \ \  \v_t = \left(1-\beta_{2}\right) \sum_{i=1}^{t} \beta_{2}^{t-i} \tilde \g_{i}^{2}.
\end{split}
\end{equation}
Let $\Delta_t = \tilde \g_t - g_t$, we obtain:
\begin{align}
 & f (\w_{t+1}) \\
\leq & f(\w_t) + \left<\g_t, \w_{t+1}-\w_t\right> + \frac{L}{2} \left\|\w_{t+1}-\w_t \right\|^2\\ \nr
=& f(\w_t) -\eta_t \left<\g_t, \tilde \g_t/(\sqrt{\v_t} +\nu) \right> + \frac{L\eta_t^2}{2} \left\|\frac{\tilde \g_t}{(\sqrt{\v_t} +\nu)} \right\|^2\\ \nr
=& f(\w_t) -\eta_t \left<\g_t, \frac{\g_t +\Delta_t}{\sqrt{\v_t} +\nu} \right> + \frac{L\eta_t^2}{2}\left\|\frac{\g_t + \Delta_t}{\sqrt{\v_t} +\nu}\right\|^2 \\ \nr
\leq& f(\w_t) -\eta_t \left<\g_t, \frac{\g_t }{\sqrt{\v_t} +\nu}\right> -\eta_t \left<\g_t, \frac{\Delta_t }{\sqrt{\v_t} +\nu} \right> + L\eta_t^2\left(\left\|\frac{\g_t }{\sqrt{\v_t} +\nu}\right\|^2 + \left\|\frac{ \Delta_t}{\sqrt{\v_t} +\nu}\right\|^2   \right) \\ \nr
 \nr =& f(\w_t) -\eta_t \sum_{i=1}^d \frac{\left[\g_t\right]_i^2}{\sqrt{\v_t^i} +\nu} - \eta_t \sum_{i=1}^d \frac{\g_t^i \Delta_t^i}{\sqrt{\v_t^i} +\nu} +  L\eta_t^2\left(\sum_{i=1}^d\frac{[\g_t]_i^2 }{(\sqrt{\v_t^i} +\nu)^2} + \sum_{i=1}^d\frac{[\Delta_t]_i^2 }{(\sqrt{\v_t^i} +\nu)^2} 
    \right) \\ \nr
 \leq& f(\w_t) -\eta_t \sum_{i=1}^d \frac{[\g_t]_i^2}{\sqrt{\v_t^i} +\nu}  + \frac{\eta_t}{2}\sum_{i=1}^d \frac{[\g_t]_i^2 + [\Delta_t]_i^2}{\sqrt{\v_t^i} +  +\nu}  + \frac{L\eta_t^2}{\nu}\left(\sum_{i=1}^d\frac{[\g_t]_i^2 }{\sqrt{\v_t^i} +\nu} + \sum_{i=1}^d\frac{[\Delta_t]_i^2 }{\sqrt{\v_t^i} +\nu}
    \right) \\ \nr
 = & f(\w_t) - \left(\eta_t -\frac{\eta_t}{2} - \frac{L\eta_t^2}{\nu} \right)\sum_{i=1}^d\frac{[\g_t]_i^2 }{\sqrt{\v_t^i} +\nu}  +\left(  \frac{\eta_t}{2} + \frac{L\eta_t^2}{\nu} \right)\sum_{i=1}^d\frac{[\Delta_t]_i^2 }{\sqrt{\v_t^i} +\nu}  \nr
\end{align}

Given the parameter setting from the theorem, we see the following condition hold:
\begin{equation} \nr
    \frac{L\eta_t}{\nu} \leq \frac{1}{4}.
\end{equation}
Then we obtain
\begin{align} 
\nr f(\w_{t+1})& \leq f(\w_t) - \frac{\eta}{4} \sum_{i=1}^{d} \frac{\left[\mathbf{g}_{t}\right]_{i}^{2}}{\sqrt{\mathbf{v}_{t}^{i}}+\nu}+\frac{3 \eta}{4} \sum_{i=1}^{d} \frac{\left[\Delta_{t}\right]_{i}^{2}}{\sqrt{\v_t^i} + \nu} \\ 
& \leq f(\w_t) - \frac{\eta}{G + \nu} \|\g_t\|^2 + \frac{3\eta}{4\epsilon} \|\Delta_t\|^2 \nr
\end{align}
The second inequality follows from the fact that $0 \leq \v_t^i \leq G^2$. Using the telescoping sum and rearranging the inequality,  we obtain
\begin{align} \nr
\frac{\eta}{G + \nu} \sum_{t=1}^T \|\g_t\|^2  \leq f(\w_1) - f^\star + \frac{3\eta}{4\epsilon} \sum_{t=1}^T  \|\Delta_t\|^2
\end{align}

Multiplying with $\frac{G +\nu}{\eta T}$ on both sides and with the guarantee in Theorem 1 that $\|\Delta_t\| \leq \alpha$ with probability at least $1-\xi$,  we obtain 
\begin{equation} \nr
\min_{t = 1,..., T}\|\g_t\|^2 \leq \left(G+\nu \right) \times \left(   \frac{ f(\w_1) - f^\star}{\eta T} + \frac{3 \alpha^2}{4\nu}\right)
\end{equation}
with probability at least $1-T\xi$.\\


\end{proof}

\vspace{0.1in}

\textbf{Proof of Theorem \ref{thm: main_rmsprop}}:

\begin{proof}
First consider the gradient concentration bound achieved by SAGD (Theorem \ref{thm: acc_basic} and Theorem \ref{thm: acc_sparse}) that if $ \frac{2n\sigma^2}{G_1^2}\leq T \leq \frac{n^2 \sigma^4}{169 \ln(1/(\sigma \beta))G_1^2}$, we have 
\begin{equation}
\begin{split}
\mathbb{P}\left\{\|\tilde \g_t - \g_t\| \geq \sqrt{d}\sigma(1+\mu)\right\} \leq d\beta + d\exp(-\mu), \ \ \forall t \in [T].
\end{split} \nr
\end{equation}
Then bring the setting in Theorem \ref{thm: main_rmsprop} that $\sigma = 1/n^{1/3}$, let $\mu = \ln (1/\beta)$ and $\beta = 1/(d n^{5/3})$, we have
\begin{equation}
 \|\tilde \g_t - \g_t\|^2 \leq d(1+\ln d + \frac{5}{3}\ln n)^2/n^{2/3}    \nr
\end{equation}
with probability at least $1- 1/n^{5/3}$, when we set $T = n^{2/3}/\left(169G_1^2(\ln d + \frac{7}{3}\ln n)\right)$. 

Connect this result with Theorem \ref{thm: opt_rmsprop}, so that we have $\alpha^2 = d(1+\ln d + \frac{5}{3}\ln n)^2/n^{2/3}$ and $\xi = 1/n^{5/3}$. Bring the value $\alpha^2$, $\xi$ and $T = n^{2/3}/\left(169G_1^2(\ln d + \frac{7}{3}\ln n)\right)$ into \eqref{eq: opt_rmsprop}, with $\rho_{n,d} = O \left(\ln n + \ln d \right)$, we have
\begin{align*}
\min_{t = 1,..., T}\|\nabla f(\w_t)\|^2 \leq O\left( \frac{\rho_{n,d} \left(f(\w_1) - f^\star \right)}{n^{2/3}} \right) + O \left( \frac{d \rho_{n,d}^2}{n^{2/3}}\right)
\end{align*}
with probability at least $1-O\left(\frac{1}{\rho_{n,d} n}\right)$ which concludes the proof.
\end{proof}


\theormspropsparse*


\begin{proof}
The proof of Theorem \ref{thm: main_rmsprop_sparse} follows the proof of Theorem \ref{thm: main_rmsprop} by considering the case $C_{s} = T$.
\end{proof}


\subsection{Proof of Theorem \ref{thm: main_rmsprop_mini}} 

\theomini*

\begin{proof} When mini-batch SAGD calls \textbf{DPG} to access each batch $s_k$ with size $m$ for $T$ times, we have mini-batch SAGD preserves $(\frac{\sqrt{T \ln(1/\delta)} G_1}{m\sigma}, \delta)$-deferential privacy for each batch $s_k$. Now consider the gradient concentration bound achieved by DPG-Lap (Theorem \ref{thm: acc_basic}) that if $ \frac{2m\sigma^2}{G_1^2}\leq T \leq \frac{m^2 \sigma^4}{169 \ln(1/(\sigma \beta))G_1^2}$, we have 
\begin{align*}
\mathbb{P}\left\{\|\tilde \g_t - \g_t\| \geq \sqrt{d}\sigma(1+\mu)\right\} \leq d\beta + d\exp(-\mu), \ \ \forall t \in [T]. 
\end{align*}
Then bring the setting in Theorem \ref{thm: main_rmsprop_mini} that $\sigma = 1/(nm)^{1/6}$, let $\mu = \ln (1/\beta)$ and $\beta = 1/(d n^{5/3})$, we have
\begin{equation}
 \|\tilde \g_t - \g_t\|^2 \leq d(1+\ln d + \frac{5}{3}\ln n)^2/n^{2/3}   \nr
\end{equation}
with probability at least $1- 1/n^{5/3}$, when we set\\ $T = (mn)^{1/3}/\left(169G_1^2(\ln d + \frac{7}{3}\ln n)\right)$. 


Connect this result with Theorem \ref{thm: opt_rmsprop}, so that we have $\alpha^2 = d(1+\ln d + \frac{5}{3}\ln n)^2/(mn)^{1/3}$ and $\xi = 1/n^{5/3}$. Bring the value $\alpha^2$, $\xi$ and $T = (mn)^{1/3}/\left(169G_1^2(\ln d + \frac{7}{3}\ln n)\right)$ into \eqref{eq: opt_rmsprop}, with $\rho_{n,d} = O \left(\ln n + \ln d \right)$, we have
\begin{align*}
\min_{t = 1,..., T}\|\nabla f(\w_t)\|^2 \leq O\left( \frac{\rho_{n,d} \left(f(\w_1) - f^\star \right)}{(mn)^{1/3}} \right) + O \left( \frac{d \rho_{n,d}^2}{(mn)^{1/3}}\right)
\end{align*} 
with probability at least $1-O\left(\frac{1}{\rho_{n,d} n}\right)$. Here we complete the proof.

\end{proof}

