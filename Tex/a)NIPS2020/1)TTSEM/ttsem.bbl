\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allassonni{\`e}re et~al.(2007)Allassonni{\`e}re, Amit, and
  Trouv{\'e}]{allassonniere2007towards}
S.~Allassonni{\`e}re, Y.~Amit, and A.~Trouv{\'e}.
\newblock Towards a coherent statistical framework for dense deformable
  template estimation.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 69\penalty0 (1):\penalty0 3--29, 2007.

\bibitem[Baey et~al.(2016)Baey, Trevezas, and Courn{\`e}de]{baey2016nonlinear}
C.~Baey, S.~Trevezas, and P.-H. Courn{\`e}de.
\newblock A non linear mixed effects model of plant growth and estimation via
  stochastic variants of the em algorithm.
\newblock \emph{Communications in Statistics-Theory and Methods}, 45\penalty0
  (6):\penalty0 1643--1669, 2016.

\bibitem[Capp{\'e}(2011)]{cappe2011online}
O.~Capp{\'e}.
\newblock Online em algorithm for hidden markov models.
\newblock \emph{Journal of Computational and Graphical Statistics}, 20\penalty0
  (3):\penalty0 728--749, 2011.

\bibitem[Capp{\'e} and Moulines(2009)]{cappe2009line}
O.~Capp{\'e} and E.~Moulines.
\newblock On-line expectation--maximization algorithm for latent data models.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 71\penalty0 (3):\penalty0 593--613, 2009.

\bibitem[Carlin and Chib(1995)]{carlin1995bayesian}
B.~P. Carlin and S.~Chib.
\newblock Bayesian model choice via markov chain monte carlo methods.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 57\penalty0 (3):\penalty0 473--484, 1995.

\bibitem[Chakraborty and Das(2010)]{das2010Inferences}
A.~Chakraborty and K.~Das.
\newblock Inferences for joint modelling of repeated ordinal scores and time to
  event data.
\newblock \emph{Computational and mathematical methods in medicine},
  11\penalty0 (3):\penalty0 281--295, 2010.

\bibitem[Delyon et~al.(1999)Delyon, Lavielle, and Moulines]{delyon1999}
B.~Delyon, M.~Lavielle, and E.~Moulines.
\newblock Convergence of a stochastic approximation version of the em
  algorithm.
\newblock \emph{Ann. Statist.}, 27\penalty0 (1):\penalty0 94--128, 03 1999.
\newblock \doi{10.1214/aos/1018031103}.
\newblock URL \url{https://doi.org/10.1214/aos/1018031103}.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977Maximum}
A.~P. Dempster, N.~M. Laird, and D.~B. Rubin.
\newblock Maximum likelihood from incomplete data via the {EM} algorithm.
\newblock \emph{Journal of the royal statistical society. Series B
  (methodological)}, pages 1--38, 1977.

\bibitem[Hughes(1999)]{hughes1999mixed}
J.~P. Hughes.
\newblock Mixed effects models with censored data with application to hiv rna
  levels.
\newblock \emph{Biometrics}, 55\penalty0 (2):\penalty0 625--629, 1999.

\bibitem[Hull(1994)]{hull1994database}
J.~J. Hull.
\newblock A database for handwritten text recognition research.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 16\penalty0 (5):\penalty0 550--554, 1994.

\bibitem[Karimi et~al.(2019)Karimi, Wai, Moulines, and
  Lavielle]{karimi2019global}
B.~Karimi, H.-T. Wai, {\'E}.~Moulines, and M.~Lavielle.
\newblock On the global convergence of (fast) incremental expectation
  maximization methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2833--2843, 2019.

\bibitem[Kuhn and Lavielle(2004)]{kuhn2004coupling}
E.~Kuhn and M.~Lavielle.
\newblock Coupling a stochastic approximation version of em with an mcmc
  procedure.
\newblock \emph{ESAIM: Probability and Statistics}, 8:\penalty0 115--131, 2004.

\bibitem[Kuhn et~al.(2019)Kuhn, Matias, and Rebafka]{kuhn2019properties}
E.~Kuhn, C.~Matias, and T.~Rebafka.
\newblock Properties of the stochastic approximation em algorithm with
  mini-batch sampling.
\newblock \emph{arXiv preprint arXiv:1907.09164}, 2019.

\bibitem[Liang and Klein(2009)]{liang2009online}
P.~Liang and D.~Klein.
\newblock Online em for unsupervised models.
\newblock In \emph{Proceedings of human language technologies: The 2009 annual
  conference of the North American chapter of the association for computational
  linguistics}, pages 611--619, 2009.

\bibitem[Maire et~al.(2016)Maire, Moulines, and Lefebvre]{maire2016online}
F.~Maire, E.~Moulines, and S.~Lefebvre.
\newblock Online em for functional data, 2016.
\newblock URL \url{http://arxiv.org/abs/1604.00570}.
\newblock cite arxiv:1604.00570v1.pdf.

\bibitem[McCulloch(1997)]{mcculloch1997maximum}
C.~E. McCulloch.
\newblock Maximum likelihood algorithms for generalized linear mixed models.
\newblock \emph{Journal of the American statistical Association}, 92\penalty0
  (437):\penalty0 162--170, 1997.

\bibitem[McLachlan and Krishnan(2007)]{mclachlan2007algorithm}
G.~McLachlan and T.~Krishnan.
\newblock \emph{The {EM} algorithm and extensions}, volume 382.
\newblock John Wiley \& Sons, 2007.

\bibitem[Meyn and Tweedie(2012)]{meyn2012markov}
S.~P. Meyn and R.~L. Tweedie.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Neal and Hinton(1998)]{neal1998view}
R.~M. Neal and G.~E. Hinton.
\newblock A view of the {EM} algorithm that justifies incremental, sparse, and
  other variants.
\newblock In \emph{Learning in graphical models}, pages 355--368. Springer,
  1998.

\bibitem[Nguyen et~al.(2020)Nguyen, Forbes, and McLachlan]{nguyen2020mini}
H.~D. Nguyen, F.~Forbes, and G.~J. McLachlan.
\newblock Mini-batch learning of exponential family finite mixture models.
\newblock \emph{Statistics and Computing}, pages 1--18, 2020.

\bibitem[Reddi et~al.(2016)Reddi, Sra, P{\'o}czos, and Smola]{reddi2016fast}
S.~J. Reddi, S.~Sra, B.~P{\'o}czos, and A.~Smola.
\newblock Fast incremental method for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1603.06159}, 2016.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Wei and Tanner(1990)]{wei1990monte}
G.~C. Wei and M.~A. Tanner.
\newblock A monte carlo implementation of the em algorithm and the poor man's
  data augmentation algorithms.
\newblock \emph{Journal of the American statistical Association}, 85\penalty0
  (411):\penalty0 699--704, 1990.

\bibitem[Zhu et~al.(2017)Zhu, Wang, Zhai, and Gu]{zhu2017high}
R.~Zhu, L.~Wang, C.~Zhai, and Q.~Gu.
\newblock High-dimensional variance-reduced stochastic gradient
  expectation-maximization algorithm.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 4180--4188. JMLR. org, 2017.

\end{thebibliography}
