%% LaTeX Template for ISIT 2020
%%
%% by Stefan M. Moser, October 2017
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

\documentclass[conference,letterpaper]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex
\topmargin 0pt\headheight 0pt\headsep 2pt\textheight 660pt\footskip
30pt\oddsidemargin 10pt\textwidth 440pt\marginparsep 10pt
\usepackage{mathrsfs,amsmath,amssymb,amsfonts}
\numberwithin{equation}{section}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{stmaryrd}
\usepackage{color, verbatim}

%\usepackage[notref,notcite]{showkeys}
%\usepackage{refcheck}

\newcommand{\lbl}{\label}
%\newcommand{\lbl}[1]{\hspace{1cm} \underline{({#1})} \label{#1}}
\newcommand{\proof}{{\it Proof. \ }}

\newcommand{\ignore}[1]{}{}
%\newcommand{\ignore}[1]{#1}
\newcommand{\be}{\begin{equation}}               %\be=\begin{equation}
\newcommand{\ee}{\end{equation}}                 %\ee=\end{equation}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\var}{\mbox{Var}}
\newcommand{\etab}{\kappa}
\newcommand{\B}{\alpha}
\newcommand{\D}{\beta}
\newcommand{\bd}{\bold}

\newcommand{\beaa}{\begin{eqnarray*}}
\newcommand{\eeaa}{\end{eqnarray*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}

%\newcommand{\binom}[2]{\left(
%\begin{array}{c} #1 \\ #2 \end{array} \right)}
\newcommand{\bis}[2]{
 \begin{array}{c} \ds #1 \\ \vs{-2.5}\\  \Large{#2} \end{array} }
%\newcommand{\bis}[2]{
%\begin{array}{c} \ds #1 \\ \footnotesize{#2} \end{array} }

\newcommand{\noi}{\noindent}
\newcommand{\beqn}{\begin{eqnarray}}             %\beqn=\begin{eqnarray}
\newcommand{\eeqn}{\end{eqnarray}}               %\eeqn=\end{eqnarray}
\newcommand{\beq}{\begin{eqnarray*}}             %\beq=\begin{eqnarray*}
\newcommand{\eeq}{\end{eqnarray*}}               %\eeq=\end{eqnarray*}
\newcommand{\mb}{\mbox}                          %\mb=\mbox
\newcommand{\lb}{\left\{ }                       %\lb=\left\{
\newcommand{\rb}{\right\} }                      %\rb=\right\}
\newcommand{\re}{\right.}                        %\re=\right.
\newcommand{\nn}{\nonumber}
\newcommand{\ds}{\displaystyle}



\newcommand{\bbox}{\nobreak\quad\vrule width4pt depth2pt height4pt}
\newcommand{\eq}[1]{$(\ref{#1})$}

\newcommand{\al}{\alpha}                         %\al=\alpha
\newcommand{\ga}{\gamma}                         %\ga=\gamma
\newcommand{\Ga}{\Gamma}                         %\Ga=\Gamma
\newcommand{\ep}{\epsilon}                       %\ep=\epilon
\newcommand{\vp}{\varepsilon}                   %\vp=\varepsilon
\newcommand{\la}{\lambda}                        %\la=\lambda
\newcommand{\La}{\Lambda}                        %\La=\Lambda
%\newcommand{\th}{\theta}                         %\th=\theta
\newcommand{\bt}{\beta}                           %\bt=\beta
\newcommand{\sg}{\sigma}                         %\sg=\sigma
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\ssb}{\scriptstyle \footnotesize % \scriptsize
                 \begin{array}{c}}
\newcommand{\esb}{\end{array}}
\newcommand{\ra}{\rightarrow}                    %\ra=\rightarrow
\newcommand{\lra}{\longrightarrow}               %\lra=\longrightarrow
\newcommand{\Ra}{\Rightarrow}                    %\Ra=\Rightarrow
\newcommand{\Lra}{\Longrightarrow}               %\Lra=\Longrightarrow
\newcommand{\sta}{\stackrel}                    %\sta=\stackrel
\newcommand{\xn}{$\{X_n, \, n \geq 1\} \ $}
\newcommand{\wip}{weak invariance principle}
\newcommand{\sa}{strong approximation}
\newcommand{\clt}{central limit theorem}
\newcommand{\lil}{law of the iterated logarithm}
\newcommand{\iid}{independent and identically distributed random variables}
\newcommand{\irv}{independent random variables \ }
\newcommand{\inrv}{independent normal random variables \ }
\newcommand{\Let}[1]{Let $\{X_n, n \geq 1 \}$ be a stationary  $#1$-mixing
          sequence of random variables \ }
\newcommand{\Les}[1]{Let $\{X_n, n \geq 1 \}$ be a  $#1$-mixing
          sequence of random variables \ }
\newcommand{\Leta}[1]{Let $\{X_n, n \geq 1 \}$ be an  $#1$-mixing
          sequence of random variables \ }
\newcommand{\CsR}{Cs\"org\H{o} and R\'ev\'esz}
\newcommand{\kmt}{Koml\'os, Major and Tusn\'ady}
\newcommand{\A}{{\cal A}}
\newcommand{\C}{{\cal C}_{bd}}
\baselineskip=7.0mm
\newtheorem{theorem}{{\sc Theorem}}[section]
 \newtheorem{prop}{Proposition}[section]
 \newtheorem{coro}{{\sc Corollary}}[section]
 \newtheorem{lemma}{{\sc Lemma}}[section]
 \newtheorem{remark}{{\sc Remark}}[section]
\newtheorem{definition}{{\sc Definition}}[section]
\newtheorem{example}{{\sc Example}}[section]
 \newcommand{\vs}{\vspace{.3cm}}
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}%%%%%%
% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

% ------------------------------------------------------------
\begin{document}
\title{Positivity of Hadamard Powers of Random Matrices}
% \footnote{ }}
\author{Tiefeng Jiang\thanks{ \textsl{E-mail address}:
\texttt{jiang040@umn.edu}} and  Ping Li\thanks{ \textsl{E-mail address}:
\texttt{liping11@baidu.com}} }
\date{\small \it University of Minnesota and Cognitive Computing Lab, Baidu Research, USA}
\maketitle
%\mbox{}\hrule\mbox{}\\[0.5cm]
%\noindent\textbf{Abstract}
%\\[-0.2cm]^^L
\begin{abstract}
\noindent The paper studies
\end{abstract}



\noindent\textbf{Keywords:} Hadamard matrix function, Hadamard power, non-negative definite matrix, positive definite matrix, random matrix.

\medskip

\noindent\textbf{MSC(2010):} Primary .\\
[0.5cm]
%\mbox{}\hrule\mbox{}
%------------------------------------------------------------------------------------------

%\newpage

\section{Introduction}
Random matrices 


\section{Main results and discussions}\lbl{Section_horse}

Let $f(x)$ be a real-valued function defined on $\mathbb{R}.$ Let $\bd{A}=(a_{ij})$ be an $n\times n$ matrix, where $a_{ij}$'s are real numbers.  Define $f: \bd{A} \rightarrow f(\bd{A)}=(f(a_{i j}))$. We call $f(\bd{A})$  a Hadamard function to distinguish it from the usual notion of matrix functions. In particular, if $\alpha>0$ and  $f(x)=x^{\alpha}$, then we call  $\bd{A}^{(\alpha)}:=f(\bd{A})$ the Hadamard power of $\alpha$. Here we need to pay attention to the domain of the function $f(x)=x^{\alpha}$. If $\alpha>0$ is an integer, the function is defined for every $x\in \mathbb{R}.$ If $\alpha>0$ is not an integer, the function $f(x)=x^{\alpha}$ is defined only on $[0, \infty).$ By the Schur product theorem, it is known that $\bd{A}^{(\alpha)}$ is a positive definite matrix if $\bd{A}=(a_{ij})$ is a positive definite matrix and $\alpha =1,2, \cdots$; see, for example, Theorem 5.2.1 from Horn and Johnson (1991). In fact, we know more about this conclusion. For positive definite matrices $\bd{U}=(u_{ij})_{n\times n}$ and $\bd{V}=(v_{ij})_{n\times}$, set $\bd{U}\circ \bd{V}=(u_{ij}v_{ij})_{n\times n}$. Then  $\lambda_{min}(\bd{U})\cdot\min_{1\leq i \leq n}{v_{ii}}\leq \lambda_i(\bd{U}\circ \bd{V})\leq \lambda_{max}(\bd{U})\cdot\max_{1\leq i \leq n}{v_{ii}}$ for each $1\leq i \leq n$; see, for example, Schur (1911) or Theorem 5.3.4 from Horn and Johnson (1991). If $\alpha$ is a positive integer, then  $\bd{A}^{(\alpha)}=\bd{A}\circ\cdots\circ\bd{A}$ from which there are $\alpha$ many $\bd{A}$ in the product. Thus, $\lambda_{min}(\bd{A})>0$ by induction  if $\bd{A}$ is positive definite.

\subsection{Some Known Results}\lbl{known_results}

 Let $a \in (0, \infty]$ and $f(x): (0, a)\to \mathbb{R}$. We say $f(x)$ is {\it absolutely monotonic} on  $(0, a)$ if $f^{(k)}(x)\geq 0$ for every $x\in (0, \alpha)$ and $k=0,1,2, \cdots.$ The following general conclusion can be seen from, for example,   Schoenberg (1942),  Vasudeva (1979) and Hiai (2009).

\begin{theorem} Assume $a \in (0, \infty]$ and $f(x)$ is a real function defined on $(-a, a)$.  Then $f(\bd{A})$ is non-negative definite for  every  non-negative definite matrix  $\bd{A}$ with  entries  in $(-a, a)$ if  and  only  if $f(x)$ is  analytic  and absolutely monotonic on $(0, a).$
\end{theorem}


\begin{theorem} (Theorem 6.3.7 from Horn and Johnson, 1991)
	Let $f(\cdot)$ be an $(n-1)$-times continuously differentiable real valued function on $(0,\infty)$, and suppose that the Hadamard function $f(\bd{A})=(f(a_{ij}))$ is non-negative definite for every non-negative definite matrix $\bd{A}$ that has positive entries. Then $f^{(k)}(t)\geq 0$ for all $t\in(0,\infty)$ and all $k=0,1,\cdots,n-1$.
\end{theorem}
\begin{coro}(Corollary 6.3.8 from Horn and Johnson, 1991)
	Let $0<\alpha<n-2$, $\alpha$ not an integer. There is some $n\times n$ non-negative definite matrix $\bd{A}$ with positive entries such that the Hadamard power $\bd{A}^{(\alpha)}=(a_{ij}^\alpha)$ is not non-negative definite.
\end{coro}
\begin{theorem} (Theorem 6.3.9 from Horn and Johnson, 1991)
	Let $\bd{A}=(a_{ij})$ be a non-negative definite matrix with nonnegative entries. If $\alpha \geq n-2$, then the Hadamard power $\bd{A}^{(\alpha)}$ is non-negative definite. Furthermore, the lower bound $n=2$ is, in general, the best possible.
\end{theorem}


\subsection{New Results}\lbl{Section_main}
To make statement of our results clearer, we will use the following notation.  For a matrix $\bd{M}$, we write $\bd{M}>0$ if $\bd{M}$ is positive definite; $\bd{M}\geq 0$ if $\bd{M}$ is non-negative definite; $\bd{M}\ngeqslant 0$ if $\bd{M}$ is not  non-negative definite.


\begin{example} Consider  $3\times 3$ matrix
\beaa
\bd{M}=
\begin{pmatrix}
1 & \frac{1}{2} & 0\\
\frac{1}{2} & 1 & \frac{1}{2}\\
0 & \frac{1}{2} & 1
\end{pmatrix}
.
\eeaa
Its Hadamard power of $\alpha$ is given by
\beaa
\bd{M}^{(\alpha)}=
\begin{pmatrix}
1 & \frac{1}{2^{\alpha}} & 0\\
\frac{1}{2^{\alpha}} & 1 & \frac{1}{2^{\alpha}}\\
0 & \frac{1}{2^{\alpha}} & 1
\end{pmatrix}
.
\eeaa
 Then
\beaa
\mbox{det}(\bd{M}^{(\alpha)})=1-\frac{2}{4^{\alpha}}.
\eeaa
Therefore, $\bd{M}=\bd{M}^{(1)}>0$. However, $\bd{M}^{(\alpha)}\ngeqslant 0$ if $\alpha \in (0, \frac{1}{2}).$

For any $n\geq 3$, define $\bd{M}_n=\bd{M}$ for $n=3$ and
\beaa
\bd{M}_n=
\begin{pmatrix}
\bd{M} & \bd{0}\\
\bd{0} & \bd{I}_{n-3}
\end{pmatrix}
,\ \ \ n\geq 4.
\eeaa
Then,  the matrix $\bd{M}_n$ is positive definite as $n\geq 3$. However, the Hadamard power matrix $\bd{M}_n^{(\alpha)}\ngeqslant 0$  as $\alpha \in (0, \frac{1}{2}).$
\end{example}



\begin{example}\lbl{good_example} Consider $4\times 4$ matrix  $\bd{M}=\bd{a}\bd{a}'+\bd{b}\bd{b}' + 10^{-4}\bd{I}_4$, where $\bd{a}^T=(1,1,1,1)$ and $\bd{b}^T=(0,1,2,3)$. The term $10^{-4}\bd{I}_4$ purely ensures the positivity of $\bd{M}$. The matrix $\bd{a}\bd{a}'+\bd{b}\bd{b}'$  is of rank $2$. Obviously, $\bd{M}>0$. It is easy to check that
\beaa
\mbox{det}(\bd{M}^{1.1})=-0.000118654.
\eeaa
Hence, $\bd{M}^{1.1}\ngeqslant 0$.
%\end{example}
%\begin{example}\lbl{day}. Let $\bd{M}$ be the $4\times 4$ matrix as in Example \ref{good_example}.
Define $\bd{M}_n=\bd{M}$ for $n=4$ and
\beaa
\bd{M}_n=
\begin{pmatrix}
\bd{M} & \bd{0}\\
\bd{0} & \bd{I}_{n-4}
\end{pmatrix}
\eeaa
for $n\geq 5.$
Then, $\bd{M}_n>0$ if $n\geq 4$. However, the Hadamard power matrix $\bd{M}_n^{1.1}\ngeqslant 0$.
\end{example}

\begin{theorem}\lbl{milk} Assume $n\geq 4$.   Let $\bd{M}=(\xi_{ij})$ be an $n\times n$ symmetric matrix, where $\{\xi_{ij};\, 1\leq i \leq j \leq n\}$ are independent random variables. Suppose all of the supports of $\xi_{ij}$'s contain  a common interval $[u, v]$ for some $v>u>0$. Then there exists $\alpha \in (1, 2)$ for which
\beaa
P\big(\bd{M}\geq 0\ \mbox{and}\ \bd{M}^{(\alpha)} \ngeqslant 0\big)>0.
\eeaa
\end{theorem}

\begin{theorem}\lbl{waffle} Assume $n\geq 4$.   Let $\bd{X}=(x_{ij})$ be an $n\times p$  matrix, where $\{x_{ij};\, 1\leq i \leq n, 1\leq j \leq p\}$ are independent random variables. Suppose all of the supports of $\xi_{ij}$'s contain  a common interval $[u, v]$ for some $v>u>0$. Then there exists $\alpha \in (1, 2)$ such that
\beaa
P\big(\bd{X}^T\bd{X}>0\ \mbox{and}\ (\bd{X}^T\bd{X})^{(\alpha)}\ngeqslant 0\big)>0.
\eeaa
\end{theorem}

\begin{theorem}\lbl{my_country} Let $\bd{A}=(a_{ij})$ be an $n\times n$ matrix of which the entries are non-negative. Assume $a_{ii}\geq  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n.$ Then $\bd{A} \geq 0$ and $\bd{A}^{(\alpha)}\geq  0$ for all $\alpha\geq 1.$ The conclusion still holds if all three ``$\geq$" are replaced by ``$>$", respectively.
\end{theorem}


\subsection{Proofs}\lbl{Proofs}
\begin{lemma}\lbl{what_is} For any $n\geq 4$, there exist $\alpha\in (1,2)$, $\delta>0$ and $n\times n$ symmetric matrix  $\bd{M}=(m_{ij})$ with $m_{ij}\geq 0$ for all $1\leq i, j \leq n$ such that the following holds.
%the $n\times n$ matrix $\bd{M}^{(\alpha)}=(m_{ij}^{(\alpha)})$ has the following property.

(i) $\bd{M}=(m_{ij})>0$  for every $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and $1\leq i, j \leq n.$

(ii) $\bd{M}^{(\alpha)}=(m_{ij}^{\alpha})\ngeqslant 0$  for any $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and any $1\leq i, j \leq n.$
\end{lemma}
\noindent\textbf{Proof of Lemma \ref{what_is}}. For any $n\times n$ symmetric matrix $\bd{M}=(m_{ij})$, let  $\|\bd{M}\|$ be the spectral norm of $\bd{M}$. We use $\lambda_1(\bd{M})\geq \lambda_2(\bd{M})\geq \cdots \geq \lambda_n(\bd{M})$ to denote  the eigenvalues of $\bd{M}$.  Evidently, $\|\bd{M}\|\leq (\sum_{1\leq i, j \leq n}|m_{ij}|^2)^{1/2}$. Let $\bd{M}_1=(m_{ij})$ and $\bd{M}_2=(\tilde{m}_{ij})$ be $n\times n$ symmetric matrices. The Weyl's perturbation theorem [see, e.g., Horn and Johnson (1985)] says that
$\max_{1\leq i \leq n}|\lambda_i(\bd{M}_1)-\lambda_i(\bd{M}_2)|\leq \|\bd{M}_1-\bd{M}_2\|$.  Therefore,
\bea\lbl{flower_could}
\max_{1\leq i \leq n}|\lambda_i(\bd{M}_1)-\lambda_i(\bd{M}_2)|\leq \Big(\sum_{1\leq i, j \leq n}|m_{ij}-\tilde{m}_{ij}|^2\Big)^{1/2}.
\eea
This concludes that the eigenvalues of a matrix are continuous functions of its entries. This is particularly true for smallest eigenvalues.

According to Example \ref{good_example}, there exists  $\alpha\in (1, 2)$ and an $n\times n$ symmetric matrix $\bd{A}=(a_{ij})$ such that $a_{ij}\geq 0$ for all $1\leq i, j \leq n$, $\bd{A}>0$ and the Hadamard power matrix $\bd{A}^{(\alpha)} \ngeqslant 0$. For any $n\times n$ symmetric matrix  $\bd{M}=(m_{ij})$, define
\beaa
f(\bd{M}):=\min\big\{\lambda_n(\bd{M}),\, -\lambda_n(\bd{M}^{(\alpha)})\big\}.
\eeaa
As explained earlier, $f(\bd{M})$ is a continuous function in $\{m_{ij};\, 1\leq i\leq j \leq n\}$. Since $f(\bd{A})>0$, there exist $\{\delta_{ij}>0;\, 1\leq i, j\leq n\}$ with $\delta_{ij}=\delta_{ji}$ for all $1\leq i, j\leq n$ such that $f(\bd{M})>0$ for any  $m_{ij}\in [a_{ij}, a_{ij}+\delta_{ij}]$ with $1\leq i, j\leq n.$ Set $\delta= \min\{\delta_{ij};\, 1\leq i\leq j\leq n\}.$ Then, $\delta>0$. Also, $\lambda_n(\bd{M})>0$ and $\lambda_n(\bd{M}^{(\alpha)})<0$ for every $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and every $1\leq i, j\leq n.$ That is, $\bd{M}>0$  and $\bd{M}^{(\alpha)} \ngeqslant 0$  for any $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and any $1\leq i, j\leq n.$
\hfill$\square$



\begin{lemma}\lbl{good_night} Let $\bd{X}=(x_{ij})_{n\times p}$ be an $n\times p$ matrix. For any $n$ and $p$ with $n\geq p\geq 4$, there exist $\alpha\in (1,2)$, $\delta>0$ and $n\times p$ matrix  $\bd{A}=(a_{ij})$ with $a_{ij}\geq 0$ for all $1\leq i\leq n$ and $1\leq  j \leq q$ such that the following holds.
%the $n\times n$ matrix $\bd{M}^{(\alpha)}=(m_{ij}^{\alpha})$ has the following property.

(i)  The matrix $\bd{X}'\bd{X}$ is positive definite for every $x_{ij}\in [a_{ij}, a_{ij}+\delta]$ and $1\leq i\leq n$ and $1\leq j \leq p.$

(ii) The Hadamard power $(\bd{X}'\bd{X})^{(\alpha)}\ngeqslant 0$  for any $x_{ij}\in [a_{ij}, a_{ij}+\delta]$, $1\leq i\leq n$ and $1\leq j \leq p$.
\end{lemma}
\noindent\textbf{Proof of Lemma \ref{good_night}}. Let $\bd{a}^T=(1,1,1,1)$ and $\bd{b}^T=(0,1,2,3)$ be as in Example \ref{good_example}.
%Set $\bd{M}=\bd{a}\bd{a}^T+\bd{b}\bd{b}^T.$
It is checked that the Hadamard power $(\bd{a}\bd{a}^T+\bd{b}\bd{b}^T)^{(1.1)}$ has determinant $-1.1856\times 10^{-4}.$
% eigenvalues $-0.0002, 0.0221, 1.2815$ and $20.3030.$
Set
\beaa
\bd{A}(\epsilon)=
%\begin{pmatrix}
%1 & 0& 0 & 0\\
%1 & 1& 0 & 0\\
%1 & 2 & \epsilon & 0\\
%1 & 3& 0 & \epsilon
%\end{pmatrix}
\begin{pmatrix}
1 & 1& 1 & 1\\
0 & 1& 2 & 3\\
0 & 0 & \epsilon & 0\\
0 & 0& 0 & \epsilon
\end{pmatrix}
,\ \ \ \epsilon>0.
\eeaa
It is easy to see $\lim_{\epsilon \to 0^+}[\bd{A}(\epsilon)^T\bd{A}(\epsilon)]^{(1.1)}=(\bd{a}\bd{a}^T+\bd{b}\bd{b}^T)^{1.1}$ with the entrywise convergence. By continuity of determinants, there exists $\epsilon_0>0$ such that the determinant of $[\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0)]^{(1.1)}$ is negative. That is,  $\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0)>0$ but the Hadmard power $[\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0)]^{(1.1)} \ngeqslant 0.$ Now we define an $n\times p$ matrix $\bd{A}$ such that
%$\bd{A}=\bd{A}$  for $n=4$ and
\beaa
\bd{A}=
\begin{pmatrix}
  \bd{A}(\epsilon_0) & \bd{0}\\
\bd{0} & \bd{I}_{p-4}\\
\bd{0} & \bd{0}
\end{pmatrix}
_{n\times p},
\eeaa
where the size of each submatrix $\bd{0}$ appeared in $\bd{A}$ can be seen from those of $\bd{A}(\epsilon_0)$ and $\bd{I}_{p-4}.$ In particular, the size of the  ``$\bd{0}$" in the bottom-right of $\bd{A}$ is $(n-p)\times (p-4).$ In case $n=p$, there is no  third row of submatrices in $\bd{A}$; in case $p=4$, there is no  second row of submatrices of $\bd{A}.$ Since
\beaa
\bd{A}^T\bd{A}
=
\begin{pmatrix}
\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0) & \bd{0}\\
\bd{0} & \bd{I}_{p-4}
\end{pmatrix}
.
\eeaa
Hence, $\bd{A}^T\bd{A}>0$ but the Hadamard power $(\bd{A}^T\bd{A})^{(1.1)} \ngeqslant 0.$


 The inequality \eqref{flower_could} shows that the smallest eigenvalue $\lambda_n(\bd{M})$ of $\bd{M}=\bd{X}\bd{X}^T$ is a continuous function of the entries of $\bd{M}$, which in turn are the continuous functions of the entries of $\bd{X}$. Write $\bd{X}=(x_{ij})_{n\times p}$.  Hence, $\lambda_n(\bd{M})$ is a continuous function of $x_{ij}$'s. Set
 \beaa
f(\bd{M}):=\min\big\{\lambda_n(\bd{M}),\, -\lambda_n(\bd{M}^{(\alpha)})\big\}.
\eeaa
Then $f(\bd{M})$ is a continuous function of $x_{ij}$'s and $f(\bd{A}\bd{A}^T)>0.$ Write  $\bd{A}=(a_{ij})_{n\times p}$. Then there exist $\delta_{ij}>0$ for all $1\leq i\leq n$ and $1\leq j \leq p$ such that $f(\bd{M})>0$ for all $x_{ij}\in [a_{ij}, a_{ij}+\delta_{ij}]$ with $1\leq i\leq n$ and $1\leq j \leq p$. Denote $\delta=\min\{\delta_{ij};\, 1\leq i\leq n, 1\leq j \leq p\}.$ Then $\delta>0$ and $f(\bd{X}\bd{X}^T)>0$ for all $x_{ij}\in [a_{ij}, a_{ij}+\delta]$ with $1\leq i\leq n$ and $1\leq j \leq p$. Hence, under these restrictions of $x_{ij}$'s, we have $\lambda_n(\bd{X}\bd{X}^T)>0$ and $\lambda_n((\bd{X}\bd{X}^T)^{(\alpha)}) < 0$. This yields (i) and (ii). \hfill$\square$

\medskip

\noindent\textbf{Proof of Theorem \ref{milk}}. Review Lemma \ref{what_is}. Let $\delta >0$ be as in the lemma. Since $[a_{ij}+\frac{1}{2}\delta, a_{ij}+\delta] \subset [a_{ij}, a_{ij}+\delta]$  for each pair of $(i, j)$ with $1\leq i\leq j \leq n.$ Then Lemma \ref{what_is} still holds if we strengthen the conclusion by requiring  that $a_{ij}>0$ for all $1\leq i\leq j \leq n.$ Therefore,
\bea\lbl{poker}
&0\\
&<\alpha:=\min\{a_{ij};\, 1\leq i\leq j \leq n\}\\
& < \beta:=\max\{a_{ij};\, 1\leq i\leq j \leq n\}+\delta.
\eea
For a random variable $\xi$, we use $\mbox{support}(\xi)$ to denote its support. In particular, $P(a\leq \xi\leq  b)>0$ provided $[a, b]\subset \mbox{support}(\xi)$.
Notice $\mbox{support}(\lambda\xi_{ij})=\lambda\cdot \mbox{support}(\xi_{ij})$ for each $i, j$. Choose $\lambda>0$ such that $\lambda [u, v]\supset [\alpha , \beta]$. It follows that
\bea\lbl{red}
& \bigcup_{1\leq i\leq  j\leq n}[a_{ij}, a_{ij}+\delta] \subset [\alpha, \beta]\\
& \subset \bigcap_{1\leq i\leq  j\leq n}  \mbox{support}(\lambda\xi_{ij}).
\eea
Observe
\beaa
& P\big(\bd{M}>0\ \mbox{and}\ \bd{M}^{(\alpha)}\ngeqslant 0\big)\\
& = P\big(\lambda\bd{M}>0\ \mbox{and}\ (\lambda\bd{M})^{(\alpha)}\ngeqslant 0\big).
\eeaa
By Lemma \ref{what_is} and independence, the last probability above is at least
\beaa
&& P(\lambda \xi_{ij}\in [a_{ij}, a_{ij}+\delta]\ \mbox{for each}\ 1\leq i\leq j \leq n)\\
&=& \prod_{1\leq i\leq j \leq n}P(\lambda\xi_{ij}\in [a_{ij}, a_{ij}+\delta])\\
&>& 0,
\eeaa
where the last inequality comes from \eqref{red}. The proof is complete. \hfill$\square$

\medskip




\noindent\textbf{Proof of Theorem \ref{waffle}}. Recall Lemma \ref{good_night}. Let $\delta >0$ be as in the lemma. By the same argument as in \eqref{poker}, without loss of generality, we assume  $a_{ij}>0$ for all $1\leq i\leq n$ and $1\leq j \leq p.$ Therefore,
\beaa
%\lbl{poker1}
0&<&\alpha:=\min\{a_{ij};\, 1\leq i \leq n, 1\leq j \leq p\}\\
&<& \beta:=\max\{a_{ij};\, 1\leq i\leq n, 1\leq j \leq p\}+\delta.
\eeaa
By choosing $\lambda>0$ such that $\lambda [u, v]\supset [\alpha , \beta]$, we then have
\bea\lbl{red1}
\bigcup [a_{ij}, a_{ij}+\delta] \subset [\alpha, \beta]\subset \bigcap   \mbox{support}(\lambda x_{ij}),
\eea
where the union and the intersection are taken over $1\leq i\leq n$ and $1\leq j \leq p.$ Let $\bd{X}=(x_{ij})$ be an $n\times p $ matrix.
By setting $\bd{Y}=\lambda\bd{X}$, we have
\beaa
& P\big(\bd{X}^T\bd{X}>0\ \mbox{and}\ (\bd{X}^T\bd{X})^{(\alpha)}\ngeqslant 0\big)\\
& =  P\big(\bd{Y}^T\bd{Y}>0\ \mbox{and}\ (\bd{Y}^T\bd{Y})^{(\alpha)}\ngeqslant 0\big).
\eeaa
From Lemma \ref{good_night}, the above is at least
\beaa
&& P\big(\lambda x_{ij}\in [a_{ij}, a_{ij}+\delta]\ \mbox{for each}\ 1\leq i\leq n\ \mbox{and}\, 1\leq j \leq p\big)\\
&=& \prod_{1\leq i\leq n, 1\leq j \leq p}P\big(\lambda x_{ij}\in [a_{ij}, a_{ij}+\delta]\big)\\
&>& 0
\eeaa
where the last step follows from \eqref{red1} and independence. The proof is completed. \hfill$\square$

\medskip

\noindent\textbf{Proof of Theorem \ref{my_country}}. By the Gershgorin disc theorem [see e.g., Horn and Johnson (1985)], all eigenvalues of $\bd{A}$ are in the set
\bea\lbl{bro_pro}
\bigcup_{1\leq i \leq n}\Big(a_{ii}- \sum_{j\ne i}a_{ij}, a_{ii}+ \sum_{j\ne i}a_{ij}\Big).
\eea
By assumption, all eigenvalues are non-negative, hence $\bd{A}\geq 0$. On the other hand,
\beaa
a_{ii}^{\alpha}\geq  \Big(\sum_{j\ne i}a_{ij}\Big)^{\alpha}\geq \sum_{j\ne i}a_{ij}^{\alpha}
\eeaa
for all $\alpha\geq 1$ by the given condition. By the Gershgorin disc theorem again, all of the eigenvalues of the Hadamard power matrix $\bd{A}^{(\alpha)}$ are non-negative. Therefore,  $\bd{A}^{(\alpha)}\geq  0$.

Evidently, if $a_{ii}>  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n$ then all of the eigenvalues of  $\bd{A}$  and $\bd{A}^{(\alpha)}$ are positive by \eqref{bro_pro} with ``$a_{ij}$" being replaced by $a_{ij}^{\alpha}$ for all $i$ and $j$. Hence $\bd{A} > 0$ and $\bd{A}^{(\alpha)}> 0$ for all $\alpha\geq 1.$ \hfill$\square$\\



\noindent\textbf{Acknowledgements}. We thank Hongru Zhao very much for very fruitful   discussions.


\begin{thebibliography}{99}

\bibitem{Hiai}
Hiai, F. (2009).  Monotonicity for entrywise functions of matrices. {\it Linear  Algebra  Appl.} 431(8), 1125-1146.

\bibitem{s1} Horn, R. and Johnson, C. (1991). {\em Topics in matrix analysis}. Cambridge University Press, Cambridge.


\bibitem{s2} Horn, R. and Johnson, C. (1985). {\em Matrix Analysis}. Cambridge Univesity Press, Cambridge.

\bibitem{s3}
Schoenberg, I. J. (1942). Positive definite functions on spheres. {\it Duke Math. J.}   9, 96-108.

\bibitem{s4}
Schur, J. (1911). Bemerkungen zur theorie der beschr\"{a}nkten bilinearformen mit unendlich vielenver\"{a}nderlichen. {\it Journal f\"{u}r die reine und angewandte Mathematik} 140, 1-28.

\bibitem{s5} Vasudeva, H. (1979).  Positive definite matrices and absolutely monotonic functions. {\it Indian J. Pure Appl. Math.} 10(7), 854-858.

\end{thebibliography}




\end{document}
