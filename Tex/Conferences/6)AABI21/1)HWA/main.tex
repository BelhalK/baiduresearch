\documentclass[tablecaption=bottom,wcp]{jmlr}

\usepackage{booktabs}
\usepackage{bm,amsmath,amssymb,algorithmic,algorithm}
\usepackage{graphicx}
\usepackage{xargs}

\input{shortcuts_OPT.sty}
\jmlrproceedings{AABI 2020}{3rd Symposium on Advances in Approximate Bayesian Inference, 2020}

 
\title[Hyperparameters Weight Averaging in BNNs]{HWA: Hyperparameters Weight Averaging in Bayesian Neural Networks}


% Anonymous authors (leave as is; do not reveal author names for your submission)
\author{\Name{Anonymous Authors}\\
  \addr Anonymous Institution}
% THE SUBMISSION MUST REMAIN ANONYMOUS

% Two authors with the same address
% \author{\Name{Author Name1\nametag{\thanks{A note}}} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}


\begin{document}

\maketitle


\begin{abstract}
Bayesian neural networks attempt to combine the strong predictive performance of neural networks with formal quantification of uncertainty of the predicted output in the Bayesian framework.
%Bayesian Deep Learning presents itself as the most useful tool for adding uncertainty estimation to traditional Deep Learning models that only produce point estimates predictions as outputs. 
In deterministic deep neural network, confidence of the model and the predictions at inference time are left alone. 
Applying randomness and Bayes Rule to the weights of a deep neural network is a step towards achieving this goal. Current state of the art optimization methods for training Bayesian Neural Networks are relatively slow and inefficient, compared to their deterministic counterparts. 
In this paper, we propose \textsc{HWA} (Hyperparameters Weight Averaging) algorithm that exploits an averaging procedure in order to optimize faster and achieve better accuracy. 
We develop our main algorithm using the simple averaging heuristic and demonstrate its effectiveness on the space of the hyperparameters of the networks random weights. 
Numerical applications are presented to confirm the empirical benefits of our method.
\end{abstract}

\section{Introduction}
While Deep Learning methods have shown increasing efficiency in various domains such as natural language processing, computer vision or robotics, sensible areas including autonomous driving or medical imaging not only require accurate predictions but also uncertainty quantification.
In~\citep{neal2012bayesian}, authors develop a bayesian variant of plain feedforward multilayer neural networks in which weights and biases are considered as random variables.
For supervised learning tasks, deterministic models are prone to overfitting and are not capable of estimating uncertainty in the training data which results in making overly confident decisions about the correct class, also known as \emph{miscalibration}~\citep{guo2017calibration,kendall2017uncertainties}.
Nevertheless, representing that aforementioned uncertainty is crucial for decision making.
Bayesian methods display a hierarchical probabilistic model that assume a (prior) random distribution over the parameters of the parameters and are useful for assessing the uncertainty of the model via posterior predictive distribution quantification~\citep{blundell2015weight,kingma2015variational}.
Current training methods for Bayesian Neural Networks (BNN)~\citep{neal2012bayesian} include Variational Inference~\citep{graves2011practical, hoffman2013stochastic} or BayesByBackprop~\citep{blundell2015weight} based on the Evidence Lower Bound (ELBO) maximization task.
Naturally, Bayesian methods, and in particular BNNs, are highly sensitive to choice of the prior distribution parameters.
Besides, current state-of-the-art models are not as efficient and robust as traditional deep learning models.

In this paper, we introduce a new \emph{optimization} algorithm to alleviate those challenges.
Our main contributions read as follows:
\begin{itemize}
\item We introduce \textsc{Hyperparameter Weight Averaging} (HWA), a training algorithm that leverages stochastic averaging techniques~\citep{polyak1992acceleration} and posterior sampling methods to efficiently train bayesian neural networks.
\item Given the high nonconvexity of the loss landscape, our method finds heuristic explanation from theoretical works on averaging and generalization such as~\citep{keskar2016large,he2019asymmetric} and more practical work on Deep Neural Networks (DNN) optimization such as~\citep{izmailov2018averaging}.
\item We provide numerical examples showcasing the effectiveness of our method on simple and complex supervised classification tasks.
\end{itemize}
The remaining of the paper is organized as follows.
Section~\ref{sec:related} presents the related works in the fields of optimization, Variational Inference and posterior sampling.
Section~\ref{sec:main} introduces our main contribution, namely the HWA algorithm.
Section~\ref{sec:numerical} highlights the benefits of our procedure on various classification tasks.
Section~\ref{sec:conclusion} concludes our work.
 
\section{Related Work}\label{sec:related}

\textbf{Posterior Prediction.}
Due to the nonconvexity of the loss landscapes involved in modern deep learning tasks, sampling directly from the posterior distribution of the weights is not an option.
Depending on the nature and the dimensionality of the problem, Markov Chain Monte Carlo (MCMC) methods have been employed to overcome this intractability issue.
By constructing a Markov chain, the samples drawn at convergence are guaranteed to be drawn from the target distribution.
Hamiltonian Monte Carlo (HMC)~\citep{neal2011mcmc} or Metropolis Hastings (MH)~\citep{hastings1970monte} are two standard solutions used in practice.
Their stochastic gradients counterpart are extensively studied in \citep{ma2015complete}.

\vspace{0.08in}
\noindent \textbf{Variational Inference (VI).}
When tackling an optimization problem, exact posterior sampling may be computationally involved and not even required.
Variational inference was proposed in~\citep{graves2011practical}, in the particular case of BNNs, in order to fit a Gaussian variational posterior approximation over the weights of neural networks.
Via a simple reparameterization trick~\citep{blundell2015weight}, several methods have emerged to train BNNs leveraging the ease of use and implementation of VI~\citep{kingma2015variational,blundell2015weight,molchanov2017variational}.
Though, those methods appear to be inefficient for large datasets and newer ones were proposed to alleviate this issue such as normalizing flows~\citep{louizos2017multiplicative}, deterministic VI~\citep{wu2018deterministic} or dropout VI~\citep{gal2016dropout}.

\vspace{0.08in}
\noindent \textbf{Stochastic Averaging.}
Averaging methods include the seminal papers of~\citep{polyak1990sa} and~\citep{ruppert1988efficient}, both based on the combination of past iterates along a stochastic approximation trajectory.
For nonconvex objectives, this averaging procedure has been adapted to Stochastic Gradient Descent (SGD) trajectory in \citep{zhou2017convergence}.
In particular, for recent deep learning examples, \citet{izmailov2018averaging} develops a  method that averages snapshots of a DNN through the iterations leading to a better empirical generalization.
Those experimental discoveries are then backed by theoretical understanding of the multilayer neural network loss landscape in~\citep{keskar2016large,he2019asymmetric}.


\section{Hyperparameters Averaging in Bayesian Neural Networks}\label{sec:main}

In this section, we introduce the basic concepts of bayesian neural networks and their associated loss function which plays a key role in this paper.
From an optimization perspective, we first review the Stochastic Weight Averaging (SWA)~\citep{izmailov2018averaging} procedure, which can be seen as an approximation of the mean trajectory of the SGD iterates and then introduce our method -- namely \textsc{HWA}.
\textit{While SWA averages snapshots of the weights of the neural networks from successive past iterations, our method HWA only captures snapshots of the hyperparameters, and not of the weights that are sampled at each training iteration.}
We then discuss the uncertainty estimation prediction of such method and how our proposed extra step, combining \emph{posterior sampling} and \emph{optimization}, can lead to better generalization of the trained model on test sets.


\subsection{Bayesian Neural Networks and ELBO Maximization}
Let $((x_i,y_i), i \in [1,n])$ be i.i.d.~input-output pairs and $w \in \mathcal{W} \subseteq \mathbb{R}^{d}$ be a latent variable. 
When conditioned on the input data $x = (x_i, i \in [n])$, the joint distribution of $y = (y_i, i \in [n])$ and $w$ is given by:
\begin{equation}\label{eq:vi} \textstyle
    p(y,w | x) = \prior(w)\prod_{i=1}^{n}{p(y_i | x_i, w)} \eqsp.
\end{equation}
In the particular case of BNN, this likelihood function is parameterized by a multilayer neural network, which can be convolutional or not.
The latent variables $w$ are thus the weights and the biases of the model and are considered as latent (and random) variables.
Training such hierarchical models involves sampling from the posterior distribution of the weights $w$ conditioned on the data $(x,y)$, noted $p(w|y,x)$.
In most cases, this posterior distribution $p(w|y,x)$ is intractable and is approximated using a family of parametric distributions, $\{q(w, \param ), \param \in \Param \}$. 
The variational inference (VI) problem~\citep{blei2017variational} boils down to minimizing the Kullback-Leibler (KL) divergence between $q(w, \param )$ and the posterior distribution $p(w|y,x)$. 
The objective is the ELBO (Evidence Lower BOund) and reads:
\begin{equation}\label{eq:variationalobjective}
{\cal L}(\param) \eqdef -\EE_{ q( w; \param )} \big[\log p(y | x, w) \big]+  \EE_{ q( w; \param )} \big[ \log q(w; \param )/\prior(w) \big]  \eqsp.
\end{equation}
Directly optimizing the objective function in \eqref{eq:variationalobjective} can be difficult.
First, with $n \gg 1$, evaluating the objective function ${\cal L}( \param )$ requires a full pass over the entire dataset.
Second, for some complex models, the expectations in \eqref{eq:variationalobjective} can be intractable even if we assume a simple parametric model for $q(w; \param)$.
Thus, the computation of the gradient requires an approximation step usually invoking a Monte Carlo (MC) approximation step. 
% For completeness, the full variational inference procedure for training bayesian neural networks, including \textit{the MC approximation} step and the \textit{parameter update} step, is summarized in Alg.~\ref{alg:trainingbnn} of the supplementary material.

Training solutions simply include using SGD~\citep{bottou2008tradeoffs} where the gradient of the individual ELBO~\eqref{eq:variationalobjective} is computed using Automatic Differentiation~\citep{kucukelbir2017automatic}. 
The final update goes in the opposite direction of that gradient up to a learning rate factor.
In the sequel, we develop an improvement over baseline SGD, invoking averaging virtue of several successive snapshots of the gradients.
The method, called Hyperparameters Weight Averaging (HWA), aims at improving the generalization property of the trained model on unseen data.


\subsection{Averaging model snapshots through hyperparameters loss landscapes}
Consider a deterministic deep neural network, the idea behind the Stochastic Weight Averaging (SWA) procedure, developed in~\citep{izmailov2018averaging}, is to run several iterates of the classical SGD procedure, starting from a pre-trained model.
At each timestep noted $T_{\mathsf{avg}}$, the model estimate is equal to the average of the last $T_{\mathsf{avg}}$ iterates.
After establishing the connectivity between several modes (point estimates of minimal loss) of the same deep neural network (after different training procedures) in~\citep{garipov2018loss}, the ability to average all those iterates probably traversing several models or at least model estimates that belong to low loss region would make the resulting trained model more robust and thus generalize better to unseen data.
Several theoretical papers such as~\citep{he2019asymmetric} or \citep{keskar2016large} provide an attempt at explaining this phenomena.


\textbf{-- Hyperparameters Weight Averaging:}
Based on the probabilistic model developed above, the loss function~\eqref{eq:variationalobjective} is defined on the space of the hyperparameters, \textit{i.e.} the mean and the variance of the variational candidate distribution.
Regarding the parameterization choice of the variational candidate $q( w; \param )$, we choose for simplicity a scalar mean $\mu_{\ell}$ depending on the layer $\ell \in [1,L]$ and constant between each neuron of the same layer. 
Classically, the covariance of this variational distribution is diagonal, see~\citep{kirkpatrick2017overcoming, blundell2015weight}, yet this assumption can be too restrictive.
We follow the direction taken in~\citep{maddox2019simple}, where the covariance of $q(w, \param)$ is a diagonal matrix.
As a result, the averaging procedure practically occurs on the set of hyperparameters and requires updating the mean and the variance of the variational candidate distribution, at iteration $k+1$, if $k$, the iteration index, is a multiple of the cycle length $c$,  as below:
\begin{equation}\label{eq:hwa_updates}
\begin{split}
 \mu_{\ell}^{HWA}  =  \frac{n_{\textrm{m}}\mu_{\ell}^{HWA} + \mu_{\ell}^{k+1}}{n_{\textrm{m}}+1}  \quad \textrm{and} \quad  \sigma^{HWA}   =  \frac{n_{\textrm{m}}\sigma^{HWA} + (\mu_{\ell}^{k+1})^2}{n_{\textrm{m}}+1} -( \mu_{\ell}^{HWA})^2 \, ,
\end{split}
\end{equation}
where for all $\ell \in [1,L]$, $\mu_{\ell}^{k+1}$ and $\sigma^{k+1}$ are obtained via Stochastic VI~\citep{hoffman2013stochastic}.


\begin{algorithm}[H]
\algsetup{indent=0.25em}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Iteration index $k$. Trained hyperparameters $\hat{\mu}_{\ell}$ and $\hat{\sigma}$. LR $\gamma_k$. Cycle length $c$. Gradient vector $\nabla \mathcal{L}_{i_{k}}(\theta^{k})$
%\STATE Initialize the hyperparameters of the weights and 
%$\mu_{\ell} = \hat{\mu}_{\ell}$ and $\mu^{HWA}_{\ell} = \mu_{\ell}$.
\STATE $\gamma \leftarrow \gamma(k)$ (Cyclical LR for the iteration)
\STATE \textbf{SVI updates:}
\STATE \quad $\mu_{\ell}^{k+1} \leftarrow \mu_{\ell}^{k} - \gamma_k \nabla_{\mu_{\ell}} \mathcal{L}_{i_{k}}(\mu_{\ell}^{k})$  \label{line:svi}
\STATE \quad $\sigma^{k+1} \leftarrow \sigma^{k} - \gamma_k \nabla_{\sigma} \mathcal{L}_{i_{k}}(\sigma^{k})$ \label{line:svisigma}
\IF{$\textrm{mod}(k,c) = 0$}
	\STATE \quad $n_{\textrm{m}} \leftarrow k/c$ \quad (Number of models to average over)
\begin{equation}\notag
\mu_{\ell}^{HWA} \leftarrow \frac{n_{\textrm{m}}\mu_{\ell}^{HWA} + \mu_{\ell}^{k+1}}{n_{\textrm{m}}+1} \quad \textrm{and} \quad \sigma^{HWA} \leftarrow \frac{n_{\textrm{m}}\sigma^{HWA} + (\mu_{\ell}^{k+1})^2}{n_{\textrm{m}}+1} -( \mu_{\ell}^{HWA})^2
\end{equation}
\ENDIF
\STATE \textbf{Return} hyperparameters $(\{\mu_{\ell}^{HWA}\}_{l=1}^L, \sigma^{HWA})$.
\end{algorithmic}
\caption{HWA: Hyperparameters Weight Averaging}
\label{alg:hwa}
\end{algorithm}

The main contribution of our paper is in Alg.~\ref{alg:hwa}.
Stochastic Variational updates are executed Line~\ref{line:svi}.
The stochastic averaging procedure happens every $c$ iterations, and consists in computing the weighted sum between the latest model estimate and the running average noted by the superscript $\textrm{HWA}$.
Note that in the above procedure, the variational candidate $q(w, \param)$ has a diagonal covariance matrix where the scalar standard deviations are obtained through Alg.~\ref{alg:hwa}.
Once the parameter estimates are updated via \eqref{eq:hwa_updates}, the neural network weights are then \emph{sampled} according to the updated variational candidate distributions in order to compute the next iteration approximate stochastic gradient, see Alg.~\ref{alg:trainingbnn} in supplementary material for more details on the end-to-end VI procedure embedding HWA.

Yet, it is also possible build a non diagonal proposal covariance to bypass the restriction of such structure.
Besides, given the nonconvexity and high dimensionality of the true posterior distribution, adding a \emph{low rank non diagonal} structure to the covariance of our proposal would yield a gain in efficiency in the VI procedure.
Of course the ideal option would be to build a posterior curvature-informed covariance but at a higher cost.
%The trade-off between computational costs and proposal efficiency is detailed in the following.
The low-rank plus diagonal posterior approximation matrix, noted $\Sigma$ of $q(w, \param)$ introduced in \citep{maddox2019simple} reads:
\begin{equation}\label{eq:lowrankcov}
\Sigma = \frac{1}{2} \Sigma_{\text {diag }}+\frac{\widehat{D} \widehat{D}^{\top}}{2(R-1)}
\end{equation}
where $\mu^{HWA} = (\mu_{\ell}^{HWA}, \ell \in [1,L])$, $R$ is the maximum number of columns in the low rank deviation matrix $\widehat{D}$ and $\Sigma_{\text {diag }}$ is the diagonal covariance defined above. 
The $r$-th component of the  low rank deviation matrix $\widehat{D}$ is defined as the gap between the current estimate and the running average: $\widehat{D}_r = \theta_r - \theta^{HWA}_r$. 
It quantifies how far the current estimate parameter deviate from the current average.
%Then the covariance of the proposal $q(\cdot)$ in Alg.~\ref{alg:trainingbnn} is either set to \eqref{eq:hwa_updates} or \eqref{eq:lowrankcov}
Several hyperparameters are worth highlighting here.
The standard learning rate $\gamma_k$ plays a key role and is either equal to a constant or a cyclical learning rate, see~\citep{zhang2019cyclical}.
The cycle length $c$ which monitors the number of times snapshots of the model estimates are being averaged is also of utmost importance and needs careful tuning.


\section{Numerical Experiments}\label{sec:numerical}
We provide experiments on classification tasks with various neural network architectures and datasets to demonstrate the effectiveness of method, namely HWA.

\vspace{0.08in}
\noindent \textbf{Methods.}\hspace{0.1in}
We consider three baselines: standard \textsc{BayesByBackprop} (BBB) developed in~\citep{blundell2015weight}, the Stochastic Gradient Langevin Dynamics (SGLD) method introduced in~\citep{welling2011bayesian} and its cyclical variant in~\citep{zhang2019cyclical}.
The algorithms are initialized at the same point and the results are averaged over 5 repetitions.

\vspace{0.08in}
\noindent \textbf{Datasets.}\hspace{0.1in}
We compare the different algorithms on \textit{MNIST}~\citep{lecun-mnisthandwrittendigit-2010} and \textit{CIFAR10}~\citep{krizhevsky2009learning} datasets.
%The \textit{MNIST} training set is composed of $n=55\,000$ handwritten digits, $28 \times 28$ images. Each image is labelled with its corresponding number (from zero to nine).
%\textit{CIFAR10} is a popular computer-vision datasets of $50\,000$ training images and $10\,000$ test images, of size $32\times 32$. 

\vspace{0.08in}
\noindent \textbf{Network architectures.} \hspace{0.1in}
(MNIST) We train a Bayesian variant of LeNet-5 convolutional neural network~\citep{lecun1998gradient} on the MNIST dataset. 
Under the prior distribution $\prior$, see \eqref{eq:vi}, the weights are assumed independent and identically distributed according to ${\cal N}(0,1)$.
We also assume a Gaussian variational candidate distribution such that $q(\cdot; \param) \equiv  \mathcal{N}(\mu,\sigma^2 \Id )$, where $\Id$ is the identity matrix.
The variational posterior parameters are thus $\param = (\mu, \sigma) $ where $\mu = (\mu_\ell, \ell \in [d])$ with $d$ the number of weights in the neural network. 
(CIFAR-10) We train the Bayesian variant of the VGG neural network~\citep{simonyan2014very} on the CIFAR-10 dataset.
As in the previous example, the weights are assumed i.i.d. according to ${\cal N}(0,\Id)$.
Standard hyperparameters values found in the literature, such as the annealing constant or the number of MC samples, were used for the benchmark methods. 
For better efficiency and lower variance, the Flipout estimator~\citep{wen2018flipout} is used to compute the MC approximation of the gradient of the loss function.

\vspace{0.08in}
\noindent \textbf{Results.} \hspace{0.1in} 
Results for both datasets and network architectures are reported Figure~\ref{fig:all}.
While for the MNIST dataset the runs for \textsc{HWA} and \textsc{SGLD} are comparable both in terms of train and testing loss and accuracy, they both highlights better convergence properties compared to \textsc{BayesByBackprop} (BBB).
It is worth mentioning that our method \textsc{HWA} displays a similar behavior as a gradient based method, such as \textsc{SGLD}, by only leveraging the average of past snapshots of the variational candidate hyperparameters. 
Regarding the CIFAR10 experiment, our method shows the lowest training loss and generalize better to unseen data (cf. last figure on bottom line in Figure~\ref{fig:all}).
In conclusion, \textsc{HWA} achieves state-of-the-art results for either small or large bayesian variants of standard network architectures while using a simple and efficient averaging update at each \textit{cycle}.
\begin{figure}[H]
\mbox{
\includegraphics[width=0.25\textwidth]{newfigs/MNIST_train_loss.eps}
\includegraphics[width=0.25\textwidth]{newfigs/MNIST_train_acc.eps}
\includegraphics[width=0.25\textwidth]{newfigs/MNIST_test_loss.eps}
\includegraphics[width=0.25\textwidth]{newfigs/MNIST_test_acc.eps}
}
\mbox{
\includegraphics[width=0.25\textwidth]{newfigs/CIFAR_train_loss.eps}
\includegraphics[width=0.25\textwidth]{newfigs/CIFAR_train_acc.eps}
\includegraphics[width=0.25\textwidth]{newfigs/CIFAR_test_loss.eps}
\includegraphics[width=0.25\textwidth]{newfigs/CIFAR_test_acc.eps}
}
\vspace{-0.2in}
 \caption{Comparison for Bayesian LeNet CNN architecture on MNIST dataset (top) and Bayesian VGG architecture on CIFAR-10 dataset (bottom). The plots are averaged over 5 repetitions.}
\label{fig:all}
\end{figure}
\section{Conclusion}\label{sec:conclusion}
We present in this paper an averaging procedure on the hyperparameters of the weights of a bayesian neural network architecture.
Based on both empirical and theoretical results regarding stochastic averaging, we propose HWA in order to increase the generalization ability of a BNN.
The procedure is easily implementable on top of any vanilla optimizer with standard design choices for prior and candidate distributions, crucial in variational inference.
Numerical runs show the advantage of our method matching and sometimes surpassing baselines such as SGLD or CSGMCMC, which require additional expensive gradient computation.

%\acks{Acknowledgements go here.}

\clearpage

\bibliography{ref}

\clearpage
\appendix

  \hsize\textwidth
  \linewidth\hsize \toptitlebar {\centering
  {\Large\bfseries Appendix for HWA: Hyperparameters Weight Averaging in Bayesian Neural Networks \par}}
 \bottomtitlebar 
 
\section{Comparison with other classical averaging procedures in nonconvex optimization}
From Alg.~\ref{alg:hwa}, we note that the averaging procedure happens once at each cycle $c$, a tuning hyperparameter, on the parameter estimates resulting from a simple stochastic gradient descent update.
Yet, another natural averaging step would be to keep $K$ snapshots of the past stochastic gradients and compute an aggregated sum used as the drift term in the general update rule, see \citep{zhou2017convergence}.
Nevertheless, in our setting, the objective function while being nonconvex is (possibly) parameterized by a high dimensional neural network making it computationally involved to store those $K$ gradients.


\textbf{Incremental Aggregated Gradients methods:} Popular optimization methods, such as SAG~\citep{schmidt2017minimizing} or SAGA~\citep{defazio2014saga}, make use of the past individual gradient and compute a moving average of those vectors as the final drift term. Those methods are proven to be faster than plain SGD in both convex and nonconvex cases, leveraging among other reasons variance reduction effect, but suffer from a high storage cost. Indeed the drift term is composed of the sum of the $n$ past individual gradient where $n$ is equal to the size of the training set.


\textbf{MISO~\citep{mairal2015incremental}:} Another important method invoking variance reduction through incremental update of the drift term in a gradient descent step is the Minimization by Incremental Surrogate Optimization method, namely MISO, developed in \citet{mairal2015incremental}.
Contrary to the method mentionned above, the accumulation does not happen on the gradient but on the sum of individual surrogate objective functions.
While this framework is more general than SAG or SAGA, and also does not require storing $n$ past gradients, it is still computationally heavy to store those $n$ past objective functions, rather their model parameter estimates, when tackling deep neural networks training.

For all those reasons, \textsc{HWA} surely combines the virtue of the accumulation/aggregation effect and the low computing cost of vanilla SGD.



\section{Embedding HWA in Variational Inference}\label{app:viandhwa}
Note that in the above procedure, the variational candidate $q(w, \param)$ has a diagonal covariance matrix where the scalar standard deviations are obtained through Alg.~\ref{alg:hwa}.
One parameter estimates are updated via \eqref{eq:hwa_updates}, the neural network weights are then sampled according to the updated variational candidate distributions in order to compute the next iteration approximate stochastic gradient $\nabla \mathcal{L}_{i_{k}}(\theta)$.
Yet it is also possible build a non diagonal proposal covariance to bypass the restriction of such structure.
Besides, given the nonconvexity and high dimensionality of the true posterior distribution, adding even a low rank non diagonal structure to the covariance of our proposal would yield a gain in efficiency in the variational inference procedure.
Of course the ideal option would be to build a posterior curvature-informed covariance but at a higher cost.
The trade-off between computational costs and proposal efficiency is detailed in the following.
The low-rank plus diagonal posterior approximation matrix, noted $\Sigma$ of $q(w, \param)$ introduced in \citep{maddox2019simple} reads:
\begin{equation}
\Sigma = \frac{1}{2} \Sigma_{\text {diag }}+\frac{\widehat{D} \widehat{D}^{\top}}{2(R-1)}
\end{equation}
where $\mu^{HWA} = (\mu_{\ell}^{HWA}, \ell \in [1,L])$, $R$ is the maximum number of columns in the low rank deviation matrix $\widehat{D}$ and $\Sigma_{\text {diag }}$ is the diagonal covariance defined above. The $r$-th component, where $r \in [R]$, of the  low rank deviation matrix $\widehat{D}$ is defined as the gap between the current estimate and the running average: $\widehat{D}_r = \theta_r - \theta^{HWA}_r$. It quantifies how far the current estimate parameter deviate from the current averaged parameter.

Then the covariance of the proposal $q(\cdot)$ in Alg.~\ref{alg:trainingbnn} is either set to \eqref{eq:hwa_updates} or \eqref{eq:lowrankcov}.

\textbf{Discussion on the choice of the variational candidate distribution:} 
The general aim of the update rules presented above is to construct an efficient variational candidate distribution that would provide an approximate shape of the true posterior. Our method acts on the mean and covariance of a simple Gaussian distribution where the covariance matrix is either diagonal or low rank.
Nevertheless, other choice of proposal can be employed such as the spike and slab variational distribution, in \citep{gal2016dropout}, leveraging dropout mechanism in VI. 
The other similar idea, namely concrete dropout in \citep{gal2017concrete} not only optimizes the hyperparameters of the weights but also the dropout probabilities.
We do not consider those variants as our work focuses on Gaussian approximations of the posterior distribution and how their parameters are updated, see Section~\ref{sec:numerical} for a description of baseline methods used in our numerical experiments.


We now give in Alg.~\ref{alg:trainingbnn}, the overall training algorithm of the bayesian neural network using the proposed HWA algorithm to update the parameters.
\begin{algorithm}[H]
\algsetup{indent=0.25em}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Trained hyperparameters $\hat{\mu}_{\ell}$ and $\hat{\sigma}$. Sequence of LR $\{\gamma_k\}_{k > 0}$. Cycle length $c$. $K$ iterations.
\FOR {$k=0,1,...$}
\STATE Sample an index $i_k$ uniformly on $[n]$
\STATE Sample MC batch of weights $\{w^m_k\}_{m=1}^{M_k}$  from variational candidate $q(w, \theta^k)$ with $\theta^k = (\mu^k, \Sigma^k)$ and the covariance is either diagonal~\eqref{eq:hwa_updates} or low rank~\eqref{eq:lowrankcov}.
\STATE Compute MC approximation of the gradient vectors:
$$ \nabla \mathcal{L}_{i_{k}}(\theta^{k}) \approx \frac{1}{M_k} \sum_{m=1}^{M_k} \log p(y_{i_k} | x_{i_k}, w^k_m)  + \nabla KL(q(w, \theta^k)||\pi(w)) $$
\STATE Update the vector of parameter estimates calling Alg.~\ref{alg:hwa}: $(\mu^K, \Sigma^K) =  \textsc{HWA}(k, c, \gamma_k, \nabla \mathcal{L}_{i_{k}}(\theta^{k}) )$
\ENDFOR
\STATE \textbf{Return} Fitted parameters $(\mu^K, \Sigma^K)$.
\end{algorithmic}
\caption{Variational Inference with HWA for BNNs}
\label{alg:trainingbnn}
\end{algorithm}

\end{document}
