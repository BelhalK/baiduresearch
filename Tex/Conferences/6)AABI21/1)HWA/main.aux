\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{neal2012bayesian}
\citation{guo2017calibration,kendall2017uncertainties}
\citation{blundell2015weight,kingma2015variational}
\citation{neal2012bayesian}
\citation{graves2011practical,hoffman2013stochastic}
\citation{blundell2015weight}
\providecommand \oddpage@label [2]{}
\jmlr@title{Hyperparameters Weight Averaging in BNNs}{HWA: Hyperparameters Weight Averaging in Bayesian Neural Networks}
\jmlr@author{\Name {Anonymous Authors}\\ \addr Anonymous Institution}{\Name {Anonymous Authors}\\ \addr Anonymous Institution}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{polyak1992acceleration}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\citation{neal2011mcmc}
\citation{hastings1970monte}
\citation{ma2015complete}
\citation{graves2011practical}
\citation{blundell2015weight}
\citation{kingma2015variational,blundell2015weight,molchanov2017variational}
\citation{louizos2017multiplicative}
\citation{wu2018deterministic}
\citation{gal2016dropout}
\citation{polyak1990sa}
\citation{ruppert1988efficient}
\citation{zhou2017convergence}
\citation{izmailov2018averaging}
\citation{keskar2016large,he2019asymmetric}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.0.2}}
\newlabel{sec:related}{{2}{2}{Related Work}{section.0.2}{}}
\citation{izmailov2018averaging}
\citation{blei2017variational}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hyperparameters Averaging in Bayesian Neural Networks}{3}{section.0.3}}
\newlabel{sec:main}{{3}{3}{Hyperparameters Averaging in Bayesian Neural Networks}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Neural Networks and ELBO Maximization}{3}{subsection.0.3.1}}
\newlabel{eq:vi}{{1}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.0.3.1}{}}
\newlabel{eq:VI}{{2}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.0.3.2}{}}
\newlabel{eq:variationalobjective}{{3}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.0.3.3}{}}
\citation{bottou2008tradeoffs}
\citation{kucukelbir2017automatic}
\citation{izmailov2018averaging}
\citation{garipov2018loss}
\citation{he2019asymmetric}
\citation{keskar2016large}
\citation{kirkpatrick2017overcoming,blundell2015weight}
\citation{maddox2019simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Averaging model snapshots through hyperparameters loss landscapes}{4}{subsection.0.3.2}}
\newlabel{eq:hwa_updates}{{4}{4}{Averaging model snapshots through hyperparameters loss landscapes}{equation.0.3.4}{}}
\citation{maddox2019simple}
\newlabel{line:svi}{{4}{5}{Averaging model snapshots through hyperparameters loss landscapes}{ALC@unique.4}{}}
\newlabel{line:svisigma}{{5}{5}{Averaging model snapshots through hyperparameters loss landscapes}{ALC@unique.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces HWA: Hyperparameters Weight Averaging}}{5}{algorithm.1}}
\newlabel{alg:hwa}{{1}{5}{Averaging model snapshots through hyperparameters loss landscapes}{algorithm.1}{}}
\newlabel{eq:lowrankcov}{{5}{5}{Averaging model snapshots through hyperparameters loss landscapes}{equation.0.3.5}{}}
\citation{blundell2015weight}
\citation{welling2011bayesian}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{krizhevsky2009learning}
\citation{simonyan2014very}
\citation{wen2018flipout}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{6}{section.0.4}}
\newlabel{sec:numerical}{{4}{6}{Numerical Experiments}{section.0.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison for Bayesian LeNet CNN architecture on MNIST dataset (top) and Bayesian VGG architecture on CIFAR-10 dataset (bottom). The plots are averaged over 5 repetitions.}}{7}{figure.1}}
\newlabel{fig:all}{{1}{7}{Comparison for Bayesian LeNet CNN architecture on MNIST dataset (top) and Bayesian VGG architecture on CIFAR-10 dataset (bottom). The plots are averaged over 5 repetitions}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{7}{section.0.5}}
\newlabel{sec:conclusion}{{5}{7}{Conclusion}{section.0.5}{}}
\bibdata{ref}
\bibcite{blei2017variational}{{1}{2017}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{blundell2015weight}{{2}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{bottou2008tradeoffs}{{3}{2008}{{Bottou and Bousquet}}{{}}}
\bibcite{defazio2014saga}{{4}{2014}{{Defazio et~al.}}{{Defazio, Bach, and Lacoste-Julien}}}
\bibcite{gal2016dropout}{{5}{2016}{{Gal and Ghahramani}}{{}}}
\bibcite{gal2017concrete}{{6}{2017}{{Gal et~al.}}{{Gal, Hron, and Kendall}}}
\bibcite{garipov2018loss}{{7}{2018}{{Garipov et~al.}}{{Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson}}}
\bibcite{graves2011practical}{{8}{2011}{{Graves}}{{}}}
\bibcite{guo2017calibration}{{9}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{hastings1970monte}{{10}{1970}{{Hastings}}{{}}}
\bibcite{he2019asymmetric}{{11}{2019}{{He et~al.}}{{He, Huang, and Yuan}}}
\bibcite{hoffman2013stochastic}{{12}{2013}{{Hoffman et~al.}}{{Hoffman, Blei, Wang, and Paisley}}}
\bibcite{izmailov2018averaging}{{13}{2018}{{Izmailov et~al.}}{{Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson}}}
\bibcite{kendall2017uncertainties}{{14}{2017}{{Kendall and Gal}}{{}}}
\bibcite{keskar2016large}{{15}{2016}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2015variational}{{16}{2015}{{Kingma et~al.}}{{Kingma, Salimans, and Welling}}}
\bibcite{kirkpatrick2017overcoming}{{17}{2017}{{Kirkpatrick et~al.}}{{Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.}}}
\bibcite{krizhevsky2009learning}{{18}{2009}{{Krizhevsky et~al.}}{{Krizhevsky, Hinton, et~al.}}}
\bibcite{kucukelbir2017automatic}{{19}{2017}{{Kucukelbir et~al.}}{{Kucukelbir, Tran, Ranganath, Gelman, and Blei}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{20}{2010}{{LeCun and Cortes}}{{}}}
\bibcite{louizos2017multiplicative}{{21}{2017}{{Louizos and Welling}}{{}}}
\bibcite{ma2015complete}{{22}{2015}{{Ma et~al.}}{{Ma, Chen, and Fox}}}
\bibcite{maddox2019simple}{{23}{2019}{{Maddox et~al.}}{{Maddox, Izmailov, Garipov, Vetrov, and Wilson}}}
\bibcite{mairal2015incremental}{{24}{2015}{{Mairal}}{{}}}
\bibcite{molchanov2017variational}{{25}{2017}{{Molchanov et~al.}}{{Molchanov, Ashukha, and Vetrov}}}
\bibcite{neal2012bayesian}{{26}{2012}{{Neal}}{{}}}
\bibcite{neal2011mcmc}{{27}{2011}{{Neal et~al.}}{{}}}
\bibcite{polyak1990sa}{{28}{1990}{{Polyak}}{{}}}
\bibcite{polyak1992acceleration}{{29}{1992}{{Polyak and Juditsky}}{{}}}
\bibcite{ruppert1988efficient}{{30}{1988}{{Ruppert}}{{}}}
\bibcite{schmidt2017minimizing}{{31}{2017}{{Schmidt et~al.}}{{Schmidt, Le~Roux, and Bach}}}
\bibcite{simonyan2014very}{{32}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{welling2011bayesian}{{33}{2011}{{Welling and Teh}}{{}}}
\bibcite{wen2018flipout}{{34}{2018}{{Wen et~al.}}{{Wen, Vicol, Ba, Tran, and Grosse}}}
\bibcite{wu2018deterministic}{{35}{2018}{{Wu et~al.}}{{Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and Gaunt}}}
\bibcite{zhou2017convergence}{{36}{2017}{{Zhou and Cong}}{{}}}
\citation{zhou2017convergence}
\citation{schmidt2017minimizing}
\citation{defazio2014saga}
\citation{mairal2015incremental}
\citation{mairal2015incremental}
\citation{maddox2019simple}
\@writefile{toc}{\contentsline {section}{\numberline {A}Comparison with other classical averaging procedures in nonconvex optimization}{11}{section.0.A}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Embedding HWA in Variational Inference}{11}{section.0.B}}
\citation{gal2016dropout}
\citation{gal2017concrete}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Variational Inference with HWA for BNNs}}{12}{algorithm.2}}
\newlabel{alg:trainingbnn}{{2}{12}{Embedding HWA in Variational Inference}{algorithm.2}{}}
\newlabel{jmlrend}{{B}{12}{end of Hyperparameters Weight Averaging in BNNs}{section*.2}{}}
