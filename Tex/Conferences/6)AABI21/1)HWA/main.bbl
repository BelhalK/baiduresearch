\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1505.05424}, 2015.

\bibitem[Bottou and Bousquet(2008)]{bottou2008tradeoffs}
L{\'e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  161--168, 2008.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pages
  1646--1654, 2014.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pages
  1050--1059, 2016.

\bibitem[Gal et~al.(2017)Gal, Hron, and Kendall]{gal2017concrete}
Yarin Gal, Jiri Hron, and Alex Kendall.
\newblock Concrete dropout.
\newblock In \emph{Advances in neural information processing systems}, pages
  3581--3590, 2017.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and
  Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8789--8798, 2018.

\bibitem[Graves(2011)]{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  2348--2356, 2011.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock \emph{arXiv preprint arXiv:1706.04599}, 2017.

\bibitem[Hastings(1970)]{hastings1970monte}
W.~K. Hastings.
\newblock {Monte Carlo sampling methods using Markov chains and their
  applications}.
\newblock \emph{Biometrika}, 57\penalty0 (1):\penalty0 97--109, 04 1970.
\newblock ISSN 0006-3444.
\newblock \doi{10.1093/biomet/57.1.97}.
\newblock URL \url{https://doi.org/10.1093/biomet/57.1.97}.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{he2019asymmetric}
Haowei He, Gao Huang, and Yang Yuan.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2553--2564, 2019.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
Matthew~D Hoffman, David~M Blei, Chong Wang, and John Paisley.
\newblock Stochastic variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Kendall and Gal(2017)]{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in bayesian deep learning for computer
  vision?
\newblock In \emph{Advances in neural information processing systems}, pages
  5574--5584, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Durk~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in neural information processing systems}, pages
  2575--2583, 2015.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Kucukelbir et~al.(2017)Kucukelbir, Tran, Ranganath, Gelman, and
  Blei]{kucukelbir2017automatic}
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David~M Blei.
\newblock Automatic differentiation variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 430--474, 2017.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, Haffner,
  et~al.]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, Patrick Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Louizos and Welling(2017)]{louizos2017multiplicative}
Christos Louizos and Max Welling.
\newblock Multiplicative normalizing flows for variational bayesian neural
  networks.
\newblock \emph{arXiv preprint arXiv:1703.01961}, 2017.

\bibitem[Ma et~al.(2015)Ma, Chen, and Fox]{ma2015complete}
Yi-An Ma, Tianqi Chen, and Emily Fox.
\newblock A complete recipe for stochastic gradient mcmc.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2917--2925, 2015.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019simple}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13153--13164, 2019.

\bibitem[Mairal(2015)]{mairal2015incremental}
Julien Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  829--855, 2015.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock \emph{arXiv preprint arXiv:1701.05369}, 2017.

\bibitem[Neal(2012)]{neal2012bayesian}
Radford~M Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Polyak(1990)]{polyak1990sa}
Boris~T Polyak.
\newblock A new method of stochastic approximation type.
\newblock \emph{Avtomat. i Telemekh}, 7:\penalty0 98:107, 1990.

\bibitem[Polyak and Juditsky(1992)]{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM journal on control and optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Ruppert(1988)]{ruppert1988efficient}
David Ruppert.
\newblock Efficient estimations from a slowly convergent robbins-monro process.
\newblock Technical report, Cornell University Operations Research and
  Industrial Engineering, 1988.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{wen2018flipout}
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock \emph{arXiv preprint arXiv:1803.04386}, 2018.

\bibitem[Wu et~al.(2018)Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and
  Gaunt]{wu2018deterministic}
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard~E Turner, Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato, and Alexander~L Gaunt.
\newblock Deterministic variational inference for robust bayesian neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.03958}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Li, Zhang, Chen, and
  Wilson]{zhang2019cyclical}
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew~Gordon Wilson.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock \emph{arXiv preprint arXiv:1902.03932}, 2019.

\bibitem[Zhou and Cong(2017)]{zhou2017convergence}
Fan Zhou and Guojing Cong.
\newblock On the convergence properties of a $ k $-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1708.01012}, 2017.

\end{thebibliography}
