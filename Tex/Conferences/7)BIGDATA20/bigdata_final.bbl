\begin{thebibliography}{10}

\bibitem{Proc:Bach_NIPS11}
Francis~R. Bach and Eric Moulines.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 451--459, Granada, Spain, 2011.

\bibitem{Bach_NIPS13}
Francis~R. Bach and Eric Moulines.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate {O}(1/n).
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 773--781, Lake Tahoe, NV, 2013.

\bibitem{Proc:Blum_COLT99}
Avrim Blum, Adam Kalai, and John Langford.
\newblock Beating the hold-out: Bounds for k-fold and progressive
  cross-validation.
\newblock In Shai Ben{-}David and Philip~M. Long, editors, {\em Proceedings of
  the Twelfth Annual Conference on Computational Learning Theory (COLT)}, pages
  203--208, Santa Cruz, CA, 1999.

\bibitem{Proc:Bottou_COMPSTAT10}
L{\'{e}}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In Yves Lechevallier and Gilbert Saporta, editors, {\em Proceedings
  of the 19th International Conference on Computational Statistics (COMPSTAT)},
  pages 177--186, Paris, France, 2010.

\bibitem{Collect:Bottou_LNCS12}
L{\'{e}}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In Gr{\'{e}}goire Montavon, Genevieve~B. Orr, and Klaus{-}Robert
  M{\"{u}}ller, editors, {\em Neural Networks: Tricks of the Trade - Second
  Edition}, volume 7700 of {\em Lecture Notes in Computer Science}, pages
  421--436. Springer, 2012.

\bibitem{Article:Bottou_SIREV18}
L{\'{e}}on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em {SIAM} Review}, 60(2):223--311, 2018.

\bibitem{Proc:Chee_AISTATS18}
Jerry Chee and Panos Toulis.
\newblock Convergence diagnostics for stochastic gradient descent with constant
  learning rate.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 1476--1485, Playa Blanca, Lanzarote, Canary
  Islands, Spain, 2018.

\bibitem{Proc:Chen_FODS20}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In {\em ACM-IMS Foundations of Data Science Conference (FODS)},
  Seattle, WA, 2020.

\bibitem{Article:Chen_MP19}
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma.
\newblock Gradient descent with random initialization: fast global convergence
  for nonconvex phase retrieval.
\newblock {\em Math. Program.}, 176(1-2):5--37, 2019.

\bibitem{Article:Delyon_SIOPT93}
Bernard Delyon and Anatoli~B. Juditsky.
\newblock Accelerated stochastic approximation.
\newblock {\em {SIAM} Journal on Optimization}, 3(4):868--881, 1993.

\bibitem{BooK:Ermoliev_1988}
Yu~M Ermoliev and RJ-B Wets.
\newblock {\em Numerical techniques for stochastic optimization}.
\newblock Springer-Verlag, 1988.

\bibitem{Proc:Ge_NeurIPS19}
Rong Ge, Sham~M. Kakade, Rahul Kidambi, and Praneeth Netrapalli.
\newblock The step decay schedule: {A} near optimal, geometrically decaying
  learning rate procedure for least squares.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 14951--14962, Vancouver, Canada, 2019.

\bibitem{Proc:He_CVPR16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the 2016 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 770--778, Las Vegas, NV, 2016.

\bibitem{Report:Shirish_arXiv17}
Nitish~Shirish Keskar and Richard Socher.
\newblock Improving generalization performance by switching from adam to {SGD}.
\newblock Technical report, arXiv:1712.07628, 2017.

\bibitem{Article:Kesten_AOMS58}
Harry Kesten.
\newblock Accelerated stochastic approximation.
\newblock {\em The Annals of Mathematical Statistics}, 29(1):41--59, 1958.

\bibitem{Proc:Kingma_ICLR15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem{Article:Krizhevsky_CACM17}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Commun. {ACM}}, 60(6):84--90, 2017.

\bibitem{Proc:Lang_NeurIPS19}
Hunter Lang, Lin Xiao, and Pengchuan Zhang.
\newblock Using statistics to automate stochastic optimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 9536--9546, Vancouver, Canada, 2019.

\bibitem{Report:Loizou_arXiv17}
Nicolas Loizou and Peter Richt{\'{a}}rik.
\newblock Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods.
\newblock Technical report, arXiv:1712.09677, 2017.

\bibitem{Article:Murata_1998}
Noboru Murata.
\newblock A statistical study of on-line learning.
\newblock {\em Online Learning and Neural Networks. Cambridge University Press,
  Cambridge, UK}, pages 63--92, 1998.

\bibitem{Article:Needell_MP16}
Deanna Needell, Nathan Srebro, and Rachel Ward.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock {\em Math. Program.}, 155(1-2):549--573, 2016.

\bibitem{Article:Nemirovski_SIOPT09}
Arkadi Nemirovski, Anatoli~B. Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em {SIAM} Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem{Article:Pflug_1990}
Georg~Ch Pflug.
\newblock Non-asymptotic confidence bounds for stochastic approximation
  algorithms with constant step size.
\newblock {\em Monatshefte f{\"u}r Mathematik}, 110(3):297--314, 1990.

\bibitem{Article:Polyak_1964}
B.T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem{Proc:Reddi_NIPS15}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab{\'{a}}s P{\'{o}}czos, and
  Alexander~J. Smola.
\newblock On variance reduction in stochastic gradient descent and its
  asynchronous variants.
\newblock In Corinna Cortes, Neil~D. Lawrence, Daniel~D. Lee, Masashi Sugiyama,
  and Roman Garnett, editors, {\em Advances in Neural Information Processing
  Systems (NIPS)}, pages 2647--2655, Montreal, Canada, 2015.

\bibitem{Proc:Roux_NIPS12}
Nicolas~Le Roux, Mark Schmidt, and Francis~R. Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 2672--2680, Lake Tahoe, NV, 2012.

\bibitem{matteo2019JSM}
Matteo Sordello and Weijie Su.
\newblock Data-adaptive learning rate selection for stochastic gradient descent
  using convergence diagnostic.
\newblock Joint Statistics Meeting, 2019.

\bibitem{Report:Su_arXiv18}
Weijie Su and Yuancheng Zhu.
\newblock Uncertainty quantification for online learning and stochastic
  approximation via hierarchical incremental gradient descent.
\newblock Technical report, arXiv:1802.04876, 2018.

\bibitem{Report:Yang_arXiv16}
Zhe~Li Tianbao~Yang, Qihang~Lin.
\newblock Unified convergence analysis of stochastic momentum methods for
  convex and non-convex optimization.
\newblock Technical report, arXiv:1604.03257, 2016.

\bibitem{Article:Toulis_SAC15}
Panos Toulis and Edoardo~M. Airoldi.
\newblock Scalable estimation strategies based on stochastic approximations:
  classical results and new insights.
\newblock {\em Stat. Comput.}, 25(4):781--795, 2015.

\bibitem{Article:Toulis_AOS17}
Panos Toulis, Edoardo~M Airoldi, et~al.
\newblock Asymptotic and finite-sample properties of estimators based on
  stochastic gradients.
\newblock {\em The Annals of Statistics}, 45(4):1694--1727, 2017.

\bibitem{Proc:Yaida_ICLR19}
Sho Yaida.
\newblock Fluctuation-dissipation relations for stochastic gradient descent.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{Proc:Yin_1989}
George Yin.
\newblock Stopping times for stochastic approximation.
\newblock In {\em Modern Optimal Control: A Conference in Honor of Solomon
  Lefschetz and Joseph P. LaSalle}, pages 409--420, 1989.

\bibitem{Proc:Zhang_ICML04}
Tong Zhang.
\newblock Solving large scale linear prediction problems using stochastic
  gradient descent algorithms.
\newblock In {\em Proceedings of the Twenty-first International Conference on
  Machine Learning (ICML)}, Banff, Canada, 2004.

\end{thebibliography}
