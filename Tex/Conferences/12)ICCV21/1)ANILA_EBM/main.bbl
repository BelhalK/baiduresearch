\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Atchad{\'e}(2006)]{atchade2006adaptive}
Yves~F Atchad{\'e}.
\newblock An adaptive version for the metropolis adjusted langevin algorithm
  with a truncated drift.
\newblock \emph{Methodology and Computing in applied Probability}, 8\penalty0
  (2):\penalty0 235--254, 2006.

\bibitem[Bottou and Bousquet(2008)]{bottou2008}
L\'{e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In J.~C. Platt, D.~Koller, Y.~Singer, and S.~T. Roweis, editors,
  \emph{Advances in Neural Information Processing Systems 20}, pages 161--168.
  Curran Associates, Inc., 2008.

\bibitem[Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and
  Ranzato]{deng2020residual}
Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato.
\newblock Residual energy-based models for text generation.
\newblock \emph{arXiv preprint arXiv:2004.11714}, 2020.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock 2019.

\bibitem[Du et~al.(2020)Du, Li, Tenenbaum, and Mordatch]{du2020improved}
Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch.
\newblock Improved contrastive divergence training of energy based models.
\newblock \emph{arXiv preprint arXiv:2012.01316}, 2020.

\bibitem[Gao et~al.(2018)Gao, Lu, Zhou, Zhu, and Wu]{gao2018learning}
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning generative convnets via multi-grid modeling and sampling.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9155--9164, 2018.

\bibitem[Girolami and Calderhead(2011)]{girolami2011riemann}
Mark Girolami and Ben Calderhead.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73\penalty0 (2):\penalty0 123--214, 2011.

\bibitem[Grenander and Miller(1994)]{grenander1994representations}
Ulf Grenander and Michael~I Miller.
\newblock Representations of knowledge in complex systems.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 56\penalty0 (4):\penalty0 549--581, 1994.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pages
  1352--1361. PMLR, 2017.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Jacob et~al.(2020)Jacob, O~Leary, and Atchad{\'e}]{jacob2020unbiased}
Pierre~E Jacob, John O~Leary, and Yves~F Atchad{\'e}.
\newblock Unbiased markov chain monte carlo methods with couplings.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82\penalty0 (3):\penalty0 543--600, 2020.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, and F~Huang.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting structured data}, 1\penalty0 (0), 2006.

\bibitem[Marshall and Roberts(2012)]{marshall2012adaptive}
Tristan Marshall and Gareth Roberts.
\newblock An adaptive approach to langevin mcmc.
\newblock \emph{Statistics and Computing}, 22\penalty0 (5):\penalty0
  1041--1057, 2012.

\bibitem[Meyn and Tweedie(2012)]{meyn2012markov}
Sean~P Meyn and Richard~L Tweedie.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and
  Dean]{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock \emph{arXiv preprint arXiv:1310.4546}, 2013.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Ngiam et~al.(2011)Ngiam, Chen, Koh, and Ng]{ngiam2011learning}
Jiquan Ngiam, Zhenghao Chen, Pang~W Koh, and Andrew~Y Ng.
\newblock Learning deep energy models.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 1105--1112, 2011.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock \emph{arXiv preprint arXiv:1904.09770}, 2019.

\bibitem[Qiu et~al.(2019)Qiu, Zhang, and Wang]{qiu2019unbiased}
Yixuan Qiu, Lingsong Zhang, and Xiao Wang.
\newblock Unbiased contrastive divergence algorithm for training energy-based
  latent variable models.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Robbins and Monro(1951)]{robbins1951A}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock \emph{Annals of Mathematical Statistics}, 22:\penalty0 400--407,
  1951.

\bibitem[Roberts et~al.(1996)Roberts, Tweedie, et~al.]{roberts1996exponential}
Gareth~O Roberts, Richard~L Tweedie, et~al.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock \emph{Bernoulli}, 2\penalty0 (4):\penalty0 341--363, 1996.

\bibitem[Song and Kingma(2021)]{song2021train}
Yang Song and Diederik~P Kingma.
\newblock How to train your energy-based models.
\newblock \emph{arXiv preprint arXiv:2101.03288}, 2021.

\bibitem[Song et~al.(2020)Song, Garg, Shi, and Ermon]{song2020sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 574--584.
  PMLR, 2020.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tijmen Tieleman.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 1064--1071, 2008.

\bibitem[Welling and Hinton(2002)]{welling2002new}
Max Welling and Geoffrey~E Hinton.
\newblock A new learning algorithm for mean field boltzmann machines.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pages 351--357. Springer, 2002.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem[Wenliang et~al.(2019)Wenliang, Sutherland, Strathmann, and
  Gretton]{wenliang2019learning}
Li~Wenliang, Dougal Sutherland, Heiko Strathmann, and Arthur Gretton.
\newblock Learning deep kernels for exponential family densities.
\newblock In \emph{International Conference on Machine Learning}, pages
  6737--6746. PMLR, 2019.

\bibitem[Xie et~al.(2016)Xie, Lu, Zhu, and Wu]{xie2016theory}
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning}, pages
  2635--2644. PMLR, 2016.

\bibitem[Xie et~al.(2020)Xie, Zheng, Gao, Wang, Zhu, and Wu]{xie2020generative}
Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and
  Ying~Nian Wu.
\newblock Generative voxelnet: Learning energy-based models for 3d shape
  synthesis and analysis.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2020.

\bibitem[Zhu et~al.(1998)Zhu, Wu, and Mumford]{zhu1998filters}
Song~Chun Zhu, Yingnian Wu, and David Mumford.
\newblock Filters, random fields and maximum entropy (frame): Towards a unified
  theory for texture modeling.
\newblock \emph{International Journal of Computer Vision}, 27\penalty0
  (2):\penalty0 107--126, 1998.

\end{thebibliography}
