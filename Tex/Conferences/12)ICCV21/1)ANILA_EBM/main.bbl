\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{allassonniere2015convergent}
St{\'e}phanie Allassonniere and Estelle Kuhn.
\newblock Convergent stochastic expectation maximization algorithm with
  efficient sampling in high dimension. application to deformable template
  model estimation.
\newblock {\em Computational Statistics \& Data Analysis}, 91:4--19, 2015.

\bibitem{atchade2006adaptive}
Yves~F Atchad{\'e}.
\newblock An adaptive version for the metropolis adjusted langevin algorithm
  with a truncated drift.
\newblock {\em Methodology and Computing in applied Probability},
  8(2):235--254, 2006.

\bibitem{bottou2008}
L\'{e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In J.~C. Platt, D. Koller, Y. Singer, and S.~T. Roweis, editors, {\em
  Advances in Neural Information Processing Systems 20}, pages 161--168. Curran
  Associates, Inc., 2008.

\bibitem{brosse2017tamed}
Nicolas Brosse, Alain Durmus, {\'E}ric Moulines, and Sotirios Sabanis.
\newblock The tamed unadjusted langevin algorithm.
\newblock {\em arXiv preprint arXiv:1710.05559}, 2017.

\bibitem{cotter2013mcmc}
Simon~L Cotter, Gareth~O Roberts, Andrew~M Stuart, and David White.
\newblock Mcmc methods for functions: modifying old algorithms to make them
  faster.
\newblock {\em Statistical Science}, pages 424--446, 2013.

\bibitem{freitas}
Nando de Freitas, Pedro H{\o}jen-S{\o}rensen, Michael~I Jordan, and Stuart
  Russell.
\newblock Variational mcmc.
\newblock {\em Proceedings of the Seventeenth Conference on Uncertainty in
  Artificial Intelligence}, pages 120--127, 2001.

\bibitem{deng2020residual}
Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato.
\newblock Residual energy-based models for text generation.
\newblock {\em arXiv preprint arXiv:2004.11714}, 2020.

\bibitem{doucet2000sequential}
Arnaud Doucet, Simon Godsill, and Christophe Andrieu.
\newblock On sequential monte carlo sampling methods for bayesian filtering.
\newblock {\em Statistics and computing}, 10(3):197--208, 2000.

\bibitem{du2020improved}
Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch.
\newblock Improved contrastive divergence training of energy based models.
\newblock {\em arXiv preprint arXiv:2012.01316}, 2020.

\bibitem{du2019energy}
Yilun Du, Joshua Meier, Jerry Ma, Rob Fergus, and Alexander Rives.
\newblock Energy-based models for atomic-resolution protein conformations.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{du2020energy}
Yilun Du, Joshua Meier, Jerry Ma, Rob Fergus, and Alexander Rives.
\newblock Energy-based models for atomic-resolution protein conformations.
\newblock {\em arXiv preprint arXiv:2004.13167}, 2020.

\bibitem{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alche-Buc, E. Fox,
  and R. Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{durmus2017fast}
Alain Durmus, Gareth~O. Roberts, Gilles Vilmart, and Konstantinos~C. Zygalakis.
\newblock Fast langevin based algorithm for mcmc in high dimensions.
\newblock {\em Ann. Appl. Probab.}, 27(4):2195--2237, 08 2017.

\bibitem{gao2018learning}
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning generative convnets via multi-grid modeling and sampling.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9155--9164, 2018.

\bibitem{gao2020flow}
Ruiqi Gao, Erik Nijkamp, Diederik~P Kingma, Zhen Xu, Andrew~M Dai, and
  Ying~Nian Wu.
\newblock Flow contrastive estimation of energy-based models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7518--7528, 2020.

\bibitem{girolami}
Mark Girolami and Ben Calderhead.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73(2):123--214, 2011.

\bibitem{girolami2011riemann}
Mark Girolami and Ben Calderhead.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73(2):123--214, 2011.

\bibitem{goodfellow2014generative}
Ian~J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1406.2661}, 2014.

\bibitem{grathwohl2020your}
Will Grathwohl, Kuan-Chieh Wang, J{\"o}rn-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{grenander1994representations}
Ulf Grenander and Michael~I Miller.
\newblock Representations of knowledge in complex systems.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 56(4):549--581, 1994.

\bibitem{gustafsson2020energy}
Fredrik~K Gustafsson, Martin Danelljan, Goutam Bhat, and Thomas~B Sch{\"o}n.
\newblock Energy-based models for deep probabilistic regression.
\newblock In {\em European Conference on Computer Vision}, pages 325--343.
  Springer, 2020.

\bibitem{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em International Conference on Machine Learning}, pages
  1352--1361. PMLR, 2017.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock {\em arXiv preprint arXiv:1706.08500}, 2017.

\bibitem{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural computation}, 14(8):1771--1800, 2002.

\bibitem{ingraham2018learning}
John Ingraham, Adam Riesselman, Chris Sander, and Debora Marks.
\newblock Learning protein structure with a differentiable simulator.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{jacob2020unbiased}
Pierre~E Jacob, John O~Leary, and Yves~F Atchad{\'e}.
\newblock Unbiased markov chain monte carlo methods with couplings.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82(3):543--600, 2020.

\bibitem{jarner2000geometric}
S{\o}ren~Fiig Jarner and Ernst Hansen.
\newblock Geometric ergodicity of metropolis algorithms.
\newblock {\em Stochastic processes and their applications}, 85(2):341--361,
  2000.

\bibitem{KB15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em ICLR}, 2015.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang.
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting structured data}, 1(0), 2006.

\bibitem{ebmood2020}
Weitang Liu, Xiaoyun Wang, John~D. Owens, and Yixuan Li.
\newblock Energy-based out-of-distribution detection.
\newblock In {\em Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{marshall2012adaptive}
Tristan Marshall and Gareth Roberts.
\newblock An adaptive approach to langevin mcmc.
\newblock {\em Statistics and Computing}, 22(5):1041--1057, 2012.

\bibitem{meyn2012markov}
Sean~P Meyn and Richard~L Tweedie.
\newblock {\em Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock {\em arXiv preprint arXiv:1310.4546}, 2013.

\bibitem{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock {\em Handbook of markov chain monte carlo}, 2(11):2, 2011.

\bibitem{ngiam2011learning}
Jiquan Ngiam, Zhenghao Chen, Pang~W Koh, and Andrew~Y Ng.
\newblock Learning deep energy models.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 1105--1112, 2011.

\bibitem{nijkamp2020anatomy}
Erik Nijkamp, Mitch Hill, Tian Han, Song{-}Chun Zhu, and Ying~Nian Wu.
\newblock On the anatomy of mcmc-based maximum likelihood learning of
  energy-based models.
\newblock In {\em The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 5272--5280. {AAAI} Press, 2020.

\bibitem{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock {\em arXiv preprint arXiv:1904.09770}, 2019.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em 2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pages 722--729. IEEE, 2008.

\bibitem{qiu2019unbiased}
Yixuan Qiu, Lingsong Zhang, and Xiao Wang.
\newblock Unbiased contrastive divergence algorithm for training energy-based
  latent variable models.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{robbins1951A}
H. Robbins and S. Monro.
\newblock A stochastic approximation method.
\newblock {\em Annals of Mathematical Statistics}, 22:400--407, 1951.

\bibitem{mh:robert}
Christian~P. Robert and George Casella.
\newblock {\em Metropolis--Hastings Algorithms}, pages 167--197.
\newblock Springer New York, New York, NY, 2010.

\bibitem{roberts}
G.~O. Roberts and J.~S. Rosenthal.
\newblock Optimal scaling of discrete approximations to langevin diffusions.
\newblock {\em J. R. Statist. Soc. B}, 60:255--268, 1997.

\bibitem{roberts2004general}
Gareth~O Roberts, Jeffrey~S Rosenthal, et~al.
\newblock General state space markov chains and mcmc algorithms.
\newblock {\em Probability surveys}, 1:20--71, 2004.

\bibitem{robertsmala}
Gareth~O. Roberts and Richard~L. Tweedie.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock {\em Bernoulli}, 2(4):341--363, 12 1996.

\bibitem{roberts1996exponential}
Gareth~O Roberts, Richard~L Tweedie, et~al.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock {\em Bernoulli}, 2(4):341--363, 1996.

\bibitem{rue2009approximate}
H{\aa}vard Rue, Sara Martino, and Nicolas Chopin.
\newblock Approximate bayesian inference for latent gaussian models by using
  integrated nested laplace approximations.
\newblock {\em Journal of the royal statistical society: Series b (statistical
  methodology)}, 71(2):319--392, 2009.

\bibitem{song2020sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 574--584.
  PMLR, 2020.

\bibitem{song2021train}
Yang Song and Diederik~P Kingma.
\newblock How to train your energy-based models.
\newblock {\em arXiv preprint arXiv:2101.03288}, 2021.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{tieleman2008training}
Tijmen Tieleman.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 1064--1071, 2008.

\bibitem{jordanvi}
Martin~J. Wainwright and Michael~I. Jordan.
\newblock Graphical models, exponential families, and variational inference.
\newblock {\em Found. Trends Mach. Learn.}, 1(1-2):1--305, Jan. 2008.

\bibitem{welling2002new}
Max Welling and Geoffrey~E Hinton.
\newblock A new learning algorithm for mean field boltzmann machines.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 351--357. Springer, 2002.

\bibitem{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem{wenliang2019learning}
Li Wenliang, Dougal Sutherland, Heiko Strathmann, and Arthur Gretton.
\newblock Learning deep kernels for exponential family densities.
\newblock In {\em International Conference on Machine Learning}, pages
  6737--6746. PMLR, 2019.

\bibitem{wolfinger}
Russ Wolfinger.
\newblock Laplace's approximation for nonlinear mixed models.
\newblock {\em Biometrika}, 80(4):791--795, 1993.

\bibitem{xie2018cooperative}
Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Cooperative training of descriptor and generator networks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  42(1):27--45, 2018.

\bibitem{xie2016theory}
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.
\newblock A theory of generative convnet.
\newblock In {\em International Conference on Machine Learning}, pages
  2635--2644. PMLR, 2016.

\bibitem{xie2021GPointNet}
Jianwen Xie, Yifei Xu, Zilong Zheng, Song{-}Chun Zhu, and Ying~Nian Wu.
\newblock Generative pointnet: energy-based learning on unordered point sets
  for 3d generation, reconstruction and classification.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2021.

\bibitem{xie2021cooperative}
Jianwen Xie, Zilong Zheng, Xiaolin Fang, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Cooperative training of fast thinking initializer and slow thinking
  solver for conditional learning.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2021.

\bibitem{xie2021cycleCoopNets}
Jianwen Xie, Zilong Zheng, Xiaolin Fang, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning cycle-consistent cooperative networks via alternating mcmc
  teaching for unsupervised cross-domain translation.
\newblock In {\em Proceedings of The Thirty-Fifth AAAI Conference on Artificial
  Intelligence (AAAI)}, 2021.

\bibitem{xie2018learning}
Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying
  Nian~Wu.
\newblock Learning descriptor networks for 3d shape synthesis and analysis.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 8629--8638, 2018.

\bibitem{xie2020generative}
Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and
  Ying~Nian Wu.
\newblock Generative voxelnet: Learning energy-based models for 3d shape
  synthesis and analysis.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2020.

\bibitem{XieCVPR17}
Jianwen Xie, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Synthesizing dynamic patterns by spatial-temporal generative convnet.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 7093--7101, 2017.

\bibitem{xie2019learning}
Jianwen Xie, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning energy-based spatial-temporal generative convnets for
  dynamic patterns.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)}, 2019.

\bibitem{xu2019energy}
Yifei Xu, Jianwen Xie, Tianyang Zhao, Chris Baker, Yibiao Zhao, and Ying~Nian
  Wu.
\newblock Energy-based continuous inverse optimal control.
\newblock {\em arXiv preprint arXiv:1904.05453}, 2019.

\bibitem{zhu1998filters}
Song~Chun Zhu, Yingnian Wu, and David Mumford.
\newblock Filters, random fields and maximum entropy (frame): Towards a unified
  theory for texture modeling.
\newblock {\em International Journal of Computer Vision}, 27(2):107--126, 1998.

\end{thebibliography}
