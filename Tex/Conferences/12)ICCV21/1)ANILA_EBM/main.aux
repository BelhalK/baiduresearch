\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhu1998filters,lecun2006tutorial}
\citation{ngiam2011learning,xie2016theory,xie2020generative,du2019implicit}
\citation{mikolov2013distributed,deng2020residual}
\citation{wenliang2019learning,song2020sliced}
\citation{haarnoja2017reinforcement}
\citation{xie2016theory}
\citation{song2021train}
\citation{song2021train}
\citation{lecun2006tutorial,ngiam2011learning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}On MCMC based Energy Based Models}{1}{section.2}}
\newlabel{eq:ebm}{{1}{1}{On MCMC based Energy Based Models}{equation.2.1}{}}
\citation{welling2011bayesian}
\@writefile{toc}{\contentsline {paragraph}{Energy Based Models: }{2}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{MCMC procedures: }{2}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Informed Langevin Diffusion}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminaries and Bottlenecks of Langevin MCMC based EBM}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Curvature informed MCMC}{2}{subsection.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {StAnLey}for Energy-Based Model}}{2}{algorithm.1}}
\newlabel{alg:anila}{{1}{2}{Curvature informed MCMC}{algorithm.1}{}}
\newlabel{line:step}{{3}{2}{Curvature informed MCMC}{ALC@unique.3}{}}
\newlabel{line:langevin}{{4}{2}{Curvature informed MCMC}{ALC@unique.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Geometric ergodicity of AniLA sampler}{2}{section.4}}
\citation{meyn2012markov}
\newlabel{ass:cont}{{1}{3}{}{assumption.1}{}}
\newlabel{ass:contlogpi}{{2}{3}{}{assumption.2}{}}
\newlabel{ass:V2}{{3}{3}{}{assumption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{5}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Application on Toy Example: Gaussian Mixture Model}{5}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (Rings Toy Dataset) }}{5}{figure.1}}
\newlabel{fig:results}{{1}{5}{(Rings Toy Dataset)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Flowers Dataset}{5}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (Flowers Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations.}}{5}{figure.2}}
\newlabel{fig:flowers}{{2}{5}{(Flowers Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}CIFAR Dataset}{5}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (CIFAR Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations.}}{5}{figure.3}}
\newlabel{fig:cifar}{{3}{5}{(CIFAR Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{6}{section.6}}
\bibstyle{plainnat}
\bibdata{ref}
\bibcite{deng2020residual}{{1}{2020}{{Deng et~al.}}{{Deng, Bakhtin, Ott, Szlam, and Ranzato}}}
\bibcite{du2019implicit}{{2}{2019}{{Du and Mordatch}}{{}}}
\bibcite{haarnoja2017reinforcement}{{3}{2017}{{Haarnoja et~al.}}{{Haarnoja, Tang, Abbeel, and Levine}}}
\bibcite{lecun2006tutorial}{{4}{2006}{{LeCun et~al.}}{{LeCun, Chopra, Hadsell, Ranzato, and Huang}}}
\bibcite{meyn2012markov}{{5}{2012}{{Meyn and Tweedie}}{{}}}
\bibcite{mikolov2013distributed}{{6}{2013}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{ngiam2011learning}{{7}{2011}{{Ngiam et~al.}}{{Ngiam, Chen, Koh, and Ng}}}
\bibcite{song2021train}{{8}{2021}{{Song and Kingma}}{{}}}
\bibcite{song2020sliced}{{9}{2020}{{Song et~al.}}{{Song, Garg, Shi, and Ermon}}}
\bibcite{welling2011bayesian}{{10}{2011}{{Welling and Teh}}{{}}}
\bibcite{wenliang2019learning}{{11}{2019}{{Wenliang et~al.}}{{Wenliang, Sutherland, Strathmann, and Gretton}}}
\bibcite{xie2016theory}{{12}{2016}{{Xie et~al.}}{{Xie, Lu, Zhu, and Wu}}}
\bibcite{xie2020generative}{{13}{2020}{{Xie et~al.}}{{Xie, Zheng, Gao, Wang, Zhu, and Wu}}}
\bibcite{zhu1998filters}{{14}{1998}{{Zhu et~al.}}{{Zhu, Wu, and Mumford}}}
