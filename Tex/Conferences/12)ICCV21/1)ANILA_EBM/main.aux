\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhu1998filters,lecun2006tutorial}
\citation{ngiam2011learning,xie2016theory,xie2020generative,du2019implicit}
\citation{mikolov2013distributed,deng2020residual}
\citation{wenliang2019learning,song2020sliced}
\citation{haarnoja2017reinforcement}
\citation{xie2016theory}
\citation{song2021train}
\citation{song2021train}
\citation{meyn2012markov}
\citation{nijkamp2019learning}
\citation{hinton2002training}
\citation{tieleman2008training}
\citation{welling2002new,gao2018learning,du2019implicit}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{meyn2012markov}
\citation{qiu2019unbiased,jacob2020unbiased}
\citation{du2020improved}
\citation{robbins1951A,bottou2008}
\@writefile{toc}{\contentsline {section}{\numberline {2}On MCMC based Energy Based Models}{2}{section.2}}
\newlabel{sec:mcmc}{{2}{2}{On MCMC based Energy Based Models}{section.2}{}}
\newlabel{eq:ebm}{{1}{2}{On MCMC based Energy Based Models}{equation.2.1}{}}
\newlabel{eq:mle}{{2}{2}{On MCMC based Energy Based Models}{equation.2.2}{}}
\citation{grenander1994representations,roberts1996exponential}
\citation{neal2011mcmc}
\citation{lecun2006tutorial,ngiam2011learning}
\citation{kingma2013auto}
\citation{goodfellow2014generative}
\citation{du2019implicit}
\citation{song2020score}
\citation{gao2020flow}
\citation{welling2011bayesian}
\newlabel{eq:mcapprox}{{4}{3}{On MCMC based Energy Based Models}{equation.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Energy Based Models: }{3}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{MCMC procedures: }{3}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Informed Langevin Diffusion}{3}{section.3}}
\newlabel{sec:main}{{3}{3}{Gradient Informed Langevin Diffusion}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminaries and Bottlenecks of Langevin MCMC based EBM}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Curvature informed MCMC}{3}{subsection.3.2}}
\citation{atchade2006adaptive,marshall2012adaptive}
\citation{girolami2011riemann}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {StAnLey}for Energy-Based Model}}{4}{algorithm.1}}
\newlabel{alg:anila}{{1}{4}{Curvature informed MCMC}{algorithm.1}{}}
\newlabel{line:step}{{3}{4}{Curvature informed MCMC}{ALC@unique.3}{}}
\newlabel{line:langevin}{{4}{4}{Curvature informed MCMC}{ALC@unique.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Geometric ergodicity of AniLA sampler}{4}{section.4}}
\newlabel{sec:theory}{{4}{4}{Geometric ergodicity of AniLA sampler}{section.4}{}}
\citation{meyn2012markov}
\newlabel{ass:cont}{{1}{5}{}{assumption.1}{}}
\newlabel{ass:contlogpi}{{2}{5}{}{assumption.2}{}}
\newlabel{ass:V2}{{3}{5}{}{assumption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{7}{section.5}}
\newlabel{sec:numericals}{{5}{7}{Numerical Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Application on Toy Example: Gaussian Mixture Model}{7}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (Rings Toy Dataset) }}{7}{figure.1}}
\newlabel{fig:results}{{1}{7}{(Rings Toy Dataset)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Flowers Dataset}{7}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (Flowers Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations.}}{7}{figure.2}}
\newlabel{fig:flowers}{{2}{7}{(Flowers Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}CIFAR Dataset}{7}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (CIFAR Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations.}}{7}{figure.3}}
\newlabel{fig:cifar}{{3}{7}{(CIFAR Dataset). Left: Langevin Method. Right: AniLA method. After 100k iterations}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}}
\newlabel{sec:conclusion}{{6}{8}{Conclusion}{section.6}{}}
\bibstyle{plainnat}
\bibdata{ref}
\bibcite{atchade2006adaptive}{{1}{2006}{{Atchad{\'e}}}{{}}}
\bibcite{bottou2008}{{2}{2008}{{Bottou and Bousquet}}{{}}}
\bibcite{deng2020residual}{{3}{2020}{{Deng et~al.}}{{Deng, Bakhtin, Ott, Szlam, and Ranzato}}}
\bibcite{du2019implicit}{{4}{2019}{{Du and Mordatch}}{{}}}
\bibcite{du2020improved}{{5}{2020}{{Du et~al.}}{{Du, Li, Tenenbaum, and Mordatch}}}
\bibcite{gao2018learning}{{6}{2018}{{Gao et~al.}}{{Gao, Lu, Zhou, Zhu, and Wu}}}
\bibcite{gao2020flow}{{7}{2020}{{Gao et~al.}}{{Gao, Nijkamp, Kingma, Xu, Dai, and Wu}}}
\bibcite{girolami2011riemann}{{8}{2011}{{Girolami and Calderhead}}{{}}}
\bibcite{goodfellow2014generative}{{9}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{grenander1994representations}{{10}{1994}{{Grenander and Miller}}{{}}}
\bibcite{haarnoja2017reinforcement}{{11}{2017}{{Haarnoja et~al.}}{{Haarnoja, Tang, Abbeel, and Levine}}}
\bibcite{hinton2002training}{{12}{2002}{{Hinton}}{{}}}
\bibcite{jacob2020unbiased}{{13}{2020}{{Jacob et~al.}}{{Jacob, O~Leary, and Atchad{\'e}}}}
\bibcite{kingma2013auto}{{14}{2013}{{Kingma and Welling}}{{}}}
\bibcite{lecun2006tutorial}{{15}{2006}{{LeCun et~al.}}{{LeCun, Chopra, Hadsell, Ranzato, and Huang}}}
\bibcite{marshall2012adaptive}{{16}{2012}{{Marshall and Roberts}}{{}}}
\bibcite{meyn2012markov}{{17}{2012}{{Meyn and Tweedie}}{{}}}
\bibcite{mikolov2013distributed}{{18}{2013}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{neal2011mcmc}{{19}{2011}{{Neal et~al.}}{{}}}
\bibcite{ngiam2011learning}{{20}{2011}{{Ngiam et~al.}}{{Ngiam, Chen, Koh, and Ng}}}
\bibcite{nijkamp2019learning}{{21}{2019}{{Nijkamp et~al.}}{{Nijkamp, Hill, Zhu, and Wu}}}
\bibcite{qiu2019unbiased}{{22}{2019}{{Qiu et~al.}}{{Qiu, Zhang, and Wang}}}
\bibcite{robbins1951A}{{23}{1951}{{Robbins and Monro}}{{}}}
\bibcite{roberts1996exponential}{{24}{1996}{{Roberts et~al.}}{{Roberts, Tweedie, et~al.}}}
\bibcite{song2021train}{{25}{2021}{{Song and Kingma}}{{}}}
\bibcite{song2020sliced}{{26}{2020{a}}{{Song et~al.}}{{Song, Garg, Shi, and Ermon}}}
\bibcite{song2020score}{{27}{2020{b}}{{Song et~al.}}{{Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole}}}
\bibcite{tieleman2008training}{{28}{2008}{{Tieleman}}{{}}}
\bibcite{welling2002new}{{29}{2002}{{Welling and Hinton}}{{}}}
\bibcite{welling2011bayesian}{{30}{2011}{{Welling and Teh}}{{}}}
\bibcite{wenliang2019learning}{{31}{2019}{{Wenliang et~al.}}{{Wenliang, Sutherland, Strathmann, and Gretton}}}
\bibcite{xie2016theory}{{32}{2016}{{Xie et~al.}}{{Xie, Lu, Zhu, and Wu}}}
\bibcite{xie2020generative}{{33}{2020}{{Xie et~al.}}{{Xie, Zheng, Gao, Wang, Zhu, and Wu}}}
\bibcite{zhu1998filters}{{34}{1998}{{Zhu et~al.}}{{Zhu, Wu, and Mumford}}}
