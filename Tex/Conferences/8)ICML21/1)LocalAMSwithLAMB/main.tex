\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}

\input{shortcuts}




\begin{document}



\title{Layerwise and Dimensionwise Adaptive Local AMSMethod for Federated Learning}

% \author{\textbf{Belhal Karimi, Xiaoyun Li, Ping Li} \\\\
% Cognitive Computing Lab\\
% Baidu Research\\
%   10900 NE 8th St. Bellevue, WA 98004, USA
% }

\date{}
\maketitle

\begin{abstract}
To be completed...
\end{abstract}

\section{Introduction}\label{sec:introduction}

A growing and important task while learning models on observed data, is the ability to train the latter over a large number of clients which could either be devices or distinct entities.
In the paradigm of Federated Learning (FL)~\citep{konevcny2016federated,mcmahan2017communication}, the focus of our paper, a central server orchestrates the optimization over those clients under the constraint that the data can neither be centralized nor shared among the clients.
Most modern machine learning tasks can be casted as a large finite-sum optimization problem written as:
\begin{equation}
\min \limits_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n f_i(\theta)
\end{equation}
where $n$ denotes the number of workers, $f_i$ represents the average loss for worker $i$ and $\theta$ the global model parameter taking value in $\Theta$ a subset of $\mathbb{R}^d$.
While this formulation recalls that of distributed optimization, the core principle of FL is different that standard distributed paradigm.

FL currently suffers from two bottlenecks: communication efficiency and privacy.
We focus on the former in this paper.
While local updates, updates during which each client learn their local models, can reduce drastically the number of communication rounds between the central server and devices, new techniques must be employed to tackle this challenge.
Some quantization~\citep{alistarh2017qsgd, wangni2018gradient} or compression~\citep{lin2017deep} methods allow to decrease the number of bits communicated at each round and are efficient method in a distributed setting.
The other approach one can take is to accelerate the local training on each device and thus sending a better local model to the server at each round.

Under the important setting of heterogenous data, i.e. the data among each device can be distributed according to different distributions, current local optimization algorithms are perfectible.
The most popular method for FL is using multiple local Stochastic Gradient Descent (\textsc{SGD}) steps in each device, sending those local models to the server that computes the average over those received local vector of parameters and broadcasts it back to the devices. This is called \textsc{FedAvg} and has been introduced in~\citep{mcmahan2017communication}.

In \citep{chen2020toward}, the authors motivate the usage of adaptive gradient optimization methods as a better alternative to the standard \textsc{SGD} inner loop in \textsc{FedAvg}.
They propose an adaptive gradient method, namely \textsc{Local AMSGrad}, with communication cost sublinear in $T$ that is guaranteed to converge to stationary points in $\mathcal{O}(\sqrt{d/Tn})$, where T is the number of iterations.

Based on recent progress in adaptive methods for accelerating the training procedure, see~\citep{you2019large}, we propose a variant of \textsc{Local AMSGrad} integrating dimensionwise and layerwise adaptive learning rate in each device's local update.
Our contributions are as follows:
\begin{itemize}
\item We develop a novel optimization algorithm for federated learning, namely \textsc{Fed-LAMB}, following a principled layerwise adaptation strategy to accelerate training of deep neural networks.
\item theoretical results
\item We exhibit the advantages of our method on several benchmarks supervised learning methods on both homogeneous and heterogeneous settings.
\end{itemize}

\subsection{Related Work}\label{sec:related}

\paragraph{Federated learning.}



\paragraph{Adaptive gradient methods.}


\section{Layerwise and Dimensionwise Adaptive Methods}\label{sec:main}

\paragraph{Notations:} We denote by $\theta$ the vector of parameters taking values in $\rset^d$. 
For each layer $\ell \in \llbracket \tot \rrbracket$, where $\tot$ is the total number of layers of the neural networks, and each coordinate $j \in \llbracket p_\ell \rrbracket$ where $p_\ell$ is the dimension per layer $\ell$, we note $\theta^{\ell, j}$ its $j$th coordinate.
The gradient of $f$ with respect to $\theta^\ell$ is denoted by $\nabla_{\ell} f(\theta)$.
The index $i \in \inter$ denotes the index of the worker $i$ in our federated framework.
$r$ and $t$ are used as the round and local iteration numbers respectively.
The smoothness per layer is denoted by $L_\ell$ for each layer $\ell \in \llbracket \tot \rrbracket$.


\subsection{Local AMS with LAMB}

We propose a layerwise and dimensionwise local AMS algorithm in the following:

\begin{algorithm}[H]
\caption{\textsc{L\&D Local AMS for Federated Learning}} \label{alg:ldams}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: parameter $\beta_1$, $\beta_2$, and learning rate $\alpha_t$. 
\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model shared by all devices and $v_{0} = \epsilon \mathsf{1} \in \mathbb R^{d}$ and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
\FOR{$r=1$ to $R$}
\STATE Set $\theta_{r,i}^{0} = \bar{\theta}_{r-1}$
\STATE\textbf{parallel for device $d \in D^{r}$ do}:
\STATE Compute stochastic gradient $g_{r,i}$ at $\theta_r$.
\quad \quad \FOR{$t=1$ to $T$}
\STATE $m^t_{r,i} = \beta_1 m^{t-1}_{r-1,i} + (1 - \beta_1) g_{r,i}$.
\STATE $m^{t}_{r,i}=m^{t}_{r,i} /\left(1-\beta_{1}^{r}\right)$.
\STATE $v^{t,i}_r = \beta_2 v^{t}_{r-1,i} + (1 - \beta_2) g_{r,i}^2$.
\STATE $v^{t}_{r,i}=v^{t}_{r,i} /\left(1-\beta_{2}^{r}\right)$.
\STATE $\hat{v}^{t}_{r} = \max( \hat{v}^{t}_{r-1} , \frac{1}{n} \sum_{i=1}^n v^t_{r,i} )$.
\STATE Compute ratio  $p_{r,i}=\frac{m^{t}_{r,i}}{\sqrt{v^{t}_{r}}+\epsilon}$.
\STATE Update local model for each layer $\ell$:
$$\theta_{r,i}^{\ell,t}=\theta_{r,i}^{\ell,t-1}-\alpha_{r} \phi(\|\theta_{r,i}^{\ell,t-1}\|)(p_{r,i}^{\ell}+\lambda \theta_{r,i}^{\ell,t-1}) / \|p_{r,i}^{\ell}+\lambda \theta_{r,i}^{\ell,t-1}\|$$
\ENDFOR
\STATE Devices send local model $\theta_{r,i}^{T} = [\theta_{r,i}^{\ell,T}]_{\ell =1}^{\tot}$ to the server
\STATE Server computes the averages of the local models $\bar{\theta}_r^\ell = \frac{1}{n} \sum_{i=1}^n \theta_{r,i}^{\ell,T}$ and send it back to the devices.
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Finite time convergence bounds}



In the context of nonconvex stochastic optimization for distributed devices, assume the following:

\begin{assumption}\label{ass:smooth}
For $i \in \inter$ and $\ell \in \interl$, $f_i$ is  L-smooth: $\norm{\nabla f_i (\theta) - \nabla f_i (\vartheta)} \leq L_\ell \norm{\theta^\ell-\vartheta^\ell}$.
\end{assumption}
We add some classical assumption in the unbiased stochastic optimization realm, on the gradient of the objective function:
\begin{assumption}\label{ass:boundgrad}
The stochastic gradient is unbiased for any iteration $r>0$: $\EE[g_r] = \nabla f(\theta_r)$ and is bounded from above, i.e., $\norm{g_t} \leq M$.
\end{assumption}

\begin{assumption}\label{ass:var}
The variance of the stochastic gradient is bounded for any iteration $r>0$ and any dimension $j \in \llbracket d \rrbracket$: $\EE[|g_r^j - \nabla f(\theta_r)^j|^2] < \sigma^2$.
\end{assumption}

\begin{assumption}\label{ass:phi}
For any value $a \in \rset^*_+$, there exists strictly positive constants such that $\phi_m \leq  \phi(a) \leq \phi_M$.
\end{assumption}


We now state our main result regarding the non asymptotic convergence analysis of our Algorithm~\ref{alg:ldams}:
\begin{Theorem}\label{th:main}
Consider $\{\overline{\theta_r}\}_{r>0}$, the sequence of parameters obtained running Algorithm~\ref{alg:ldams}. Then, if the number of local epochs is set to $T=1$ and $\epsilon = \lambda = 0$, we have:
\beq
\frac{1}{R} \sum_{r=1}^R \EE[\| \nabla f(\overline{\theta_r}) \|^2 \leq dd
\eeq
\end{Theorem}

\section{Numerical experiments}\label{sec:numerical}

\begin{figure}[h]
    \begin{center}
        \mbox{
        \includegraphics[width=2.5in]{figure/mnist_trainloss_cnn_ep1_client60_iid0.eps}
        \includegraphics[width=2.5in]{figure/mnist_testerror_cnn_ep1_client60_iid0.eps}
        }
    \end{center}
    \vspace{-0.1in}
	\caption{Test accuracy on CNN + MNIST. Non-iid data distribution.}
	\label{fig:mnist-cnn-noniid}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \mbox{
        \includegraphics[width=2.5in]{figure/cifar_testerror_cnn_ep1_client10_iid1.eps}
        \includegraphics[width=2.5in]{figure/cifar_testerror_cnn_ep1_client50_iid1.eps}
        }
        \mbox{
        \includegraphics[width=2.5in]{figure/cifar_testerror_cnn_ep3_client10_iid1.eps}
        \includegraphics[width=2.5in]{figure/cifar_testerror_cnn_ep3_client50_iid1.eps}
        }
    \end{center}
    \vspace{-0.1in}
	\caption{Test accuracy on CNN + CIFAR10. iid data distribution.}
	\label{fig:cifar-cnn-iid}
\end{figure}

\begin{figure}[h]
    \begin{center}
        \mbox{
        \includegraphics[width=2.5in]{figure/cifar_testerror_resnet_ep1_client10_iid1.eps}
        \includegraphics[width=2.5in]{figure/cifar_testerror_resnet_ep1_client50_iid1.eps}
        }
        \mbox{
        \includegraphics[width=2.5in]{figure/cifar_testerror_resnet_ep3_client10_iid1.eps}
        \includegraphics[width=2.5in]{figure/cifar_testerror_resnet_ep3_client50_iid1.eps}
        }
    \end{center}
    \vspace{-0.1in}
	\caption{Test accuracy on ResNet + CIFAR10. iid data distribution.}
	\label{fig:cifar-resnet-iid}
\end{figure}

\section{Conclusion}\label{sec:conclusion}

\clearpage

\bibliographystyle{plain}
\bibliography{ref}


\clearpage
\appendix 

\section{Appendix}\label{sec:appendix}


\section{Theoretical Analysis}

\subsection{Intermediary Lemmas}

\begin{Lemma}\label{lemma:iterates}
Consider $\{\overline{\theta_r}\}_{r>0}$, the sequence of parameters obtained running Algorithm~\ref{alg:ldams}. Then for $i \in \inter$:
\beq
\| \overline{\theta_r} - \theta_{r,i} \| \leq \alpha^2 M^2 \phi_M^2 \frac{(1-\beta_2)p}{v_0}
\eeq
\end{Lemma}

\begin{proof}
Assuming the simplest case when $T=1$, i.e. one local iteration, then by construction of Algorithm~\ref{alg:ldams}, we have for all $\ell \in \llbracket \tot \rrbracket$, $i \in \inter$ and $r >0$:
\beq
 \theta^{\ell}_{r,i} =  \overline{\theta_r}^{\ell}  - \alpha \phi(\|\theta_{r,i}^{\ell,t-1}\|)p_{r,i}^{j} / \|p_{r,i}^{\ell}\|=  \overline{\theta_r}^{\ell}  - \alpha \phi(\|\theta_{r,i}^{\ell,t-1}\|)  
 \frac{m^{t}_{r,i}}{\sqrt{v^{t}_{r}}} \frac{1}{\|p_{r,i}^{\ell}\|}
\eeq
leading to 
\beq
\begin{split}
\|\overline{\theta_r}   -  \theta_{r,i}\|^2 & = \pscal{\overline{\theta_r}^{\ell}   -  \theta^{\ell}_{r,i}}{\overline{\theta_r}^{\ell}   -  \theta^{\ell}_{r,i}} \\
& \leq \alpha^2 M^2 \phi_M^2 \frac{(1-\beta_2)p}{v_0}
\end{split}
\eeq
which concludes the proof.
\end{proof}


\subsection{Proof of Theorem~\ref{th:main}}

\begin{Theorem*}
Consider $\{\overline{\theta_r}\}_{r>0}$, the sequence of parameters obtained running Algorithm~\ref{alg:ldams}. Then, if the number of local epochs is set to $T=1$ and $\epsilon = \lambda = 0$, we have:
\beq
\frac{1}{R} \sum_{r=1}^R \EE[\| \nabla f(\overline{\theta_r}) \|^2 \leq dd
\eeq
\end{Theorem*}


\textbf{Case with $T=1$, $\epsilon = 0$ and $\lambda = 0$:}
Using H\ref{ass:smooth}, we have:
\begin{align}\notag
f(\bar{\vartheta}_{r+1}) &  \leq f(\bar{\vartheta}_r) + \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r} + \sum_{\ell =1}^L \frac{L_\ell}{2} \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2\\
&  \leq f(\bar{\vartheta}_r) + \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j (\bar{\vartheta}^{\ell,j}_{r+1} - \bar{\vartheta}^{\ell,j}_r) + \sum_{\ell =1}^L \frac{L_\ell}{2} \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2
\end{align}

Taking expectations on both sides leads to:
\begin{align}\label{eq:main}
- \EE[  \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r}]  \leq  \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{\ell =1}^L \frac{L_\ell}{2} \EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2]
\end{align}

Yet, we observe that, using the classical intermediate quantity, used for proving convergence results of adaptive optimization methods, see \citep{}, we have:
\beq\label{eq:defseq}
\bar{\vartheta}_r = \bar{\theta}_r +  \frac{\beta_1}{1-\beta_1}(\bar{\theta}_{r} - \bar{\theta}_{r-1})
\eeq
where $\bar{\theta_r}$ denotes the average of the local models at round $r$.
Then for each layer $\ell$,
\begin{align}\label{eq:gap}
\bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r  & = \frac{1}{1-\beta_1}(\bar{\theta}^\ell_{r+1} - \bar{\theta}^\ell_{r}) - \frac{\beta_1}{1-\beta_1}(\bar{\theta}^\ell_{r} - \bar{\theta}^\ell_{r-1})\\
& = \frac{\alpha_{r}}{1-\beta_1} \frac{1}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\|p_{r,i}^{\ell}\|} p_{r,i}^{\ell}  - \frac{\alpha_{r-1}}{1-\beta_1} \frac{1}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\|p_{r-1,i}^{\ell}\|} p_{r-1,i}^{\ell}\\
& = \frac{\alpha \beta_1}{1-\beta_1} \frac{1}{n}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1} + \frac{\alpha}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}
\end{align}
where we have assumed a constant learning rate $\alpha$.


We note for all $\theta \in \Theta$, the majorant $G > 0$ such that $\phi(\|\theta \|) \leq G$. Then, following \eqref{eq:main}, we obtain:
\begin{align}\label{eq:main2}
- \EE[  \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r}]  \leq  \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{\ell =1}^L \frac{L_\ell}{2} \EE[  \| \bar{\vartheta}_{r+1} - \bar{\vartheta}_r \|^2]
\end{align}

Developing the LHS of \eqref{eq:main2} using \eqref{eq:gap} leads to

\begin{align}\label{eq:inner}
\pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r} &= \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j (\bar{\vartheta}^{\ell,j}_{r+1} - \bar{\vartheta}^{\ell,j}_r) \\
& =  \frac{\alpha \beta_1}{1-\beta_1}\frac{1}{n}  \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j \left[   \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}  \right] \\
& - \underbrace{ \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j  \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}}_{= A_1}
\end{align}

\textbf{ Term $A_1$:}
Since we have that $\|p_{r,i}^{\ell}\| \leq \sqrt{\frac{p_\ell}{1-\beta_2}}$ and $1/\sqrt{v^{t}_{r}} \leq 1/\sqrt{v_{0}}$, using H\ref{ass:boundgrad}, we develop the term $A_1$ as follows:
\begin{align}
A_1 & \leq - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j  \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\\
& \leq - \frac{\alpha}{n} \sum_{\ell=1}^\tot  \sqrt{\frac{1-\beta_2}{M^2 p_\ell}} \sum_{i = 1}^n \sum_{j=1}^{p_\ell}   \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  g^{\ell, j}_{r,i}\\
& - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell}   \left( \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\right)\mathsf{1}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell}) \right)
\end{align}

Taking the expectations on both sides yields:

\begin{align}
\EE[A_1]  & \leq - \alpha \sum_{\ell=1}^\tot  \sqrt{\frac{1-\beta_2}{M^2 p_\ell}} \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \EE \left[  \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  g^{\ell, j}_{r,i}\right]\\
& - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell}   \EE\left[ \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\mathsf{1} \left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell}) \right) \right]\\
& \leq - \frac{\alpha}{n} \sum_{\ell=1}^\tot  \phi_m \sqrt{\frac{1-\beta_2}{M^2 p_\ell}} \sum_{i = 1}^n \sum_{j=1}^{p_\ell}    (\nabla_{\ell} f(\bar{\vartheta}_r)^j)^2\\
& - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \phi_M  \EE\left[ \left| \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\right| \mathsf{1}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell})\right) \right]\\
\end{align}
where we have used assumption H\ref{ass:phi}.

Since for any $\ell, i , j$, we have
\beq
  \EE\left[ \left| \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\right| \mathsf{1}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell})\right) \right] \leq   \left| \nabla_{\ell} f(\bar{\vartheta}_r)^j \right| \mathbb{P}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell})\right) 
\eeq

Then, we obtain

\beq\label{eq:finala1}
\EE[A_1]  \leq -\alpha \phi_m \sqrt{\frac{\tot(1-\beta_2)}{M^2 p}}  \EE[ \| \overline{\nabla f}(\bar{\vartheta_r})\|^2]  -  \alpha \phi_M \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \frac{\sigma_{i}^{\ell, j}}{\sqrt{n}}
\eeq

where $\overline{\nabla f}( \cdot) = \sum_{i=1}^n \nabla f_i(\cdot)$

We now need to bound the following terms:
\begin{align}
& A_r^2 \eqdef \EE[  \| \bar{\vartheta}_{r+1} - \bar{\vartheta}_r \|^2]\\
& A_r^3 \eqdef  \frac{\alpha \beta_1}{1-\beta_1}  \frac{1}{n} \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j \left[   \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}  \right]
\end{align}

\textbf{ Term $A_r^2$:}
According to definition \eqref{eq:defseq}, for each layer $\ell \in \llbracket \tot \rrbracket$, we have, using the Cauchy-Schwartz inequality, that:
\begin{align}
  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2 & = \left\| \frac{\alpha \beta_1}{1-\beta_1} \frac{1}{n}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1} + \frac{\alpha}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\right \|^2 \\
&  \leq 2\frac{\alpha^2}{n^2} \left\| \frac{ \beta_1}{1-\beta_1}   \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 + \frac{1}{n^2} \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\right\|^2
\end{align}

Taking the expectation on both sides leads to:
\begin{align}\label{eq:maina2}
\begin{split}
\EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2] &  \leq 2\alpha^2 \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right] +  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\right\|^2 \right]\\
& \leq 2 \frac{\alpha^2}{n^2} \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right] \\
& +  \frac{1}{n^2} \EE\left[ \left| \sum_{i = 1}^n\sum_{j = 1}^p    \pscal{\Gamma_{r,i}^j (\nabla f_i(\theta_r)^j + g_{r,i}^j - \nabla f_i(\theta_r)^j) }{\Gamma_{r,i}^j (\nabla f_i(\theta_r)^j + g_{r,i}^j - \nabla f_i(\theta_r)^j)}\right| \right]\\
& \leq 2\frac{\alpha^2}{n^2} \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right] \\
& +  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] +  \frac{1}{n}\left\| \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\\
\end{split}
\end{align}
where the last line uses assumptions H\ref{ass:boundgrad} and H\ref{ass:var} (unbiased gradient and bounded variance of the stochastic gradient) and $\Gamma \eqdef  \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} $.

On the other hand, using the bound on the gradient H\ref{ass:boundgrad},
\begin{align}\label{eq:first}
\begin{split}
& \sum_{r=1}^R \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right]  \\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} M^2 \phi^2_M \sum_{r=1}^R  \EE \left[  \left\| \sum_{i = 1}^n  \left( \frac{1}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{1}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) \right\|^2 \right]\\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \sum_{r=1}^R  \EE \left[  \left\| \sum_{i = 1}^n  \left( \frac{1}{\sqrt{v^{t}_{r}}} - \frac{1}{\sqrt{v^{t}_{r-1}}} \right) \right\|^2 \right]\\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \sum_{r=1}^R  \EE \left[  \left| \sum_{i = 1}^n  \sum_{j= 1}^p  \left( \frac{1}{\sqrt{v^{t,j}_{r}}} - \frac{1}{\sqrt{v^{t,j}_{r-1}}} \right) \right| \right]\\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0}\\
\end{split}
\end{align}
where, in the telescopic sum, we have used the initial value $v_0$ of the non decreasing sequence $\{v^t_r\}_{r >0}$ by construction (max operator).
 
Combining \eqref{eq:first} into \eqref{eq:maina2} and summing over the total number of rounds $R$ yields
\begin{align}\label{eq:finala2}
\begin{split}
\sum_{r=1}^R A_r^2 \eqdef \sum_{r=1}^R \EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2] & \leq \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0} \\
& + \sum_{r=1}^R  \left[  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] +  \frac{1}{n}\left\| \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\right]
\end{split}
\end{align}


\textbf{ Term $A_r^3$:}
According to similar arguments on the non decreasing sequence involved in the algorithm as in the previous series of calculations, observe that

\beq\label{eq:finala3}
\sum_{r=1}^R A_r^3 \leq  \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}}
\eeq

Plugging \eqref{eq:finala1} into \eqref{eq:main2} combined with \eqref{eq:finala2} and \eqref{eq:finala3} injected into the original smoothness definition \eqref{eq:main} summed over the total number of rounds:
\begin{align}
- \sum_{r=1}^R \EE[  \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r}]  \leq  \sum_{r=1}^R \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{r=1}^R \sum_{\ell =1}^L \frac{L_\ell}{2} \EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2]
\end{align}

gives:

\beq
\begin{split}
&  \sum_{r=1}^R \alpha \phi_m \sqrt{\frac{\tot(1-\beta_2)}{M^2 p}}  \EE[ \| \overline{\nabla f}(\bar{\vartheta_r})\|^2]  -  \alpha \phi_M  \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \frac{\sigma_{i}^{\ell, j}}{\sqrt{n}}  + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} \\
& \leq  \sum_{r=1}^R \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{\ell =1}^L \frac{L_\ell}{2} \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0} \\
& \quad - \sum_{r=1}^R  \left[  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] +  \frac{1}{n}\left\| \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\right]
\end{split}
\eeq
Noting that $ \sum_{r=1}^R \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] =   f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})] $, we obtain


\beq
\begin{split}
&    \sum_{r=1}^R \alpha \phi_m \sqrt{\frac{\tot(1-\beta_2)}{M^2 p}}  \EE[ \| \overline{\nabla f}(\bar{\vartheta_r})\|^2]  +  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] \\
&\leq   f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})] +   \frac{1}{n}\left\| \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right] +\alpha \phi_M  \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \frac{\sigma_{i}^{\ell, j}}{\sqrt{n}}  + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} \\
& +  \sum_{\ell =1}^L \frac{L_\ell}{2} \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0} 
\end{split}
\eeq


leading to
\beq
\begin{split}
   \sum_{r=1}^R   \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] &\leq   f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})] +   \frac{1}{n}\left\| \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\\
&   +\alpha \phi_M \sigma \tot p \sqrt{n}+ \frac{\overline{L}_\ell\beta_1^2\tot(1-\beta_2)M^2 \phi^2_M n}{2(1-\beta_1)^2 v_0}    + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} 
\end{split}
\eeq
where $ \overline{L}_\ell = \sum_{\ell=1}^\tot L_{\ell}$ is the sum of all smoothness constants.

Consider the following inequality:

\beq
\frac{1}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell} \|}\nabla f_i(\theta_r) \leq   \phi_M (1-\beta_2) \frac{\overline{\nabla}f(\theta_r)}{\sqrt{ v_r^t}}
\eeq
where $\overline{\nabla}f(\theta_r) \eqdef \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\theta_r) $.
And using the Cauchy-Schwartz inequality we have
\beq
\begin{split}
\left\| \frac{\overline{\nabla}f(\theta_r)}{\sqrt{ v_r^t}} \right\| \geq \frac{1}{2} \left\| \frac{\nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\| - \left\| \frac{\overline{\nabla}f(\theta_r)- \nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\|
\end{split}
\eeq


Using Lemma~\ref{lemma:iterates} and the smoothness assumption H\ref{ass:smooth}, we have
\beq
\begin{split}
\left\| \frac{\overline{\nabla}f(\theta_r)}{\sqrt{ v_r^t}} \right\| \geq \frac{1}{2} \left\| \frac{\nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\| - \left\| \frac{\overline{\nabla}f(\theta_r)- \nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\|
\end{split}
\eeq


\end{document} 