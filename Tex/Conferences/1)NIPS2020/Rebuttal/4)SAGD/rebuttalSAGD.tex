\documentclass{article}

\usepackage{neurips_2020_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}

\usepackage[]{color-edits}
\addauthor{yz}{magenta}

\begin{document}


We thank all the reviewers for their insightful feedback which do help us improve the quality of our paper. We explain how we address your concerns and revise our paper based on your comments. Based on \textbf{R1} and \textbf{R3} concerns about the code, we are more than happy to share it. 
If they wish to see the code right away, we can share the code through AC/PC. 

\textbf{Reviewer 1:}
\vspace{-4pt}

-- \textit{ ``There are relatively few stable facts. This paper does not necessarily reduce the entropy.''}
The reviewer raised a very important point. We agree that there are tremendous amount of papers on this topic with few stable facts. We expect our work to bring new insights to this field, especially in understanding the generalization via the lens of differential privacy.\vspace{-4pt} 

-- \textit{ `` Figure 2''}
We thank the reviewer for pointing this out. We will improve the quality of the plots in the revision.\vspace{-4pt}

-- \textit{ ``broader impact''} This paragraph will be improved to reflect the idea of avoiding over-fitting (see plot (b) Figure 2).

% \textit{ ``broader impact''} This paragraph will be improved. The main motivation for our paper's impact is to avoid over-fitting (see plot (b) Figure 2) which can be valued when strong biases exist in the training set.\vspace{-4pt}
% \textit{ ``there is no code''} We are happy to share the code, of course. If reviewers wish to see the code right away, we are happy to share the code through area chair or program chair

\textbf{Reviewer 2:} \vspace{-4pt}
% \textit{``Adding Laplace noise fix the convergence issue?''}
% Reddi et al., 2018 paper studies the convergence of Adam under the stochastic oracle setting, where the optimization oracle can obtain an unbiased estimation of the gradient of the objective. 
% However, our work studies the SGD with sample reuse setting (finite samples). Since the problem considered in our work and Reddi et al., 2018 work, it is unclear if we can draw the conclusion that Laplace noise fix the convergence issue. \yzcomment{not sure if we need to respond to this, or shall we just ignore it?}

-- \textit{ 
``I would have liked to see more thorough and rigorous experiments.'' 
%``if the baselines were tuned properly''.
%``both ResNet18 and VGG19 should be reaching slightly higher test accuracies with SGD/Adam''
}
%Our experiments mainly 
We mainly  follow the method in [Wilson et al., 2017] to tune the step size, since they highlight that the initial step size and the scheme of decaying have a considerable impact. 
%We agree that the mini-batch size would also play an important role in the performance of the training algorithms. 
% Studying the effect of batch size will be an interesting perspective as well. 
We agree with the reviewer that the mini-batch size and hyper-parameter tuning would also play an important role in the performance.
%of the training algorithms. 
Still, we think that our experiments provide an extensive experimental evaluation of variants of training algorithms for various tasks such as image classification and language modeling. 
% We agree that the SGD/Adam or any other optimizers (including our SAGD) can reach better accuracy with much more effort in tuning the hyper-parameters such as moment, weight decay etc. 
We believe our experiments offer a fair comparison since the same effort was done to tune the hyper-parameters for each baseline. \vspace{-4pt}

    
-- \textit{``Does RMSProp offer any particular advantage...''}
We agree that DPG-LAG/DPG-SPARSE can be used with any first order optimization algorithm. 
The RMSProp can be viewed as SGD when $\beta_2 = 1$. 
We plan to provide a generic stable adaptive algorithm that encapsulates many popular adaptive and \emph{non-adaptive} methods in the Appendix. \vspace{-4pt}
% We consider giving a theoretical guarantee and empirical study of Adam with DPG-LAG/DPG-SPARSE as our future work. 
    
    
-- \textit{``How do the high probability bounds change when using mini-batches of size m?''} 
The high probability bounds on the gradient mainly follow the generalization guarantee of differential privacy with conditions on the privacy parameters $(\epsilon, \delta)$ and sample complexity. 
In the case of mini-batch, the value of privacy parameters $(\epsilon, \delta)$ and the condition on sample complexity get modified. We have provided details in the proof of Theorem 5, see Section B.2 of the Appendix.

% The high probability bounds on the gradient mainly follow the generalization guarantee of differential privacy, which has a conditions on privacy parameter $(\epsilon, \delta)$ and sample complexity.  
% , which shows that an $(\epsilon, \delta)$-algorithm can guarantee a certain  generalization error if $(\epsilon, \delta)$ and sample size $n$ used to evaluate the gradient satisfy some conditions (Lemma 1). 
%In the case of mini-batch, the sample size becomes $m$ and the value of $(\epsilon, \delta)$ is modified. 
% Thus, the sample complexity for the high probability bound changes. 
%We have provided details in the proof of Theorem 5.
\vspace{-4pt}
    
-- \textit{``Is data augmentation used in the experiments?''}
We used data augmentation for MNIST and CIFAR-10. 
For MNIST, we normalize the value of each feature to [0,1]. 
For CIFAR-10, we normalize, randomly crop and rotate the images.
%, using standard functions from

%such as RandomCrop, RandomHorizontalFlip. 
%RandomCrop, transforms.RandomHorizontalFlip, and transforms.Normalize. 

    


\textbf{Reviewer 3:}
\vspace{-4pt}

--\textit{``It is unclear how guaranteeing stationary points that have small gradient norms translates to good generalization''}
Our main theoretical results provide the convergence to the \emph{`population stationary point'}. 
Note that Theorems 2, 4 and 5 show the convergence of the norm of the \emph{population gradient} instead of the empirical gradient. 
%Specifically, while SAGD only has access to $n$ samples, it converges to the \emph{population stationary point}. 
Also, one will be able to use our results to establish the generalization error of the loss function based on arguments such as the PL condition. \vspace{-4pt}
    
--\textit{``The Hoeffding's bound holds true as long as the samples are drawn independently''.}
 Yes, Hoeffding's bound holds as long as the samples are drawn independently. 
However, in the setting of
%optimization with 
\emph{sample reuse} (setting in this paper) such as SGD with multi-pass, the reused samples are not independent anymore,
%for any iteration $t > 0$, 
since the posterior distributions of samples change after training on the reused samples.
%finite set of $n$ observations. 
\vspace{-4pt}
% For stochastic oracle (meaning that if every iteration, there are fresh samples), the Hoeffding's bound holds for sure. But this is not the setting considered in this paper. 
 

 -- \textit{``The bounds in Theorem 1 have a dependence on d''.}
The reviewer raised a very interesting question!
Yes, the dependence on $d$ is a known result for differential privacy (DP) and is hard to avoid (see ref. [1]). 
Some works on DP try to improve this dependence on $d$ by leveraging special structures of the gradients. 
This will be considered in the future. \vspace{-4pt}
    

--\textit{``do not depend on the initialization $\mathbf{w}_0$ but on $\mathbf{w}_1$.``}
We thank the reviewer for this typo: should be $\mathbf{w}_0$ instead of $\mathbf{w}_1$.\vspace{-4pt}
    
--\textit{ ``For Penn-Tree bank,[...] algorithms are not stable w.r.t. train perplexity.''}
With respect to train perplexity, all methods stabilize around a target value (which is of course different given the highly nonconvex loss). 
We note that the test perplexity increases after several epochs for most baselines while our method keeps a low and steady one.
%  \textit{``the code with respect to reproducibility``} We are happy to share the code, of course. If reviewers wish to see the code right away, we are happy to share the code through area chair or program chair

\textbf{Reviewer 4:}
\vspace{-4pt}

--\textit{``experiment design mainly follows [Wilson et al., 2017]''}
The design is different from [Wilson et al., 2017] (except for the stepsize tuning, see \textbf{Reviewer 2}).
Indeed, we study the \emph{generalization} performance of each algorithm with an \emph{increasing} training sample size $n$ (see Fig. 1, x-axis is $n$).
This is consistent with our theoretical results which show the convergence of SAGD in terms of $n$. 
%We can compare the performance of those algorithms when $n$ is small as well.
% Thus, as shown in Fig 1, the x-axis is the number of training samples  and y-axis is the training/test accuracy. 
However, [Wilson et al., 2017] mainly plotted the training/test accuracy against the number of epochs. 
% The RMSProp can be viewed as SGD when $\beta_2 = 1$. 
We agree that it would be interesting to add experiments to compare SGD with differential privacy. \vspace{-4pt}
% We can set up such experiments. 
    
--\textit{``SGD with gradient corrupted by Gaussian noise performs well or not''}
Excellent question and nice reference! 
Actually, one can also use Gaussian noise to design a differentially private algorithm (namely Gaussian Mechanism [7]). 
Also, 
%the connection between SGLD (Stochastic Gradient Langevin Dynamics) and differential privacy has been studied. 
there are papers showing the connection between SGLD (Stochastic Gradient Langevin Dynamics) and differential privacy. 
Yet, the existing generalization bound of SGLD is established by the techniques of algorithmic stability [23, 26], which scales with $(\sqrt{T})$. 
We believe it is of great interest to show how Gaussian noise works in our setting. 
%provide a theoretical analysis of SGLD via the generalization of differential privacy. 
%It is also interesting to show how Gaussian noise works in our setting. 
We will add a discussion in the paper. 
We consider the theoretical details and experimental results as a future work.\vspace{-4pt}

    
--\textit{``whether the proposed method works well for small datasets in terms of generalization``}
Figure 1 shows that SAGD has a slightly better test accuracy than other algorithms when the training sample size $n$ is small (x-axis). 
    


\end{document}



