\begin{thebibliography}{10}

\bibitem{basm2014}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In {\em 2014 IEEE 55th Annual Symposium on Foundations of Computer
  Science}, pages 464--473. IEEE, 2014.

\bibitem{boel02}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em Journal of machine learning research}, 2(Mar):499--526, 2002.

\bibitem{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock {\em arXiv preprint arXiv:1508.05326}, 2015.

\bibitem{chmo2011}
Kamalika Chaudhuri, Claire Monteleoni, and Anand~D Sarwate.
\newblock Differentially private empirical risk minimization.
\newblock {\em Journal of Machine Learning Research}, 12(Mar):1069--1109, 2011.

\bibitem{chgu2018}
Jinghui Chen and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock {\em arXiv preprint arXiv:1806.06763}, 2018.

\bibitem{cheli2019}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{conneau2017supervised}
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes.
\newblock Supervised learning of universal sentence representations from
  natural language inference data.
\newblock {\em arXiv preprint arXiv:1705.02364}, 2017.

\bibitem{duha11}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12(Jul):2121--2159, 2011.

\bibitem{dwfe2015a}
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toni Pitassi, Omer Reingold, and
  Aaron Roth.
\newblock Generalization in adaptive data analysis and holdout reuse.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2350--2358, 2015.

\bibitem{dwfe2015b}
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold,
  and Aaron Roth.
\newblock The reusable holdout: Preserving validity in adaptive data analysis.
\newblock {\em Science}, 349(6248):636--638, 2015.

\bibitem{dwfe2015c}
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold,
  and Aaron~Leon Roth.
\newblock Preserving statistical validity in adaptive data analysis.
\newblock In {\em Proceedings of the forty-seventh annual ACM symposium on
  Theory of computing}, pages 117--126. ACM, 2015.

\bibitem{dwro2014}
Cynthia Dwork, Aaron Roth, et~al.
\newblock The algorithmic foundations of differential privacy.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 9(3--4):211--407, 2014.

\bibitem{ghla2013}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{hare2016}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em International Conference on Machine Learning}, pages
  1225--1234, 2016.

\bibitem{hezh2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{keso2017}
Nitish~Shirish Keskar and Richard Socher.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock {\em arXiv preprint arXiv:1712.07628}, 2017.

\bibitem{kiba15}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em In Proceedings of the 3rd International Conference on
  Learning Representations (ICLR)}, 2015.

\bibitem{krhi2009}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{kula2018}
Ilja Kuzborskij and Christoph Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In {\em International Conference on Machine Learning}, pages
  2820--2829, 2018.

\bibitem{lebo1998}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, Patrick Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lilu2019}
Jian Li, Xuanyuan Luo, and Mingda Qiao.
\newblock On generalization error bounds of noisy gradient methods for
  non-convex learning.
\newblock {\em arXiv preprint arXiv:1902.00621}, 2019.

\bibitem{luxi2019}
Liangchen Luo, Yuanhao Xiong, and Yan Liu.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{mama1993}
Mitchell Marcus, Beatrice Santorini, and Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of english: the penn treebank.
\newblock {\em Computational linguistics-Association for Computational
  Linguistics}, 19(2):313--330, 1993.

\bibitem{stni2018}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mowa2018}
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng.
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock In {\em Conference On Learning Theory}, pages 605--638, 2018.

\bibitem{pejo2018}
Ankit Pensia, Varun Jog, and Po-Ling Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In {\em 2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 546--550. IEEE, 2018.

\bibitem{qian1999momentum}
Ning Qian.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock {\em Neural networks}, 12(1):145--151, 1999.

\bibitem{rara2017}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In {\em Conference on Learning Theory}, pages 1674--1703, 2017.

\bibitem{reka2018}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{romo51}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{shbe14}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{sizi2014}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{tige12}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 2012.

\bibitem{waxu2019}
Di~Wang and Jinhui Xu.
\newblock Differentially private empirical risk minimization with smooth
  non-convex loss functions: A non-stationary view.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 1182--1189, 2019.

\bibitem{waye2017}
Di~Wang, Minwei Ye, and Jinhui Xu.
\newblock Differentially private empirical risk minimization revisited: Faster
  and more general.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2722--2731, 2017.

\bibitem{wawu19}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock In {\em International Conference on Machine Learning}, pages
  6677--6686, 2019.

\bibitem{wiro17}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4148--4158, 2017.

\bibitem{zare18}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9793--9803, 2018.

\bibitem{zhta18}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em arXiv preprint arXiv:1808.05671}, 2018.

\bibitem{zhch2018}
Yingxue Zhou, Sheng Chen, and Arindam Banerjee.
\newblock Stable gradient descent.
\newblock In {\em UAI}, pages 766--775, 2018.

\bibitem{zosh2019}
Fangyu Zou, Li~Shen, Zequn Jie, Weizhong Zhang, and Wei Liu.
\newblock A sufficient condition for convergences of adam and rmsprop.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 11127--11135, 2019.

\end{thebibliography}
