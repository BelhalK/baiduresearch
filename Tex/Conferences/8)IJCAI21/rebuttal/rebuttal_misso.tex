% IJCAI-21 Author's response

% Template file with author's reponse

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai21-authors-response}


\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\urlstyle{same}
\setlength{\parindent}{0pt}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\begin{document}
We would like to thank the five reviewers for their feedback. We provides point-by-point response to their concerns.
We first discuss a few common concerns shared by \textbf{\textcolor{green!50!black}{Reviewer 50}} and \textbf{\textcolor{brown}{Reviewer 90}} on the performance of MISSO in he numerical section:

\vspace{0.03in}
${\color{green!50!black}\bullet}\!~{\color{brown}\bullet}$ \textbf{Comparison with MC-ADAM (Figure 2):} 
In order to avoid the confusion in the convergence of MISSO in the Resnet-18 example, we will extend the number of training epochs to 200.
200 epochs will stabilize the ELBO for MISSO, reaching similar values as MC-Adam.
While both methods reach the same local minima, we acknowledge that in this particular example MC-Adam does reach an $\epsilon-$stationary points in fewer iterations than MISSO, probably suffering from a high variance, hence the twists and turns in the objective function.
At this stage, we are not sure if it is possible to generalize a class of problems where baselines can beat MISSO in terms of convergence speed.

\medskip
\textbf{\textcolor{blue}{Reviewer 5:}} We thank the reviewer for valuable comments and reference. We would like to make the following clarification regarding the stationary measure we use: 

\vspace{0.03in}
\textbf{-- Stationary Measure:} In Theorem 1, both rates in (16) and (17) are given with respect to first, the second order moment of the gap between the (potentially nonsmooth) objective function and the surrogates, and to secondly, the negative part of the directional derivative.
For (16), the existence of the gradient of the gap is ensured by assumption H2 (adding the nonsmooth components of the objective function in the surrogates is an easy trick to ensure its existence).
For (17), since our problem is constrained, we must have recourse to the directional derivative of the objective function, and more precisely to the convergence of its negative part to $0$ as a characterization of stationarity.

Techniques in [Shamir, 2020] are useful when the suboptimality condition is derived on a nonsmooth quantity, which is not the case here.


\medskip
\textbf{\textcolor{red}{Reviewer 9:}} We thank the reviewer for the useful comments. Our point-to-point response is as follows: 

\vspace{0.03in}
\textbf{-- Exact Surrogate Minimization:} 
The advantage of our method is that the surrogate function is user designed.
hence, its choice can be made so that the maximization step can be done in closed form at each iteration.
In our numerical examples, we use quadratic surrogate of the objective function, leading to its exact minimization via a gradient step.
The updates are detailed in the supplementary material for completeness, see for instance section B.3, equation (42) for the logistic regression example.

\vspace{0.03in}
\textbf{-- Proof Techniques:} 
\textcolor{red}{The technicality of our proofs partly relies on mixing results from the MCMC literature, such as the Bracketing number and the control of the Monte Carlo noise, see references [25;26;27], with stochastic nonconvex optimization convergence bounds proofs techniques, generally used for gradient descent type of algorithms.
The handling of both sampling and optimization procedure into a single theoretical framework makes the derivation of our bound challenging.
While Theorem 2 is somewhat related to the convergence theorems in [Mairal, 2015], the finite-time analysis for such \emph{incremental} and \emph{doubly stochastic} method is not common in the literature.}

\medskip
\textbf{\textcolor{magenta}{Reviewer 35:}} We thank the reviewer for valuable comments. We clarify the following point on the experiments: 

\vspace{0.03in}
\textbf{-- Numerical Experiments:} 
Thank you for the pointer.
We plan on extending the number of epochs for all methods.
With 200 epochs, MISSO does converge and reaches similar local minima as MC-Adam, while MC-Adagrad stays stuck in a worse local minima, as per our runs.
We will plot a stabilized curve for MISSO in the revised paper.


\medskip
\textbf{\textcolor{green!50!black}{Reviewer 50:}} We thank the reviewer for the valuable comments and typos. 
We would like to emphasize on the nature of our contribution, given some runs where MISSO does not display a particular edge over baselines: 


\vspace{0.03in}
\textbf{-- Nature of the contribution:} 
We want to stress on the generality of our incremental optimization framework, which tackles a \emph{constrained}, \emph{non-convex} and \emph{non-smooth} optimization problem. 
The main contribution of this paper is to propose and analyze a \textbf{unifying framework} for a large class of optimization algorithms which includes many well-known but not so well-studied algorithms.
The major idea here is to relax the class of surrogate functions used in MISO [Mairal, 2015] and to allow for intractable surrogate that can only be evaluated by Monte-Carlo approximations.
We provide a general algorithm and global convergence rate analysis under mild assumptions on the model and show that two examples, MLE for latent data models and Variational Inference, are its special instances.
Working at the crossroads of \emph{Optimization} and \emph{Sampling} constitutes what we believe to be the novelty and the technicality of our theoretical results.


\medskip
\textbf{\textcolor{brown}{Reviewer 90:}} We thank the reviewer for the interest in our contribution. We also precise the following: 

\vspace{0.03in}
\textbf{-- Comparison with Monte Carlo variants of baselines:}
Given the generality of the MISSO framework, as recalled at the beginning of this rebuttal, the comparison with Monte Carlo variants of other baselines will have to be on a case by case study.
As MISSO encompasses several methods such as MCEM or Variational Inference, their comparable baselines are usually different.
Those competitors also highly depend on the type of model that is being trained. 
Here again, MISSO aims at tackling a large collection of latent variable models.
Yet, when this latent variable model is a multilayered network or a plain linear mixed model, the most performing baselines will be of different nature.
Hence, we argue that MISSO enjoys from its broad scope of possible algorithms for a large class of models, while displaying strong convergence guarantees.

From a practical point of view, the simplicity of its update in Algorithm 2 is enjoyable compared to various baselines such as Adam where several (possibly computationally heavy) estimations are calculated.
Besides, to the best of our knowledge, the Monte Carlo variants of those baselines have not been studied.
The convergence of relatively complex algorithms such Adagrad or Adam, while adding this layer of stochasticity, is far from easy to obtain.

\end{document}

