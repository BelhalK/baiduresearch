%% LaTeX Template for ISIT 2020
%%
%% by Stefan M. Moser, October 2017
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

\documentclass[conference,letterpaper]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex
\topmargin 0pt\headheight 0pt\headsep 2pt\textheight 660pt\footskip
30pt\oddsidemargin 10pt\textwidth 440pt\marginparsep 10pt
\usepackage{mathrsfs,amsmath,amssymb,amsfonts}
\numberwithin{equation}{section}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{stmaryrd}
\usepackage{color, verbatim}

%\usepackage[notref,notcite]{showkeys}
%\usepackage{refcheck}

\newcommand{\lbl}{\label}
%\newcommand{\lbl}[1]{\hspace{1cm} \underline{({#1})} \label{#1}}
\newcommand{\proof}{{\it Proof. \ }}

\newcommand{\ignore}[1]{}{}
%\newcommand{\ignore}[1]{#1}
\newcommand{\be}{\begin{equation}}               %\be=\begin{equation}
\newcommand{\ee}{\end{equation}}                 %\ee=\end{equation}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\var}{\mbox{Var}}
\newcommand{\etab}{\kappa}
\newcommand{\B}{\alpha}
\newcommand{\D}{\beta}
\newcommand{\bd}{\bold}

\newcommand{\beaa}{\begin{eqnarray*}}
\newcommand{\eeaa}{\end{eqnarray*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}

%\newcommand{\binom}[2]{\left(
%\begin{array}{c} #1 \\ #2 \end{array} \right)}
\newcommand{\bis}[2]{
 \begin{array}{c} \ds #1 \\ \vs{-2.5}\\  \Large{#2} \end{array} }
%\newcommand{\bis}[2]{
%\begin{array}{c} \ds #1 \\ \footnotesize{#2} \end{array} }

\newcommand{\noi}{\noindent}
\newcommand{\beqn}{\begin{eqnarray}}             %\beqn=\begin{eqnarray}
\newcommand{\eeqn}{\end{eqnarray}}               %\eeqn=\end{eqnarray}
\newcommand{\beq}{\begin{eqnarray*}}             %\beq=\begin{eqnarray*}
\newcommand{\eeq}{\end{eqnarray*}}               %\eeq=\end{eqnarray*}
\newcommand{\mb}{\mbox}                          %\mb=\mbox
\newcommand{\lb}{\left\{ }                       %\lb=\left\{
\newcommand{\rb}{\right\} }                      %\rb=\right\}
\newcommand{\re}{\right.}                        %\re=\right.
\newcommand{\nn}{\nonumber}
\newcommand{\ds}{\displaystyle}



\newcommand{\bbox}{\nobreak\quad\vrule width4pt depth2pt height4pt}
\newcommand{\eq}[1]{$(\ref{#1})$}

\newcommand{\al}{\alpha}                         %\al=\alpha
\newcommand{\ga}{\gamma}                         %\ga=\gamma
\newcommand{\Ga}{\Gamma}                         %\Ga=\Gamma
\newcommand{\ep}{\epsilon}                       %\ep=\epilon
\newcommand{\vp}{\varepsilon}                   %\vp=\varepsilon
\newcommand{\la}{\lambda}                        %\la=\lambda
\newcommand{\La}{\Lambda}                        %\La=\Lambda
%\newcommand{\th}{\theta}                         %\th=\theta
\newcommand{\bt}{\beta}                           %\bt=\beta
\newcommand{\sg}{\sigma}                         %\sg=\sigma
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\ssb}{\scriptstyle \footnotesize % \scriptsize
                 \begin{array}{c}}
\newcommand{\esb}{\end{array}}
\newcommand{\ra}{\rightarrow}                    %\ra=\rightarrow
\newcommand{\lra}{\longrightarrow}               %\lra=\longrightarrow
\newcommand{\Ra}{\Rightarrow}                    %\Ra=\Rightarrow
\newcommand{\Lra}{\Longrightarrow}               %\Lra=\Longrightarrow
\newcommand{\sta}{\stackrel}                    %\sta=\stackrel
\newcommand{\xn}{$\{X_n, \, n \geq 1\} \ $}
\newcommand{\wip}{weak invariance principle}
\newcommand{\sa}{strong approximation}
\newcommand{\clt}{central limit theorem}
\newcommand{\lil}{law of the iterated logarithm}
\newcommand{\iid}{independent and identically distributed random variables}
\newcommand{\irv}{independent random variables \ }
\newcommand{\inrv}{independent normal random variables \ }
\newcommand{\Let}[1]{Let $\{X_n, n \geq 1 \}$ be a stationary  $#1$-mixing
          sequence of random variables \ }
\newcommand{\Les}[1]{Let $\{X_n, n \geq 1 \}$ be a  $#1$-mixing
          sequence of random variables \ }
\newcommand{\Leta}[1]{Let $\{X_n, n \geq 1 \}$ be an  $#1$-mixing
          sequence of random variables \ }
\newcommand{\CsR}{Cs\"org\H{o} and R\'ev\'esz}
\newcommand{\kmt}{Koml\'os, Major and Tusn\'ady}
\newcommand{\A}{{\cal A}}
\newcommand{\C}{{\cal C}_{bd}}
\baselineskip=7.0mm
\newtheorem{theorem}{{\sc Theorem}}[section]
 \newtheorem{prop}{Proposition}[section]
 \newtheorem{coro}{{\sc Corollary}}[section]
 \newtheorem{lemma}{{\sc Lemma}}[section]
 \newtheorem{remark}{{\sc Remark}}[section]
\newtheorem{definition}{{\sc Definition}}[section]
\newtheorem{example}{{\sc Example}}[section]
 \newcommand{\vs}{\vspace{.3cm}}
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}%%%%%%
% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

% ------------------------------------------------------------
\begin{document}
\title{Positivity of Hadamard Powers of Random Matrices}
% \footnote{ }}
%\author{Tiefeng Jiang\thanks{ \textsl{E-mail address}:
%\texttt{jiang040@umn.edu}} and  Ping Li\thanks{ \textsl{E-mail address}:
%\texttt{liping11@baidu.com}} }
%\date{\small \it University of Minnesota and Cognitive Computing Lab, Baidu Research, USA}


\author{%
  \IEEEauthorblockN{Tiefeng Jiang}
  \IEEEauthorblockA{School of Statistics\\
  		University of Minnesota \\
		224 Church Street SE,MN 55455\\
                    Email: jiang040@umn.edu}
  \and
  \IEEEauthorblockN{Belhal Karimi and Ping Li}
  \IEEEauthorblockA{Cognitive Computing Lab\\
  			Baidu Research \\
			10900 NE 8th St. Bellevue, WA 98004\\
                    Email: \{belhal.karimi, pingli98\}@gmail.com}
}



\maketitle
%\mbox{}\hrule\mbox{}\\[0.5cm]
%\noindent\textbf{Abstract}
%\\[-0.2cm]^^L


\begin{abstract}
\noindent The paper studies
\end{abstract}



\noindent\textbf{Keywords:} Hadamard matrix function, Hadamard power, non-negative definite matrix, positive definite matrix, random matrix.

\medskip

\noindent\textbf{MSC(2010):} Primary .\\
[0.5cm]
%\mbox{}\hrule\mbox{}
%------------------------------------------------------------------------------------------

%\newpage

\section{Introduction}
Random matrices and Hadamard product, the two main topics of this paper, are of utmost importance in various applications, such as physics, finance, telecommunication, computational biology and machine learning for the random matrices, and combinatorial analysis, number theory or regular graphs to name a few for the Hadamard product, and in information theory.
The Hadamard product, also known as the entry-wise or Schur product, is a type of matrix multiplication that is commutative and displays  virtues, as put in \cite{horadam2012hadamard}. 
Several noteworthy applications and studies of the Hadamard product can be found in the literature, as in \cite{hedayat1978hadamard,agaian2006hadamard,bulutoglu2009counterexample,audenaert2010spectral}.

In particular, in our contribution, we focus on the Hadamard power of random matrices. 
Given $f(x)$, a real-valued function defined on $\mathbb{R}.$ Let $\bd{A}=(a_{ij})$ be an $n\times n$ matrix, where $a_{ij}$'s are real numbers.  Define $f: \bd{A} \rightarrow f(\bd{A)}=(f(a_{i j}))$. We call $f(\bd{A})$  a Hadamard function to distinguish it from the usual notion of matrix functions, as here the function is applied to each element of the matrix. 
In particular, if $\alpha>0$ and  $f(x)=x^{\alpha}$, then we call  $\bd{A}^{(\alpha)}:=f(\bd{A})$ the Hadamard power of $\alpha$. 

\textcolor{red}{continue the introduction with references on random matrices, hadamard power}

\textbf{Main Motivations: }
Kernel learning, with the exponent used as tuning parameter

\textcolor{red}{Motivate the application of such results. How they provide benefits or drawbacks}

\textbf{Our Contributions: }
Collection of theoretical results giving 


Section~\ref{sec:notations} is devoted to the main concepts, notations and existing results to lay the core foundation of our study.
Several illustrative examples are also provided for the sake of clarity.
Section~\ref{sec:main} develops the main results of our paper for positive definite random matrices.
The proofs of our results are presented in Section~\ref{sec:proofs}.
Section~\ref{sec:conclusion} concludes our work.

\section{Notations, Existing Results and Examples}\lbl{sec:notations}
We recall in this section some important results in the literature followed by our main theoretical contributions to the domain of random matrices.
In the definition of the Hadamard power stated in the introduction, careful attention needs to be paid to the domain of the function $f(x)=x^{\alpha}$. 
If the power $\alpha>0$ is an integer, the function is defined for every $x\in \mathbb{R}.$ If $\alpha>0$ is not an integer, the function $f(x)=x^{\alpha}$ is defined only on $[0, \infty).$ By the Schur product theorem, it is known that $\bd{A}^{(\alpha)}$ is a positive definite matrix if $\bd{A}=(a_{ij})$ is a positive definite matrix and $\alpha =1,2, \cdots$; see, for example, Theorem 5.2.1 from \cite{horn_johnson_1991}. In fact, we know more about this conclusion. For positive definite matrices $\bd{U}=(u_{ij})_{n\times n}$ and $\bd{V}=(v_{ij})_{n\times}$, set $\bd{U}\circ \bd{V}=(u_{ij}v_{ij})_{n\times n}$. Then  $\lambda_{min}(\bd{U})\cdot\min_{1\leq i \leq n}{v_{ii}}\leq \lambda_i(\bd{U}\circ \bd{V})\leq \lambda_{max}(\bd{U})\cdot\max_{1\leq i \leq n}{v_{ii}}$ for each $1\leq i \leq n$; see, for example, \cite{schur1911bemerkungen} or Theorem 5.3.4 from \cite{horn_johnson_1991}. If $\alpha$ is a positive integer, then  $\bd{A}^{(\alpha)}=\bd{A}\circ\cdots\circ\bd{A}$ from which there are $\alpha$ times $\bd{A}$ in the product. Thus, by induction, we have that $\lambda_{min}(\bd{A})>0$ if $\bd{A}$ is positive definite.


\subsection{Exisiting Results}\lbl{known_results}

 Let $a \in (0, \infty]$ and $f(x): (0, a)\to \mathbb{R}$. 
 We say that $f(x)$ is {\it absolutely monotonic} on  $(0, a)$ if $f^{(k)}(x)\geq 0$ for every $x\in (0, \alpha)$ and $k\geq0$ 
 The following general conclusion can be found in several research articles including \cite{schoenberg1988positive},  \cite{vasudeva1979positive} and \cite{hiai2009monotonicity}.

First, Theorem~\ref{oldth1} gives a formal result regarding the monotonicity of a value function noted $f(\cdot)$ and a non-negative definite matrix $\bd{A}$.
\begin{theorem} \label{oldth1}
Assume $a \in (0, \infty]$ and $f(x)$ is a real function defined on $(-a, a)$.  
Then $f(\bd{A})$ is non-negative definite for every non-negative definite matrix  $\bd{A}$ with  entries in $(-a, a)$ if and only if $f(x)$ is analytic and absolutely monotonic on $(0, a).$
\end{theorem}


\begin{theorem}\label{oldth2} (Theorem 6.3.7 from \cite{horn_johnson_1991})
	Let $f(\cdot)$ be an $(n-1)$-times continuously differentiable real valued function on $(0,\infty)$, and suppose that the Hadamard function $f(\bd{A})=(f(a_{ij}))$ is non-negative definite for every non-negative definite matrix $\bd{A}$ that has positive entries. Then:
$$f^{(k)}(t)\geq 0$$
 for all $t\in(0,\infty)$ and all $k \in [0,n-1]$.
\end{theorem}
An additional result can be derived from Theorem~\ref{oldth2} stated above, and reads:
\begin{coro}(Corollary 6.3.8 from\cite{horn_johnson_1991})
	Let $0<\alpha<n-2$, where $\alpha$ is not an integer. 
	There exists some $n\times n$ non-negative definite matrix noted $\bd{A}$ with positive entries, such that the Hadamard power of  $\bd{A}$ noted $\bd{A}^{(\alpha)}=(a_{ij}^\alpha)$ is not non-negative definite.
\end{coro}
This Corollary translates the result of the Theorems stated above in the particular case when the value function $f(\cdot)$ maps a non-negative definite matrix into its Hadamard power. 
The equivalence with a non-negative definite Hadamard power matrix is then consequent.
Another important existing result worth noting, as an introduction to our novel results introduced in the sequel, is a quantification of a infimum value for the power $\alpha$ used in the Hadamard power operation. The following theorem states as follows:
\begin{theorem} (Theorem 6.3.9 from \cite{horn_johnson_1991})
	Let $\bd{A}=(a_{ij})$ be a non-negative definite matrix with nonnegative entries. If $\alpha \geq n-2$, then the Hadamard power $\bd{A}^{(\alpha)}$ is non-negative definite. Furthermore, the lower bound $n=2$ is, in general, the best possible.
\end{theorem}


\section{Illustrative Examples}\lbl{sec:main}

For the sake of clarity, we introduce the following notations, used throughout the following sections including the several examples we provide for illustrative purposes and the statements of our main theoretical results.  

\medskip
\textbf{Notations: } For a matrix $\bd{M}$, we write $\bd{M}>0$ if $\bd{M}$ is positive definite; $\bd{M}\geq 0$ if $\bd{M}$ is non-negative definite; $\bd{M}\ngeqslant 0$ if $\bd{M}$ is not non-negative definite.

\medskip

We now provide an example of such \emph{not non-negative definite} matrix through the use of Hadamard power operation.
\medskip
\begin{example} Consider  the following $3\times 3$ matrix
\beaa
\bd{M}=
\begin{pmatrix}
1 & \frac{1}{2} & 0\\
\frac{1}{2} & 1 & \frac{1}{2}\\
0 & \frac{1}{2} & 1
\end{pmatrix}
.
\eeaa
Its Hadamard power of power $\alpha$, where $\alpha$ is not an integer, is given by
\beaa
\bd{M}^{(\alpha)}=
\begin{pmatrix}
1 & \frac{1}{2^{\alpha}} & 0\\
\frac{1}{2^{\alpha}} & 1 & \frac{1}{2^{\alpha}}\\
0 & \frac{1}{2^{\alpha}} & 1
\end{pmatrix}
.
\eeaa
 Then, the computation of its determinant yields
\beaa
\mbox{det}(\bd{M}^{(\alpha)})=1-\frac{2}{4^{\alpha}}.
\eeaa
Therefore, we have that $\bd{M}=\bd{M}^{(1)}>0$. 
However, $\bd{M}^{(\alpha)}\ngeqslant 0$ if $\alpha \in (0, \frac{1}{2}).$

Now, for any integer $n\geq 3$, we define the matrix noted $\bd{M}_n$ and defined as 
$\bd{M}_n=\bd{M}$ for $n=3$ and

\[
    \bd{M}_n= 
\begin{cases}
    \bd{M},& \text{if } n=3\\
    \begin{pmatrix}
\bd{M} & \bd{0}\\
\bd{0} & \bd{I}_{n-3}
\end{pmatrix},              & \text{if } n\geq 4
\end{cases}
\]
where $\bd{I}_{n-3}$ denotes the identity matrix of dimension $n-3$.
%\beaa
%\bd{M}_n=
%\begin{pmatrix}
%\bd{M} & \bd{0}\\
%\bd{0} & \bd{I}_{n-3}
%\end{pmatrix}
%,\ \ \ n\geq 4.
%\eeaa
Then,  the matrix $\bd{M}_n$ is positive definite as $n\geq 3$. 
However, it is easy to note that its Hadamard power matrix $\bd{M}_n^{(\alpha)}\ngeqslant 0$ is not non-negative definite when $\alpha \in (0, \frac{1}{2})$.
\end{example}

\medskip

The example given above shows that the construction of a sequence of positive definite matrix, in dimension $3$, while its Hadamard power is not non-negative, for a certain value of power $\alpha$, is achievable. 
For better understanding, we provide the following additional example in dimension $4$. 

\medskip

\begin{example}\lbl{good_example} 
Consider a $4\times 4$ matrix defined as 
$$\bd{M}=\bd{a}\bd{a}'+\bd{b}\bd{b}' + 10^{-4}\bd{I}_4 \ ,$$ 
where $\bd{a}^T=(1,1,1,1)$ and $\bd{b}^T=(0,1,2,3)$. 
Note that the additive term $10^{-4}\bd{I}_4$ purely ensures the positivity of the resulting $\bd{M}$, thus being a positive definite one and falling into the framework of our study.
The matrix $\bd{a}\bd{a}'+\bd{b}\bd{b}'$  is of rank $2$. Obviously, $\bd{M}>0$. 
It is also easy to check that
\beaa
\mbox{det}(\bd{M}^{1.1})=-0.000118654.
\eeaa
Hence, $\bd{M}^{1.1}\ngeqslant 0$.
%\end{example}
%\begin{example}\lbl{day}. Let $\bd{M}$ be the $4\times 4$ matrix as in Example \ref{good_example}.
Similarly to the previous example, define the sequence of matrices $\bd{M}_n$ defined as follows:
\[
    \bd{M}_n= 
\begin{cases}
    \bd{M},& \text{if } n=4\\
    \begin{pmatrix}
\bd{M} & \bd{0}\\
\bd{0} & \bd{I}_{n-4}
\end{pmatrix},              & \text{if } n\geq 5
\end{cases}
\]
Then, we observe that for $n\geq 4$ we have that $\bd{M}_n>0$. 
However, its Hadamard power matrix $\bd{M}_n^{1.1}\ngeqslant 0$ is not non-negative definite.
\end{example}
\medskip
Those simple, yet illustrative examples lay the context of the next section, where several new results are provided, along with their proofs.

\subsection{Main Results}\lbl{Section_main}
After having established the notations, the relevant existing theoretical results and given important illustrative examples, we introduce a collection of theorems aiming at improving the theoretical understanding of Hadamard powers of random matrices.
All proofs are deferred to Section~\ref{sec:proofs}.

\subsection{Non-negative matrices}
Following the extensive literature presented in the introduction of our paper, we provide the following result for non-negative (and deterministic) definite matrices:

\begin{theorem}\lbl{th:th1} Let $\bd{A}=(a_{ij})$ be an $n\times n$ matrix of which the entries are non-negative. Assume $a_{ii}\geq  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n.$ Then $\bd{A} \geq 0$ and $\bd{A}^{(\alpha)}\geq  0$ for all $\alpha\geq 1.$ 
Note that the conclusion still holds if all three ``$\geq$" are replaced by ``$>$", \textit{i.e.}, if $a_{ii} >  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n.$, then $\bd{A} > 0$ and $\bd{A}^{(\alpha)}>  0$ for all $\alpha> 1.$ 

\end{theorem}


\subsection{Random Matrices}
Here, we consider random matrices, in the sense that the entries of the matrices we study are independent random variables.
Then, under some mild assumptions, that we rigorously give in each statement, we provide the existence, in probability, of positive definitive matrices while their Hadamard power, for some values of exponent $\alpha$, is not non-negative definite.
To begin with, considering large symmetric matrices of size $n \geq 4$, it is possible to derive the following theoretical result:
\begin{theorem}\lbl{th:th2} Assume $n\geq 4$.   Let $\bd{M}=(\xi_{ij})$ be an $n\times n$ symmetric matrix, where $\{\xi_{ij};\, 1\leq i \leq j \leq n\}$ are independent random variables. 
Suppose all of the supports of $\xi_{ij}$'s contain  a common interval $[u, v]$ for some $v>u>0$. Then there exists $\alpha \in (1, 2)$ for which
\beaa
P\big(\bd{M}\geq 0\ \mbox{and}\ \bd{M}^{(\alpha)} \ngeqslant 0\big)>0 \ .
\eeaa
\end{theorem}

On the same line of work, we also develop a similar result for non-square matrices. Then under the same mild assumptions on the support of the random variables constituting the matrices involved and the value of the non integer exponent, we state the following:
\begin{theorem}\lbl{th:th3} Assume $n\geq 4$.   Let $\bd{X}=(x_{ij})$ be an $n\times p$  matrix, where $\{x_{ij};\, 1\leq i \leq n, 1\leq j \leq p\}$ are independent random variables. Suppose all of the supports of $\xi_{ij}$'s contain  a common interval $[u, v]$ for some $v>u>0$. Then there exists $\alpha \in (1, 2)$ such that
\beaa
P\big(\bd{X}^T\bd{X}>0\ \mbox{and}\ (\bd{X}^T\bd{X})^{(\alpha)}\ngeqslant 0\big)>0 \ .
\eeaa
\end{theorem}




\section{Proofs of the main results}\lbl{sec:proofs}
Using intermediary Lemmas, that we rigorously state and prove in this section, we now provide the proofs of the main results presented above.

\subsection{Intermediary Theoretical Resuls}
The following result guarantees the existence of a positive definite symmetric matrix, where the support of its entries are well defined, such that its Hadamard power, in a range of exponent, is not non-negative.
\begin{lemma}\lbl{lemma:lem1} For any $n\geq 4$, there exist $\alpha\in (1,2)$, $\delta>0$ and an $n\times n$ symmetric matrix $\bd{M}=(m_{ij})$ where for all $1\leq i, j \leq n$ we have $m_{ij}\geq 0$, such that the following holds:
(i) $\bd{M}=(m_{ij})>0$  for every $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and $1\leq i, j \leq n.$

(ii) $\bd{M}^{(\alpha)}=(m_{ij}^{\alpha})\ngeqslant 0$  for any $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and any $1\leq i, j \leq n.$
\end{lemma}
\medskip
\noindent\textbf{Proof of Lemma~\ref{lemma:lem1}}. For any $n\times n$ symmetric matrix $\bd{M}=(m_{ij})$, we denote by $\|\bd{M}\|$ the spectral norm of $\bd{M}$. 
We use $\lambda_1(\bd{M})\geq \lambda_2(\bd{M})\geq \cdots \geq \lambda_n(\bd{M})$ to denote  the eigenvalues of $\bd{M}$.  
It is well established that $\|\bd{M}\|\leq (\sum_{1\leq i, j \leq n}|m_{ij}|^2)^{1/2}$. 
Also denote by $\bd{M}_1=(m_{ij})$ and $\bd{M}_2=(\tilde{m}_{ij})$, two $n\times n$ symmetric matrices. 
The Weyl's perturbation theorem [see, e.g., \cite{horn1985}] states that
$$\max_{1\leq i \leq n}|\lambda_i(\bd{M}_1)-\lambda_i(\bd{M}_2)|\leq \|\bd{M}_1-\bd{M}_2\| \ .$$  
Therefore, we obtain the following
\bea
& \max_{1\leq i \leq n}|\lambda_i(\bd{M}_1)-\lambda_i(\bd{M}_2)|\\\notag
&\leq \Big(\sum_{1\leq i, j \leq n}|m_{ij}-\tilde{m}_{ij}|^2\Big)^{1/2}.\lbl{flower_could}
\eea
The inequality above concludes that the eigenvalues of a matrix are continuous functions of its entries. 
This is particularly true for the smallest eigenvalues.

According to Example \ref{good_example}, there exists  $\alpha\in (1, 2)$ and an $n\times n$ symmetric matrix $\bd{A}=(a_{ij})$ such that $a_{ij}\geq 0$ for all $1\leq i, j \leq n$, we have $\bd{A}>0$ and its Hadamard power matrix $\bd{A}^{(\alpha)} \ngeqslant 0$. 
For any $n\times n$ symmetric matrix  $\bd{M}=(m_{ij})$, let us define
\beaa
f(\bd{M}):=\min\big\{\lambda_n(\bd{M}),\, -\lambda_n(\bd{M}^{(\alpha)})\big\}.
\eeaa
As explained prior, $f(\bd{M})$ is a continuous function in the entries $\{m_{ij};\, 1\leq i\leq j \leq n\}$ of the matrix $\bd{M}$. 
Since $f(\bd{A})>0$, there exist $\{\delta_{ij}>0;\, 1\leq i, j\leq n\}$ with $\delta_{ij}=\delta_{ji}$ for all $1\leq i, j\leq n$ such that $f(\bd{M})>0$ for any entry $m_{ij}\in [a_{ij}, a_{ij}+\delta_{ij}]$ where $1\leq i, j\leq n$.
Set $\delta := \min\{\delta_{ij};\, 1\leq i\leq j\leq n\}.$ Then, $\delta>0$. Also, $\lambda_n(\bd{M})>0$ and $\lambda_n(\bd{M}^{(\alpha)})<0$ for every $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and every $1\leq i, j\leq n.$ 
In summary, this boils down to observing that $\bd{M}>0$  and $\bd{M}^{(\alpha)} \ngeqslant 0$ for any $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and any $1\leq i, j\leq n.$ which conludes the proof of our Lemma.
\hfill$\square$
\medskip


We now derive a similar result true for any non-square matrix whose entries supports are within a range defined by an assumed existing matrix.
Note that the following Lemma holds for any matrix $\bd{X}$.
\begin{lemma}\lbl{lemma:lem2} Let $\bd{X}=(x_{ij})_{n\times p}$ be an $n\times p$ matrix. For any $n$ and $p$ with $n\geq p\geq 4$, there exist $\alpha\in (1,2)$, $\delta>0$ and $n\times p$ matrix  $\bd{A}=(a_{ij})$ with $a_{ij}\geq 0$ for all $1\leq i\leq n$ and $1\leq  j \leq q$ such that the following holds.
%the $n\times n$ matrix $\bd{M}^{(\alpha)}=(m_{ij}^{\alpha})$ has the following property.

(i)  The matrix $\bd{X}'\bd{X}$ is positive definite for every $x_{ij}\in [a_{ij}, a_{ij}+\delta]$ and $1\leq i\leq n$ and $1\leq j \leq p.$

(ii) The Hadamard power $(\bd{X}'\bd{X})^{(\alpha)}\ngeqslant 0$  for any $x_{ij}\in [a_{ij}, a_{ij}+\delta]$, $1\leq i\leq n$ and $1\leq j \leq p$.
\end{lemma}
\medskip

\noindent\textbf{Proof of Lemma~\ref{lemma:lem2}}. Let $\bd{a}^T=(1,1,1,1)$ and $\bd{b}^T=(0,1,2,3)$ be as in Example \ref{good_example}.
%Set $\bd{M}=\bd{a}\bd{a}^T+\bd{b}\bd{b}^T.$
It is checked that the Hadamard power $(\bd{a}\bd{a}^T+\bd{b}\bd{b}^T)^{(1.1)}$ has determinant $-1.1856\times 10^{-4}.$
% eigenvalues $-0.0002, 0.0221, 1.2815$ and $20.3030.$
Set
\beaa
\bd{A}(\epsilon)=
%\begin{pmatrix}
%1 & 0& 0 & 0\\
%1 & 1& 0 & 0\\
%1 & 2 & \epsilon & 0\\
%1 & 3& 0 & \epsilon
%\end{pmatrix}
\begin{pmatrix}
1 & 1& 1 & 1\\
0 & 1& 2 & 3\\
0 & 0 & \epsilon & 0\\
0 & 0& 0 & \epsilon
\end{pmatrix}
,\ \ \ \epsilon>0.
\eeaa
It is easy to see $\lim_{\epsilon \to 0^+}[\bd{A}(\epsilon)^T\bd{A}(\epsilon)]^{(1.1)}=(\bd{a}\bd{a}^T+\bd{b}\bd{b}^T)^{1.1}$ with the entrywise convergence. By continuity of determinants, there exists $\epsilon_0>0$ such that the determinant of $[\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0)]^{(1.1)}$ is negative. That is,  $\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0)>0$ but the Hadmard power $[\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0)]^{(1.1)} \ngeqslant 0.$ Now we define an $n\times p$ matrix $\bd{A}$ such that
%$\bd{A}=\bd{A}$  for $n=4$ and
\beaa
\bd{A}=
\begin{pmatrix}
  \bd{A}(\epsilon_0) & \bd{0}\\
\bd{0} & \bd{I}_{p-4}\\
\bd{0} & \bd{0}
\end{pmatrix}
_{n\times p},
\eeaa
where the size of each submatrix $\bd{0}$ appeared in $\bd{A}$ can be seen from those of $\bd{A}(\epsilon_0)$ and $\bd{I}_{p-4}.$ In particular, the size of the  ``$\bd{0}$" in the bottom-right of $\bd{A}$ is $(n-p)\times (p-4).$ In case $n=p$, there is no  third row of submatrices in $\bd{A}$; in case $p=4$, there is no  second row of submatrices of $\bd{A}.$ Since
\beaa
\bd{A}^T\bd{A}
=
\begin{pmatrix}
\bd{A}(\epsilon_0)^T\bd{A}(\epsilon_0) & \bd{0}\\
\bd{0} & \bd{I}_{p-4}
\end{pmatrix}
.
\eeaa
Hence, $\bd{A}^T\bd{A}>0$ but the Hadamard power $(\bd{A}^T\bd{A})^{(1.1)} \ngeqslant 0.$


 The inequality \eqref{flower_could} shows that the smallest eigenvalue $\lambda_n(\bd{M})$ of $\bd{M}=\bd{X}\bd{X}^T$ is a continuous function of the entries of $\bd{M}$, which in turn are the continuous functions of the entries of $\bd{X}$. Write $\bd{X}=(x_{ij})_{n\times p}$.  Hence, $\lambda_n(\bd{M})$ is a continuous function of $x_{ij}$'s. Set
 \beaa
f(\bd{M}):=\min\big\{\lambda_n(\bd{M}),\, -\lambda_n(\bd{M}^{(\alpha)})\big\}.
\eeaa
Then $f(\bd{M})$ is a continuous function of $x_{ij}$'s and $f(\bd{A}\bd{A}^T)>0.$ Write  $\bd{A}=(a_{ij})_{n\times p}$. Then there exist $\delta_{ij}>0$ for all $1\leq i\leq n$ and $1\leq j \leq p$ such that $f(\bd{M})>0$ for all $x_{ij}\in [a_{ij}, a_{ij}+\delta_{ij}]$ with $1\leq i\leq n$ and $1\leq j \leq p$. Denote $\delta=\min\{\delta_{ij};\, 1\leq i\leq n, 1\leq j \leq p\}.$ Then $\delta>0$ and $f(\bd{X}\bd{X}^T)>0$ for all $x_{ij}\in [a_{ij}, a_{ij}+\delta]$ with $1\leq i\leq n$ and $1\leq j \leq p$. Hence, under these restrictions of $x_{ij}$'s, we have $\lambda_n(\bd{X}\bd{X}^T)>0$ and $\lambda_n((\bd{X}\bd{X}^T)^{(\alpha)}) < 0$. This yields (i) and (ii). \hfill$\square$

\medskip


\subsection{Proofs of Theorems ~\ref{th:th1}-\ref{th:th2}-\ref{th:th3}}
The proof of Theorem~\ref{th:th1}, dealing with non-square deterministic matrices, reads:

\noindent\textbf{Proof of Theorem~\ref{th:th1}}. By the Gershgorin Disk Theorem [see e.g., \cite{horn1985}], all eigenvalues of $\bd{A}$ are in the set
\bea\lbl{bro_pro}
\bigcup_{1\leq i \leq n}\Big(a_{ii}- \sum_{j\ne i}a_{ij}, a_{ii}+ \sum_{j\ne i}a_{ij}\Big).
\eea
By assumption, all eigenvalues are non-negative, hence $\bd{A}\geq 0$. On the other hand,
\beaa
a_{ii}^{\alpha}\geq  \Big(\sum_{j\ne i}a_{ij}\Big)^{\alpha}\geq \sum_{j\ne i}a_{ij}^{\alpha}
\eeaa
for all $\alpha\geq 1$ by the given condition. By the Gershgorin disc theorem again, all of the eigenvalues of the Hadamard power matrix $\bd{A}^{(\alpha)}$ are non-negative. Therefore,  $\bd{A}^{(\alpha)}\geq  0$.

Evidently, if $a_{ii}>  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n$ then all of the eigenvalues of  $\bd{A}$  and $\bd{A}^{(\alpha)}$ are positive by \eqref{bro_pro} with ``$a_{ij}$" being replaced by $a_{ij}^{\alpha}$ for all $i$ and $j$. Hence $\bd{A} > 0$ and $\bd{A}^{(\alpha)}> 0$ for all $\alpha\geq 1.$ \hfill$\square$
\medskip


Based on the results obtained in Lemma~\ref{lemma:lem1}, we give a detailed proof of Theorem~\ref{th:th2} below:

\noindent\textbf{Proof of Theorem~\ref{th:th2}}. Let $\delta >0$ be the value defined in Lemma~\ref{lemma:lem1} as the length of the interval defining the supports of the entries of a matrix. 
Since $[a_{ij}+\frac{1}{2}\delta, a_{ij}+\delta] \subset [a_{ij}, a_{ij}+\delta]$  for each pair of $(i, j)$ with $1\leq i\leq j \leq n$, then Lemma~\ref{lemma:lem1} still holds if we strengthen its conclusion with the requirement of having $a_{ij}>0$ for all $1\leq i\leq j \leq n$.
Therefore, we obtain
\bea\lbl{poker}
&0\\\notag
&<\alpha:=\min\{a_{ij};\, 1\leq i\leq j \leq n\}\\\notag
& < \beta:=\max\{a_{ij};\, 1\leq i\leq j \leq n\}+\delta.\notag
\eea
In the sequel, we denote by $\mbox{support}(\xi)$, the support of a random variable $\xi$.
In particular, we have that $P(a\leq \xi\leq  b)>0$ provided $[a, b]\subset \mbox{support}(\xi)$.
Notice that $\mbox{support}(\lambda\xi_{ij})=\lambda\cdot \mbox{support}(\xi_{ij})$ for each $i, j$ where $1\leq i\leq j \leq n$. 
Choose $\lambda>0$ such that $\lambda [u, v]\supset [\alpha , \beta]$. 
Then, it follows that
\bea\lbl{red}
& \bigcup_{1\leq i\leq  j\leq n}[a_{ij}, a_{ij}+\delta] \subset [\alpha, \beta]\\\notag
& \subset \bigcap_{1\leq i\leq  j\leq n}  \mbox{support}(\lambda\xi_{ij}).\notag
\eea
Besides, observe that
\bea\label{eq:th2proba}
& P\big(\bd{M}>0\ \mbox{and}\ \bd{M}^{(\alpha)}\ngeqslant 0\big)\\\notag
& = P\big(\lambda\bd{M}>0\ \mbox{and}\ (\lambda\bd{M})^{(\alpha)}\ngeqslant 0\big).\notag
\eea
By Lemma~\ref{lemma:lem1} and the independence assumption, the last probability in \eqref{eq:th2proba} reads
\begin{align}\notag
& P(\lambda \xi_{ij}\in [a_{ij}, a_{ij}+\delta]\ \mbox{for each}\ 1\leq i\leq j \leq n)\\\notag
=& \prod_{1\leq i\leq j \leq n}P(\lambda\xi_{ij}\in [a_{ij}, a_{ij}+\delta])\\\notag
>& 0,\notag
\end{align}
where the last inequality comes from \eqref{red}. 
The proof is thus complete. \hfill$\square$

\medskip



Having established Lemma~\ref{lemma:lem2} leads to the following proof of Theorem~\ref{th:th3}:

\noindent\textbf{Proof of Theorem~\ref{th:th3}}. Let $\delta >0$ be the strictly positive value defined in Lemma~\ref{lemma:lem2}. 
Leveraging the same argument as in \eqref{poker} (see Proof of Theorem~\ref{th:th2}), and without loss of generality, we assume that $a_{ij}>0$ for all $1\leq i\leq n$ and $1\leq j \leq p$. 
Therefore,
\beaa
%\lbl{poker1}
0&<&\alpha:=\min\{a_{ij};\, 1\leq i \leq n, 1\leq j \leq p\}\\
&<& \beta:=\max\{a_{ij};\, 1\leq i\leq n, 1\leq j \leq p\}+\delta.
\eeaa
By choosing $\lambda>0$ such that $\lambda [u, v]\supset [\alpha , \beta]$, we then have
\bea\lbl{red1}
\bigcup [a_{ij}, a_{ij}+\delta] \subset [\alpha, \beta]\subset \bigcap   \mbox{support}(\lambda x_{ij}),
\eea
where the union and the intersection are taken over $1\leq i \leq n$ and $1\leq j \leq p.$ 
Let $\bd{X}=(x_{ij})$ be an $n\times p $ matrix.
By setting $\bd{Y}=\lambda\bd{X}$, we have
\beaa
& P\big(\bd{X}^T\bd{X}>0\ \mbox{and}\ (\bd{X}^T\bd{X})^{(\alpha)}\ngeqslant 0\big)\\
& =  P\big(\bd{Y}^T\bd{Y}>0\ \mbox{and}\ (\bd{Y}^T\bd{Y})^{(\alpha)}\ngeqslant 0\big).
\eeaa
From Lemma~\ref{lemma:lem2}, the above is at least
\begin{align}\notag
& P\big(\lambda x_{ij}\in [a_{ij}, a_{ij}+\delta]\ \mbox{for}\ 1\leq i\leq n\ \,, 1\leq j \leq p\big)\\ \notag
=& \prod_{1\leq i\leq n, 1\leq j \leq p}P\big(\lambda x_{ij}\in [a_{ij}, a_{ij}+\delta]\big)\\\notag
>& 0\notag
\end{align}
where the last step follows from \eqref{red1} and the independence assumption of $\{\xi_{ij};\, 1\leq i \leq j \leq n\}$ which concludes our proof. \hfill$\square$



\section{Conclusion}\lbl{sec:conclusion}

We have studied in this paper, 


\noindent\textbf{Acknowledgements}. We thank Hongru Zhao very much for very fruitful   discussions.

\bibliographystyle{IEEEtran}
\bibliography{references}

%
%\begin{thebibliography}{99}
%
%\bibitem{Hiai}
%Hiai, F. (2009).  Monotonicity for entrywise functions of matrices. {\it Linear  Algebra  Appl.} 431(8), 1125-1146.
%
%\bibitem{s1} Horn, R. and Johnson, C. (1991). {\em Topics in matrix analysis}. Cambridge University Press, Cambridge.
%
%
%\bibitem{s2} Horn, R. and Johnson, C. (1985). {\em Matrix Analysis}. Cambridge Univesity Press, Cambridge.
%
%\bibitem{s3}
%Schoenberg, I. J. (1942). Positive definite functions on spheres. {\it Duke Math. J.}   9, 96-108.
%
%\bibitem{s4}
%Schur, J. (1911). Bemerkungen zur theorie der beschr\"{a}nkten bilinearformen mit unendlich vielenver\"{a}nderlichen. {\it Journal f\"{u}r die reine und angewandte Mathematik} 140, 1-28.
%
%\bibitem{s5} Vasudeva, H. (1979).  Positive definite matrices and absolutely monotonic functions. {\it Indian J. Pure Appl. Math.} 10(7), 854-858.
%
%\bibitem{horadam2012hadamard} Horadam, K. J. (2012).  Hadamard matrices and their applications. {\it Princeton university press}
%
%
%\end{thebibliography}




\end{document}
