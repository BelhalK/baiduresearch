\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{RKK18}
\citation{LFDA17}
\citation{Rnet16,goodfellow2014generative}
\citation{Atari13}
\citation{GMH13}
\citation{RKK18}
\citation{KB15}
\citation{TH12}
\citation{Z12}
\citation{D16}
\citation{DHS11,MS10}
\citation{N04}
\citation{P64}
\citation{P64}
\citation{RKK18}
\citation{KB15}
\citation{CJ12,RS13b,SALS15,ALLW18,mertikopoulos2018optimistic}
\citation{DISZ18}
\citation{RS13b}
\citation{goodfellow2014generative}
\citation{CJ12,RS13b,SALS15}
\citation{DISZ18}
\providecommand \oddpage@label [2]{}
\citation{KB15,RKK18}
\citation{CJ12,RS13b,SALS15,ALLW18}
\citation{H14}
\citation{SALS15}
\citation{SALS15}
\citation{RS13b}
\citation{KB15}
\citation{P64}
\citation{DHS11}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\newlabel{optFTRL}{{1}{2}{Preliminaries}{equation.2.1}{}}
\citation{RKK18}
\citation{DHS11}
\citation{KB15}
\citation{KB15}
\citation{RKK18}
\citation{RKK18}
\citation{RS13b}
\citation{CJ12}
\citation{DHS11}
\citation{N04}
\citation{P64}
\citation{CJ12,RS13b,SALS15}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:amsgrad}{{1}{3}{Preliminaries}{algocf.1}{}}
\newlabel{line:maxop}{{7}{3}{Preliminaries}{ALC@unique.7}{}}
\newlabel{sec:opt}{{3}{3}{\textsc {OPT-AMSGRAD} Algorithm}{section.3}{}}
\newlabel{alg:optamsgrad}{{2}{3}{\textsc {OPT-AMSGRAD} Algorithm}{algocf.2}{}}
\newlabel{fig:scheme}{{1}{3}{\textsc {OPT-AMSGrad} underlying structure.\relax }{figure.caption.3}{}}
\citation{RKK18,KB15}
\citation{RKK18}
\citation{RS13b}
\citation{ghadimi2013stochastic}
\newlabel{sec:analysis}{{4}{4}{Convergence Analysis}{section.4}{}}
\newlabel{sec:convex}{{4.1}{4}{Convex Regret Analysis}{subsection.4.1}{}}
\newlabel{thm:mainconvex}{{1}{4}{}{Theorem.1}{}}
\newlabel{cor:corollary}{{1}{4}{}{Corollary.1}{}}
\newlabel{eq:boundAMS}{{2}{4}{Convex Regret Analysis}{equation.4.2}{}}
\newlabel{eq:minproblem}{{3}{4}{Finite-Time Analysis in Nonconvex Case}{equation.4.3}{}}
\newlabel{eq:random}{{4}{4}{Finite-Time Analysis in Nonconvex Case}{equation.4.4}{}}
\newlabel{ass:boundedparam}{{1}{4}{}{assumption.1}{}}
\newlabel{ass:smooth}{{2}{4}{}{assumption.2}{}}
\citation{ghadimi2013stochastic}
\citation{ghadimi2013stochastic}
\citation{ZTYCG18}
\citation{ZRSKK18,CLSH19,WWB18,ZTYCG18,ZS18,LO18}
\citation{CLSH19}
\citation{Princeton18}
\citation{CYYZC19}
\newlabel{fig:assumption}{{2}{5}{Assumption H\ref {ass:guessbound} on gradient prediction.\relax }{figure.caption.4}{}}
\newlabel{ass:guessbound}{{3}{5}{}{assumption.3}{}}
\newlabel{ass:bounded}{{4}{5}{}{assumption.4}{}}
\newlabel{lem:bound}{{1}{5}{}{Lemma.1}{}}
\newlabel{thm:boundopt}{{2}{5}{}{Theorem.2}{}}
\newlabel{eq:dnnmodel}{{5}{5}{Checking H\ref {ass:boundedparam} for a Deep Neural Network}{equation.4.5}{}}
\newlabel{lem:dnnh2}{{2}{5}{}{Lemma.2}{}}
\newlabel{sec:related}{{5}{5}{Comparison to related methods}{section.5}{}}
\citation{MY16}
\citation{MY16}
\citation{MY16}
\citation{MY16}
\citation{DISZ18}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{DISZ18}
\citation{WN11}
\citation{CJ76}
\citation{E79}
\citation{SAB16}
\citation{BZ13}
\citation{SAB16}
\citation{Scieur18}
\newlabel{OPT-DISZ}{{3}{6}{Comparison to related methods}{algocf.3}{}}
\newlabel{sec:numerical}{{6}{6}{Numerical Experiments}{section.6}{}}
\newlabel{nox}{{6}{6}{Gradient Estimation}{equation.6.6}{}}
\newlabel{alg:algex}{{4}{6}{Gradient Estimation}{algocf.4}{}}
\citation{RKK18}
\citation{DISZ18}
\citation{RKK18}
\citation{KB15}
\citation{MNIST07}
\citation{Rnet16}
\citation{gers1999learning}
\newlabel{fig:train_loss}{{3}{7}{Training loss vs. Number of iterations for fully connected NN, CNN, LSTM and ResNet.\relax }{figure.caption.7}{}}
\newlabel{fig:testandtrain}{{4}{8}{\textit {MNIST-back-image} + CNN, \textit {CIFAR10} + Res-18 and \textit {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy.\relax }{figure.caption.8}{}}
\newlabel{sec:choicer}{{6.3}{8}{Choice of parameter $r$}{subsection.6.3}{}}
\newlabel{fig:compare}{{5}{8}{Training loss w.r.t. different $r$ values.\relax }{figure.caption.9}{}}
\bibstyle{plain}
\bibdata{reference}
\bibcite{ALLW18}{{1}{}{{}}{{}}}
\bibcite{Princeton18}{{2}{}{{}}{{}}}
\bibcite{BZ13}{{3}{}{{}}{{}}}
\bibcite{CJ76}{{4}{}{{}}{{}}}
\bibcite{CLSH19}{{5}{}{{}}{{}}}
\bibcite{CYYZC19}{{6}{}{{}}{{}}}
\bibcite{CJ12}{{7}{}{{}}{{}}}
\bibcite{DISZ18}{{8}{}{{}}{{}}}
\bibcite{defossez2020convergence}{{9}{}{{}}{{}}}
\bibcite{D16}{{10}{}{{}}{{}}}
\bibcite{DHS11}{{11}{}{{}}{{}}}
\bibcite{E79}{{12}{}{{}}{{}}}
\bibcite{gers1999learning}{{13}{}{{}}{{}}}
\bibcite{ghadimi2013stochastic}{{14}{}{{}}{{}}}
\bibcite{goodfellow2014generative}{{15}{}{{}}{{}}}
\bibcite{GMH13}{{16}{}{{}}{{}}}
\bibcite{H14}{{17}{}{{}}{{}}}
\bibcite{Rnet16}{{18}{}{{}}{{}}}
\bibcite{KB15}{{19}{}{{}}{{}}}
\bibcite{MNIST07}{{20}{}{{}}{{}}}
\bibcite{LFDA17}{{21}{}{{}}{{}}}
\bibcite{LO18}{{22}{}{{}}{{}}}
\bibcite{MS10}{{23}{}{{}}{{}}}
\bibcite{mertikopoulos2018optimistic}{{24}{}{{}}{{}}}
\bibcite{Atari13}{{25}{}{{}}{{}}}
\bibcite{MY16}{{26}{}{{}}{{}}}
\bibcite{N04}{{27}{}{{}}{{}}}
\bibcite{P64}{{28}{}{{}}{{}}}
\bibcite{RS13b}{{29}{}{{}}{{}}}
\bibcite{RKK18}{{30}{}{{}}{{}}}
\bibcite{SAB16}{{31}{}{{}}{{}}}
\bibcite{Scieur18}{{32}{}{{}}{{}}}
\bibcite{SALS15}{{33}{}{{}}{{}}}
\bibcite{TH12}{{34}{}{{}}{{}}}
\bibcite{T08}{{35}{}{{}}{{}}}
\bibcite{WN11}{{36}{}{{}}{{}}}
\bibcite{WWB18}{{37}{}{{}}{{}}}
\bibcite{yan2018unified}{{38}{}{{}}{{}}}
\bibcite{ZRSKK18}{{39}{}{{}}{{}}}
\bibcite{Z12}{{40}{}{{}}{{}}}
\bibcite{ZTYCG18}{{41}{}{{}}{{}}}
\bibcite{ZS18}{{42}{}{{}}{{}}}
\citation{T08}
\newlabel{app:thmmainconvex}{{A}{12}{Proof of Theorem~\ref {thm:mainconvex}}{appendix.A}{}}
\newlabel{nn1}{{8}{12}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.8}{}}
\newlabel{ii}{{9}{12}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.9}{}}
\newlabel{nc1}{{10}{12}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.10}{}}
\newlabel{nn2}{{11}{12}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.11}{}}
\newlabel{nc2}{{12}{12}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.12}{}}
\newlabel{nn3}{{13}{13}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.13}{}}
\newlabel{nnn}{{14}{13}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.14}{}}
\newlabel{nnnn}{{15}{13}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.15}{}}
\newlabel{nn5}{{16}{13}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.16}{}}
\newlabel{nn4}{{17}{13}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.17}{}}
\citation{yan2018unified}
\newlabel{eq:deftilde}{{18}{14}{Proofs of Auxiliary Lemmas}{equation.C.18}{}}
\newlabel{lem:momentum}{{3}{14}{}{Lemma.3}{}}
\newlabel{lem:squarev}{{4}{15}{}{Lemma.4}{}}
\newlabel{app:lembound}{{C.1}{16}{Proof of Lemma~\ref {lem:bound}}{subsection.C.1}{}}
\newlabel{app:thmboundopt}{{D}{16}{Proof of Theorem~\ref {thm:boundopt}}{appendix.D}{}}
\newlabel{eq:smoothness}{{27}{16}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.27}{}}
\newlabel{eq:termA1}{{28}{17}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.28}{}}
\newlabel{eq:termA2}{{29}{17}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.29}{}}
\newlabel{eq:termA}{{30}{17}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.30}{}}
\newlabel{eq:termB1}{{31}{17}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.31}{}}
\newlabel{eq:termB2}{{32}{17}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.32}{}}
\newlabel{eq:termB3}{{34}{17}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.34}{}}
\newlabel{eq:termB}{{35}{18}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.35}{}}
\newlabel{eq:term3}{{36}{18}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.36}{}}
\newlabel{eq:expectationtildegrad}{{37}{18}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.37}{}}
\newlabel{eq:bound1}{{38}{18}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.38}{}}
\citation{ZTYCG18}
\citation{defossez2020convergence}
\newlabel{app:lemdnnh2}{{E}{20}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{appendix.E}{}}
\newlabel{eq:mildassumptions}{{39}{20}{}{equation.E.39}{}}
\newlabel{eq:boundderivativeloss}{{40}{20}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.E.40}{}}
\newlabel{eq:decrease}{{41}{20}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.E.41}{}}
\citation{RKK18}
\citation{RKK18}
\citation{KB15}
\newlabel{eq:gradientatell}{{42}{21}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.E.42}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{simu}{{6}{22}{\small (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. \relax }{figure.caption.11}{}}
