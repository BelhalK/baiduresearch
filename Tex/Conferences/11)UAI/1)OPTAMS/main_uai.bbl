\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2018)Abernethy, Lai, Levy, and Wang]{ALLW18}
Jacob Abernethy, Kevin~A. Lai, Kfir~Y. Levy, and Jun-Kun Wang.
\newblock Faster rates for convex-concave games.
\newblock \emph{COLT}, 2018.

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{Princeton18}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock \emph{ICML}, 2019.

\bibitem[Brezinski and Zaglia(2013)]{BZ13}
C.~Brezinski and M.~R. Zaglia.
\newblock Extrapolation methods: theory and practice.
\newblock \emph{Elsevier}, 2013.

\bibitem[Cabay and Jackson(1976)]{CJ76}
S.~Cabay and L.~Jackson.
\newblock A polynomial extrapolation method for finding limits and antilimits
  of vector sequences.
\newblock \emph{SIAM Journal on Numerical Analysis}, 1976.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Liu, Sun, and Hong]{CLSH19}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Yuan, Yi, Zhou, Chen, and
  Yang]{CYYZC19}
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao
  Yang.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock \emph{ICLR}, 2019{\natexlab{b}}.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and Zhu]{CJ12}
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
  Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock \emph{COLT}, 2012.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and Zeng]{DISZ18}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock \emph{ICLR}, 2018.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020convergence}
Alexandre D{\'e}fossez, L{\'e}on Bottou, Francis Bach, and Nicolas Usunier.
\newblock On the convergence of adam and adagrad.
\newblock \emph{arXiv preprint arXiv:2003.02395}, 2020.

\bibitem[Dozat(2016)]{D16}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock \emph{ICLR (Workshop Track)}, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{DHS11}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2011.

\bibitem[Eddy(1979)]{E79}
R.~Eddy.
\newblock Extrapolating to the limit of a vector sequence.
\newblock \emph{Information linkage between applied mathematics and industry,
  Elsevier}, 1979.

\bibitem[Gers et~al.(2000)Gers, Schmidhuber, and Cummins]{gers1999learning}
Felix~A. Gers, J\"{u}rgen~A. Schmidhuber, and Fred~A. Cummins.
\newblock Learning to forget: Continual prediction with lstm.
\newblock \emph{Neural Comput.}, 12\penalty0 (10):\penalty0 2451--2471, October
  2000.
\newblock ISSN 0899-7667.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{NIPS}, 2014.

\bibitem[Graves et~al.(2013)Graves, rahman Mohamed, and Hinton]{GMH13}
Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock \emph{ICASSP}, 2013.

\bibitem[Hazan(2016)]{H14}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Rnet16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{CVPR}, 2016.

\bibitem[Kingma and Ba(2015)]{KB15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Larochelle et~al.(2007)Larochelle, Erhan, Courville, Bergstra, and
  Bengio]{MNIST07}
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua
  Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock \emph{ICML}, 2007.

\bibitem[Levine et~al.(2017)Levine, Finn, Darrell, and Abbeel]{LFDA17}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{NIPS}, 2017.

\bibitem[Li and Orabona.(2019)]{LO18}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock \emph{AISTAT}, 2019.

\bibitem[McMahan and Streeter(2010)]{MS10}
H.~Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{COLT}, 2010.

\bibitem[Mertikopoulos et~al.(2018)Mertikopoulos, Lecouat, Zenati, Foo,
  Chandrasekhar, and Piliouras]{mertikopoulos2018optimistic}
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay
  Chandrasekhar, and Georgios Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock \emph{arXiv preprint arXiv:1807.02629}, 2018.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{Atari13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{NIPS (Deep Learning Workshop)}, 2013.

\bibitem[Mohri and Yang(2016)]{MY16}
Mehryar Mohri and Scott Yang.
\newblock Accelerating optimization via adaptive prediction.
\newblock \emph{AISTATS}, 2016.

\bibitem[Nesterov(2004)]{N04}
Yurii Nesterov.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock \emph{Springer}, 2004.

\bibitem[Polyak(1964)]{P64}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Mathematics and Mathematical Physics}, 1964.

\bibitem[Rakhlin and Sridharan(2013)]{RS13b}
Sasha Rakhlin and Karthik Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3066--3074, 2013.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{RKK18}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{ICLR}, 2018.

\bibitem[Scieur et~al.(2016)Scieur, d'Aspremont, and Bach]{SAB16}
Damien Scieur, Alexandre d'Aspremont, and Francis Bach.
\newblock Regularized nonlinear acceleration.
\newblock \emph{NIPS}, 2016.

\bibitem[Scieur et~al.(2018)Scieur, Oyallon, d'Aspremont, and Bach]{Scieur18}
Damien Scieur, Edouard Oyallon, Alexandre d'Aspremont, and Francis Bach.
\newblock Nonlinear acceleration of deep neural networks.
\newblock \emph{CoRR}, abs/1805.09639, 2018.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and Schapire]{SALS15}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock \emph{NIPS}, 2015.

\bibitem[Tieleman and Hinton(2012)]{TH12}
T.~Tieleman and G.~Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Tseng(2008)]{T08}
Paul Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock 2008.

\bibitem[Walker and Ni.(2011)]{WN11}
H.~F. Walker and P.~Ni.
\newblock Anderson acceleration for fixed-point iterations.
\newblock \emph{SIAM Journal on Numerical Analysis}, 2011.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou.]{WWB18}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from
  any initialization.
\newblock \emph{ICML}, 2019.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock \emph{arXiv preprint arXiv:1808.10396}, 2018.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and Kumar]{ZRSKK18}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Zeiler(2012)]{Z12}
Matthew~D. Zeiler.
\newblock Adadelta: An adaptive learning rate method.
\newblock \emph{arXiv:1212.5701}, 2012.

\bibitem[Zhou et~al.(2018)Zhou, Tang, Yang, Cao, and Gu]{ZTYCG18}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv:1808.05671}, 2018.

\bibitem[Zou and Shen(2018)]{ZS18}
Fangyu Zou and Li~Shen.
\newblock On the convergence of adagrad with momentum for training deep neural
  networks.
\newblock \emph{arXiv:1808.03408}, 2018.

\end{thebibliography}
