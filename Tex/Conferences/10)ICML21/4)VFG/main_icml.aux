\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{koller2007graphical}
\citation{bilmes2005graphical}
\citation{shwe1990probabilistic}
\citation{jordan1999graphical}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{sanner2012symbolic,kahle2008junction}
\citation{jordan1999introduction,hoffman2013stochastic,kingma2013auto,liu2016stein}
\citation{xing2012generalized}
\citation{bishop2003vibes,winn2005variational}
\citation{kingma2013auto}
\citation{kingma2018glow,rezende2015variational}
\citation{tabak2010density}
\citation{Dinh2016DensityEU,rippel2013high}
\citation{rezende2015variational}
\citation{Dinh2016DensityEU,dinh2014nice,de2020block,ho2019flow++,papamakarios2019normalizing}
\citation{rezende2015variational}
\newlabel{sec:prelim}{{2}{2}{}{section.2}{}}
\newlabel{eq:vi_elbo}{{1}{2}{}{equation.2.1}{}}
\newlabel{eq:vae_recon}{{2}{2}{}{equation.2.2}{}}
\newlabel{eq:flow}{{3}{2}{}{equation.2.3}{}}
\citation{rezende2015variational,berg2018sylvester}
\citation{kingma2013auto}
\newlabel{eq:flow2}{{4}{3}{}{equation.2.4}{}}
\newlabel{sec:main}{{3}{3}{}{section.3}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tree}{{1}{3}{\small (Left) Node $\mathbf {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf {h}^{2,1}$. (Right) An illustration of the latent structure from layer $l-1$ to $l+1$. $\oplus $ is an aggregation node, and circles stand for non-aggregation~nodes. \relax }{figure.caption.1}{}}
\newlabel{eq:posterior}{{5}{3}{}{equation.3.5}{}}
\newlabel{eq:elbo}{{6}{3}{}{equation.3.6}{}}
\newlabel{eq:kl}{{7}{3}{}{equation.3.7}{}}
\newlabel{eq:post_smp}{{8}{3}{}{equation.3.8}{}}
\newlabel{eq:prior_smp}{{9}{3}{}{equation.3.9}{}}
\citation{kingma2013auto}
\citation{rezende2015variational,berg2018sylvester}
\newlabel{fig:message}{{2}{4}{{\small Layer-wise sampling of the posterior and the prior corresponds to forward~(encoding) and backward~(decoding) message passing, respectively.}\relax }{figure.caption.2}{}}
\newlabel{eq:kl_est}{{10}{4}{}{equation.3.10}{}}
\newlabel{eq:elbo_dag}{{11}{4}{}{equation.3.11}{}}
\newlabel{sec:node_aggr}{{3.3}{4}{}{subsection.3.3}{}}
\newlabel{fig:node_aggre}{{3}{5}{(Left) Aggregation node $\mathbf {h}^{l+1,1}$ has three children, $\mathbf {h}^{l,1}$, $\mathbf {h}^{l,2}$, and $\mathbf {h}^{l,3}$. Thin lines are identity functions, and thick lines are flow functions. (Right) A VFG model with one aggregation node, $\mathbf {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }{figure.caption.3}{}}
\newlabel{eq:one_agg_node}{{12}{5}{}{equation.3.12}{}}
\newlabel{fig:two_layer_infer}{{4}{5}{{\small (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }{figure.caption.4}{}}
\newlabel{sec:infer}{{3.4}{5}{}{subsection.3.4}{}}
\newlabel{eq:aggr_obs_ch}{{14}{5}{}{equation.3.14}{}}
\newlabel{lm:apprx}{{1}{5}{}{lemma.1}{}}
\newlabel{eq:cond_llk}{{15}{6}{}{equation.3.15}{}}
\newlabel{rmk:apprx_mul}{{1}{6}{}{remark.1}{}}
\newlabel{alg:main}{{1}{6}{Inference model parameters with forward and backward message propagation\relax }{algorithm.1}{}}
\newlabel{line:for2}{{1}{6}{Inference model parameters with forward and backward message propagation\relax }{algorithm.1}{}}
\newlabel{line:forward}{{1}{6}{Inference model parameters with forward and backward message propagation\relax }{algorithm.1}{}}
\newlabel{line:backward}{{1}{6}{Inference model parameters with forward and backward message propagation\relax }{algorithm.1}{}}
\newlabel{line:update}{{1}{6}{Inference model parameters with forward and backward message propagation\relax }{algorithm.1}{}}
\newlabel{alg:rand_mask}{{2}{6}{Inference model parameters with random masking\relax }{algorithm.2}{}}
\citation{Khemakhem20a,Sorrenson2020}
\citation{efron1975defining}
\citation{bengio2013representation}
\newlabel{eq:elbo_tree_mask}{{16}{7}{}{equation.3.16}{}}
\newlabel{sec:theory}{{4}{7}{}{section.4}{}}
\newlabel{eq:exp_h}{{17}{7}{}{equation.4.17}{}}
\newlabel{eq:xt_gen}{{18}{7}{}{equation.4.18}{}}
\newlabel{thm:identif}{{1}{7}{}{theorem.1}{}}
\newlabel{sec:numerical}{{5}{7}{}{section.5}{}}
\citation{Dinh2016DensityEU}
\citation{Lecunmnist2010}
\citation{Sorrenson2020}
\citation{maaten2008visualizing}
\citation{langley00}
\bibdata{example_paper}
\bibcite{bengio2013representation}{{1}{2013}{{Bengio et~al.}}{{Bengio, Courville, and Vincent}}}
\newlabel{fig:sim}{{5}{8}{Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }{figure.caption.5}{}}
\newlabel{sec:exp:mnist}{{5.2}{8}{}{subsection.5.2}{}}
\newlabel{fig:reconst}{{6}{8}{(Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }{figure.caption.6}{}}
\newlabel{fig:z_tsne}{{7}{8}{MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }{figure.caption.7}{}}
\newlabel{sec:conclusion}{{6}{8}{}{section.6}{}}
\bibcite{berg2018sylvester}{{2}{2018}{{Berg et~al.}}{{Berg, Hasenclever, Tomczak, and Welling}}}
\bibcite{bilmes2005graphical}{{3}{2005}{{Bilmes \& Bartels}}{{Bilmes and Bartels}}}
\bibcite{bishop2003vibes}{{4}{2003}{{Bishop et~al.}}{{Bishop, Spiegelhalter, and Winn}}}
\bibcite{de2020block}{{5}{2020}{{De~Cao et~al.}}{{De~Cao, Aziz, and Titov}}}
\bibcite{dinh2014nice}{{6}{2014}{{Dinh et~al.}}{{Dinh, Krueger, and Bengio}}}
\bibcite{Dinh2016DensityEU}{{7}{2016}{{Dinh et~al.}}{{Dinh, Sohl-Dickstein, and Bengio}}}
\bibcite{efron1975defining}{{8}{1975}{{Efron et~al.}}{{}}}
\bibcite{Book:Hanson_1994}{{9}{1994}{{Hanson}}{{}}}
\bibcite{ho2019flow++}{{10}{2019}{{Ho et~al.}}{{Ho, Chen, Srinivas, Duan, and Abbeel}}}
\bibcite{hoffman2013stochastic}{{11}{2013}{{Hoffman et~al.}}{{Hoffman, Blei, Wang, and Paisley}}}
\bibcite{hruschka2007bayesian}{{12}{2007}{{Hruschka et~al.}}{{Hruschka, Hruschka, and Ebecken}}}
\bibcite{jordan1999graphical}{{13}{1999}{{Jordan}}{{}}}
\bibcite{jordan1999introduction}{{14}{1999}{{Jordan et~al.}}{{Jordan, Ghahramani, Jaakkola, and Saul}}}
\bibcite{kahle2008junction}{{15}{2008}{{Kahle et~al.}}{{Kahle, Savitsky, Schnelle, and Cevher}}}
\bibcite{Khemakhem20a}{{16}{2020}{{Khemakhem et~al.}}{{Khemakhem, Kingma, Monti, and Hyvarinen}}}
\bibcite{kingma2018glow}{{17}{2018}{{Kingma \& Dhariwal}}{{Kingma and Dhariwal}}}
\bibcite{kingma2013auto}{{18}{2013}{{Kingma \& Welling}}{{Kingma and Welling}}}
\bibcite{koller2007graphical}{{19}{2007}{{Koller et~al.}}{{Koller, Friedman, Getoor, and Taskar}}}
\bibcite{Article:Krantz_2008}{{20}{2008}{{Krantz \& Parks}}{{Krantz and Parks}}}
\bibcite{langley00}{{21}{2000}{{Langley}}{{}}}
\bibcite{Lecunmnist2010}{{22}{2010}{{LeCun \& Cortes}}{{LeCun and Cortes}}}
\bibcite{liu2016stein}{{23}{2016}{{Liu \& Wang}}{{Liu and Wang}}}
\bibcite{maaten2008visualizing}{{24}{2008}{{Maaten \& Hinton}}{{Maaten and Hinton}}}
\bibcite{madigan1995bayesian}{{25}{1995}{{Madigan et~al.}}{{Madigan, York, and Allard}}}
\bibcite{papamakarios2019normalizing}{{26}{2019}{{Papamakarios et~al.}}{{Papamakarios, Nalisnick, Rezende, Mohamed, and Lakshminarayanan}}}
\bibcite{rezende2015variational}{{27}{2015}{{Rezende \& Mohamed}}{{Rezende and Mohamed}}}
\bibcite{rippel2013high}{{28}{2013}{{Rippel \& Adams}}{{Rippel and Adams}}}
\bibcite{sanner2012symbolic}{{29}{2012}{{Sanner \& Abbasnejad}}{{Sanner and Abbasnejad}}}
\bibcite{shwe1990probabilistic}{{30}{1990}{{Shwe et~al.}}{{Shwe, Middleton, Heckerman, Henrion, Horvitz, Lehmann, and Cooper}}}
\bibcite{Sorrenson2020}{{31}{2020}{{Sorrenson et~al.}}{{Sorrenson, Rother, and K{\" o}the}}}
\bibcite{tabak2010density}{{32}{2010}{{Tabak et~al.}}{{Tabak, Vanden-Eijnden, et~al.}}}
\bibcite{winn2005variational}{{33}{2005}{{Winn \& Bishop}}{{Winn and Bishop}}}
\bibcite{xing2012generalized}{{34}{2012}{{Xing et~al.}}{{Xing, Jordan, and Russell}}}
\bibstyle{icml2021}
\newlabel{appd:tree_elbo}{{A.1}{11}{}{subsection.A.1}{}}
\newlabel{fig:tree-d}{{8}{11}{Tree structure.\relax }{figure.caption.9}{}}
\newlabel{eq:chain}{{19}{11}{}{equation.A.19}{}}
\citation{rezende2015variational,berg2018sylvester}
\newlabel{eq:kl_l}{{20}{12}{}{equation.A.20}{}}
\newlabel{eq:kl_la}{{21}{12}{}{equation.A.21}{}}
\newlabel{eq:kl_lb}{{23}{13}{}{equation.A.23}{}}
\newlabel{fig:message_tree}{{9}{13}{Message passing on a tree.\relax }{figure.caption.10}{}}
\citation{Dinh2016DensityEU}
\newlabel{eq:elbo2}{{24}{14}{}{equation.A.24}{}}
\newlabel{appd:dag_elbo}{{A.3}{14}{}{subsection.A.3}{}}
\newlabel{fig:dag}{{10}{14}{DAG structure. The inverse topology order is \big \{ \{1,2,3\}, \{4,5\}, \{6\}, \{7\} \big \}, and it corresponds to layers 0 to 3. \relax }{figure.caption.11}{}}
\newlabel{eq:dag_elbo}{{25}{15}{}{equation.A.25}{}}
\newlabel{eq:KL_dag1}{{26}{15}{}{equation.A.26}{}}
\newlabel{eq:KL_dag2}{{27}{15}{}{equation.A.27}{}}
\newlabel{eq:kl_dag3}{{28}{15}{}{equation.A.28}{}}
\newlabel{appd:proof_lm1}{{B.1}{15}{}{subsection.B.1}{}}
\citation{Book:Hanson_1994,Article:Krantz_2008}
\newlabel{appd:proof_thm1}{{B.2}{16}{}{subsection.B.2}{}}
\newlabel{eq:u_diff}{{29}{16}{}{equation.B.29}{}}
\citation{Khemakhem20a}
\citation{Dinh2016DensityEU}
\newlabel{eq:A_sim}{{30}{17}{}{equation.B.30}{}}
\newlabel{fig:z_no_Y}{{11}{18}{MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }{figure.caption.12}{}}
\newlabel{fig:mnist_dis}{{12}{19}{MNIST: Increasing each latent variable from a small value to a larger one.\relax }{figure.caption.13}{}}
