\documentclass{article}

\usepackage{icml2021_author_response}
% \usepackage{fullpage}
\setlength\parindent{0pt}
\usepackage{bbm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{color}

\begin{document}
We thank the five reviewers for their valuable feedback. 
We address the common concern of \textbf{Reviewer 3, Reviewer 7 and Reviewer 8:} \vspace{-5pt}

-- \textit{On :} 


\textbf{Reviewer 3:} We thank the reviewer for the comments and clarify:\vspace{-5pt}

-- \textit{Notations:} 

2) 
4) We referred to Lemma 1 indeed.
5) 
6) The Top-k operation alone is biased which is what we want to avoid with the HEAVYMIX update.
7) We chose to encapsulate the update allowing to compute the sketch of the gradient vector. This allows to use any sketching method as a plug-and-play update.
8) HEAVYMIX does require to extract the exact $Top_m$ values of the gradient.
9) See 7)
11) The function $S_j(\cdot)$ is any sketching operator that depends on the random hash tables used. This will be clarified.
12) We will add a table to summarize the metrics from the plots.

% \textbf{Reviewer 6:} \vspace{-5pt}

% -- \textit{On :} 


\textbf{Reviewer 7:} We will proofread the paper and add the following remarks:\vspace{-5pt}

-- \textit{Privacy:} HEAPRIX sketches are based on random hashes that by construction ensures the attacker to access exact values of the gradients.

-- \textit{Compressing and Sketching:} Thank you for the references.
Compression and quantization are two alternatives to the overall objective of making algorithms communication efficient.
Though, our paper focuses on sketching techniques in the spirit of tackling the privacy issue of FL along its communication efficiency bottleneck.


-- \textit{Numerical Experiments:} 
We stress on the observable gap between our method and FedSGD in the numerical runs. FedSGD is a method using the full gradient at each round of communication and thus displaying a higher computation cost than any other methods using sketches that we plot.
While $(50 \times 100)$ may seem large, it still represents and $12 \times$ compressing ratio, which is high. 
Hence, not matching FedSGD performances while reducing by $12$ the size of the transmitted vector is not an issue.

\textbf{Reviewer 8:} Thank you for your reviews.\vspace{-5pt}

-- \textit{Numerical Experiments:} We have developed numerical experiments on both MNIST and CIFAR-10 under iid and non-idd settings using shallow and deeper neural networks for completeness. Those runs are also reported in the supplementary material.
More experiments are in progress for inclusion in the revised paper.

-- \textit{Comparison with other unbiased compressors:} 

-- \textit{Heuristic behind FedSKETCH:} 

\end{document}
