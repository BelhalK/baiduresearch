
\documentclass{article} % For LaTeX2e




\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{xcolor} 
\usepackage{pdfpages}
\usepackage{natbib}

\usepackage{enumerate}
\usepackage{enumitem}
%\usepackage[nonatbib]{neurips_2020}
\usepackage{icml2021_author_response}


  





\title{Convergent Adaptive Gradient Methods in Decentralized Optimization\vspace{-0.15in}}
% \title{On the Convergence of Decentralized Adaptive Gradient Methods}

\allowdisplaybreaks[2]



\begin{document}

\maketitle


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



We would like to thank the five reviewers for their feedback. Upon acceptance, we will include in the final version \emph{{\sf (a)} improved comparison with prior work}, and \emph{{\sf (c)} missing references}. 
We first discuss a few common concerns shared by \textbf{Reviewer 1},  \textbf{Reviewer 5}, and  \textbf{Reviewer 6}.

 \textsc{Numerical Experiments:} 


\textbf{Reviewer 1:}
We thank the reviewer for the comments/remarks on our paper. \vspace{-4pt}

\textsc{Comparison with [Chen et. al, 2020]:}

 \vspace{-2pt}
\textsc{Bias of $v$, the estimate of the second order moment:}


\textbf{Reviewer 5:}
We thank the reviewer for valuable comments. We add the following: \vspace{-4pt}




\textsc{Comparison with [Chen et. al, 2019] and [Zhou, Dongruo, et al., 2018]:}



\textbf{Reviewer 6:}
We thank you for the valuable comments on our submission. We are revising our paper and will update as soon as it is done. Following is our answer to your questions. \vspace{-4pt}


\textsc{Explanations on the assumptions:}
As rightly mentioned by the reviewer, the stepsize is in order $\alpha_t = 1/\sqrt{T}$. The dependence in $d$ leads to a small learning rate in the presence of large networks but our theorem states that the rate would then be as fast as we present it. 
Hence, the bound in our Theorem prevails over the intuition that the convergence will be slow due to a small learning rate.

\textcolor{red}{discussion on matrix $W$ when number of nodes is large}






\textbf{Reviewer 8:} 
We thank the reviewer for his/her interest in our paper. Below we address your concerns about our contribution. \vspace{-4pt}


\textsc{Discussion on the matrix $W$:}





\end{document}
