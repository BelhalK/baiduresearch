
\documentclass{article} % For LaTeX2e




\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{xcolor} 
\usepackage{pdfpages}
\usepackage{natbib}

\usepackage{enumerate}
\usepackage{enumitem}
%\usepackage[nonatbib]{neurips_2020}
\usepackage{icml2021_author_response}


  





\title{Convergent Adaptive Gradient Methods in Decentralized Optimization\vspace{-0.15in}}
% \title{On the Convergence of Decentralized Adaptive Gradient Methods}

\allowdisplaybreaks[2]



\begin{document}

\maketitle


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



We would like to thank the five reviewers for their feedback. Upon acceptance, we will include in the final version \emph{{\sf (a)} improved comparison with prior work}, and \emph{{\sf (c)} missing references}. 
We first discuss a few common concerns shared by \textbf{Reviewer 1},  \textbf{Reviewer 5}, and  \textbf{Reviewer 6}.

 \textsc{-- Numerical Experiments:} 
Our experiments in the main paper aim at showing the advantages over DADAM, a decentralized variant of Adam method, developed in [Nazari et. al., 2019].
We recall that the purpose of this paper is to provide both an \emph{algorithmic} and {theoretical} framework for decentralized variant of adaptive gradient methods (focussing on AMSGrad for illustration purposes). 
Hence, single-server Adam method does not constitute a baseline for our method, rather its decentralized version does, as plotted in our numerical section.
We also highlight the advantages over SGD comparing Figure 2, 3 and Figure 4 in section F of the supplementary material where we note that the proposed algorithm is less sensitive to the learning rate, which is one advantage of adaptive methods.
While we acknowledge that the numerical experiments can be improved by adding runs on larger datasets (which we plan on doing for the revised paper), we stress on the fact that the current experiments support our theory. 
The current experiments we are displaying in our paper are informative on how our newly proposed decentralized framework behaves with respect to baseline methods. In Figure 1 (b), we show a very bad convergence behavior of DADAM on heterogeneous data, in echo of the theoretical divergence that we claim in the paper. 
Nevertheless, our decentralized framework, using AMSGrad as a prototype, and D-PSGD of [Lian et. al., 2017] are exhibiting great convergence. 
Our framework is similar and sometimes better than D-PSGD. While D-PSGD is a non-adaptive decentralized method, Figure 1 is convincing on the need for a convergent decentralized adaptive method, thus fixing the divergence issue of DADAM (shown both theoretically and empirically through Figure 1).

\textbf{Reviewer 1:}
We thank the reviewer for the comments/remarks on our paper. \vspace{-4pt}

\textsc{-- Comparison with [Chen et. al, 2020]:}

 \vspace{-2pt}
\textsc{-- Bias of $v$, the estimate of the second order moment:}


\textbf{Reviewer 5:}
We thank the reviewer for valuable comments. We add the following: \vspace{-4pt}




\textsc{-- Comparison with [Chen et. al, 2019] and [Zhou, Dongruo, et al., 2018]:}



\textbf{Reviewer 6:}
We thank you for the valuable comments on our submission. We are revising our paper and will update as soon as it is done. Following is our answer to your questions. \vspace{-4pt}


\textsc{-- Explanations on the assumptions:}
As rightly mentioned by the reviewer, the stepsize is in order $\alpha_t = 1/\sqrt{T}$. The dependence in $d$ leads to a small learning rate in the presence of large networks but our theorem states that the rate would then be as fast as we present it. 
Hence, the bound in our Theorem prevails over the intuition that the convergence will be slow due to a small learning rate.

\textcolor{red}{discussion on matrix $W$ when number of nodes is large}






\textbf{Reviewer 8:} 
We thank the reviewer for his/her interest in our paper. Below we address your concerns about our contribution. \vspace{-4pt}


\textsc{-- Discussion on the matrix $W$:}





\end{document}
