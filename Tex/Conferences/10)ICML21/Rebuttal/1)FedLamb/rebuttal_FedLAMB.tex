\documentclass{article}
\usepackage{icml2021_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % define colors in text
\usepackage{xspace}         % fix spacing around commands


\begin{document}


We sincerely thank the five reviewers for their valuable feedback. 
We address the following concern common to \textbf{\textcolor{green!50!black}{R4}}, \textbf{\textcolor{purple}{R5}} and \textbf{\textcolor{magenta}{R6}} about the nature of our contribution:\vspace{-1pt}

\textbf{-- Purpose of FED-LAMB:} 
In the context of Federated Learning, in particular in the \emph{cross-device} settings with a large volume of devices, training deep neural networks is a burning challenge.
Given the potentially cumbersome amount of data present in each device, being able to learn high dimensional and nonconvex models per device is of utmost importance.
-- The nature of our contribution is thus \emph{to improve the local optimization method for each device} so that a better local model is learned in fewer iterations, in order to obtain \emph{a natural improvement of the communication efficiency} of the overall federated method, hence requiring less rounds of communication to reach similar accuracy.


 \vspace{1pt}
\textbf{\textcolor{blue}{R1:}} We thank the reviewer for the valuable comments. A proofreading is being done and we clarify:\vspace{-1pt}

\textbf{-- Notations and Precisions:} 
$\bar{L}$ denotes the sum of the smoothness constants and is stated in the supplementary material. 
The function $\phi(\cdot)$ is set to the identity function in our runs. 
The typo in Corollary 1 has been fixed.
The usage of $d$ is replaced by $i$ in the algorithm.
$\lambda$ is a weight decaying parameter similar to the original LAMB method. It is tuned on a grid-search for our experiments.

\vspace{-0.5pt}
\textbf{-- Bounds on Theorem 1:} The bounds do depend on the number of devices noted $n$.
We precise that our theoretical results assumes full participation. Thus, the total number of devices is $n$, as defined in (1), appearing in our bound.
Similar dependence is observed in related works.
 
 \vspace{-0.5pt}
\textbf{-- Bounds on Corollary 1:} 
The bound does depend on $\mathsf{L}$ which is the total number of layers. 
It also depends on the total smoothness, included in the $\mathcal{O}$ notation.
The dependence on the number of devices $n$ is in the denominator of the RHS, which is in accordance with the bound of local AMS in [Chen et. al. 2020].

 \vspace{1pt}
\textbf{\textcolor{red}{R3:}} We thank the reviewer for valuable comments.  Our point-to-point response is as follows: \vspace{-1pt}

\textbf{-- Partial selection of devices:} The partial selection of devices has indeed a practical virtue which we respected in the numerical experiments.
Though, as far as convergence bounds, it is common in the literature to consider the total number of workers participating in each round.
Either for simplicity or to avoid cumbersome notations, deriving the result for the general case is not an obstacle for the understanding of the convergence behaviour.

\vspace{-0.5pt}
\textbf{-- Theorem 1 for multiple local updates:} 
We presented $T =1$ for simplifying the discussion and presentation.
For \emph{multiple local updates}, please refer to Theorem 3, section A.4, in the supplementary, and related discussion on the communication efficiency of Fed-LAMB. 
 
 \vspace{1pt}
\textbf{\textcolor{green!50!black}{R4:}} We thank the reviewer for valuable comments. Below we address your concerns about our contribution: \vspace{-1pt}

\textbf{-- Various remarks:}
We agree that strictly speaking, the LAMB technique we introduce in our federated method is not a modification of Adam as in the original paper.
Yet, Line 56, we explicitly state that we develop a variant of local AMSGrad using the layerwise adaptivity technique.
We would argue that Local AMSGrad is used as a backbone and that periodic averaging is used as the most efficient way to compute a global model from several local ones.

\vspace{-0.5pt}
\textbf{-- Comparison with "Adaptive Federated Optimization":} 
We thank the reviewer for the reference.
The difference is, that paper does SGD updates in each local worker with ADAM optimizer \emph{applied to the global model}, while our method runs layerwise-adaptive AMSGrad at local workers with standard FedAvg for global aggregation.
It would be interesting to compare these two different schemes. 
Thank you for the suggestion.

 \vspace{1pt}
\textbf{\textcolor{purple}{R5:}} We thank the reviewer for valuable comments. Our response is as follows: \vspace{-1pt}

\textbf{-- Notations:} We have added some clarification on several notations in our revised paper. 
$p_{r,i}^{t} = \{p_{r,i}^{\ell,t}\}_{\ell=1}^{\mathsf{L}}$ is the ratio computed at round $r$, local iteration $t$ and for device $i$.

\vspace{-0.5pt}
\textbf{-- Comparison with FedBN:} We thank the reviewer for this reference that we did not consider.
After careful reading of the contribution, we argue that our method is purely on the optimization algorithm aspect of things while FedBN is a new modeling consideration.
Our method can be used with any model, a large variety for numerical runs and certain models satisfying our assumptions for the theoretical part. 
Batch Normalization could even be an option on top of Fed-LAMB.
We agree that it would be interesting to include a comparison with FedBN in our numerical runs to compare how better or worse such layerwise adaptivity is when performed on the algorithmic level.

 \vspace{1pt}
\textbf{\textcolor{magenta}{R6:}} We thank the reviewer for valuable comments. We first would like to clarify that we did present results on non-iid MNIST in Figure 1.
\vspace{-1pt}

\textbf{-- Assumption H5:} Specializing the upperbound of the estimation of the second order moment is doable and would lead to similar result. For the sake of simplicity, we assumed a constant upperbound.

\vspace{-0.5pt}
\textbf{-- Convergence of Fed-LAMB:} It is well known that federated learning itself saves communication by periodic aggregation. Our method improves the communication in the sense that for fewer rounds of communications, it reaches a similar accuracy as other methods.
Fair comparison are thus given when the number of local updates are equal for each method, where our method outperforms the competing ones. For multiple local updates, i.e. $T > 1$, we present the results in the supplementary materials, see Theorem 3, section A.4, and following discussion on the communication efficiency.

%\vspace{-0.5pt}
%\textbf{Comparison with baselines like SGD and ADAM:}  We are doing Federated Learning.

\end{document}
