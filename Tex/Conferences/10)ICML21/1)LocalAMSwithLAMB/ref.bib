@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}


@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}

@inproceedings{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1709--1720},
  year={2017}
}

@inproceedings{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1299--1309},
  year={2018}
}

@inproceedings{chen2020toward,
  title={Toward Communication Efficient Adaptive Gradient Method},
  author={Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
  booktitle={ACM-IMS Foundations of Data Science Conference (FODS)},
  address  = {Seattle, WA},
  year={2020}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}


@ARTICLE{RKK18,
  title={On the Convergence of Adam and Beyond },
  author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  journal={ICLR},
  year={2018}
}

@ARTICLE{KB15,
  title = {Adam: A Method for Stochastic Optimization},
  author    = {Diederik P. Kingma and Jimmy Ba},
  journal = {ICLR},
  year = {2015}
}

@ARTICLE{TH12,
  title = {RmsProp: Divide the gradient by a running average of its recent magnitude},
  author    = {T. Tieleman and G. Hinton},
  journal = {COURSERA: Neural Networks for Machine Learning},
  year = {2012}
}


@ARTICLE{Z12,
  title = {ADADELTA: An Adaptive Learning Rate Method},
  author    = {Matthew D. Zeiler},
  journal = {arXiv:1212.5701},
  year = {2012}
}

@ARTICLE{D16,
  title = {Incorporating Nesterov Momentum into Adam},
  author    = {Timothy Dozat},
  journal = {ICLR (Workshop Track)},
  year = {2016}
}


@ARTICLE{DHS11,
title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
author = {John Duchi and Elad Hazan and Yoram Singer},
journal = { Journal of Machine Learning Research (JMLR)},
year = {2011},
}


@ARTICLE{MS10,
  title = {Adaptive bound optimization for online convex optimization},
  author    = {H. Brendan McMahan and Matthew J. Streeter},
  journal = {COLT},
  year = {2010}
}

@ARTICLE{N04,
    title = {Introductory Lectures on Convex Optimization:
A Basic Course},
    Author = {Yurii Nesterov},
    journal = {Springer},
    year = {2004},
}


@ARTICLE{P64,
  title = {Some methods of speeding up the convergence of iteration methods},
  author    = {B. T. Polyak},
  journal = {Mathematics and Mathematical Physics},
  year = {1964}
}



@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}

@article{zhao2020distributed,
  title={Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems},
  author={Zhao, Weijie and Xie, Deping and Jia, Ronglai and Qian, Yulei and Ding, Ruiquan and Sun, Mingming and Li, Ping},
  journal={arXiv preprint arXiv:2003.05622},
  year={2020}
}

@article{recht2011hogwild,
  title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={693--701},
  year={2011}
}



@article{zhou2017convergence,
  title={On the convergence properties of a $ K $-step averaging stochastic gradient descent algorithm for nonconvex optimization},
  author={Zhou, Fan and Cong, Guojing},
  journal={arXiv preprint arXiv:1708.01012},
  year={2017}
}

@article{stich2018local,
  title={Local SGD converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1805.09767},
  year={2018}
}


@article{yu2019linear,
  title={On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  journal={arXiv preprint arXiv:1905.03817},
  year={2019}
}

@inproceedings{Proc:He-resnet16,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {770--778},
  publisher = {{IEEE} Computer Society},
  year      = {2016}
}

@article{ghadimi2013stochastic,
	Author = {Ghadimi, Saeed and Lan, Guanghui},
	Date-Added = {2018-10-27 15:50:56 -0400},
	Date-Modified = {2018-10-27 15:50:56 -0400},
	Journal = {SIAM Journal on Optimization},
	Number = {4},
	Pages = {2341--2368},
	Publisher = {SIAM},
	Title = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
	Volume = {23},
	Year = {2013}}
	
@article{karimireddy2019scaffold,
  title={SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:1910.06378},
  year={2019}
}

@article{reddi2020adaptive,
  title={Adaptive federated optimization},
  author={Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
  journal={arXiv preprint arXiv:2003.00295},
  year={2020}
}