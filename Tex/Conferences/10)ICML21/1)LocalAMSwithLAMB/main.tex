%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

\input{shortcuts}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Layerwise and Dimensionwise Adaptive Local Adaptive Gradient Method for Federated Learning}

\begin{document}

\twocolumn[
\icmltitle{Layerwise and Dimensionwise Adaptive Local Adaptive Gradient Method for Federated Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Belhal Karimi}{comp}
\icmlauthor{Xiaoyun Li}{comp}
\icmlauthor{Ping Li}{comp}
\end{icmlauthorlist}

\icmlaffiliation{com}{Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, WA 98004, USA}

\icmlcorrespondingauthor{Belhal Karimi}{belhal.karimi@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Adaptive, DNN, Federated, Distributed}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In the emerging paradigm of Federated Learning, large amount of clients, such as mobile devices, are used to train possibly high-dimensional models on their respective data.
Under the orchestration of a central server, the data needs to remain decentralized, as it can not be shared among clients or with the central server.
Then, due to the low bandwidth of mobile devices, decentralized optimization methods need to shift the computation burden from those clients to the computation server while preserving \emph{privacy} and reasonable \emph{communication cost}.
In the particular case of training Deep, as in multilayered, Neural Networks, under such settings, we propose in this paper, \algo, a novel Federated Learning method based on a Layerwise and Dimensionwise updates of the local models. 
A periodic averaging is added to obtain estimates of the desired global model parameters.
We provide a thorough finite time convergence analysis for our algorithm, substantiated by numerical runs on benchmark datasets.
\end{abstract}

\section{Introduction}\label{sec:introduction}

A growing and important task while learning models on observed data, is the ability to train the latter over a large number of clients which could either be devices or distinct entities.
In the paradigm of Federated Learning (FL)~\citep{konevcny2016federated,mcmahan2017communication}, the focus of our paper, a central server orchestrates the optimization over those clients under the constraint that the data can neither be centralized nor shared among the clients.
Most modern machine learning tasks can be casted as a large finite-sum optimization problem written as:
\begin{equation}\label{eq:opt}
\min \limits_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n f_i(\theta)
\end{equation}
where $n$ denotes the number of workers, $f_i$ represents the average loss for worker $i$ and $\theta$ the global model parameter taking value in $\Theta$ a subset of $\mathbb{R}^d$.
While this formulation recalls that of distributed optimization, the core principle of FL is different that standard distributed paradigm.

FL currently suffers from two bottlenecks: communication efficiency and privacy.
We focus on the former in this paper.
While local updates, updates during which each client learn their local models, can reduce drastically the number of communication rounds between the central server and devices, new techniques must be employed to tackle this challenge.
Some quantization~\citep{alistarh2017qsgd, wangni2018gradient} or compression~\citep{lin2017deep} methods allow to decrease the number of bits communicated at each round and are efficient method in a distributed setting.
The other approach one can take is to accelerate the local training on each device and thus sending a better local model to the server at each round.

Under the important setting of heterogenous data, i.e. the data among each device can be distributed according to different distributions, current local optimization algorithms are perfectible.
The most popular method for FL is using multiple local Stochastic Gradient Descent (\textsc{SGD}) steps in each device, sending those local models to the server that computes the average over those received local vector of parameters and broadcasts it back to the devices. This is called \textsc{FedAvg} and has been introduced in~\citep{mcmahan2017communication}.

In \citep{chen2020toward}, the authors motivate the usage of adaptive gradient optimization methods as a better alternative to the standard \textsc{SGD} inner loop in \textsc{FedAvg}.
They propose an adaptive gradient method, namely \textsc{Local AMSGrad}, with communication cost sublinear in $T$ that is guaranteed to converge to stationary points in $\mathcal{O}(\sqrt{d/Tn})$, where T is the number of iterations.

Based on recent progress in adaptive methods for accelerating the training procedure, see~\citep{you2019large}, we propose a variant of \textsc{Local AMSGrad} integrating dimensionwise and layerwise adaptive learning rate in each device's local update.
Our contributions are as follows:
\begin{itemize}
\item We develop a novel optimization algorithm for federated learning, namely \textsc{Fed-LAMB}, following a principled layerwise adaptation strategy to accelerate training of deep neural networks.
\item We provide a rigorous theoretical understanding of the non asymptotic convergence rate of \textsc{Fed-LAMB}. Based on the recent progress on nonconvex stochastic optimization, we derive for a any finite number of rounds performed by our method, a characterization of the rate at which the classical suboptimilality condition, \ie, the second order moment of the gradient of the objective function, decreases. Our bound  in $\mathcal{O}(\sqrt{\frac{p \tot}{nR}})$ matches state of the art methods in Federated Learning reaching a sublinear convergence in $R$, the total number of rounds.
\item We exhibit the advantages of our method on several benchmarks supervised learning methods on both homogeneous and heterogeneous settings.
\end{itemize}

The plan of our paper is as follows.
After having established a literature review of both realms of federated and adaptive learning in subsection~\ref{sec:related}, we develop in Section~\ref{sec:main}, our method, namely \algo, based on the computation per layer and per dimension, of a scaling factor in the traditional stepsize of AMSGrad.
Theoretical understanding of our method's behaviour with respect to convergence towards a stationary point is developed in Section~\ref{sec:theory}.
We present numerical illustrations showing the advantages of our method in Section~\ref{sec:numerical}.

\subsection{Related Work}\label{sec:related}

\paragraph{Adaptive gradient methods.}
In classical stochastic nonconvex optimization, adaptive methods have proven to be the spearhead of any practicioner.
Those gradient based optimization algorithms alleviate the possibly high nonconvexity of the objective function by adaptively updating each coordinate of their learning rate using past gradients. Most used examples \textsc{AMSGrad} \citep{RKK18}, \textsc{Adam} \citep{KB15}, \textsc{RMSPROP} \citep{TH12}, \textsc{AdADELTA} \citep{Z12}, and \textsc{NADAM} \citep{D16}.

Their popularity and efficiency are due to their great performance at training deep neural networks.
They generally combine the idea of adaptivity from \textsc{AdaGrad} \citep{DHS11,MS10}, as explained above, and the idea of momentum from \textsc{Nesterov's Method} \citep{N04} or \textsc{Heavy ball} method \citep{P64} using past gradients.
\textsc{AdaGrad} displays a great edge when the gradient is sparse compared to other classical methods.
Its update has a notable feature: it leverages an anisotropic learning rate depending on the magnitude of the gradient for each dimension which helps in exploiting the geometry of the data. 

The anisotropic nature of this update represented a real breakthrough in the training of high dimensional and nonconvex loss functions.
This adaptive learning rate helps accelerating the convergence when the gradient vector is sparse \citep{DHS11}, yet, when applying \textsc{AdaGrad} to train deep neural networks, it is observed that the learning rate might decay too fast, see \citep{KB15} for more details.
Consequently, \cite{KB15} develops \textsc{Adam} leveraging a moving average of the gradients divided by the square root of the second moment of this moving average (element-wise multiplication).
A variant, called \textsc{AMSGrad} described in \citep{RKK18} ought to fix \textsc{Adam} failures and is presented in Algorithm~\ref{alg:amsgrad}.
\begin{algorithm}[H]
\caption{\textsc{AMSGrad} \citep{RKK18}} \label{alg:amsgrad}
\begin{algorithmic}[1]
\small
\STATE \textbf{Required}: parameter $\beta_1$, $\beta_2$, and $\eta_t$. 
\STATE Init: $w_{1} \in \Theta \subseteq \mathbb R^d $ and $v_{0} = \epsilon 1 \in \mathbb R^{d}$.
\FOR{$t=1$ to $T$}
\STATE Get mini-batch stochastic gradient $g_t$ at $w_t$.
\STATE $\theta_t = \beta_1 \theta_{t-1} + (1 - \beta_1) g_t$.
\STATE $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$. 
\STATE \label{line:maxop}$\hat{v}_t = \max( \hat{v}_{t-1} , v_t )$. 
\STATE $w_{t+1} = w_t - \eta_t \frac{\theta_t}{ \sqrt{\hat{v}}_t }$.
\text{ (element-wise division)}
\ENDFOR
\end{algorithmic}
\end{algorithm}\vspace{-0.1in}

The difference between \textsc{Adam} and \textsc{AMSGrad} lies in Line~\ref{line:maxop} of Algorithm~\ref{alg:amsgrad}.

A natural extension of Algorithm~\ref{alg:amsgrad} has been developed in \citep{you2019large} specifically for multi layered neural network. 
A principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches is proposed using either a standard stochastic gradient update or a generalized adaptive method under the setting of a classical single server empirical risk minimization problem.

\medskip
\paragraph{Federated learning.}
An extension of the well known parameter server framework, where a model is being trained on several servers in a distributed manner, is called Federated Learning, see \citep{konevcny2016federated}.
Here, the central server only plays the role of compute power for aggregation and global update of the model.
Compared with the distributed learning paradigm, in Federated Learning, the data stored in each worker must not be seen by the central server -- preserving privacy is key -- and the nature of those workers, which can be mobile devices, combined with their usually large amount make communication between the devices and the central server less appealing -- communication cost needs to be controlled.

Thus, while traditional distributed gradient methods~\citep{recht2011hogwild,li2014scaling,zhao2020distributed} do not respect those constraints, it has been proposed in \citep{mcmahan2017communication}, an algorithm called Federated Averaging -- \textsc{Fed-Avg} -- extending parallel SGD with local updates performed on each device. 
In \textsc{Fed-Avg}, each worker updates their own model parameters locally using SGD, and the local models are synchronized by periodic averaging on the central parameter server.


\section{Layerwise and Dimensionwise Adaptive Methods}\label{sec:main}
Beforehand, it is important to provide useful and important notations used throughout our paper.

\paragraph{Notations:} We denote by $\theta$ the vector of parameters taking values in $\rset^d$. 
For each layer $\ell \in \llbracket \tot \rrbracket$, where $\tot$ is the total number of layers of the neural networks, and each coordinate $j \in \llbracket p_\ell \rrbracket$ where $p_\ell$ is the dimension per layer $\ell$, we note $\theta^{\ell, j}$ its $j$th coordinate.
The gradient of $f$ with respect to $\theta^\ell$ is denoted by $\nabla_{\ell} f(\theta)$.
The index $i \in \inter$ denotes the index of the worker $i$ in our federated framework.
$r$ and $t$ are used as the round and local iteration numbers respectively.
The smoothness per layer is denoted by $L_\ell$ for each layer $\ell \in \llbracket \tot \rrbracket$.
We note for each communication $r>0$, the set of randomly drawn devices $D^{r}$ performing local updates.


\subsection{AMSGrad, Local AMSGrad and Periodic Averaging}
Under our Federated setting, we stress on the important of reducing the communication cost at each round between the central server, used mainly for aggregation purposes, and the many clients used for gradient computation and local updates.
Using Periodic Averaging after few local epochs, updating local models on each device, as developed in \cite{mcmahan2017communication} is the gold standard for achieving such communication cost reduction.
Intuitively, one rather shift the computation burden from the many clients to the central server as much as possible. This allows for fewer local epochs and a better global model, from a loss minimization (or model fitting) perspective.

The premises of that new paradigm are SGD updates performed locally on each device then averaged periodically, see \cite{konevcny2016federated, zhou2017convergence}.
The heuristic efficiency of local updates using SGD and periodic averaging has been studied in \cite{stich2018local,yu2019linear} and shown to reach a similar sublinear convergence rate as in the standard distributed optimization settings.

Then, with the growing need of training far more complex models, such as deep neural networks, several efficient methods, built upon adaptive gradient algorithms, such as Local AMSGrad in \cite{chen2020toward}, extended both empirically and theoretically, the benefits of performing local updates coupled with periodic averaging.



\subsection{Layerwise and Dimensionwise Learning with Periodic Averaging}
Recall that our original problem is the following optimization task:
\begin{equation}\notag
\min \limits_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n f_i(\theta)
\end{equation}
where $f_i(\theta)$ is the loss function associated to the client $i \in \inter$ and is parameterized, in our paper, by a deep neural network.
The multilayer and nonconvex nature of the loss function implies having recourse to particular optimization methods in order to efficiently train our model.
Besides, the distributed and clients low bandwidth constraints are strong motivations for improving existing methods performing \eqref{eq:opt}.  


Based on the periodic averaging and local AMSGrad algorithms, presented prior, we propose a layerwise and dimensionwise local AMS algorithm described in the following:

\begin{algorithm}[H]
\caption{\algo\ for Federated Learning} \label{alg:ldams}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: parameter $\beta_1$, $\beta_2$, and learning rate $\alpha_t$. 
\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\hat v_0=v_{0} = \epsilon \mathsf{1} \in \mathbb R^{d}$ and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
\FOR{$r=1$ to $R$}
\STATE Set $\theta_{r,i}^{0} = \bar{\theta}_{r-1}$

\FOR{parallel for device $d \in D^{r}$}
%\STATE\textbf{parallel for device $d \in D^{r}$ do}:
\STATE Compute stochastic gradient $g_{r,i}$ at $\theta_r$.
\FOR{$t=1$ to $T$}
\STATE $m^t_{r,i} = \beta_1 m^{t-1}_{r-1,i} + (1 - \beta_1) g_{r,i}$. \label{line:first}
\STATE $m^{t}_{r,i}=m^{t}_{r,i} /\left(1-\beta_{1}^{r}\right)$. \label{line:new1}
\STATE $v^{t,i}_r = \beta_2 v^{t}_{r-1,i} + (1 - \beta_2) g_{r,i}^2$. \label{line:second}
\STATE $v^{t}_{r,i}=v^{t}_{r,i} /\left(1-\beta_{2}^{r}\right)$. \label{line:new2}
\STATE Compute ratio  $p_{r,i}=\frac{m^{t}_{r,i}}{\sqrt{\hat v_{r}}+\epsilon}$. \label{line:scale}
\STATE Update local model for each layer $\ell$: \label{line:layer}
$$\theta_{r,i}^{\ell,t}=\theta_{r,i}^{\ell,t-1}-\alpha_{r} \phi(\|\theta_{r,i}^{\ell,t-1}\|)\frac{p_{r,i}^{\ell}+\lambda \theta_{r,i}^{\ell,t-1}}{ \|p_{r,i}^{\ell}+\lambda \theta_{r,i}^{\ell,t-1}\|}$$
\ENDFOR
\STATE Devices send $\theta_{r,i}^{T} = [\theta_{r,i}^{\ell,T}]_{\ell =1}^{\tot}$ and $v_{r,i}^T$ to server.
\ENDFOR
\STATE Server computes the averages of the local models $\bar{\theta}_r^\ell = \frac{1}{n} \sum_{i=1}^n \theta_{r,i}^{\ell,T}$ and $\hat{v}_{r+1} = \max( \hat{v}_{r},\frac{1}{n} \sum_{i=1}^n v^T_{r,i} )$ and send them back to the devices. \label{line:final}
\ENDFOR
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:ldams} is a natural adaptation of the vanilla AMSGrad method, for \emph{multilayer} neural networks under the \emph{distributed} settings.
In particular, while Line~\ref{line:first} and Line~\ref{line:second} corresponds to the standard approximation of the first and second moments, via the smooth updates allowed by the tuning parameters $\beta_1$ and $\beta_2$ respectively and that both Lines~\ref{line:new1}-\ref{line:new2} are correct the biases of those estimates, the final local update in Line~\ref{line:layer} is novel and corresponds to the specialization per layer of our federated method.
Note that a scaling factor is applied to the learning rate $\alpha_r$ at each round $r>0$ via the quantity $\phi(\|\theta_{r,i}^{\ell,t-1}\|)$ depending on the dimensionwise and layerwise quantity computed in Line~\ref{line:scale}.
This function is user designed and can be set to the identity function.

The adaptivity of our federated method is thus manifold. 
There occurs a per dimension normalization with respect to the square root of the second moment used in adaptive gradient methods and a layerwise normalization obtained via the final local update (Line~\ref{line:layer}).




\section{On The Convergence of \algo}\label{sec:theory}
We develop in this section, the theoretical analysis of Algorithm\ref{alg:ldams}.
based on classical result for stochastic nonconvex optimization, we 


\subsection{Finite Time Analysis of \algo}
In the context of nonconvex stochastic optimization for distributed devices, assume the following:

\begin{assumption}\label{ass:smooth}
For $i \in \inter$ and $\ell \in \interl$, $f_i$ is  L-smooth: $\norm{\nabla f_i (\theta) - \nabla f_i (\vartheta)} \leq L_\ell \norm{\theta^\ell-\vartheta^\ell}$.
\end{assumption}
We add some classical assumption in the unbiased stochastic optimization realm, on the gradient of the objective function:
\begin{assumption}\label{ass:boundgrad}
The stochastic gradient is unbiased for any iteration $r>0$: $\EE[g_r] = \nabla f(\theta_r)$ and is bounded from above, i.e., $\norm{g_t} \leq M$.
\end{assumption}

\begin{assumption}\label{ass:var}
The variance of the stochastic gradient is bounded for any iteration $r>0$ and any dimension $j \in \llbracket d \rrbracket$: $\EE[|g_r^j - \nabla f(\theta_r)^j|^2] < \sigma^2$.
\end{assumption}

\begin{assumption}\label{ass:phi}
For any value $a \in \rset^*_+$, there exists strictly positive constants such that $\phi_m \leq  \phi(a) \leq \phi_M$.
\end{assumption}


We now state our main result regarding the non asymptotic convergence analysis of our Algorithm~\ref{alg:ldams}:
\begin{Theorem}\label{th:main}
Consider $\{\overline{\theta_r}\}_{r>0}$, the sequence of parameters obtained running Algorithm~\ref{alg:ldams}. Then, if the number of local epochs is set to $T=1$ and $\epsilon = \lambda = 0$, we have:
\beq
\begin{split}
  & \frac{1}{R}\sum_{r=1}^R  \EE\left[ \left\| \frac{\nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}}   \right \|^2 \right] \\
   \leq & \frac{ f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})]}{R}+      \frac{\phi_M   \sigma^2}{n} \sqrt{\frac{1 - \beta_2}{M^2 p}  } \\
&   +\alpha \phi_M \sigma \tot p \sqrt{n}+ \frac{ \overline{L}\beta_1^2\tot(1-\beta_2)M^2 \phi^2_M n}{2(1-\beta_1)^2 v_0}    \\
& + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} +\overline{L} \alpha^2 M^2 \phi_M^2 \frac{(1-\beta_2)p}{Rv_0} 
   \end{split}
\eeq
\end{Theorem}


\subsection{Comparison with LAMB and Local-AMS}

\textbf{Bound in \citep{you2019large} for LAMB: }



\textbf{Bound in \citep{chen2020toward} for Local-AMS: }

\clearpage

\section{Numerical experiments}\label{sec:numerical}

In this section, we conduct numerical experiments on various datasets and network architectures to testify the effectiveness of our proposed method in practice. 

% \vspace{0.1in}
\noindent\textbf{Settings.} In our experiment, we will evaluate three federated learning algorithms: 1) Fed-SGD, 2) Fed-AMS and 3) our proposed Fed-LAMB (Algorithm~\ref{alg:ldams}), where the first two serve as the baseline methods. For adaptive methods 2) and 3), we set $\beta_1=0.9$, $\beta_2=0.999$ as default and recommended~\citep{RKK18}. Regarding federated learning environment, we use 50 local workers with 0.5 participation rate. That means, we randomly pick half of the workers to be active for training in each round. To best accord with real scenarios where the local training batch size is usually limited, we set a relatively small local update batch size as 32. In each round, the training samples are allocated to the active devices, and one local epoch is finished after all the local devices run one epoch over their received samples by batch training. We test different number of local epochs in our experiments. For each dataset and number of local epochs, we tune the constant learning rate $\alpha$ for each algorithm in logarithm scale. For LocalLAMB, the parameter $\lambda$ in Algorithm~\ref{alg:ldams} controlling the overall scale of the layerwise gradients is tuned from $\{0,0.01,0.1\}$. For each run, we take the model performance with the best $\alpha$ and $\lambda$. The reported results are averaged over three independent runs each with same initialization.

\noindent\textbf{Models.} We test the performance of different federated learning algorithms on MNIST and CIFAR10 image classification datasets. For MNIST, we apply 1) a simple multilayer perceptron (MLP), which has one hidden layer containing 200 cells with dropout; 2) Convolutional Neural Network (CNN), which has two max-pooled convolutional layers followed by a dropout layer and two fully-connected layers with 320 and 50 cells respectively. For CIFAR10, we implement: 1) a CNN with three convolutional layers followed by two fully-connected layers, and 2) a ResNet-9 model proposed by~\cite{Proc:He-resnet16}. All the networks use ReLU as the activation function.



\begin{figure}[H]
    \begin{center}
        \mbox{
        \includegraphics[width=0.25\textwidth]{figure/mnist_testerror_mlp_ep1_client50_iid0.eps}
        \includegraphics[width=0.25\textwidth]{figure/mnist_testerror_mlp_ep5_client50_iid0.eps}
        }
    \end{center}
	\caption{Test accuracy on CNN + MNIST. Non-iid data distribution.}
	\label{fig:mnist-mlp-noniid}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \mbox{
        \includegraphics[width=0.25\textwidth]{figure/mnist_testerror_cnn_ep1_client60_iid0.eps}
        \includegraphics[width=0.25\textwidth]{figure/mnist_testerror_cnn_ep5_client50_iid0.eps}
        }
    \end{center}
	\caption{Test accuracy on CNN + MNIST. Non-iid data distribution.}
	\label{fig:mnist-cnn-noniid}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \mbox{
        \includegraphics[width=0.25\textwidth]{figure/cifar_testerror_cnn_ep1_client50_iid1.eps}
        \includegraphics[width=0.25\textwidth]{figure/cifar_testerror_cnn_ep3_client50_iid1.eps}
        }
    \end{center}
	\caption{Test accuracy on CNN + CIFAR10. iid data distribution.}
	\label{fig:cifar-cnn-iid}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \mbox{
        \includegraphics[width=0.25\textwidth]{figure/cifar_testerror_resnet_ep1_client10_iid1.eps}
        \includegraphics[width=0.25\textwidth]{figure/cifar_testerror_resnet_ep3_client10_iid1.eps}
        }
        \mbox{
        \includegraphics[width=0.25\textwidth]{figure/cifar_testerror_resnet_ep1_client50_iid1.eps}
        \includegraphics[width=0.25\textwidth]{figure/cifar_testerror_resnet_ep3_client50_iid1.eps}
        }
    \end{center}
	\caption{Test accuracy on ResNet + CIFAR10. iid data distribution.}
	\label{fig:cifar-resnet-iid}
\end{figure}




\section{Conclusion}\label{sec:conclusion}

\clearpage

\bibliographystyle{icml2021}
\bibliography{ref}


\clearpage

\onecolumn
\appendix 

\section{Appendix}\label{sec:appendix}


\section{Theoretical Analysis}

\subsection{Intermediary Lemmas}

\begin{Lemma}\label{lemma:iterates}
Consider $\{\overline{\theta_r}\}_{r>0}$, the sequence of parameters obtained running Algorithm~\ref{alg:ldams}. Then for $i \in \inter$:
\beq
\| \overline{\theta_r} - \theta_{r,i} \| \leq \alpha^2 M^2 \phi_M^2 \frac{(1-\beta_2)p}{v_0}
\eeq
where $\phi_M$ is defined in H\ref{ass:phi} and p is the total number of dimensions $p = \sum_{\ell = 1}^\tot p_\ell$.
\end{Lemma}

\begin{proof}
Assuming the simplest case when $T=1$, i.e. one local iteration, then by construction of Algorithm~\ref{alg:ldams}, we have for all $\ell \in \llbracket \tot \rrbracket$, $i \in \inter$ and $r >0$:
\beq
 \theta^{\ell}_{r,i} =  \overline{\theta_r}^{\ell}  - \alpha \phi(\|\theta_{r,i}^{\ell,t-1}\|)p_{r,i}^{j} / \|p_{r,i}^{\ell}\|=  \overline{\theta_r}^{\ell}  - \alpha \phi(\|\theta_{r,i}^{\ell,t-1}\|)  
 \frac{m^{t}_{r,i}}{\sqrt{v^{t}_{r}}} \frac{1}{\|p_{r,i}^{\ell}\|}
\eeq
leading to 
\beq
\begin{split}
\|\overline{\theta_r}   -  \theta_{r,i}\|^2 & = \pscal{\overline{\theta_r}^{\ell}   -  \theta^{\ell}_{r,i}}{\overline{\theta_r}^{\ell}   -  \theta^{\ell}_{r,i}} \\
& \leq \alpha^2 M^2 \phi_M^2 \frac{(1-\beta_2)p}{v_0}
\end{split}
\eeq
which concludes the proof.
\end{proof}


\subsection{Proof of Theorem~\ref{th:main}}

\begin{Theorem*}
Consider $\{\overline{\theta_r}\}_{r>0}$, the sequence of parameters obtained running Algorithm~\ref{alg:ldams}. Then, if the number of local epochs is set to $T=1$ and $\epsilon = \lambda = 0$, we have:
\beq
\frac{1}{R} \sum_{r=1}^R \EE[\| \nabla f(\overline{\theta_r}) \|^2 \leq dd
\eeq
\end{Theorem*}


\textbf{Case with $T=1$, $\epsilon = 0$ and $\lambda = 0$:}
Using H\ref{ass:smooth}, we have:
\begin{align}\notag
f(\bar{\vartheta}_{r+1}) &  \leq f(\bar{\vartheta}_r) + \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r} + \sum_{\ell =1}^L \frac{L_\ell}{2} \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2\\
&  \leq f(\bar{\vartheta}_r) + \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j (\bar{\vartheta}^{\ell,j}_{r+1} - \bar{\vartheta}^{\ell,j}_r) + \sum_{\ell =1}^L \frac{L_\ell}{2} \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2
\end{align}

Taking expectations on both sides leads to:
\begin{align}\label{eq:main}
- \EE[  \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r}]  \leq  \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{\ell =1}^L \frac{L_\ell}{2} \EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2]
\end{align}

Yet, we observe that, using the classical intermediate quantity, used for proving convergence results of adaptive optimization methods, see for instance \citep{RKK18}, we have:
\beq\label{eq:defseq}
\bar{\vartheta}_r = \bar{\theta}_r +  \frac{\beta_1}{1-\beta_1}(\bar{\theta}_{r} - \bar{\theta}_{r-1})
\eeq
where $\bar{\theta_r}$ denotes the average of the local models at round $r$.
Then for each layer $\ell$,
\begin{align}\label{eq:gap}
\bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r  & = \frac{1}{1-\beta_1}(\bar{\theta}^\ell_{r+1} - \bar{\theta}^\ell_{r}) - \frac{\beta_1}{1-\beta_1}(\bar{\theta}^\ell_{r} - \bar{\theta}^\ell_{r-1})\\
& = \frac{\alpha_{r}}{1-\beta_1} \frac{1}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\|p_{r,i}^{\ell}\|} p_{r,i}^{\ell}  - \frac{\alpha_{r-1}}{1-\beta_1} \frac{1}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\|p_{r-1,i}^{\ell}\|} p_{r-1,i}^{\ell}\\
& = \frac{\alpha \beta_1}{1-\beta_1} \frac{1}{n}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1} + \frac{\alpha}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}
\end{align}
where we have assumed a constant learning rate $\alpha$.


We note for all $\theta \in \Theta$, the majorant $G > 0$ such that $\phi(\|\theta \|) \leq G$. 
Then, following \eqref{eq:main}, we obtain:
\begin{align}\label{eq:main2}
- \EE[  \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r}]  \leq  \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{\ell =1}^L \frac{L_\ell}{2} \EE[  \| \bar{\vartheta}_{r+1} - \bar{\vartheta}_r \|^2]
\end{align}

Developing the LHS of \eqref{eq:main2} using \eqref{eq:gap} leads to

\begin{align}\label{eq:inner}
\pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r} &= \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j (\bar{\vartheta}^{\ell,j}_{r+1} - \bar{\vartheta}^{\ell,j}_r) \\
& =  \frac{\alpha \beta_1}{1-\beta_1}\frac{1}{n}  \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j \left[   \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}  \right] \\
& - \underbrace{ \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j  \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}}_{= A_1}
\end{align}

\textbf{ Term $A_1$:}
Since we have that $\|p_{r,i}^{\ell}\| \leq \sqrt{\frac{p_\ell}{1-\beta_2}}$ and $1/\sqrt{v^{t}_{r}} \leq 1/\sqrt{v_{0}}$, using H\ref{ass:boundgrad}, we develop the term $A_1$ as follows:
\begin{align}
A_1 & \leq - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j  \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\\
& \leq - \frac{\alpha}{n} \sum_{\ell=1}^\tot  \sqrt{\frac{1-\beta_2}{M^2 p_\ell}} \sum_{i = 1}^n \sum_{j=1}^{p_\ell}   \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  g^{\ell, j}_{r,i}\\
& - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell}   \left( \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\right)\mathsf{1}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell}) \right)
\end{align}

Taking the expectations on both sides yields and using H\ref{ass:phi}:

\begin{align}
\EE[A_1]  & \leq - \alpha \sum_{\ell=1}^\tot  \sqrt{\frac{1-\beta_2}{M^2 p_\ell}} \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \EE \left[  \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  g^{\ell, j}_{r,i}\right]\\
& - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell}   \EE\left[ \phi(\|\theta_{r,i}^{\ell}\|)  \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\mathsf{1} \left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell}) \right) \right]\\
& \leq - \frac{\alpha}{n} \sum_{\ell=1}^\tot  \phi_m \sqrt{\frac{1-\beta_2}{M^2 p_\ell}} \sum_{i = 1}^n \sum_{j=1}^{p_\ell}    (\nabla_{\ell} f(\bar{\vartheta}_r)^j)^2\\
& - \frac{\alpha}{n} \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \phi_M  \EE\left[ \left| \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\right| \mathsf{1}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell})\right) \right]\\
\end{align}
where we have used assumption H\ref{ass:phi}.

Since for any $\ell, i , j$, we have
\beq
  \EE\left[ \left| \nabla_{\ell} f(\bar{\vartheta}_r)^j  \frac{p_{r,i}^{\ell}}{ \|p_{r,i}^{\ell}\|}\right| \mathsf{1}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell})\right) \right] \leq   \left| \nabla_{\ell} f(\bar{\vartheta}_r)^j \right| \mathbb{P}\left( \sign(  \nabla_{\ell} f(\bar{\vartheta}_r)^j) \neq  \sign( p_{r,i}^{\ell})\right) 
\eeq

Then, we obtain

\beq\label{eq:finala1}
\EE[A_1]  \leq -\alpha \phi_m \sqrt{\frac{\tot(1-\beta_2)}{M^2 p}}  \EE[ \| \overline{\nabla f}(\bar{\vartheta_r})\|^2]  -  \alpha \phi_M \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \frac{\sigma_{i}^{\ell, j}}{\sqrt{n}}
\eeq

where  for any $\theta \in \Theta$ we define $\overline{\nabla f}( \cdot) = \sum_{i=1}^n \nabla f_i(\cdot)$.

We now need to bound the following terms:
\begin{align}
& A_r^2 \eqdef \EE[  \| \bar{\vartheta}_{r+1} - \bar{\vartheta}_r \|^2]\\
& A_r^3 \eqdef  \frac{\alpha \beta_1}{1-\beta_1}  \frac{1}{n} \sum_{\ell=1}^\tot \sum_{j=1}^{p_\ell} \nabla_{\ell} f(\bar{\vartheta}_r)^j \left[   \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}  \right]
\end{align}

\textbf{ Term $A_r^2$:}
According to definition \eqref{eq:defseq}, for each layer $\ell \in \llbracket \tot \rrbracket$, we have, using the Cauchy-Schwartz inequality, that:
\begin{align}
  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2 & = \left\| \frac{\alpha \beta_1}{1-\beta_1} \frac{1}{n}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1} + \frac{\alpha}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\right \|^2 \\
&  \leq 2\frac{\alpha^2}{n^2} \left\| \frac{ \beta_1}{1-\beta_1}   \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 + \frac{1}{n^2} \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\right\|^2
\end{align}

Taking the expectation on both sides leads to:
\begin{align}\label{eq:maina2}
\begin{split}
\EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2] &  \leq 2\alpha^2 \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right] +  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} g_{r,i}\right\|^2 \right]\\
& \leq 2 \frac{\alpha^2}{n^2} \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right] \\
& +  \frac{1}{n^2} \EE\left[ \left| \sum_{i = 1}^n\sum_{j = 1}^p    \pscal{\Gamma_{r,i}^j (\nabla f_i(\theta_r)^j + g_{r,i}^j - \nabla f_i(\theta_r)^j) }{\Gamma_{r,i}^j (\nabla f_i(\theta_r)^j + g_{r,i}^j - \nabla f_i(\theta_r)^j)}\right| \right]\\
& \leq 2\frac{\alpha^2}{n^2} \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right] \\
& +  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] +  \frac{1}{n^2} \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \left\|\frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\\
\end{split}
\end{align}
where the last line uses assumptions H\ref{ass:boundgrad} and H\ref{ass:var} (unbiased gradient and bounded variance of the stochastic gradient) and $\Gamma \eqdef  \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} $.

On the other hand, using the bound on the gradient H\ref{ass:boundgrad},
\begin{align}\label{eq:first}
\begin{split}
& \sum_{r=1}^R \EE \left[ \left\| \frac{ \beta_1}{1-\beta_1}  \sum_{i = 1}^n  \left( \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{\phi(\|\theta_{r-1,i}^{\ell}\|)}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) m^{t}_{r-1}\right\|^2 \right]  \\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} M^2 \phi^2_M \sum_{r=1}^R  \EE \left[  \left\| \sum_{i = 1}^n  \left( \frac{1}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} - \frac{1}{\sqrt{v^{t}_{r-1}} \|p_{r-1,i}^{\ell}\|} \right) \right\|^2 \right]\\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \sum_{r=1}^R  \EE \left[  \left\| \sum_{i = 1}^n  \left( \frac{1}{\sqrt{v^{t}_{r}}} - \frac{1}{\sqrt{v^{t}_{r-1}}} \right) \right\|^2 \right]\\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \sum_{r=1}^R  \EE \left[  \left| \sum_{i = 1}^n  \sum_{j= 1}^p  \left( \frac{1}{\sqrt{v^{t,j}_{r}}} - \frac{1}{\sqrt{v^{t,j}_{r-1}}} \right) \right| \right]\\
& \leq   \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0}\\
\end{split}
\end{align}
where, in the telescopic sum, we have used the initial value $v_0$ of the non decreasing sequence $\{v^t_r\}_{r >0}$ by construction (max operator).
 
Combining \eqref{eq:first} into \eqref{eq:maina2} and summing over the total number of rounds $R$ yields
\begin{align}\label{eq:finala2}
\begin{split}
\sum_{r=1}^R A_r^2 \eqdef \sum_{r=1}^R \EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2] & \leq \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0} \\
& + \sum_{r=1}^R  \left[  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] +  \frac{1}{n^2} \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \left\|\frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\right]
\end{split}
\end{align}


\textbf{ Term $A_r^3$:}
According to similar arguments on the non decreasing sequence involved in the algorithm as in the previous series of calculations, observe that

\beq\label{eq:finala3}
\sum_{r=1}^R A_r^3 \leq  \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}}
\eeq

Plugging \eqref{eq:finala1} into \eqref{eq:main2} combined with \eqref{eq:finala2} and \eqref{eq:finala3} injected into the original smoothness definition \eqref{eq:main} summed over the total number of rounds:
\begin{align}
- \sum_{r=1}^R \EE[  \pscal{\nabla f(\bar{\vartheta}_r)}{\bar{\vartheta}_{r+1} - \bar{\vartheta}_r}]  \leq  \sum_{r=1}^R \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{r=1}^R \sum_{\ell =1}^L \frac{L_\ell}{2} \EE[  \| \bar{\vartheta}^\ell_{r+1} - \bar{\vartheta}^\ell_r \|^2]
\end{align}

gives:

\beq
\begin{split}
&  \sum_{r=1}^R \alpha \phi_m \sqrt{\frac{\tot(1-\beta_2)}{M^2 p}}  \EE[ \| \overline{\nabla f}(\bar{\vartheta_r})\|^2]  -  \alpha \phi_M  \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \frac{\sigma_{i}^{\ell, j}}{\sqrt{n}}  + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} \\
& \leq  \sum_{r=1}^R \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] + \sum_{\ell =1}^L \frac{L_\ell}{2} \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0} \\
& \quad - \sum_{r=1}^R  \left[  \frac{1}{n^2} \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] +  \frac{1}{n^2} \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \left\|\frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\right]
\end{split}
\eeq
Noting that $ \sum_{r=1}^R \EE[ f(\bar{\vartheta}_r) - f(\bar{\vartheta}_{r+1})] =   f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})] $, we obtain


\beq
\begin{split}
&    \sum_{r=1}^R \alpha \phi_m \sqrt{\frac{\tot(1-\beta_2)}{M^2 p}}  \EE[ \| \overline{\nabla f}(\bar{\vartheta_r})\|^2]  +  \frac{1}{n^2}  \sum_{r=1}^R  \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] \\
&\leq   f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})] +   \frac{1}{n^2} \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \left\|\frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right] +\alpha \phi_M  \sum_{\ell=1}^\tot \sum_{i = 1}^n \sum_{j=1}^{p_\ell} \frac{\sigma_{i}^{\ell, j}}{\sqrt{n}}  + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} \\
& +  \sum_{\ell =1}^L \frac{L_\ell}{2} \frac{ \beta_1^2}{(1-\beta_1)^2} \frac{\tot(1-\beta_2)}{ p}  M^2 \phi^2_M \frac{n p}{v_0} 
\end{split}
\eeq


leading to
\beq\label{eq:final2}
\begin{split}
   \sum_{r=1}^R  \EE\left[ \left\| \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|}\nabla f_i(\theta_r) \right\|^2 \right] &\leq   f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})] +   \sum_{r=1}^R  \frac{1}{n^2} \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \left\|\frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\\
&   +\alpha \phi_M \sigma \tot p \sqrt{n}+ \frac{ \overline{L}\beta_1^2\tot(1-\beta_2)M^2 \phi^2_M n}{2(1-\beta_1)^2 v_0}    + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} 
\end{split}
\eeq
where $  \overline{L} = \sum_{\ell=1}^\tot L_{\ell}$ is the sum of all smoothness constants.

Consider the following inequality:

\beq
\frac{1}{n} \sum_{i = 1}^n \frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell} \|}\nabla f_i(\theta_r) \leq   \phi_M (1-\beta_2) \frac{\overline{\nabla}f(\theta_r)}{\sqrt{ v_r^t}}
\eeq
where $\overline{\nabla}f(\theta_r) \eqdef \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\theta_r) $.
And using the Cauchy-Schwartz inequality we have
\beq
\begin{split}
\left\| \frac{\overline{\nabla}f(\theta_r)}{\sqrt{ v_r^t}} \right\| \geq \frac{1}{2} \left\| \frac{\nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\| - \left\| \frac{\overline{\nabla}f(\theta_r)- \nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\|
\end{split}
\eeq


Using Lemma~\ref{lemma:iterates} and the smoothness assumption H\ref{ass:smooth}, we have
\beq
\begin{split}
\left\| \frac{\overline{\nabla}f(\theta_r)}{\sqrt{ v_r^t}} \right\| & \geq \frac{1}{2} \left\| \frac{\nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\| - \left\| \frac{\overline{\nabla}f(\theta_r)- \nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\|\\
& \geq \frac{1}{2} \left\| \frac{\nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}} \right\| - \overline{L} \alpha^2 M^2 \phi_M^2 \frac{(1-\beta_2)p}{v_0}
\end{split}
\eeq

Plugging the above inequality into \eqref{eq:final2} and dividing both sides by $R$ yields:
\beq
\begin{split}
   \frac{1}{R}\sum_{r=1}^R  \EE\left[ \left\| \frac{\nabla f(\overline{\theta_r})}{\sqrt{ v_r^t}}   \right \|^2 \right] & \leq  \frac{ f(\bar{\vartheta}_1)  - \EE[ f(\bar{\vartheta}_{R+1})]}{R}+   \frac{1}{n^2}  \sum_{r=1}^R  \sum_{i = 1}^n  \sigma_i^2 \EE\left[ \left\|\frac{\phi(\|\theta_{r,i}^{\ell}\|)}{\sqrt{v^{t}_{r}} \|p_{r,i}^{\ell}\|} \right\|^2 \right]\\
&   +\alpha \phi_M \sigma \tot p \sqrt{n}+ \frac{ \overline{L}\beta_1^2\tot(1-\beta_2)M^2 \phi^2_M n}{2(1-\beta_1)^2 v_0}    + \frac{\alpha \beta_1}{1-\beta_1}  \sqrt{(1-\beta_2)p} \frac{\tot M^2}{\sqrt{v_0}} +\overline{L} \alpha^2 M^2 \phi_M^2 \frac{(1-\beta_2)p}{Rv_0} 
   \end{split}
\eeq

concluding the proof of our main convergence result.
\end{document} 