\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}

\usepackage{amssymb}
% \pdfinfo{
%   /Author (Weifu Wang \and Ping Li)
%   /Title  (Categorizing diverse path from workspace)
%   /CreationDate (D:20101201120000)
%   /Subject (Robots)
%   /Keywords (Robotics)
% }

\usepackage[utf8]{inputenc}
\usepackage{comment}
% \usepackage[
% backend=biber,
% style=alphabetic,
% sorting=ynt
% ]{biblatex}
% \addbibresource{ref.bib}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\newtheorem{mydef}{Definition}


\title{Markov models in motion planning and sequential decision problems}
\author{Weifu Wang \and Belhal Karimi}
\date{}

\begin{document}

\maketitle

\section{Introduction}

Here, I briefly note down some associations between MPD models and planning scenarios. Associated with the MDP model, I will also add some other probability-based models in the planning domain, hoping to link all together. We can make notes here, and find a common interesting topic that we can manage, and work towards our first collaboration. 

One interesting paper: NeurIPS 2020: Multi-Robot collision avoidance under Uncertainty with Probabilistic Safety Barrier Certificates. 

This is not directly a MDP model, but the probability settings, if can be associated with MDP, even with some interesting look-ahead policies / strategies, may provide better certificates or guarantees; 


Motion planning: In an environment, given a start and goal, find a {\em path} that connects from start to goal. Challenge: what are legal path segments? Here, the concept of {\em control} is introduced. An agent applies control to move itself, but the control have some limits, so the agent may not always be able to move towards all directions at the same velocity from different configurations. Therefore, the planning problem, is also a sequential decision problem: at time-step $i$, what decision / control do I choose so that eventually, the agent can reach goal. 


In many cases, MDP  is used to model sequential decisions. Especially, when some states, i.e. the results of some actions / controls are not fully understood or observable, POMDP is used. The action forms a transition probability on states, but also depends on states, even though action set is limited, the result of the same action from different states can be different. Also, the number of states is usually unlimited, or to be more precisely, a discretization of the entire space (continuous). 

Normally, the states of planning is the configuration, i.e. location and other parameters describing the status of the robot. However, in different applications, one can model states differently, making some problem more complex or more interesting. For example, in a situation with planning multiple robots, the state can be the relative relation among different robots. Then, the MDP states can be of a bounded set, rather than samples from a continuous space. 

POMDP is also used when there's uncertainty, such as when the obstacles in the space are moving. 

Often, sequential planning is often modeled as temporal logic as well. I believe such models can integrate with MDP naturally. 

I will post papers on this link periodically, and if you find interesting models, please do the same, and message each other. If we have some thoughts or ideas, we can put them in the below section. 



\section{Ideas}

\subsection{Weifu}

\subsection{Belhal}
\begin{itemize}
\item Data augmentation for Control via MDP
\item Efficient latent states simulation in POMDP (how can we improve the simulation of unobserved states)
\item Energy Based Models for Motion Planning
\end{itemize}


\section{Notations and background}

\noindent\textit{Motion planning}: Given an environment with obstacles $\mathcal{O}_i$, $i = \{1, 2, \ldots, n\}$, a robot with controls $u\in\mathcal{U}$, and a start $s$ and goal $g$; find a sequence of robot configurations or a sequence of controls so that the robot can go to the goal $g$ from start $s$. 

\textcolor{magenta}{BK: so the goal is to output the vector $(u_1, \cdots, u_T)$ of controls for each timestep $t \in [1, T]$?}

Define the robot configuration as the minimum set of parameters needed to fully describe the robot's state. For example, given a car on the plane, we can describe its configuration as $(x, y, \theta)$, so that every point on the car can be described or computed based on a given $(x, y, \theta)$ value. Similarly, the configuration of a robot arm is usually described as a sequence of joint angles between adjacent links. 

\textcolor{magenta}{BK: Obstacles also come with their triplets $(x, y, \theta)$ to know where they are in advance?}

Challenge 1: collision detection. Since the obstacles are not always the same, in order to know whether a control or a configuration of a robot is valid or not, we need to perform collision detection, to make sure whatever action we choose to use or configuration we choose to move to is valid. \textbf{Collision detection} is a necessary subroutine in robot motion planning. 

Challenge 2: Local planner. It is usually not straight forward to compute what control a robot needs to perform to reach a nearby configuration even without obstacles, so we need to rely on {\em local planners}, which is another subroutine that computes the controls needed or way-points needed to pass to reach the goal configuration. 
In extreme case, we cannot even have local planner to compute how to move to a given configuration. What we have is called \emph{steering method}, only able of simulating where the robot can be after apply a given control for a small duration.

\noindent\textbf{MDP and POMDP modeling:} Given an environment, a robot, and a start and goal pair, find a sequence of actions / transitions that lead to the goal. 

\textcolor{magenta}{BK: That is where we can use Energy-Based Modeling. Learning the transitions that lead to the desired goal can be seen as a probability distribution learning problem.
We should start by checking what methods are used to learn those transitions: MPII (Model Predictive Path Integral), Maximum Entropy? }

In rare cases, if we are not sure if we will collide with the obstacles, we have an partially observed model, POMDP, simulating uncertainties of whether collision happens. Similar arguments can be made with moving obstacles, or from an unknown environment with unknown obstacles. 

\section{Related papers}


\end{document}
