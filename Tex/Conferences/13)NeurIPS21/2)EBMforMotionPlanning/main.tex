\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}

\usepackage{amssymb}
% \pdfinfo{
%   /Author (Weifu Wang \and Ping Li)
%   /Title  (Categorizing diverse path from workspace)
%   /CreationDate (D:20101201120000)
%   /Subject (Robots)
%   /Keywords (Robotics)
% }

\usepackage[utf8]{inputenc}
\usepackage{comment}
% \usepackage[
% backend=biber,
% style=alphabetic,
% sorting=ynt
% ]{biblatex}
% \addbibresource{ref.bib}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\newtheorem{mydef}{Definition}


\title{Markov models in motion planning and sequential decision problems}
\author{Weifu Wang \and Belhal Karimi}
\date{}

\begin{document}

\maketitle

\section{Introduction}

Here, I briefly note down some associations between MPD models and planning scenarios. Associated with the MDP model, I will also add some other probability-based models in the planning domain, hoping to link all together. We can make notes here, and find a common interesting topic that we can manage, and work towards our first collaboration. 

One interesting paper: NeurIPS 2020: Multi-Robot collision avoidance under Uncertainty with Probabilistic Safety Barrier Certificates. 

This is not directly a MDP model, but the probability settings, if can be associated with MDP, even with some interesting look-ahead policies / strategies, may provide better certificates or guarantees; 


Motion planning: In an environment, given a start and goal, find a {\em path} that connects from start to goal. Challenge: what are legal path segments? Here, the concept of {\em control} is introduced. An agent applies control to move itself, but the control have some limits, so the agent may not always be able to move towards all directions at the same velocity from different configurations. Therefore, the planning problem, is also a sequential decision problem: at time-step $i$, what decision / control do I choose so that eventually, the agent can reach goal. 


In many cases, MDP  is used to model sequential decisions. Especially, when some states, i.e. the results of some actions / controls are not fully understood or observable, POMDP is used. The action forms a transition probability on states, but also depends on states, even though action set is limited, the result of the same action from different states can be different. Also, the number of states is usually unlimited, or to be more precisely, a discretization of the entire space (continuous). 

Normally, the states of planning is the configuration, i.e. location and other parameters describing the status of the robot. However, in different applications, one can model states differently, making some problem more complex or more interesting. For example, in a situation with planning multiple robots, the state can be the relative relation among different robots. Then, the MDP states can be of a bounded set, rather than samples from a continuous space. 

POMDP is also used when there's uncertainty, such as when the obstacles in the space are moving. 

Often, sequential planning is often modeled as temporal logic as well. I believe such models can integrate with MDP naturally. 

I will post papers on this link periodically, and if you find interesting models, please do the same, and message each other. If we have some thoughts or ideas, we can put them in the below section. 



\section{Ideas}

\subsection{Weifu}

\subsection{Belhal}
\begin{itemize}
\item Data augmentation for Control via MDP
\item Efficient latent states simulation in POMDP (how can we improve the simulation of unobserved states)
\item Energy Based Models for Motion Planning
\end{itemize}


\section{Notations and background}



\textcolor{magenta}{BK: so the goal is to output the vector $(u_1, \cdots, u_T)$ of controls for each timestep $t \in [1, T]$?}

\textcolor{blue}{Weifu: In a sense, yes. We want the sequence of controls that can lead the robot to the goal. However, it is often complicated to get the exact controls, so usually the first step is to find sequence of intermediate configurations between start and goal, so that from the start, we can connect through the intermediate configurations to the goal. }

Define the robot configuration as the minimum set of parameters needed to fully describe the robot's state. For example, given a car on the plane, we can describe its configuration as $(x, y, \theta)$, so that every point on the car can be described or computed based on a given $(x, y, \theta)$ value. Similarly, the configuration of a robot arm is usually described as a sequence of joint angles between adjacent links. 

\textcolor{magenta}{BK: Obstacles also come with their triplets $(x, y, \theta)$ to know where they are in advance?}

\textcolor{blue}{Weifu: Obstacle descriptions are complicated, sometimes they are polyhedrons, with known geometries, or sometimes they are described as point clouds, or meshes. }

\textcolor{magenta}{BK: Ok, if it's that complicated, we should formalize the state transition probability conditioned on the obstacles. Looking at what this object looks like will be useful. Is there any references where this has been formalized?}

\begin{itemize}
\item Challenge 1: collision detection. Since the obstacles are not always the same, in order to know whether a control or a configuration of a robot is valid or not, we need to perform collision detection, to make sure whatever action we choose to use or configuration we choose to move to is valid. \textbf{Collision detection} is a necessary subroutine in robot motion planning. 

\item Challenge 2: Local planner. It is usually not straight forward to compute what control a robot needs to perform to reach a nearby configuration even without obstacles, so we need to rely on {\em local planners}, which is another subroutine that computes the controls needed or way-points needed to pass to reach the goal configuration. 
In extreme case, we cannot even have local planner to compute how to move to a given configuration. What we have is called \emph{steering method}, only able of simulating where the robot can be after apply a given control for a small duration.
\end{itemize}

\noindent\textbf{MDP and POMDP modeling:} Given an environment, a robot, and a start and goal pair, find a sequence of actions / transitions that lead to the goal. 

\textcolor{magenta}{BK: That is where we can use Energy-Based Modeling. Learning the transitions that lead to the desired goal can be seen as a probability distribution learning problem.
We should start by checking what methods are used to learn those transitions: MPII (Model Predictive Path Integral), Maximum Entropy? }

\textcolor{blue}{Weifu: It is true, we can model it like that. Question is, the environment may change after a task is performed. So, a long learning process is usually not acceptable, the whole planning should be done in a relatively short time, unless the distribution can be reused in different environments, with the fast updates. So, can you let me know usually how long will these methods learn the distribution for a task like this? Some pre-computation is ok in planning, but usually not too heavy. }

\textcolor{magenta}{BK: At each state, hence at each model update $\theta$, the learning of EBM can take few MCMC transitions (can be small) and a single gradient step to update the model parameter.}

In rare cases, if we are not sure if we will collide with the obstacles, we have an partially observed model, POMDP, simulating uncertainties of whether collision happens. Similar arguments can be made with moving obstacles, or from an unknown environment with unknown obstacles. 


\noindent\textit{Motion planning Problem description}: Let there be an environment of $\mathbb{R}^d$ where $d = 2$ or $3$; obstacles $\mathcal{O}_i\in\mathbb{R}^d$, $i = \{1, 2, \ldots, n\}$, a robot of geometry $\mathcal{B}$, denote the configuration of the robot as $q\in\mathbb{R}^n$, where $n$ is the number of Degree of Freedom (DoF) of the robot. Let the robot has controls $u\in\mathcal{U}$, and let there be start $s\in\mathbb{R}^n$ and goal $g\in\mathbb{R}^n$; find a sequence of robot configurations or a sequence of controls so that the robot can go from $s$ to $g$. We can assume there is an oracle function $\mathcal{F}:(q\times \cup\mathcal{O})\rightarrow \{0, 1\}$ that can return collision detection result in a given environment for any given configuration of the robot. 

One common approach is called sampling based motion planning, which is achieved through placing samples in $\mathcal{R}^n$, the configuration space, and retain valid non-collision samples in the configuration space by inquiring $\mathcal{F}$. The invalid samples are discarded (or retained in some cases), and the valid samples are connected if the path connecting the configurations is valid (pass the validity check after inquiring $\mathcal{F}$). A good set of samples will lead to solutions much faster compared to random samples, though may not always be optimal. 

If we want to use a learning method to create the samples, with bias, we may be able to find paths faster. 



\section{Related papers}


\end{document}
