\begin{thebibliography}{10}

\bibitem{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
  Konstantinov, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock {\em arXiv preprint arXiv:1809.10505}, 2018.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem{bottou2008}
L\'{e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In J.~C. Platt, D.~Koller, Y.~Singer, and S.~T. Roweis, editors, {\em
  Advances in Neural Information Processing Systems 20}, pages 161--168. Curran
  Associates, Inc., 2008.

\bibitem{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning},
  3(1):1--122, 2011.

\bibitem{chen2020quantized}
Congliang Chen, Li~Shen, Haozhi Huang, Qi~Wu, and Wei Liu.
\newblock Quantized adam with error feedback.
\newblock {\em arXiv preprint arXiv:2004.14180}, 2020.

\bibitem{chen2010approximate}
Yongjian Chen, Tao Guan, and Cheng Wang.
\newblock Approximate nearest neighbor search by residual vector quantization.
\newblock {\em Sensors}, 10(12):11259--11273, 2010.

\bibitem{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In {\em Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pages 561--574, 2017.

\bibitem{duchi2011dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock {\em IEEE Transactions on Automatic control}, 57(3):592--606, 2011.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{haddadpour2019trading}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe.
\newblock Trading redundancy for communication: Speeding up distributed sgd for
  non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  2545--2554. PMLR, 2019.

\bibitem{hong2017prox}
Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1529--1538, 2017.

\bibitem{jegou2010product}
Herve Jegou, Matthijs Douze, and Cordelia Schmid.
\newblock Product quantization for nearest neighbor search.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  33(1):117--128, 2010.

\bibitem{jiang2018linear}
Peng Jiang and Gagan Agrawal.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 2530--2541, 2018.

\bibitem{karimi2019non}
Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai.
\newblock Non-asymptotic analysis of biased stochastic approximation scheme.
\newblock In {\em Conference on Learning Theory}, pages 1944--1974. PMLR, 2019.

\bibitem{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock {\em arXiv preprint arXiv:1901.09847}, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In {\em International Conference on Machine Learning}, pages
  3478--3487, 2019.

\bibitem{lu2019gnsd}
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong.
\newblock Gnsd: A gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In {\em 2019 IEEE Data Science Workshop (DSW)}, pages 315--321, 2019.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48, 2009.

\bibitem{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{shi2019convergence}
Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu.
\newblock A convergence analysis of distributed sgd with
  communication-efficient gradient sparsification.
\newblock In {\em IJCAI}, pages 3411--3417, 2019.

\bibitem{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem{wangni2017gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock {\em arXiv preprint arXiv:1710.09854}, 2017.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock {\em arXiv preprint arXiv:1705.07878}, 2017.

\bibitem{yang2019swalp}
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew~Gordon
  Wilson, and Chris De~Sa.
\newblock Swalp: Stochastic weight averaging in low precision training.
\newblock In {\em International Conference on Machine Learning}, pages
  7015--7024. PMLR, 2019.

\end{thebibliography}
