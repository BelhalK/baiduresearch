\begin{thebibliography}{10}

\bibitem{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
  Konstantinov, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock {\em arXiv preprint arXiv:1809.10505}, 2018.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem{bottou2008}
L\'{e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In J.~C. Platt, D.~Koller, Y.~Singer, and S.~T. Roweis, editors, {\em
  Advances in Neural Information Processing Systems 20}, pages 161--168. Curran
  Associates, Inc., 2008.

\bibitem{chen2020quantized}
Congliang Chen, Li~Shen, Haozhi Huang, Qi~Wu, and Wei Liu.
\newblock Quantized adam with error feedback.
\newblock {\em arXiv preprint arXiv:2004.14180}, 2020.

\bibitem{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In {\em Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pages 561--574, 2017.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{haddadpour2019trading}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe.
\newblock Trading redundancy for communication: Speeding up distributed sgd for
  non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  2545--2554. PMLR, 2019.

\bibitem{jiang2018linear}
Peng Jiang and Gagan Agrawal.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 2530--2541, 2018.

\bibitem{karimi2019non}
Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai.
\newblock Non-asymptotic analysis of biased stochastic approximation scheme.
\newblock In {\em Conference on Learning Theory}, pages 1944--1974. PMLR, 2019.

\bibitem{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock {\em arXiv preprint arXiv:1901.09847}, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{shi2019convergence}
Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu.
\newblock A convergence analysis of distributed sgd with
  communication-efficient gradient sparsification.
\newblock In {\em IJCAI}, pages 3411--3417, 2019.

\bibitem{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem{wangni2017gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock {\em arXiv preprint arXiv:1710.09854}, 2017.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock {\em arXiv preprint arXiv:1705.07878}, 2017.

\bibitem{yang2019swalp}
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew~Gordon
  Wilson, and Chris De~Sa.
\newblock Swalp: Stochastic weight averaging in low precision training.
\newblock In {\em International Conference on Machine Learning}, pages
  7015--7024. PMLR, 2019.

\end{thebibliography}
