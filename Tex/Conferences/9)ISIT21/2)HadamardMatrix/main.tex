%% LaTeX Template for ISIT 2020
%%
%% by Stefan M. Moser, October 2017
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

\documentclass[conference,letterpaper]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex
\topmargin 0pt\headheight 0pt\headsep 2pt\textheight 660pt\footskip
30pt\oddsidemargin 10pt\textwidth 440pt\marginparsep 10pt
\usepackage{mathrsfs,amsmath,amssymb,amsfonts}
\numberwithin{equation}{section}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{stmaryrd}
\usepackage{color, verbatim}
\usepackage{mdframed}


\newmdtheoremenv{theo}{Theorem}
\newmdtheoremenv{lem}{Lemma}


%\usepackage[notref,notcite]{showkeys}
%\usepackage{refcheck}

\newcommand{\lbl}{\label}
%\newcommand{\lbl}[1]{\hspace{1cm} \underline{({#1})} \label{#1}}
\newcommand{\proof}{{\it Proof. \ }}

\newcommand{\ignore}[1]{}{}
%\newcommand{\ignore}[1]{#1}
\newcommand{\be}{\begin{equation}}               %\be=\begin{equation}
\newcommand{\ee}{\end{equation}}                 %\ee=\end{equation}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\var}{\mbox{Var}}
\newcommand{\etab}{\kappa}
\newcommand{\B}{\alpha}
\newcommand{\D}{\beta}
\newcommand{\bd}{\bold}

\newcommand{\beaa}{\begin{eqnarray*}}
\newcommand{\eeaa}{\end{eqnarray*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}

%\newcommand{\binom}[2]{\left(
%\begin{array}{c} #1 \\ #2 \end{array} \right)}
\newcommand{\bis}[2]{
 \begin{array}{c} \ds #1 \\ \vs{-2.5}\\  \Large{#2} \end{array} }
%\newcommand{\bis}[2]{
%\begin{array}{c} \ds #1 \\ \footnotesize{#2} \end{array} }

\newcommand{\noi}{\noindent}
\newcommand{\beqn}{\begin{eqnarray}}             %\beqn=\begin{eqnarray}
\newcommand{\eeqn}{\end{eqnarray}}               %\eeqn=\end{eqnarray}
\newcommand{\beq}{\begin{eqnarray*}}             %\beq=\begin{eqnarray*}
\newcommand{\eeq}{\end{eqnarray*}}               %\eeq=\end{eqnarray*}
\newcommand{\mb}{\mbox}                          %\mb=\mbox
\newcommand{\lb}{\left\{ }                       %\lb=\left\{
\newcommand{\rb}{\right\} }                      %\rb=\right\}
\newcommand{\re}{\right.}                        %\re=\right.
\newcommand{\nn}{\nonumber}
\newcommand{\ds}{\displaystyle}



\newcommand{\bbox}{\nobreak\quad\vrule width4pt depth2pt height4pt}
\newcommand{\eq}[1]{$(\ref{#1})$}

\newcommand{\al}{\alpha}                         %\al=\alpha
\newcommand{\ga}{\gamma}                         %\ga=\gamma
\newcommand{\Ga}{\Gamma}                         %\Ga=\Gamma
\newcommand{\ep}{\epsilon}                       %\ep=\epilon
\newcommand{\vp}{\varepsilon}                   %\vp=\varepsilon
\newcommand{\la}{\lambda}                        %\la=\lambda
\newcommand{\La}{\Lambda}                        %\La=\Lambda
%\newcommand{\th}{\theta}                         %\th=\theta
\newcommand{\bt}{\beta}                           %\bt=\beta
\newcommand{\sg}{\sigma}                         %\sg=\sigma
\newcommand{\wt}{\widetilde}
\newcommand{\wh}{\widehat}
\newcommand{\ssb}{\scriptstyle \footnotesize % \scriptsize
                 \begin{array}{c}}
\newcommand{\esb}{\end{array}}
\newcommand{\ra}{\rightarrow}                    %\ra=\rightarrow
\newcommand{\lra}{\longrightarrow}               %\lra=\longrightarrow
\newcommand{\Ra}{\Rightarrow}                    %\Ra=\Rightarrow
\newcommand{\Lra}{\Longrightarrow}               %\Lra=\Longrightarrow
\newcommand{\sta}{\stackrel}                    %\sta=\stackrel
\newcommand{\xn}{$\{X_n, \, n \geq 1\} \ $}
\newcommand{\wip}{weak invariance principle}
\newcommand{\sa}{strong approximation}
\newcommand{\clt}{central limit theorem}
\newcommand{\lil}{law of the iterated logarithm}
\newcommand{\iid}{independent and identically distributed random variables}
\newcommand{\irv}{independent random variables \ }
\newcommand{\inrv}{independent normal random variables \ }
\newcommand{\Let}[1]{Let $\{X_n, n \geq 1 \}$ be a stationary  $#1$-mixing
          sequence of random variables \ }
\newcommand{\Les}[1]{Let $\{X_n, n \geq 1 \}$ be a  $#1$-mixing
          sequence of random variables \ }
\newcommand{\Leta}[1]{Let $\{X_n, n \geq 1 \}$ be an  $#1$-mixing
          sequence of random variables \ }
\newcommand{\CsR}{Cs\"org\H{o} and R\'ev\'esz}
\newcommand{\kmt}{Koml\'os, Major and Tusn\'ady}
\newcommand{\A}{{\cal A}}
\newcommand{\C}{{\cal C}_{bd}}
\baselineskip=7.0mm
\newtheorem{theorem}{{\sc Theorem}}[section]
 \newtheorem{prop}{Proposition}[section]
 \newtheorem{coro}{{\sc Corollary}}[section]
 \newtheorem{lemma}{{\sc Lemma}}[section]
 \newtheorem{remark}{{\sc Remark}}[section]
\newtheorem{definition}{{\sc Definition}}[section]
\newtheorem{example}{{\sc Example}}[section]
 \newcommand{\vs}{\vspace{.3cm}}
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}%%%%%%
% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

% ------------------------------------------------------------
\begin{document}
\title{On the Positivity of Hadamard Powers \\
of Random Matrices}
% \footnote{ }}
%\author{Tiefeng Jiang\thanks{ \textsl{E-mail address}:
%\texttt{jiang040@umn.edu}} and  Ping Li\thanks{ \textsl{E-mail address}:
%\texttt{liping11@baidu.com}} }
%\date{\small \it University of Minnesota and Cognitive Computing Lab, Baidu Research, USA}


\author{%
  \IEEEauthorblockN{Tiefeng Jiang}
  \IEEEauthorblockA{School of Statistics\\
  		University of Minnesota \\
		224 Church Street SE,MN 55455\\
                    Email: jiang040@umn.edu}
  \and
  \IEEEauthorblockN{Belhal Karimi and Ping Li}
  \IEEEauthorblockA{Cognitive Computing Lab\\
  			Baidu Research \\
			10900 NE 8th St. Bellevue, WA 98004\\
                    Email: \{belhal.karimi, pingli98\}@gmail.com}
}



\maketitle
%\mbox{}\hrule\mbox{}\\[0.5cm]
%\noindent\textbf{Abstract}
%\\[-0.2cm]^^L


\begin{abstract}
\noindent The\footnote{The work of Tiefeng Jiang was conducted as consulting researcher at Baidu Research.} paper studies the relation between the positive definiteness of a random matrix and its Hadamard power.
Given important applications of random matrix theory and their Hadamard power, of exponent $\alpha$, in various fields such as kernel learning, graphs learning and more, we establish a collection of theoretical results ensuring the existence of an interval of non integer exponent ensuring that given a \emph{random} positive definite matrix, its \emph{Hadamard} power can be in high probability, not non-negative.
Based on mild assumptions on the support of entries of any random matrices, our results give a better understanding and a counterintuitive property of the Hadamard powers of random matrices.
Key ingredients in the proof include a novel consideration of the supports of each random entry of our considered matrices, coupled with their probabilistic interpretation.\\
\end{abstract}



\noindent\textbf{Keywords:} Hadamard matrix function, Hadamard power, non-negative definite matrix, positive definite matrix, random matrix.

%\medskip
%
%\noindent\textbf{MSC(2010):} Primary .\\
%[0.5cm]
%\mbox{}\hrule\mbox{}
%------------------------------------------------------------------------------------------

%\newpage

\section{Introduction}
Random matrices and Hadamard product, the two main topics of this paper, are of utmost importance in various applications, such as physics, finance, telecommunication, computational biology and machine learning for the random matrices, and combinatorial analysis, number theory or regular graphs to name a few for the Hadamard product, and in information theory.
The Hadamard product, also known as the entry-wise or Schur product, is a type of matrix multiplication that is commutative and displays virtues, as put in \cite{horadam2012hadamard}. 
Several noteworthy applications and studies of the Hadamard product can be found in the literature, as in \cite{hedayat1978hadamard,agaian2006hadamard,bulutoglu2009counterexample,audenaert2010spectral}.

In particular, in our contribution, we focus on the Hadamard power of random matrices. 
Given $f(x)$, a real-valued function defined on $\mathbb{R}.$ Let $\mathbf{A}=(a_{ij})$ be an $n\times n$ matrix, where $a_{ij}$'s are real numbers.  Define $f: \mathbf{A} \rightarrow f(\mathbf{A)}=(f(a_{i j}))$. We call $f(\mathbf{A})$  a Hadamard function to distinguish it from the usual notion of matrix functions, as here the function is applied to each element of the matrix. 
In particular, if $\alpha>0$ and  $f(x)=x^{\alpha}$, then we call  $\mathbf{A}^{(\alpha)}:=f(\mathbf{A})$ the Hadamard power of $\alpha$. 

\textbf{Main Motivations: }
An important application of Hadamard powers of matrices is the kernel learning problem where the exponent, used in for instance polynomial kernels or Gaussian Radial Basis Function (RBF), is a tunable parameter, see~\cite{jiang2017hadamard,li2018several, belton2019simultaneous}.
In particular, positive definite kernels and the set of methods they have inspired in the machine learning literature, namely kernel methods, are particularly sensitive to the choice of their exponent.
After having chosen a family of kernel, the task of choosing the adequate hyperparameters for this kernel is one of the biggest challenges when using kernel methods, as described in Section 12.3.4 of \cite{hastie2009elements}. 
For instance, for polynomial kernels or Gaussian kernels, the searches are limited to the exponent parameters, see \cite{bousquet2002complexity,frohlich2004feature,keerthi2007efficient} for more details on the tuning of such kernels.

%\textcolor{red}{Motivate the application of such results. How they provide benefits or drawbacks}

\textbf{Our Contributions: }
We provide in this contribution several important and intriguing results on the positivity of Hadamard powers of random matrices. 
While we show in Theorem~\ref{th:th1} that for deterministic and square matrices with non-negative entries, their Hadamard powers remain positive definite, we also provide in Theorem~\ref{th:th2} and Theorem~\ref{th:th3}, that even though Hadamard products preserve positive semidefiniteness, we are able to prove that for some non integer exponent values, the positive definite random matrices have their Hadamard powers non non-negative definite.


Section~\ref{sec:notations} is devoted to the main concepts, notations and existing results to lay the core foundation of our study.
Several illustrative examples are also provided for the sake of clarity.
Section~\ref{sec:main} develops the main results of our paper and their proofs are in Section~\ref{sec:proofs}.
Section~\ref{sec:conclusion} concludes our work.

\section{Notations, Existing Results and Examples}\lbl{sec:notations}
We recall in this section some important results in the literature followed by our main theoretical contributions to the domain of random matrices.
In the definition of the Hadamard power stated in the introduction, careful attention needs to be paid to the domain of the function $f(x)=x^{\alpha}$. 
If the power $\alpha>0$ is an integer, the function is defined for every $x\in \mathbb{R}.$ If $\alpha>0$ is not an integer, the function $f(x)=x^{\alpha}$ is defined only on $[0, \infty).$ By the Schur product theorem, it is known that $\mathbf{A}^{(\alpha)}$ is a positive definite matrix if $\mathbf{A}=(a_{ij})$ is a positive definite matrix and $\alpha =1,2, \cdots$; see, for example, Theorem 5.2.1 from \cite{horn_johnson_1991}. 
%In fact, we know more about this conclusion. 
Besides, for positive definite matrices $\mathbf{U}=(u_{ij})_{n\times n}$ and $\mathbf{V}=(v_{ij})_{n\times}$, set $\mathbf{U}\circ \mathbf{V}=(u_{ij}v_{ij})_{n\times n}$. 
Then  $\lambda_{min}(\mathbf{U})\cdot\min_{1\leq j \leq n}{v_{ij}}\leq \lambda_i(\mathbf{U}\circ \mathbf{V})\leq \lambda_{max}(\mathbf{U})\cdot\max_{1\leq j \leq n}{v_{ij}}$ for each $1\leq i \leq n$; see, for example, \cite{schur1911bemerkungen} or Theorem 5.3.4 from \cite{horn_johnson_1991}. 
If $\alpha$ is a positive integer, then  $\mathbf{A}^{(\alpha)}=\mathbf{A}\circ\cdots\circ\mathbf{A}$ from which there are $\alpha$ times $\mathbf{A}$ in the product. Thus, by induction, we have that $\lambda_{min}(\mathbf{A})>0$ if $\mathbf{A}$ is positive definite.


\subsection{Existing Results}\lbl{known_results}

 Let $a \in (0, \infty]$ and $f: (0, a) \to \mathbb{R}$. 
 We say that $f(x)$ is {\it absolutely monotonic} on  $(0, a)$ if $f^{(k)}(x)\geq 0$ for every $x\in (0, a)$ and $k\geq0$ 
 The following general conclusion can be found in several research articles including \cite{schoenberg1988positive},  \cite{vasudeva1979positive} and \cite{hiai2009monotonicity}.
Theorem~\ref{oldth1} gives a formal result regarding the monotonicity of a value function $f(\cdot)$ and a non-negative definite matrix $\mathbf{A}$.
\medskip
\begin{theorem} \label{oldth1}
Assume $a \in (0, \infty]$ and $f(x)$ is a real function defined on $(-a, a)$.  
Then $f(\mathbf{A})$ is non-negative definite for every non-negative definite matrix  $\mathbf{A}$ with  entries in $(-a, a)$ if and only if $f(x)$ is analytic and absolutely monotonic on $(0, a).$
\end{theorem}
\medskip

\begin{theorem}\label{oldth2} (Theorem 6.3.7 from \cite{horn_johnson_1991})
	Let $f(\cdot)$ be an $(n-1)$-times continuously differentiable real valued function on $(0,\infty)$, and suppose that the Hadamard function $f(\mathbf{A})=(f(a_{ij}))$ is non-negative definite for every non-negative definite matrix $\mathbf{A}$ that has positive entries. Then:
$$f^{(k)}(t)\geq 0$$
 for all $t\in(0,\infty)$ and all $k \in [0,n-1]$.
\end{theorem}
\medskip
An additional result can be derived from Theorem~\ref{oldth2} stated above, and states:
\begin{coro}(Corollary 6.3.8 from\cite{horn_johnson_1991})
	Let $0<\alpha<n-2$, where $\alpha$ is not an integer. 
	There exists some $n\times n$ non-negative definite matrix noted $\mathbf{A}$ with positive entries, such that the Hadamard power of  $\mathbf{A}$ noted $\mathbf{A}^{(\alpha)}=(a_{ij}^\alpha)$ is not non-negative definite.
\end{coro}
This Corollary translates the result of the Theorems stated above in the particular case when the value function $f(\cdot)$ maps a non-negative definite matrix into its Hadamard power. 
The equivalence with a non-negative definite Hadamard power matrix is then a consequence of the above.
Another important existing result worth noting, as an introduction to our novel results introduced in the sequel, is a quantification of an infimum value for the power $\alpha$ used in the Hadamard power operation. The following theorem states as follows:
\medskip
\begin{theorem} (Theorem 6.3.9 from \cite{horn_johnson_1991})
	Let $\mathbf{A}=(a_{ij})$ be a non-negative definite matrix with non-negative entries. If $\alpha \geq n-2$, then the Hadamard power $\mathbf{A}^{(\alpha)}$ is non-negative definite. 
	Also, the lower bound $n=2$ is, in general, the best possible.
\end{theorem}

\subsection{Some Illustrative Examples}
For the sake of clarity, we introduce the following notations, used throughout the following sections including the several examples we provide for illustrative purposes and the statements of our main theoretical results.  
For a matrix $\mathbf{M}$, we note $\mathbf{M}>0$ if $\mathbf{M}$ is positive definite; $\mathbf{M}\geq 0$ if $\mathbf{M}$ is non-negative definite; $\mathbf{M}\ngeqslant 0$ if $\mathbf{M}$ is not non-negative definite.

We now provide an example of such \emph{not non-negative definite} matrix through the use of the Hadamard power operation.
\medskip
\begin{example} Consider  the following $3\times 3$ matrix
\beaa
\mathbf{M}=
\begin{pmatrix}
1 & \frac{1}{2} & 0\\
\frac{1}{2} & 1 & \frac{1}{2}\\
0 & \frac{1}{2} & 1
\end{pmatrix}
.
\eeaa
Its Hadamard power of power $\alpha$, where $\alpha$ is not an integer, is given by
\beaa
\mathbf{M}^{(\alpha)}=
\begin{pmatrix}
1 & \frac{1}{2^{\alpha}} & 0\\
\frac{1}{2^{\alpha}} & 1 & \frac{1}{2^{\alpha}}\\
0 & \frac{1}{2^{\alpha}} & 1
\end{pmatrix}
.
\eeaa
 Then, the computation of its determinant yields
\beaa
\mbox{det}(\mathbf{M}^{(\alpha)})=1-\frac{2}{4^{\alpha}}.
\eeaa
Therefore, we have that $\mathbf{M}=\mathbf{M}^{(1)}>0$. 
However, $\mathbf{M}^{(\alpha)}\ngeqslant 0$ if $\alpha \in (0, \frac{1}{2}).$

Now, for any integer $n\geq 3$, we define the matrix noted $\mathbf{M}_n$ and defined as 
$\mathbf{M}_n=\mathbf{M}$ for $n=3$ and

\[
    \mathbf{M}_n= 
\begin{cases}
    \mathbf{M},& \text{if } n=3\\
    \begin{pmatrix}
\mathbf{M} & \mathbf{0}\\
\mathbf{0} & \mathbf{I}_{n-3}
\end{pmatrix},              & \text{if } n\geq 4
\end{cases}
\]
where $\mathbf{I}_{n-3}$ denotes the identity matrix of dimension $n-3$.
%\beaa
%\mathbf{M}_n=
%\begin{pmatrix}
%\mathbf{M} & \mathbf{0}\\
%\mathbf{0} & \mathbf{I}_{n-3}
%\end{pmatrix}
%,\ \ \ n\geq 4.
%\eeaa
Then,  the matrix $\mathbf{M}_n$ is positive definite as $n\geq 3$. 
However, we note that its Hadamard power $\mathbf{M}_n^{(\alpha)}\ngeqslant 0$ is not non-negative definite when $\alpha \in (0, \frac{1}{2})$.
\end{example}

\medskip

The example given above shows that the construction of a sequence of positive definite matrix, in dimension $3$, while its Hadamard power is not non-negative, for a certain value of power $\alpha$, is achievable. 
For better understanding, we provide the following additional example in dimension $4$. 

\medskip

\begin{example}\lbl{good_example} 
Consider a $4\times 4$ matrix defined as 
$$\mathbf{M}=\mathbf{a}\mathbf{a}'+\mathbf{b}\mathbf{b}' + 10^{-4}\mathbf{I}_4 \ ,$$ 
where $\mathbf{a}^T=(1,1,1,1)$ and $\mathbf{b}^T=(0,1,2,3)$. 
Note that the additive term $10^{-4}\mathbf{I}_4$ purely ensures the positivity of the resulting $\mathbf{M}$, thus being a positive definite one and falling into the framework of our study.
The matrix $\mathbf{a}\mathbf{a}'+\mathbf{b}\mathbf{b}'$  is of rank $2$. Obviously, $\mathbf{M}>0$. 
It is also easy to check that
\beaa
\mbox{det}(\mathbf{M}^{1.1})=-0.000118654.
\eeaa
Hence, $\mathbf{M}^{1.1}\ngeqslant 0$.
%\end{example}
%\begin{example}\lbl{day}. Let $\mathbf{M}$ be the $4\times 4$ matrix as in Example \ref{good_example}.
Similarly to the previous example, define the sequence of matrices $\mathbf{M}_n$ defined as follows:
\[
    \mathbf{M}_n= 
\begin{cases}
    \mathbf{M},& \text{if } n=4\\
    \begin{pmatrix}
\mathbf{M} & \mathbf{0}\\
\mathbf{0} & \mathbf{I}_{n-4}
\end{pmatrix},              & \text{if } n\geq 5
\end{cases}
\]
Then, we observe that for $n\geq 4$ we have that $\mathbf{M}_n>0$. 
However, its Hadamard power matrix $\mathbf{M}_n^{1.1}\ngeqslant 0$ is not non-negative definite.
\end{example}
\medskip
Those simple illustrative examples lay the context of the next section, where several new results are provided, along with their proofs.


\section{Main Results}\lbl{sec:main}

We now introduce a collection of theorems aiming at improving the theoretical understanding of Hadamard powers of random matrices.
%All proofs are deferred to Section~\ref{sec:proofs}.

\subsection{Non-negative matrices}
Following the extensive literature presented in the introduction of our paper, we provide the following result for non-negative (and deterministic) definite matrices:

\begin{theorem}\lbl{th:th1} Let $\mathbf{A}=(a_{ij})$ be an $n\times n$ matrix of which the entries are non-negative. Assume $a_{ii}\geq  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n.$ Then $\mathbf{A} \geq 0$ and $\mathbf{A}^{(\alpha)}\geq  0$ for all $\alpha\geq 1.$ 
Note that the conclusion still holds if all three ``$\geq$" are replaced by ``$>$", \textit{i.e.}, if $a_{ii} >  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n.$, then $\mathbf{A} > 0$ and $\mathbf{A}^{(\alpha)}>  0$ for all $\alpha> 1.$ 
\end{theorem}


\subsection{Random Matrices}
Here, we consider random matrices, in the sense that the entries of the matrices we study are independent random variables.
Then, under some mild assumptions, that we rigorously give in each statement, we provide the existence, in probability, of positive definitive matrices while their Hadamard power, for some values of exponent $\alpha$, is not non-negative definite.
To begin with, considering large symmetric matrices of size $n \geq 4$, it is possible to derive the following theoretical result:
\begin{theorem}\lbl{th:th2} Assume $n\geq 4$.   Let $\mathbf{M}=(\xi_{ij})$ be an $n\times n$ symmetric matrix, where $\{\xi_{ij};\, 1\leq i \leq j \leq n\}$ are independent random variables. 
Suppose all of the supports of $\xi_{ij}$'s contain  a common interval $[u, v]$ for some $v>u>0$. Then there exists $\alpha \in (1, 2)$ for which
\beaa
P\big(\mathbf{M}\geq 0\ \mbox{and}\ \mathbf{M}^{(\alpha)} \ngeqslant 0\big)>0 \ .
\eeaa
\end{theorem}

On the same line of work, we also develop a similar result for non-square matrices. Then under the same mild assumptions on the support of the random variables constituting the matrices involved and the value of the non integer exponent, we state the following:
\begin{theorem}\lbl{th:th3} Assume $n\geq 4$.   Let $\mathbf{X}=(x_{ij})$ be an $n\times p$  matrix, where $\{x_{ij};\, 1\leq i \leq n, 1\leq j \leq p\}$ are independent random variables. Suppose all of the supports of $\xi_{ij}$'s contain  a common interval $[u, v]$ for some $v>u>0$. Then there exists $\alpha \in (1, 2)$ such that
\beaa
P\big(\mathbf{X}^T\mathbf{X}>0\ \mbox{and}\ (\mathbf{X}^T\mathbf{X})^{(\alpha)}\ngeqslant 0\big)>0 \ .
\eeaa
\end{theorem}




\section{Proofs of the main results}\lbl{sec:proofs}
Using intermediary Lemmas, that we rigorously state and prove in this section, we now provide the proofs of the main results presented above.

\subsection{Intermediary Theoretical Results}
The following result guarantees the existence of a positive definite symmetric matrix, where the support of its entries are well defined, such that its Hadamard power, in a range of exponent, is not non-negative.
\begin{lemma}\lbl{lemma:lem1}
Let $\mathbf{A}=(a_{ij})$ be an $n\times n$ matrix of which the entries are non-negative.
For any $n\geq 4$, there exist $\alpha\in (1,2)$, $\delta>0$ and an $n\times n$ symmetric matrix $\mathbf{M}=(m_{ij})$ where for all $1\leq i, j \leq n$ we have $m_{ij}\geq 0$, such that the following holds:
(i) $\mathbf{M}=(m_{ij})>0$  for every $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and $1\leq i, j \leq n.$

(ii) $\mathbf{M}^{(\alpha)}=(m_{ij}^{\alpha})\ngeqslant 0$  for any $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and any $1\leq i, j \leq n.$
\end{lemma}
\medskip
\noindent\textbf{Proof of Lemma~\ref{lemma:lem1}}. For any $n\times n$ symmetric matrix $\mathbf{M}=(m_{ij})$, we denote by $\|\mathbf{M}\|$ the spectral norm of $\mathbf{M}$. 
We use $\lambda_1(\mathbf{M})\geq \lambda_2(\mathbf{M})\geq \cdots \geq \lambda_n(\mathbf{M})$ to denote  the eigenvalues of $\mathbf{M}$.  
It is well established that $\|\mathbf{M}\|\leq (\sum_{1\leq i, j \leq n}|m_{ij}|^2)^{1/2}$. 
Also denote by $\mathbf{M}_1=(m_{ij})$ and $\mathbf{M}_2=(\tilde{m}_{ij})$, two symmetric matrices of dimension $n\times n$. 
The Weyl's perturbation theorem [see, e.g., \cite{horn1985}] states that
$$\max_{1\leq i \leq n}|\lambda_i(\mathbf{M}_1)-\lambda_i(\mathbf{M}_2)|\leq \|\mathbf{M}_1-\mathbf{M}_2\| \ .$$  
Therefore, we obtain the following
\bea
& \max_{1\leq i \leq n}|\lambda_i(\mathbf{M}_1)-\lambda_i(\mathbf{M}_2)|\\\notag
&\leq \Big(\sum_{1\leq i, j \leq n}|m_{ij}-\tilde{m}_{ij}|^2\Big)^{1/2}.\lbl{flower_could}
\eea
The inequality above concludes that the eigenvalues of a matrix are continuous functions of its entries. 
This is particularly true for the smallest eigenvalues.

According to Example \ref{good_example}, there exists  $\alpha\in (1, 2)$ and an $n\times n$ symmetric matrix $\mathbf{A}=(a_{ij})$ such that $a_{ij}\geq 0$ for all $1\leq i, j \leq n$, we have $\mathbf{A}>0$ and its Hadamard power matrix $\mathbf{A}^{(\alpha)} \ngeqslant 0$. 
For any $n\times n$ symmetric matrix  $\mathbf{M}=(m_{ij})$, let us define
\beaa
f(\mathbf{M}):=\min\big\{\lambda_n(\mathbf{M}),\, -\lambda_n(\mathbf{M}^{(\alpha)})\big\}.
\eeaa
As explained above, $f(\mathbf{M})$ is a continuous function in the entries $\{m_{ij};\, 1\leq i\leq j \leq n\}$ of the matrix $\mathbf{M}$. 
Since $f(\mathbf{A})>0$, there exist $\{\delta_{ij}>0;\, 1\leq i, j\leq n\}$ with $\delta_{ij}=\delta_{ji}$ for all $1\leq i, j\leq n$ such that $f(\mathbf{M})>0$ for any entry $m_{ij}\in [a_{ij}, a_{ij}+\delta_{ij}]$ where $1\leq i, j\leq n$.
Set $\delta := \min\{\delta_{ij};\, 1\leq i\leq j\leq n\}.$ Then, $\delta>0$. Also, $\lambda_n(\mathbf{M})>0$ and $\lambda_n(\mathbf{M}^{(\alpha)})<0$ for every $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and every $1\leq i, j\leq n.$ 
In summary, this boils down to observing that $\mathbf{M}>0$  and $\mathbf{M}^{(\alpha)} \ngeqslant 0$ for any $m_{ij}\in [a_{ij}, a_{ij}+\delta]$ and any $1\leq i, j\leq n.$ which concludes the proof of our Lemma.
\hfill$\square$
\medskip

We now derive a similar result true for any non-square matrix whose entries supports are within some specific range.
Note that the following Lemma holds for any matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$.
\begin{lemma}\lbl{lemma:lem2} Let $\mathbf{X}=(x_{ij})_{n\times p}$ be an $n\times p$ matrix. For any $n$ and $p$ with $n\geq p\geq 4$, there exist $\alpha \in (1,2)$, $\delta>0$ and $n\times p$ matrix  $\mathbf{A}=(a_{ij})$ with $a_{ij}\geq 0$ for all $1\leq i\leq n$ and $1\leq  j \leq p$ such that the following holds.
%the $n\times n$ matrix $\mathbf{M}^{(\alpha)}=(m_{ij}^{\alpha})$ has the following property.

(i)  The matrix $\mathbf{X}'\mathbf{X}$ is positive definite for every $x_{ij}\in [a_{ij}, a_{ij}+\delta]$ and $1\leq i\leq n$ and $1\leq j \leq p.$

(ii) The Hadamard power $(\mathbf{X}'\mathbf{X})^{(\alpha)}\ngeqslant 0$  for any $x_{ij}\in [a_{ij}, a_{ij}+\delta]$, $1\leq i\leq n$ and $1\leq j \leq p$.
\end{lemma}
\medskip

\noindent\textbf{Proof of Lemma~\ref{lemma:lem2}}. 
Let $\mathbf{a}^T=(1,1,1,1)$ and $\mathbf{b}^T=(0,1,2,3)$ be as in Example \ref{good_example}.
%Set $\mathbf{M}=\mathbf{a}\mathbf{a}^T+\mathbf{b}\mathbf{b}^T.$
It can be shown that the Hadamard power $(\mathbf{a}\mathbf{a}^T+\mathbf{b}\mathbf{b}^T)^{(1.1)}$ has determinant $-1.1856\times 10^{-4}.$
% eigenvalues $-0.0002, 0.0221, 1.2815$ and $20.3030.$
Set
\beaa
\mathbf{A}(\epsilon)=
%\begin{pmatrix}
%1 & 0& 0 & 0\\
%1 & 1& 0 & 0\\
%1 & 2 & \epsilon & 0\\
%1 & 3& 0 & \epsilon
%\end{pmatrix}
\begin{pmatrix}
1 & 1& 1 & 1\\
0 & 1& 2 & 3\\
0 & 0 & \epsilon & 0\\
0 & 0& 0 & \epsilon
\end{pmatrix}
,\ \ \ \epsilon>0.
\eeaa
We can observe that $\lim_{\epsilon \to 0^+}[\mathbf{A}(\epsilon)^T\mathbf{A}(\epsilon)]^{(1.1)}=(\mathbf{a}\mathbf{a}^T+\mathbf{b}\mathbf{b}^T)^{1.1}$ with the entrywise convergence. 
By continuity of determinants, there exists $\epsilon_0>0$ such that the determinant of $[\mathbf{A}(\epsilon_0)^T\mathbf{A}(\epsilon_0)]^{(1.1)}$ is negative. That is,  $\mathbf{A}(\epsilon_0)^T\mathbf{A}(\epsilon_0)>0$ but the Hadmard power $[\mathbf{A}(\epsilon_0)^T\mathbf{A}(\epsilon_0)]^{(1.1)} \ngeqslant 0.$ Now we define an $n\times p$ matrix $\mathbf{A}$ such that
%$\mathbf{A}=\mathbf{A}$  for $n=4$ and
\beaa
\mathbf{A}=
\begin{pmatrix}
  \mathbf{A}(\epsilon_0) & \mathbf{0}\\
\mathbf{0} & \mathbf{I}_{p-4}\\
\mathbf{0} & \mathbf{0}
\end{pmatrix}
_{n\times p},
\eeaa
where the size of each submatrix $\mathbf{0}$ of $\mathbf{A}$ can be derived from those of $\mathbf{A}(\epsilon_0)$ and $\mathbf{I}_{p-4}.$ In particular, the size of the  ``$\mathbf{0}$" in the bottom-right of $\mathbf{A}$ is $(n-p)\times (p-4).$ In case $n=p$, there is no  third row of submatrices in $\mathbf{A}$; in case $p=4$, there is no  second row of submatrices of $\mathbf{A}$. Since we have
\beaa
\mathbf{A}^T\mathbf{A}
=
\begin{pmatrix}
\mathbf{A}(\epsilon_0)^T\mathbf{A}(\epsilon_0) & \mathbf{0}\\
\mathbf{0} & \mathbf{I}_{p-4}
\end{pmatrix}
.
\eeaa

Hence, $\mathbf{A}^T\mathbf{A}>0$, nevertheless, its Hadamard power, of exponent $1.1$ satisfies $(\mathbf{A}^T\mathbf{A})^{(1.1)} \ngeqslant 0$.
The inequality \eqref{flower_could} shows that the smallest eigenvalue $\lambda_n(\mathbf{M})$ of $\mathbf{M}=\mathbf{X}\mathbf{X}^T$ is a continuous function of the entries of $\mathbf{M}$, which in turn are the continuous functions of the entries of $\mathbf{X}$. 
Let $\mathbf{X}=(x_{ij})_{n\times p}$.
Hence, $\lambda_n(\mathbf{M})$ is a continuous function of the entries $x_{ij}$'s. 
Set
 \beaa
f(\mathbf{M}):=\min\big\{\lambda_n(\mathbf{M}),\, -\lambda_n(\mathbf{M}^{(\alpha)})\big\}.
\eeaa
Then $f(\mathbf{M})$ is a continuous function of $x_{ij}$'s and $f(\mathbf{A}\mathbf{A}^T)>0.$ Write  $\mathbf{A}=(a_{ij})_{n\times p}$. Then there exist $\delta_{ij}>0$ for all $1\leq i\leq n$ and $1\leq j \leq p$ such that $f(\mathbf{M})>0$ for all $x_{ij}\in [a_{ij}, a_{ij}+\delta_{ij}]$ with $1\leq i\leq n$ and $1\leq j \leq p$. Denote $\delta=\min\{\delta_{ij};\, 1\leq i\leq n, 1\leq j \leq p\}.$ Then $\delta>0$ and $f(\mathbf{X}\mathbf{X}^T)>0$ for all $x_{ij}\in [a_{ij}, a_{ij}+\delta]$ with $1\leq i\leq n$ and $1\leq j \leq p$. Hence, under these restrictions of $x_{ij}$'s, we have $\lambda_n(\mathbf{X}\mathbf{X}^T)>0$ and $\lambda_n((\mathbf{X}\mathbf{X}^T)^{(\alpha)}) < 0$. This yields the proofs for (i) and (ii) of Lemma~\ref{lemma:lem2}.~\hfill$\square$

\medskip


\subsection{Proofs of Theorems ~\ref{th:th1}-\ref{th:th2}-\ref{th:th3}}
The proof of Theorem~\ref{th:th1}, dealing with non-square deterministic matrices, reads as follows:

\noindent\textbf{Proof of Theorem~\ref{th:th1}}. 
By the Gershgorin Disk Theorem (see for instance \cite{horn1985}), all eigenvalues of $\mathbf{A}$ are in the set
\bea\lbl{bro_pro}
\bigcup_{1\leq i \leq n}\Big(a_{ii}- \sum_{j\ne i}a_{ij}, a_{ii}+ \sum_{j\ne i}a_{ij}\Big).
\eea
By assumption, all eigenvalues are non-negative, hence $\mathbf{A}\geq 0$. On the other hand,
\beaa
a_{ii}^{\alpha}\geq  \Big(\sum_{j\ne i}a_{ij}\Big)^{\alpha}\geq \sum_{j\ne i}a_{ij}^{\alpha}
\eeaa
for all $\alpha\geq 1$ by the given condition. 
Using the Gershgorin Disk theorem again, all of the eigenvalues of the Hadamard power matrix $\mathbf{A}^{(\alpha)}$ are non-negative. 
Therefore,  it is set that $\mathbf{A}^{(\alpha)}\geq  0$.
Evidently, if $a_{ii}>  \sum_{j\ne i}a_{ij}$ for each $1\leq i\leq n$ then all of the eigenvalues of  $\mathbf{A}$  and $\mathbf{A}^{(\alpha)}$ are positive by \eqref{bro_pro} with ``$a_{ij}$" being replaced by $a_{ij}^{\alpha}$ for all $i$ and $j$. Hence $\mathbf{A} > 0$ and $\mathbf{A}^{(\alpha)}> 0$ for all $\alpha\geq 1.$ \hfill$\square$
\medskip


\noindent\textbf{Proof of Theorem~\ref{th:th2}}. Based on the results obtained in Lemma~\ref{lemma:lem1}, we now give a detailed proof of Theorem~\ref{th:th2}. 
Let $\delta >0$ be the value defined in Lemma~\ref{lemma:lem1} as the length of the interval defining the supports of the entries of a matrix. 
Since $[a_{ij}+\frac{1}{2}\delta, a_{ij}+\delta] \subset [a_{ij}, a_{ij}+\delta]$  for each pair of $(i, j)$ with $1\leq i\leq j \leq n$, then Lemma~\ref{lemma:lem1} still holds if we strengthen its conclusion with the requirement of having $a_{ij}>0$ for all $1\leq i\leq j \leq n$.
Therefore, we obtain
\bea\lbl{poker}
0 < \alpha < \beta \, , \notag
\eea
where $\alpha:=\min\{a_{ij};\, 1\leq i\leq j \leq n\}$ and $\beta:=\max\{a_{ij};\, 1\leq i\leq j \leq n\}+\delta$ .
In the sequel, we denote by $\mbox{supp}(\xi)$, the support of a random variable $\xi$.
In particular, we have that $P(a\leq \xi\leq  b)>0$ provided $[a, b]\subset \mbox{supp}(\xi)$.
Notice that $\mbox{supp}(\lambda\xi_{ij})=\lambda\cdot \mbox{supp}(\xi_{ij})$ for each $i, j$ where $1\leq i\leq j \leq n$. 
Choose $\lambda>0$ such that $\lambda [u, v]\supset [\alpha , \beta]$. 
Then, it follows that
\bea\lbl{red}
& \bigcup_{1\leq i\leq  j\leq n}[a_{ij}, a_{ij}+\delta] \subset [\alpha, \beta]\\\notag
& \subset \bigcap_{1\leq i\leq  j\leq n}  \mbox{supp}(\lambda\xi_{ij}).\notag
\eea
Besides, observe that
\bea\label{eq:th2proba}
& P\big(\mathbf{M}>0\ \mbox{and}\ \mathbf{M}^{(\alpha)}\ngeqslant 0\big)\\\notag
& = P\big(\lambda\mathbf{M}>0\ \mbox{and}\ (\lambda\mathbf{M})^{(\alpha)}\ngeqslant 0\big).\notag
\eea
By Lemma~\ref{lemma:lem1} and the independence assumption, the last probability in \eqref{eq:th2proba} reads
\begin{align}\notag
& P(\lambda \xi_{ij}\in [a_{ij}, a_{ij}+\delta]\ \mbox{for each}\ 1\leq i\leq j \leq n)\\\notag
=& \prod_{1\leq i\leq j \leq n}P(\lambda\xi_{ij}\in [a_{ij}, a_{ij}+\delta])>0,\notag
\end{align}
where the last inequality comes from \eqref{red}. 
The proof is thus complete. \hfill$\square$

\medskip



Having established Lemma~\ref{lemma:lem2} leads to the following proof of Theorem~\ref{th:th3}:

\noindent\textbf{Proof of Theorem~\ref{th:th3}}. Let $\delta >0$ be the strictly positive value defined in Lemma~\ref{lemma:lem2}. 
Leveraging the same argument as in \eqref{poker} (see Proof of Theorem~\ref{th:th2}), and without loss of generality, we assume that $a_{ij}>0$ for all $1\leq i\leq n$ and $1\leq j \leq p$. 
Therefore,
\beaa
0 < \alpha < \beta \, , \notag \, 
\eeaa
where $\alpha:=\min\{a_{ij};\, 1\leq i \leq n, 1\leq j \leq p\}$ and $\beta:=\max\{a_{ij};\, 1\leq i\leq n, 1\leq j \leq p\}+\delta$.
By choosing $\lambda>0$ such that $\lambda [u, v]\supset [\alpha , \beta]$, we then have
\bea\lbl{red1}
\bigcup [a_{ij}, a_{ij}+\delta] \subset [\alpha, \beta]\subset \bigcap   \mbox{supp}(\lambda x_{ij}),
\eea
where the union and the intersection are taken over $1\leq i \leq n$ and $1\leq j \leq p.$ 
Let $\mathbf{X}=(x_{ij})$ be an $n\times p $ matrix.
By setting $\mathbf{Y}=\lambda\mathbf{X}$, we have
\beaa
& P\big(\mathbf{X}^T\mathbf{X}>0\ \mbox{and}\ (\mathbf{X}^T\mathbf{X})^{(\alpha)}\ngeqslant 0\big)\\
& =  P\big(\mathbf{Y}^T\mathbf{Y}>0\ \mbox{and}\ (\mathbf{Y}^T\mathbf{Y})^{(\alpha)}\ngeqslant 0\big).
\eeaa
From Lemma~\ref{lemma:lem2}, then for $1\leq i\leq n$ and $1\leq j \leq p$, the above is at least following the relationship below:
\begin{align}\notag
& P\big(\lambda x_{ij}\in [a_{ij}, a_{ij}+\delta]\big)\\ \notag
=& \prod_{1\leq i\leq n, 1\leq j \leq p}P\big(\lambda x_{ij}\in [a_{ij}, a_{ij}+\delta]\big)>0\notag
\end{align}
where the last step follows from \eqref{red1} and the independence assumption of $\{\xi_{ij};\, 1\leq i \leq j \leq n\}$, hence concluding our proof. \hfill$\square$



\section{Conclusion}\lbl{sec:conclusion}

We have studied in this paper the impact of the Hadamard product, and especially the Hadamard powers, on positive definite deterministic and random matrices.
Our main results yield the existence of an interval of non integer values for the exponent of the Hadamard powers such that for any positive definite random matrice, up to a mild assumption on the support of its entries, has non non-negative Hadamard powers.
Our initial results pave the way for a better understanding of kernel learning problems where the exponent, of either polynomial kernel or Gaussian Radial Basis Function (RBF), is considered as a tuning parameter.


% \noindent\textbf{Acknowledgements}. We thank Hongru Zhao very much for very fruitful   discussions.

\clearpage

\bibliographystyle{IEEEtran}
\bibliography{references}

%
%\begin{thebibliography}{99}
%
%\bibitem{Hiai}
%Hiai, F. (2009).  Monotonicity for entrywise functions of matrices. {\it Linear  Algebra  Appl.} 431(8), 1125-1146.
%
%\bibitem{s1} Horn, R. and Johnson, C. (1991). {\em Topics in matrix analysis}. Cambridge University Press, Cambridge.
%
%
%\bibitem{s2} Horn, R. and Johnson, C. (1985). {\em Matrix Analysis}. Cambridge Univesity Press, Cambridge.
%
%\bibitem{s3}
%Schoenberg, I. J. (1942). Positive definite functions on spheres. {\it Duke Math. J.}   9, 96-108.
%
%\bibitem{s4}
%Schur, J. (1911). Bemerkungen zur theorie der beschr\"{a}nkten bilinearformen mit unendlich vielenver\"{a}nderlichen. {\it Journal f\"{u}r die reine und angewandte Mathematik} 140, 1-28.
%
%\bibitem{s5} Vasudeva, H. (1979).  Positive definite matrices and absolutely monotonic functions. {\it Indian J. Pure Appl. Math.} 10(7), 854-858.
%
%\bibitem{horadam2012hadamard} Horadam, K. J. (2012).  Hadamard matrices and their applications. {\it Princeton university press}
%
%
%\end{thebibliography}




\end{document}
