\begin{thebibliography}{10}

\bibitem{Proc:Bassily_FOCS14}
Raef Bassily, Adam~D. Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In {\em Proceedings of the 55th {IEEE} Annual Symposium on
  Foundations of Computer Science (FOCS)}, pages 464--473, Philadelphia, PA,
  2014.

\bibitem{Article:Bousquet_JMLR02}
Olivier Bousquet and Andr{\'{e}} Elisseeff.
\newblock Stability and generalization.
\newblock {\em J. Mach. Learn. Res.}, 2:499--526, 2002.

\bibitem{Proc:Bowman_EMNLP15}
Samuel~R. Bowman, Gabor Angeli, Christopher Potts, and Christopher~D. Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In {\em Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 632--642, Lisbon, Portugal, 2015.

\bibitem{Article:Chaudhuri_JMLR11}
Kamalika Chaudhuri, Claire Monteleoni, and Anand~D. Sarwate.
\newblock Differentially private empirical risk minimization.
\newblock {\em J. Mach. Learn. Res.}, 12:1069--1109, 2011.

\bibitem{Proc:Chee_AISTATS18}
Jerry Chee and Panos Toulis.
\newblock Convergence diagnostics for stochastic gradient descent with constant
  learning rate.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 1476--1485, Playa Blanca, Lanzarote, Canary
  Islands, Spain, 2018.

\bibitem{Proc:Chen_IJCAI20}
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock In {\em Proceedings of the Twenty-Ninth International Joint
  Conference on Artificial Intelligence (IJCAI)}, pages 3267--3275, 2020.

\bibitem{Proc:Chen_ICLR19}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{Proc:Conneau_EMNLP17}
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo{\"{\i}}c Barrault, and Antoine
  Bordes.
\newblock Supervised learning of universal sentence representations from
  natural language inference data.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 670--680, Copenhagen, Denmark,
  2017.

\bibitem{Proc:Duchi_JMLR11}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em J. Mach. Learn. Res.}, 12:2121--2159, 2011.

\bibitem{Proc:Dwork_NIPS15}
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold,
  and Aaron Roth.
\newblock Generalization in adaptive data analysis and holdout reuse.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 2350--2358, Montreal, Quebec, Canada, 2015.

\bibitem{dwfe2015b}
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold,
  and Aaron Roth.
\newblock The reusable holdout: Preserving validity in adaptive data analysis.
\newblock {\em Science}, 349(6248):636--638, 2015.

\bibitem{Proc:Dwork_STOC15}
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold,
  and Aaron~Leon Roth.
\newblock Preserving statistical validity in adaptive data analysis.
\newblock In {\em Proceedings of the Forty-Seventh Annual {ACM} on Symposium on
  Theory of Computing (STOC)}, pages 117--126, Portland, OR, 2015.

\bibitem{Article:Dwork_2014}
Cynthia Dwork and Aaron Roth.
\newblock The algorithmic foundations of differential privacy.
\newblock {\em Found. Trends Theor. Comput. Sci.}, 9(3-4):211--407, 2014.

\bibitem{Article:Ghadimi_SJO13}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em {SIAM} J. Optim.}, 23(4):2341--2368, 2013.

\bibitem{Proc:Hardt_ICML16}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning (ICML)}, pages 1225--1234, New York City, NY, 2016.

\bibitem{Proc:He_CVPR16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the 2016 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 770--778, Las Vegas, NV, 2016.

\bibitem{keso2017}
Nitish~Shirish Keskar and Richard Socher.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock {\em arXiv preprint arXiv:1712.07628}, 2017.

\bibitem{Proc:Kingma_ICLR15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem{krhi2009}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{Proc:Kuzborskij_ICML18}
Ilja Kuzborskij and Christoph~H. Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 2820--2829, Stockholmsm{\"{a}}ssan, Stockholm,
  Sweden, 2018.

\bibitem{Article:Lecun_IEEE98}
Y.~{Lecun}, L.~{Bottou}, Y.~{Bengio}, and P.~{Haffner}.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{Proc:Li_ICLR20}
Jian Li, Xuanyuan Luo, and Mingda Qiao.
\newblock On generalization error bounds of noisy gradient methods for
  non-convex learning.
\newblock In {\em Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}, Addis Ababa, Ethiopia, 2020.

\bibitem{Proc:Luo_ICLR19}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{Article:Marcus_CL93}
Mitchell~P. Marcus, Beatrice Santorini, and Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock {\em Comput. Linguistics}, 19(2):313--330, 1993.

\bibitem{Proc:Merity_ICLR18}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem{Proc:Mou_COLT18}
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng.
\newblock Generalization bounds of {SGLD} for non-convex learning: Two
  theoretical viewpoints.
\newblock In {\em Conference On Learning Theory (COLT)}, pages 605--638,
  Stockholm, Sweden, 2018.

\bibitem{Proc:Pensia_ISIT18}
Ankit Pensia, Varun Jog, and Po{-}Ling Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In {\em Proceedings of the 2018 {IEEE} International Symposium on
  Information Theory (ISIT)}, pages 546--550, Vail, CO, 2018.

\bibitem{Article:Qian_NN99}
Ning Qian.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock {\em Neural Networks}, 12(1):145--151, 1999.

\bibitem{Proc:Raginsky_COLT17}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In {\em Proceedings of the 30th Conference on Learning Theory
  (COLT)}, pages 1674--1703, Amsterdam, The Netherlands, 2017.

\bibitem{Proc:Reddi_ICLR18}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem{Article:Robbins_1951}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The Annals of Mathematical Statistics}, 22(3):400--407, 1951.

\bibitem{Book:Shwartz_2014}
Shai Shalev{-}Shwartz and Shai Ben{-}David.
\newblock {\em Understanding Machine Learning - From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem{Proc:Simonyan_ICLR15}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem{tige12}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 2012.

\bibitem{Proc:Wang_AAAI19}
Di~Wang and Jinhui Xu.
\newblock Differentially private empirical risk minimization with smooth
  non-convex loss functions: {A} non-stationary view.
\newblock In {\em The Thirty-Third {AAAI} Conference on Artificial Intelligence
  (AAAI)}, pages 1182--1189, Honolulu, HI, 2019.

\bibitem{Proc:Wang_NIPS17}
Di~Wang, Minwei Ye, and Jinhui Xu.
\newblock Differentially private empirical risk minimization revisited: Faster
  and more general.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 2722--2731, Long Beach, CA, 2017.

\bibitem{Proc:Ward_ICML19}
Rachel Ward, Xiaoxia Wu, and L{\'{e}}on Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 6677--6686, Long Beach, CA, 2019.

\bibitem{Proc:Wilson_NIPS17}
Ashia~C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 4148--4158, Long Beach, CA, 2017.

\bibitem{Proc:Zaheer_NeurIPS18}
Manzil Zaheer, Sashank~J. Reddi, Devendra~Singh Sachan, Satyen Kale, and Sanjiv
  Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 9815--9825, Montr{\'{e}}al, Canada, 2018.

\bibitem{zhta18}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em arXiv preprint arXiv:1808.05671}, 2018.

\bibitem{Proc:Zhou_UAI18}
Yingxue Zhou, Sheng Chen, and Arindam Banerjee.
\newblock Stable gradient descent.
\newblock In {\em Proceedings of the Thirty-Fourth Conference on Uncertainty in
  Artificial Intelligence (UAI)}, pages 766--775, Monterey, CA, 2018.

\bibitem{Proc:Zou_CVPR19}
Fangyu Zou, Li~Shen, Zequn Jie, Weizhong Zhang, and Wei Liu.
\newblock A sufficient condition for convergences of adam and rmsprop.
\newblock In {\em Proceedings of {IEEE} Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 11127--11135, Long Beach, CA, 2019.

\end{thebibliography}
