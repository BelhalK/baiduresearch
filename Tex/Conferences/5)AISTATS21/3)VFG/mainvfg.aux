\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{madigan1995bayesian}
\citation{hruschka2007bayesian}
\citation{koller2007graphical}
\citation{bilmes2005graphical}
\citation{shwe1990probabilistic}
\citation{jordan1999graphical}
\citation{madigan1995bayesian}
\citation{hruschka2007bayesian}
\citation{sanner2012symbolic}
\citation{kahle2008junction}
\citation{jordan1999introduction}
\citation{salimans2015markov}
\citation{hoffman2013stochastic}
\citation{kingma2013auto}
\citation{liu2016stein}
\citation{xing2012generalized}
\citation{winn2005variational}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kingma2018glow}
\citation{rezende2015variational}
\citation{tabak2010density}
\citation{Dinh2016DensityEU}
\citation{rippel2013high}
\citation{rezende2015variational}
\citation{Dinh2016DensityEU}
\citation{dinh2014nice}
\citation{de2020block}
\citation{ho2019flow++}
\citation{papamakarios2019normalizing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\newlabel{eq:flow}{{1}{2}{Preliminaries}{equation.2.1}{}}
\citation{bishop2003vibes}
\citation{Dinh2016DensityEU}
\newlabel{eq:vi_elbo}{{2}{3}{Preliminaries}{equation.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variational Flow Graphical Model}{3}{section.3}}
\newlabel{sec:main}{{3}{3}{Variational Flow Graphical Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Evidence Lower Bound of Variational Flow Graphical Models}{3}{subsection.3.1}}
\newlabel{eq:elbo_tree}{{3}{3}{The Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  {\relax \fontsize  {9}{10pt}\selectfont  (Left) The structure of one node. Node $\mathbf  {h}^{2, 1}$ connects with its children with invertible functions. The messages from its children are aggregated at $\mathbf  {h}^{2,1}$. (Right) An illustration of the latent structure from layer $l-1$ to $l+1$. $\mathbf  {h}^{l, i}$ means the $i$th latent variable in layer $l$.}\relax }}{4}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:node_tree}{{1}{4}{{\small (Left) The structure of one node. Node $\mathbf {h}^{2, 1}$ connects with its children with invertible functions. The messages from its children are aggregated at $\mathbf {h}^{2,1}$. (Right) An illustration of the latent structure from layer $l-1$ to $l+1$. $\mathbf {h}^{l, i}$ means the $i$th latent variable in layer $l$.}\relax }{figure.caption.2}{}}
\newlabel{eq:elbo_dag}{{4}{4}{The Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Aggregation with average. $\mathbf  {h}^{(r)}$ has three children, $\mathbf  {h}^{(1)}$, $\mathbf  {h}^{(2)}$, and $\mathbf  {h}^{(3)}$.\relax }}{4}{figure.caption.3}}
\newlabel{fig:node_aggre}{{2}{4}{Aggregation with average. $\mathbf {h}^{(r)}$ has three children, $\mathbf {h}^{(1)}$, $\mathbf {h}^{(2)}$, and $\mathbf {h}^{(3)}$.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Node Aggregation}{4}{subsection.3.2}}
\newlabel{sec:node_aggr}{{3.2}{4}{Node Aggregation}{subsection.3.2}{}}
\newlabel{eq:one_agg_node}{{5}{4}{Node Aggregation}{equation.3.5}{}}
\newlabel{eq:reconstr}{{7}{4}{Node Aggregation}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Inference on Sub-graphs }{5}{subsection.3.3}}
\newlabel{sec:infer}{{3.3}{5}{Inference on Sub-graphs}{subsection.3.3}{}}
\newlabel{eq:aggr_obs_ch}{{8}{5}{Inference on Sub-graphs}{equation.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\relax \fontsize  {9}{10pt}\selectfont  (Left) Inference of single aggregation node model. Node 4 aggregates from node 1 and 2, and pass the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered in node 5 to predict the state of node 4.}\relax }}{5}{figure.caption.4}}
\newlabel{fig:two_layer_infer}{{3}{5}{{\small (Left) Inference of single aggregation node model. Node 4 aggregates from node 1 and 2, and pass the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered in node 5 to predict the state of node 4.}\relax }{figure.caption.4}{}}
\newlabel{lm:apprx}{{1}{5}{}{lemma.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inference model parameters with forward and backward message propagation\relax }}{5}{algorithm.1}}
\newlabel{alg:main}{{1}{5}{Inference model parameters with forward and backward message propagation\relax }{algorithm.1}{}}
\newlabel{line:for2}{{4}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.4}{}}
\newlabel{line:forward}{{6}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.6}{}}
\newlabel{line:backward}{{11}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.11}{}}
\newlabel{line:update}{{15}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.15}{}}
\newlabel{eq:cond_llk}{{9}{5}{}{equation.3.9}{}}
\newlabel{rmk:apprx_mul}{{1}{5}{}{remark.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Algorithm and Implementation}{5}{subsection.3.4}}
\citation{Khemakhem20a}
\citation{Sorrenson2020}
\citation{efron1975defining}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Layer-wise Training}{6}{subsubsection.3.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Random Masking}{6}{subsubsection.3.4.2}}
\newlabel{eq:elbo_tree_mask}{{10}{6}{Random Masking}{equation.3.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Inference model parameters with random masking\relax }}{6}{algorithm.2}}
\newlabel{alg:rand_mask}{{2}{6}{Inference model parameters with random masking\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theoretical Justifications for Latent Representation Learning}{6}{section.4}}
\newlabel{sec:theory}{{4}{6}{Theoretical Justifications for Latent Representation Learning}{section.4}{}}
\newlabel{eq:exp_h}{{11}{6}{Theoretical Justifications for Latent Representation Learning}{equation.4.11}{}}
\citation{bengio2013representation}
\citation{Dinh2016DensityEU}
\citation{ch_sklearn}
\newlabel{eq:xt_gen}{{12}{7}{Theoretical Justifications for Latent Representation Learning}{equation.4.12}{}}
\newlabel{thm:identif}{{1}{7}{}{theorem.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{7}{section.5}}
\newlabel{sec:numerical}{{5}{7}{Numerical Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Missing Entries Imputation}{7}{subsection.5.1}}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{Sorrenson2020}
\citation{maaten2008visualizing}
\bibstyle{plain}
\bibdata{references}
\bibcite{ch_sklearn}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }}{8}{figure.caption.5}}
\newlabel{fig:sim}{{4}{8}{Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }}{8}{table.caption.6}}
\newlabel{tab:imp_arrhytmia}{{1}{8}{California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Latent Representation Learning on MNIST}{8}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }}{8}{figure.caption.7}}
\newlabel{fig:reconst}{{5}{8}{(Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }}{8}{figure.caption.8}}
\newlabel{fig:z_tsne}{{6}{8}{MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}}
\newlabel{sec:conclusion}{{6}{8}{Conclusion}{section.6}{}}
\bibcite{bengio2013representation}{2}
\bibcite{bilmes2005graphical}{3}
\bibcite{bishop2003vibes}{4}
\bibcite{de2020block}{5}
\bibcite{dinh2014nice}{6}
\bibcite{Dinh2016DensityEU}{7}
\bibcite{efron1975defining}{8}
\bibcite{ho2019flow++}{9}
\bibcite{hoffman2013stochastic}{10}
\bibcite{hruschka2007bayesian}{11}
\bibcite{jordan1999graphical}{12}
\bibcite{jordan1999introduction}{13}
\bibcite{kahle2008junction}{14}
\bibcite{Khemakhem20a}{15}
\bibcite{kingma2013auto}{16}
\bibcite{kingma2018glow}{17}
\bibcite{koller2007graphical}{18}
\bibcite{lecun-mnisthandwrittendigit-2010}{19}
\bibcite{liu2016stein}{20}
\bibcite{maaten2008visualizing}{21}
\bibcite{madigan1995bayesian}{22}
\bibcite{papamakarios2019normalizing}{23}
\bibcite{rezende2015variational}{24}
\bibcite{rippel2013high}{25}
\bibcite{salimans2015markov}{26}
\bibcite{sanner2012symbolic}{27}
\bibcite{shwe1990probabilistic}{28}
\bibcite{Sorrenson2020}{29}
\bibcite{tabak2010density}{30}
\bibcite{winn2005variational}{31}
\bibcite{xing2012generalized}{32}
\@writefile{toc}{\contentsline {section}{\numberline {A}Derivation of the ELBO for both Tree and DAG structures}{11}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}ELBO of Tree Models}{11}{subsection.A.1}}
\newlabel{appd:tree_elbo}{{A.1}{11}{ELBO of Tree Models}{subsection.A.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Tree structure.\relax }}{11}{figure.caption.10}}
\newlabel{fig:tree-d}{{7}{11}{Tree structure.\relax }{figure.caption.10}{}}
\newlabel{eq:chain}{{13}{11}{ELBO of Tree Models}{equation.A.13}{}}
\newlabel{eq:tree_elbo}{{14}{12}{ELBO of Tree Models}{equation.A.14}{}}
\newlabel{eq:layer0_a}{{15}{12}{ELBO of Tree Models}{equation.A.15}{}}
\newlabel{eq:layer0_b}{{16}{12}{ELBO of Tree Models}{equation.A.16}{}}
\newlabel{eq:kl_1}{{17}{12}{ELBO of Tree Models}{equation.A.17}{}}
\newlabel{eq:kl_a}{{18}{12}{ELBO of Tree Models}{equation.A.18}{}}
\newlabel{eq:a_inner}{{19}{12}{ELBO of Tree Models}{equation.A.19}{}}
\newlabel{eq:kl_b}{{20}{12}{ELBO of Tree Models}{equation.A.20}{}}
\newlabel{eq:KL_tree}{{21}{13}{ELBO of Tree Models}{equation.A.21}{}}
\newlabel{eq:KL_all}{{22}{13}{ELBO of Tree Models}{equation.A.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}ELBO for DAG Models}{13}{subsection.A.2}}
\newlabel{appd:dag_elbo}{{A.2}{13}{ELBO for DAG Models}{subsection.A.2}{}}
\newlabel{eq:dag_elbo}{{23}{13}{ELBO for DAG Models}{equation.A.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces DAG structure. The inverse topology order is {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \{\vcenter to\@ne \big@size {}\right .$}\box \z@ } \{1,2,3\}, \{4,5\}, \{6\}, \{7\} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \}\vcenter to\@ne \big@size {}\right .$}\box \z@ }, and it corresponds to layers 0 to 3. \relax }}{14}{figure.caption.11}}
\newlabel{fig:dag}{{8}{14}{DAG structure. The inverse topology order is \big \{ \{1,2,3\}, \{4,5\}, \{6\}, \{7\} \big \}, and it corresponds to layers 0 to 3. \relax }{figure.caption.11}{}}
\newlabel{eq:KL_dag1}{{24}{14}{ELBO for DAG Models}{equation.A.24}{}}
\newlabel{eq:KL_dag2}{{25}{14}{ELBO for DAG Models}{equation.A.25}{}}
\newlabel{eq:kl_dag3}{{26}{14}{ELBO for DAG Models}{equation.A.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Theoretical Proofs}{14}{appendix.B}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Proof of Lemma\nobreakspace  {}\ref  {lm:apprx}}{14}{subsection.B.1}}
\newlabel{appd:proof_lm1}{{B.1}{14}{Proof of Lemma~\ref {lm:apprx}}{subsection.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Message passing on a tree.\relax }}{15}{figure.caption.12}}
\newlabel{fig:message}{{9}{15}{Message passing on a tree.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proof of Theorem\nobreakspace  {}\ref  {thm:identif}}{15}{subsection.B.2}}
\newlabel{appd:proof_thm1}{{B.2}{15}{Proof of Theorem~\ref {thm:identif}}{subsection.B.2}{}}
\newlabel{eq:u_diff}{{27}{15}{Proof of Theorem~\ref {thm:identif}}{equation.B.27}{}}
\citation{Khemakhem20a}
\newlabel{eq:A_sim}{{29}{16}{Proof of Theorem~\ref {thm:identif}}{equation.B.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Additional Numerical Experiments}{16}{appendix.C}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Latent Representation Learning on MNIST}{16}{subsection.C.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces t-SNE plot of latent variables learned with VFG without labels.\relax }}{16}{figure.caption.13}}
\newlabel{fig:z_no_Y}{{10}{16}{t-SNE plot of latent variables learned with VFG without labels.\relax }{figure.caption.13}{}}
