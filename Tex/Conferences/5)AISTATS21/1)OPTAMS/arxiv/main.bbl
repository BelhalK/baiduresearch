\begin{thebibliography}{10}

\bibitem{Proc:Abernethy_COLT18}
Jacob~D. Abernethy, Kevin~A. Lai, Kfir~Y. Levy, and Jun{-}Kun Wang.
\newblock Faster rates for convex-concave games.
\newblock In {\em Proceedings of Conference On Learning Theory (COLT)}, pages
  1595--1625, Stockholm, Sweden, 2018.

\bibitem{Proc:Agarwal_ICML19}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 102--110, Long Beach, CA, 2019.

\bibitem{brezinski2013extrapolation}
Claude Brezinski and M~Redivo Zaglia.
\newblock {\em Extrapolation methods: theory and practice}.
\newblock Elsevier, 2013.

\bibitem{cabay1976polynomial}
Stan Cabay and LW~Jackson.
\newblock A polynomial extrapolation method for finding limits and antilimits
  of vector sequences.
\newblock {\em SIAM Journal on Numerical Analysis}, 13(5):734--752, 1976.

\bibitem{Proc:Chen_FODS20}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In {\em Proceedintgs of {ACM-IMS} Foundations of Data Science
  Conference (FODS)}, pages 119--128, Virtual Event, 2020.

\bibitem{Proc:Chen_ICLR19}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{Proc:Chen_Yuan_ICLR19}
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao
  Yang.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{Proc:Chiang_COLT12}
Chao{-}Kai Chiang, Tianbao Yang, Chia{-}Jung Lee, Mehrdad Mahdavi, Chi{-}Jen
  Lu, Rong Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock In {\em Proceedings of the 25th Annual Conference on Learning Theory
  (COLT)}, pages 6.1--6.20, Edinburgh, Scotland, 2012.

\bibitem{Proc:Daskalakis_ICLR18}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem{defossez2020convergence}
Alexandre D{\'e}fossez, L{\'e}on Bottou, Francis Bach, and Nicolas Usunier.
\newblock On the convergence of adam and adagrad.
\newblock {\em arXiv preprint arXiv:2003.02395}, 2020.

\bibitem{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock 2016.

\bibitem{Proc:Duchi_JMLR11}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em J. Mach. Learn. Res.}, 12:2121--2159, 2011.

\bibitem{eddy1979extrapolating}
RP~Eddy.
\newblock Extrapolating to the limit of a vector sequence.
\newblock In {\em Information linkage between applied mathematics and
  industry}, pages 387--396. Elsevier, 1979.

\bibitem{Article:Gers_NC00}
Felix~A. Gers, J{\"{u}}rgen Schmidhuber, and Fred~A. Cummins.
\newblock Learning to forget: Continual prediction with {LSTM}.
\newblock {\em Neural Comput.}, 12(10):2451--2471, 2000.

\bibitem{Article:Ghadimi_SJOPT13}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em {SIAM} J. Optim.}, 23(4):2341--2368, 2013.

\bibitem{Proc:Goodfellow_NIPS14}
Ian~J. Goodfellow, Jean Pouget{-}Abadie, Mehdi Mirza, Bing Xu, David
  Warde{-}Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 2672--2680, Montreal, Canada, 2014.

\bibitem{Proc:Graves_ICASSP13}
Alex Graves, Abdel{-}rahman Mohamed, and Geoffrey~E. Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em Proceedings of the {IEEE} International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 6645--6649,
  Vancouver, Canada, 2013.

\bibitem{hazan2019introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock {\em arXiv preprint arXiv:1909.05207}, 2019.

\bibitem{Proc:He_CVPR16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the 2016 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 770--778, Las Vegas, NV, 2016.

\bibitem{Proc:Kingma_ICLR15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem{Proc:Larochelle_ICML07}
Hugo Larochelle, Dumitru Erhan, Aaron~C. Courville, James Bergstra, and Yoshua
  Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock In {\em Proceedings of the Twenty-Fourth International Conference on
  Machine Learning (ICML)}, pages 473--480, Corvalis, Oregon, 2007.

\bibitem{Article:Levine_JMLR16}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em J. Mach. Learn. Res.}, 17:39:1--39:40, 2016.

\bibitem{Proc:ABC_UAI10}
Ping Li.
\newblock Robust logitboost and adaptive base class (abc) logitboost.
\newblock In {\em Proceedings of the Twenty-Sixth Conference Annual Conference
  on Uncertainty in Artificial Intelligence (UAI)}, pages 302--311, Catalina
  Island, CA, 2010.

\bibitem{Proc:Li_AISTATS19}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In {\em Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 983--992, Naha,
  Okinawa, Japan, 2019.

\bibitem{Proc:McMahan_COLT10}
H.~Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In {\em Proceedings of the 23rd Conference on Learning Theory
  (COLT)}, pages 244--256, Haifa, Israel, 2010.

\bibitem{Proc:Mertikopoulos_ICLR19}
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan{-}Sheng Foo,
  Vijay Chandrasekhar, and Georgios Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem{Proc:Mohri_AISTATS16}
Mehryar Mohri and Scott Yang.
\newblock Accelerating online convex optimization via adaptive prediction.
\newblock In {\em Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 848--856, Cadiz,
  Spain, 2016.

\bibitem{Book:Nesterov_2004}
Yurii~E. Nesterov.
\newblock {\em Introductory Lectures on Convex Optimization - {A} Basic
  Course}, volume~87 of {\em Applied Optimization}.
\newblock Springer, 2004.

\bibitem{Article:Polyak_1964}
B.T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem{Proc:Rakhlin_NIPS13}
Alexander Rakhlin and Karthik Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 3066--3074, Lake Tahoe, NV, 2013.

\bibitem{Proc:Reddi_ICLR18}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem{Article:Scieur_MP20}
Damien Scieur, Alexandre d'Aspremont, and Francis Bach.
\newblock Regularized nonlinear acceleration.
\newblock {\em Math. Program.}, 179(1):47--83, 2020.

\bibitem{scieur2018nonlinear}
Damien Scieur, Edouard Oyallon, Alexandre d'Aspremont, and Francis Bach.
\newblock Nonlinear acceleration of deep neural networks.
\newblock 2018.

\bibitem{Proc:Syrgkanis_NIPS15}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 2989--2997, Montreal, Canada, 2015.

\bibitem{Tieleman2012}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 2012.

\bibitem{tseng2008accelerated}
Paul Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock 2008.

\bibitem{Article:Walker_SJNA11}
Homer~F. Walker and Peng Ni.
\newblock Anderson acceleration for fixed-point iterations.
\newblock {\em {SIAM} J. Numer. Anal.}, 49(4):1715--1735, 2011.

\bibitem{Proc:Ward_ICML19}
Rachel Ward, Xiaoxia Wu, and L{\'{e}}on Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 6677--6686, Long Beach, CA, 2019.

\bibitem{Proc:Yan_IJCAI18}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In {\em Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence (IJCAI)}, pages 2955--2961, Stockholm,
  Sweden, 2018.

\bibitem{Proc:Zaheer_NeurIPS18}
Manzil Zaheer, Sashank~J. Reddi, Devendra~Singh Sachan, Satyen Kale, and Sanjiv
  Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 9815--9825, Montr{\'{e}}al, Canada, 2018.

\bibitem{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock {\em arXiv preprint arXiv:1212.5701}, 2012.

\bibitem{zhou2018convergence}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em arXiv preprint arXiv:1808.05671}, 2018.

\bibitem{Proc:Zhou_NeurIPS20}
Yingxue Zhou, Belhal Karimi, Jinxing Yu, Zhiqiang Xu, and Ping Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{zou2018convergence}
Fangyu Zou and Li~Shen.
\newblock On the convergence of adagrad with momentum for training deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1808.03408}, 2018.

\end{thebibliography}
