\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{RKK18}
\citation{LFDA17}
\citation{Rnet16,goodfellow2014generative}
\citation{Atari13}
\citation{GMH13}
\citation{RKK18}
\citation{KB15}
\citation{TH12}
\citation{Z12}
\citation{D16}
\citation{Proc:Chen_FODS20}
\citation{Proc:Zhou_NeurIPS20}
\citation{DHS11,MS10}
\citation{N04}
\citation{P64}
\citation{P64}
\citation{RKK18}
\citation{KB15}
\citation{CJ12,RS13b,SALS15,ALLW18,mertikopoulos2018optimistic}
\citation{DISZ18}
\citation{RS13b}
\citation{goodfellow2014generative}
\citation{CJ12,RS13b,SALS15}
\citation{DISZ18}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{KB15,RKK18}
\citation{CJ12,RS13b,SALS15,ALLW18}
\citation{H14}
\citation{SALS15}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\newlabel{sec:prelim@cref}{{[section][2][]2}{[1][2][]2}}
\citation{SALS15}
\citation{RS13b}
\citation{KB15}
\citation{P64}
\citation{DHS11}
\citation{RKK18}
\citation{RKK18}
\citation{DHS11}
\citation{KB15}
\citation{KB15}
\citation{RKK18}
\citation{RKK18}
\citation{RS13b}
\newlabel{optFTRL}{{1}{3}{Preliminaries}{equation.2.1}{}}
\newlabel{optFTRL@cref}{{[equation][1][]1}{[1][2][]3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {AMSGrad}\nobreakspace  {}\citep  {RKK18}\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:amsgrad}{{1}{3}{\textsc {AMSGrad}~\citep {RKK18}\relax }{figure.caption.1}{}}
\newlabel{alg:amsgrad@cref}{{[algorithm][1][]1}{[1][3][]3}}
\newlabel{line:maxop}{{7}{3}{\textsc {AMSGrad}~\citep {RKK18}\relax }{ALC@unique.7}{}}
\newlabel{line:maxop@cref}{{[ALC@unique][7][]7}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\textsc  {OPT-AMSGRAD} Algorithm}{3}{section.3}}
\newlabel{sec:opt}{{3}{3}{\textsc {OPT-AMSGRAD} Algorithm}{section.3}{}}
\newlabel{sec:opt@cref}{{[section][3][]3}{[1][3][]3}}
\citation{CJ12}
\citation{DHS11}
\citation{N04}
\citation{P64}
\citation{CJ12,RS13b,SALS15}
\citation{RKK18,KB15}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textsc  {OPT-AMSGrad}\relax }}{4}{algorithm.2}}
\newlabel{alg:optamsgrad}{{2}{4}{\textsc {OPT-AMSGrad}\relax }{algorithm.2}{}}
\newlabel{alg:optamsgrad@cref}{{[algorithm][2][]2}{[1][3][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textsc  {OPT-AMSGrad} underlying structure.\relax }}{4}{figure.caption.2}}
\newlabel{fig:scheme}{{1}{4}{\textsc {OPT-AMSGrad} underlying structure.\relax }{figure.caption.2}{}}
\newlabel{fig:scheme@cref}{{[figure][1][]1}{[1][3][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Convergence Analysis}{4}{section.4}}
\newlabel{sec:analysis}{{4}{4}{Convergence Analysis}{section.4}{}}
\newlabel{sec:analysis@cref}{{[section][4][]4}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Convex Regret Analysis}{4}{subsection.4.1}}
\newlabel{sec:convex}{{4.1}{4}{Convex Regret Analysis}{subsection.4.1}{}}
\newlabel{sec:convex@cref}{{[subsection][1][4]4.1}{[1][4][]4}}
\newlabel{thm:mainconvex}{{1}{4}{}{Theorem.1}{}}
\newlabel{thm:mainconvex@cref}{{[Theorem][1][]1}{[1][4][]4}}
\citation{RKK18}
\citation{RS13b}
\citation{ghadimi2013stochastic}
\newlabel{cor:corollary}{{1}{5}{}{Corollary.1}{}}
\newlabel{cor:corollary@cref}{{[Corollary][1][]1}{[1][5][]5}}
\newlabel{eq:boundAMS}{{2}{5}{Convex Regret Analysis}{equation.4.2}{}}
\newlabel{eq:boundAMS@cref}{{[equation][2][]2}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Finite-Time Analysis in Nonconvex Case}{5}{subsection.4.2}}
\newlabel{eq:minproblem}{{3}{5}{Finite-Time Analysis in Nonconvex Case}{equation.4.3}{}}
\newlabel{eq:minproblem@cref}{{[equation][3][]3}{[1][5][]5}}
\newlabel{eq:random}{{4}{5}{Finite-Time Analysis in Nonconvex Case}{equation.4.4}{}}
\newlabel{eq:random@cref}{{[equation][4][]4}{[1][5][]5}}
\newlabel{ass:boundedparam}{{1}{5}{}{assumption.1}{}}
\newlabel{ass:boundedparam@cref}{{[assumption][1][]1}{[1][5][]5}}
\newlabel{ass:smooth}{{2}{5}{}{assumption.2}{}}
\newlabel{ass:smooth@cref}{{[assumption][2][]2}{[1][5][]5}}
\newlabel{ass:guessbound}{{3}{5}{}{assumption.3}{}}
\newlabel{ass:guessbound@cref}{{[assumption][3][]3}{[1][5][]5}}
\citation{ghadimi2013stochastic}
\citation{ghadimi2013stochastic}
\citation{ZTYCG18}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Assumption H\ref  {ass:guessbound} on gradient prediction.\relax }}{6}{figure.caption.3}}
\newlabel{fig:assumption}{{2}{6}{Assumption H\ref {ass:guessbound} on gradient prediction.\relax }{figure.caption.3}{}}
\newlabel{fig:assumption@cref}{{[figure][2][]2}{[1][5][]6}}
\newlabel{ass:bounded}{{4}{6}{}{assumption.4}{}}
\newlabel{ass:bounded@cref}{{[assumption][4][]4}{[1][6][]6}}
\newlabel{lem:bound}{{1}{6}{}{Lemma.1}{}}
\newlabel{lem:bound@cref}{{[Lemma][1][]1}{[1][6][]6}}
\newlabel{thm:boundopt}{{2}{6}{}{Theorem.2}{}}
\newlabel{thm:boundopt@cref}{{[Theorem][2][]2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Checking H\ref  {ass:boundedparam} for a Deep Neural Network}{6}{subsection.4.3}}
\citation{ZRSKK18,CLSH19,WWB18,ZTYCG18,ZS18,LO18}
\citation{CLSH19}
\citation{Princeton18}
\citation{CYYZC19}
\citation{MY16}
\citation{MY16}
\citation{MY16}
\citation{MY16}
\citation{DISZ18}
\citation{DISZ18}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{DISZ18}
\newlabel{eq:dnnmodel}{{5}{7}{Checking H\ref {ass:boundedparam} for a Deep Neural Network}{equation.4.5}{}}
\newlabel{eq:dnnmodel@cref}{{[equation][5][]5}{[1][6][]7}}
\newlabel{lem:dnnh2}{{2}{7}{}{Lemma.2}{}}
\newlabel{lem:dnnh2@cref}{{[Lemma][2][]2}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison to related methods}{7}{section.5}}
\newlabel{sec:related}{{5}{7}{Comparison to related methods}{section.5}{}}
\newlabel{sec:related@cref}{{[section][5][]5}{[1][7][]7}}
\newlabel{OPT-DISZ}{{3}{7}{\textsc {Optimistic-Adam~\citep {DISZ18}+$\hat {v}_t$}. \relax }{figure.caption.4}{}}
\newlabel{OPT-DISZ@cref}{{[algorithm][3][]3}{[1][7][]7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \textsc  {Optimistic-Adam\nobreakspace  {}\citep  {DISZ18}+$\mathaccentV {hat}05E{v}_t$}. \relax }}{7}{figure.caption.4}}
\citation{WN11}
\citation{CJ76}
\citation{E79}
\citation{SAB16}
\citation{BZ13}
\citation{SAB16}
\citation{SAB16}
\citation{Scieur18}
\citation{RKK18}
\citation{DISZ18}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Experiments}{8}{section.6}}
\newlabel{sec:numerical}{{6}{8}{Numerical Experiments}{section.6}{}}
\newlabel{sec:numerical@cref}{{[section][6][]6}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Gradient Estimation}{8}{subsection.6.1}}
\newlabel{nox}{{6}{8}{Gradient Estimation}{equation.6.6}{}}
\newlabel{nox@cref}{{[equation][6][]6}{[1][8][]8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Regularized Approximated Minimal Polynomial Extrapolation\nobreakspace  {}\citep  {SAB16} \relax }}{8}{algorithm.4}}
\newlabel{alg:algex}{{4}{8}{Regularized Approximated Minimal Polynomial Extrapolation~\citep {SAB16} \relax }{algorithm.4}{}}
\newlabel{alg:algex@cref}{{[algorithm][4][]4}{[1][8][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Classification Experiments}{8}{subsection.6.2}}
\citation{RKK18}
\citation{KB15}
\citation{MNIST07}
\citation{Rnet16}
\citation{gers1999learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training loss vs. Number of iterations for fully connected NN, CNN, LSTM and ResNet.\relax }}{9}{figure.caption.5}}
\newlabel{fig:train_loss}{{3}{9}{Training loss vs. Number of iterations for fully connected NN, CNN, LSTM and ResNet.\relax }{figure.caption.5}{}}
\newlabel{fig:train_loss@cref}{{[figure][3][]3}{[1][9][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textit  {MNIST-back-image} + CNN, \textit  {CIFAR10} + Res-18 and \textit  {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy.\relax }}{10}{figure.caption.6}}
\newlabel{fig:testandtrain}{{4}{10}{\textit {MNIST-back-image} + CNN, \textit {CIFAR10} + Res-18 and \textit {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy.\relax }{figure.caption.6}{}}
\newlabel{fig:testandtrain@cref}{{[figure][4][]4}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Choice of parameter $r$}{11}{subsection.6.3}}
\newlabel{sec:choicer}{{6.3}{11}{Choice of parameter $r$}{subsection.6.3}{}}
\newlabel{sec:choicer@cref}{{[subsection][3][6]6.3}{[1][11][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training loss w.r.t. different $r$ values.\relax }}{11}{figure.caption.7}}
\newlabel{fig:compare}{{5}{11}{Training loss w.r.t. different $r$ values.\relax }{figure.caption.7}{}}
\newlabel{fig:compare@cref}{{[figure][5][]5}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{11}{section.7}}
\bibstyle{plain}
\bibdata{standard,reference}
\bibcite{ALLW18}{{1}{}{{}}{{}}}
\bibcite{Princeton18}{{2}{}{{}}{{}}}
\bibcite{BZ13}{{3}{}{{}}{{}}}
\bibcite{CJ76}{{4}{}{{}}{{}}}
\bibcite{Proc:Chen_FODS20}{{5}{}{{}}{{}}}
\bibcite{CLSH19}{{6}{}{{}}{{}}}
\bibcite{CYYZC19}{{7}{}{{}}{{}}}
\bibcite{CJ12}{{8}{}{{}}{{}}}
\bibcite{DISZ18}{{9}{}{{}}{{}}}
\bibcite{defossez2020convergence}{{10}{}{{}}{{}}}
\bibcite{D16}{{11}{}{{}}{{}}}
\bibcite{DHS11}{{12}{}{{}}{{}}}
\bibcite{E79}{{13}{}{{}}{{}}}
\bibcite{gers1999learning}{{14}{}{{}}{{}}}
\bibcite{ghadimi2013stochastic}{{15}{}{{}}{{}}}
\bibcite{goodfellow2014generative}{{16}{}{{}}{{}}}
\bibcite{GMH13}{{17}{}{{}}{{}}}
\bibcite{H14}{{18}{}{{}}{{}}}
\bibcite{Rnet16}{{19}{}{{}}{{}}}
\bibcite{KB15}{{20}{}{{}}{{}}}
\bibcite{MNIST07}{{21}{}{{}}{{}}}
\bibcite{LFDA17}{{22}{}{{}}{{}}}
\bibcite{LO18}{{23}{}{{}}{{}}}
\bibcite{MS10}{{24}{}{{}}{{}}}
\bibcite{mertikopoulos2018optimistic}{{25}{}{{}}{{}}}
\bibcite{Atari13}{{26}{}{{}}{{}}}
\bibcite{MY16}{{27}{}{{}}{{}}}
\bibcite{N04}{{28}{}{{}}{{}}}
\bibcite{P64}{{29}{}{{}}{{}}}
\bibcite{RS13b}{{30}{}{{}}{{}}}
\bibcite{RKK18}{{31}{}{{}}{{}}}
\bibcite{SAB16}{{32}{}{{}}{{}}}
\bibcite{Scieur18}{{33}{}{{}}{{}}}
\bibcite{SALS15}{{34}{}{{}}{{}}}
\bibcite{TH12}{{35}{}{{}}{{}}}
\bibcite{T08}{{36}{}{{}}{{}}}
\bibcite{WN11}{{37}{}{{}}{{}}}
\bibcite{WWB18}{{38}{}{{}}{{}}}
\bibcite{yan2018unified}{{39}{}{{}}{{}}}
\bibcite{ZRSKK18}{{40}{}{{}}{{}}}
\bibcite{Z12}{{41}{}{{}}{{}}}
\bibcite{ZTYCG18}{{42}{}{{}}{{}}}
\bibcite{Proc:Zhou_NeurIPS20}{{43}{}{{}}{{}}}
\bibcite{ZS18}{{44}{}{{}}{{}}}
\citation{T08}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Theorem\nobreakspace  {}\ref  {thm:mainconvex}}{15}{appendix.A}}
\newlabel{app:thmmainconvex}{{A}{15}{Proof of Theorem~\ref {thm:mainconvex}}{appendix.A}{}}
\newlabel{app:thmmainconvex@cref}{{[appendix][1][2147483647]A}{[1][15][]15}}
\newlabel{nn1}{{8}{15}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.8}{}}
\newlabel{nn1@cref}{{[equation][8][2147483647]8}{[1][15][]15}}
\newlabel{ii}{{9}{15}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.9}{}}
\newlabel{ii@cref}{{[equation][9][2147483647]9}{[1][15][]15}}
\newlabel{nc1}{{10}{15}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.10}{}}
\newlabel{nc1@cref}{{[equation][10][2147483647]10}{[1][15][]15}}
\newlabel{nn2}{{11}{15}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.11}{}}
\newlabel{nn2@cref}{{[equation][11][2147483647]11}{[1][15][]15}}
\newlabel{nc2}{{12}{15}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.12}{}}
\newlabel{nc2@cref}{{[equation][12][2147483647]12}{[1][15][]15}}
\newlabel{nn3}{{13}{15}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.13}{}}
\newlabel{nn3@cref}{{[equation][13][2147483647]13}{[1][15][]15}}
\newlabel{nnn}{{14}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.14}{}}
\newlabel{nnn@cref}{{[equation][14][2147483647]14}{[1][15][]16}}
\newlabel{nnnn}{{15}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.15}{}}
\newlabel{nnnn@cref}{{[equation][15][2147483647]15}{[1][16][]16}}
\newlabel{nn5}{{16}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.16}{}}
\newlabel{nn5@cref}{{[equation][16][2147483647]16}{[1][16][]16}}
\newlabel{nn4}{{17}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.A.17}{}}
\newlabel{nn4@cref}{{[equation][17][2147483647]17}{[1][16][]16}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Corollary\nobreakspace  {}\ref  {cor:corollary}}{16}{appendix.B}}
\citation{yan2018unified}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proofs of Auxiliary Lemmas}{17}{appendix.C}}
\newlabel{eq:deftilde}{{18}{17}{Proofs of Auxiliary Lemmas}{equation.C.18}{}}
\newlabel{eq:deftilde@cref}{{[equation][18][2147483647]18}{[1][17][]17}}
\newlabel{lem:momentum}{{3}{17}{}{Lemma.3}{}}
\newlabel{lem:momentum@cref}{{[Lemma][3][2147483647]3}{[1][17][]17}}
\newlabel{lem:squarev}{{4}{18}{}{Lemma.4}{}}
\newlabel{lem:squarev@cref}{{[Lemma][4][2147483647]4}{[1][18][]18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Proof of Lemma\nobreakspace  {}\ref  {lem:bound}}{19}{subsection.C.1}}
\newlabel{app:lembound}{{C.1}{19}{Proof of Lemma~\ref {lem:bound}}{subsection.C.1}{}}
\newlabel{app:lembound@cref}{{[subappendix][1][2147483647,3]C.1}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proof of Theorem\nobreakspace  {}\ref  {thm:boundopt}}{19}{appendix.D}}
\newlabel{app:thmboundopt}{{D}{19}{Proof of Theorem~\ref {thm:boundopt}}{appendix.D}{}}
\newlabel{app:thmboundopt@cref}{{[appendix][4][2147483647]D}{[1][19][]19}}
\newlabel{eq:smoothness}{{27}{19}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.27}{}}
\newlabel{eq:smoothness@cref}{{[equation][27][2147483647]27}{[1][19][]19}}
\newlabel{eq:termA1}{{28}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.28}{}}
\newlabel{eq:termA1@cref}{{[equation][28][2147483647]28}{[1][20][]20}}
\newlabel{eq:termA2}{{29}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.29}{}}
\newlabel{eq:termA2@cref}{{[equation][29][2147483647]29}{[1][20][]20}}
\newlabel{eq:termA}{{30}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.30}{}}
\newlabel{eq:termA@cref}{{[equation][30][2147483647]30}{[1][20][]20}}
\newlabel{eq:termB1}{{31}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.31}{}}
\newlabel{eq:termB1@cref}{{[equation][31][2147483647]31}{[1][20][]20}}
\newlabel{eq:termB2}{{32}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.32}{}}
\newlabel{eq:termB2@cref}{{[equation][32][2147483647]32}{[1][20][]20}}
\newlabel{eq:termB3}{{34}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.34}{}}
\newlabel{eq:termB3@cref}{{[equation][34][2147483647]34}{[1][20][]20}}
\newlabel{eq:termB}{{35}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.35}{}}
\newlabel{eq:termB@cref}{{[equation][35][2147483647]35}{[1][21][]21}}
\newlabel{eq:term3}{{36}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.36}{}}
\newlabel{eq:term3@cref}{{[equation][36][2147483647]36}{[1][21][]21}}
\newlabel{eq:expectationtildegrad}{{37}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.37}{}}
\newlabel{eq:expectationtildegrad@cref}{{[equation][37][2147483647]37}{[1][21][]21}}
\newlabel{eq:bound1}{{38}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.D.38}{}}
\newlabel{eq:bound1@cref}{{[equation][38][2147483647]38}{[1][21][]21}}
\citation{ZTYCG18}
\citation{defossez2020convergence}
\@writefile{toc}{\contentsline {section}{\numberline {E}Proof of Lemma\nobreakspace  {}\ref  {lem:dnnh2} (Boundedness of the iterates H\ref  {ass:boundedparam})}{23}{appendix.E}}
\newlabel{app:lemdnnh2}{{E}{23}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{appendix.E}{}}
\newlabel{app:lemdnnh2@cref}{{[appendix][5][2147483647]E}{[1][23][]23}}
\newlabel{eq:mildassumptions}{{39}{23}{}{equation.E.39}{}}
\newlabel{eq:mildassumptions@cref}{{[equation][39][2147483647]39}{[1][23][]23}}
\newlabel{eq:boundderivativeloss}{{40}{23}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.E.40}{}}
\newlabel{eq:boundderivativeloss@cref}{{[equation][40][2147483647]40}{[1][23][]23}}
\newlabel{eq:decrease}{{41}{24}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.E.41}{}}
\newlabel{eq:decrease@cref}{{[equation][41][2147483647]41}{[1][23][]24}}
\newlabel{eq:gradientatell}{{42}{24}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.E.42}{}}
\newlabel{eq:gradientatell@cref}{{[equation][42][2147483647]42}{[1][24][]24}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Additional Remarks and Runs on the Gradient Prediction Process}{24}{appendix.F}}
\citation{RKK18}
\citation{RKK18}
\citation{KB15}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. \relax }}{25}{figure.caption.9}}
\newlabel{simu}{{6}{25}{\small (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. \relax }{figure.caption.9}{}}
\newlabel{simu@cref}{{[figure][6][2147483647]6}{[1][25][]25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{25}{figure.caption.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{25}{figure.caption.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{25}{figure.caption.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{25}{figure.caption.9}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
