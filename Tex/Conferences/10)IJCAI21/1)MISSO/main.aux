\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mcLachlan2008em}
\citation{jordan1999var}
\citation{razaviyayn2013unified}
\citation{lange2016mm}
\citation{mairal2015miso}
\citation{schmidt2017minimizing}
\citation{qian2019miso}
\citation{mairal2015miso}
\citation{mensch2017stochastic}
\citation{mairal2015miso}
\citation{mcLachlan2008em}
\citation{ghahramani2015probabilistic}
\citation{neal2012bayesian}
\citation{blundell2015weight}
\citation{polson2017deep}
\citation{rezende2014stochastic}
\citation{li2017dropout}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{eq:opt}{{1}{1}{Introduction}{equation.1.1}{}}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{razaviyayn2013unified}
\citation{mairal2015miso}
\@writefile{toc}{\contentsline {section}{\numberline {2}Incremental Minimization of Finite Sum Nonconvex Functions}{2}{section.2}}
\newlabel{sec:framework}{{2}{2}{Incremental Minimization of Finite Sum Nonconvex Functions}{section.2}{}}
\newlabel{ass:sur}{{1}{2}{}{assumption.1}{}}
\newlabel{eq:lowerbd}{{3}{2}{}{equation.2.3}{}}
\newlabel{ass:diff}{{2}{2}{}{assumption.2}{}}
\newlabel{eq:eq30}{{4}{2}{}{equation.2.4}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{line:upd}{{5}{2}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.5}{}}
\newlabel{miso:iter}{{6}{2}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The MISO method.\relax }}{2}{algorithm.1}}
\newlabel{alg:miso}{{1}{2}{The MISO method.\relax }{algorithm.1}{}}
\newlabel{eq:integralsurrogate}{{5}{2}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.5}{}}
\citation{bishop2006pattern}
\citation{blei2017vi}
\citation{paisley2013}
\citation{kingma}
\citation{blundell2015weight}
\citation{blundell2015weight}
\newlabel{line:unif}{{5}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.12}{}}
\newlabel{line:mcsample}{{6}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.13}{}}
\newlabel{line:ssur}{{7}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.14}{}}
\newlabel{line:iter}{{8}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.15}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The MISSO method.\relax }}{3}{algorithm.2}}
\newlabel{alg:misso}{{2}{3}{The MISSO method.\relax }{algorithm.2}{}}
\newlabel{eq:ssur}{{6}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.6}{}}
\newlabel{pairmcem}{{7}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.7}{}}
\newlabel{eq:vi}{{8}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.8}{}}
\newlabel{eq:VI}{{9}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.9}{}}
\newlabel{eq:variationalobjective}{{10}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.10}{}}
\newlabel{eq:quad_sur}{{11}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.11}{}}
\citation{van2000asymptotic}
\citation{vershynin2018high}
\citation{wainwright2019high}
\citation{vershynin2018high}
\citation{vershynin2018high}
\citation{wainwright2019high}
\citation{van2000asymptotic}
\citation{fletcher2002global}
\citation{mairal2015miso}
\newlabel{pairvi}{{13}{4}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.2.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Convergence Analysis}{4}{section.3}}
\newlabel{sec:analysis}{{3}{4}{Convergence Analysis}{section.3}{}}
\newlabel{ass:lips}{{3}{4}{}{assumption.3}{}}
\newlabel{controlapprox}{{4}{4}{}{assumption.4}{}}
\newlabel{eq:stationary_meas}{{14}{4}{Convergence Analysis}{equation.3.14}{}}
\newlabel{eq:sumsurrodet}{{15}{4}{Convergence Analysis}{equation.3.15}{}}
\citation{mairal2015miso}
\citation{jiang2018logistic}
\citation{delyon1999}
\citation{wei1990mcem}
\newlabel{thm:main}{{1}{5}{}{theo.1}{}}
\newlabel{eq:misso_rate}{{16}{5}{}{equation.3.16}{}}
\newlabel{thm:mainasymp}{{2}{5}{}{theo.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{5}{section.4}}
\newlabel{sec:numerical}{{4}{5}{Numerical Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Binary logistic regression with missing values}{5}{subsection.4.1}}
\newlabel{logisticreg}{{4.1}{5}{Binary logistic regression with missing values}{subsection.4.1}{}}
\newlabel{eq:logistic}{{18}{5}{Binary logistic regression with missing values}{equation.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Convergence of parameters ${\bm  {\delta }}$ and ${\bm  {\beta }}$ for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against number of passes over the data.\relax }}{5}{figure.caption.1}}
\newlabel{fig:misso_trauma}{{1}{5}{Convergence of parameters ${\bm \delta }$ and ${\bm \beta }$ for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against number of passes over the data.\relax }{figure.caption.1}{}}
\citation{lecun1998gradient}
\citation{lecun1998gradient}
\citation{lecun1998mnist}
\citation{he2016deep}
\citation{krizhevsky2012imagenet}
\citation{he2016deep}
\citation{wen2018flipout}
\citation{kingma:adam}
\citation{sutskever2013importance}
\citation{schmidt2017minimizing}
\citation{blundell2015weight}
\bibstyle{plain}
\bibdata{ref}
\bibcite{bishop2006pattern}{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training Bayesian CNN using MISSO}{6}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Negated ELBO versus epochs elapsed for fitting (a) Bayesian LeNet-5 on MNIST and (b) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods, and the shaded area represents the standard deviation.\relax }}{6}{figure.caption.2}}
\newlabel{fig:lenetopt}{{2}{6}{Negated ELBO versus epochs elapsed for fitting (a) Bayesian LeNet-5 on MNIST and (b) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods, and the shaded area represents the standard deviation.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {LeNet-5: MNIST}}}{6}{subfigure.2.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ResNet-18: CIFAR-10}}}{6}{subfigure.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.5}}
\bibcite{blei2017vi}{2}
\bibcite{blundell2015weight}{3}
\bibcite{delyon1999}{4}
\bibcite{fletcher2002global}{5}
\bibcite{ghahramani2015probabilistic}{6}
\bibcite{he2016deep}{7}
\bibcite{jiang2018logistic}{8}
\bibcite{jordan1999var}{9}
\bibcite{kingma:adam}{10}
\bibcite{kingma}{11}
\bibcite{krizhevsky2012imagenet}{12}
\bibcite{lange2016mm}{13}
\bibcite{lecun1998mnist}{14}
\bibcite{lecun1998gradient}{15}
\bibcite{li2017dropout}{16}
\bibcite{mairal2015miso}{17}
\bibcite{mcLachlan2008em}{18}
\bibcite{mensch2017stochastic}{19}
\bibcite{meyn2012markov}{20}
\bibcite{neal2012bayesian}{21}
\bibcite{paisley2013}{22}
\bibcite{polson2017deep}{23}
\bibcite{qian2019miso}{24}
\bibcite{razaviyayn2013unified}{25}
\bibcite{rezende2014stochastic}{26}
\bibcite{schmidt2017minimizing}{27}
\bibcite{sutskever2013importance}{28}
\bibcite{van2000asymptotic}{29}
\bibcite{vershynin2018high}{30}
\bibcite{wainwright2019high}{31}
\bibcite{wei1990mcem}{32}
\bibcite{wen2018flipout}{33}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs of the Theoretical Results}{9}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of Theorem\nobreakspace  {}\ref  {thm:main}}{9}{subsection.A.1}}
\newlabel{eq:surbd}{{19}{9}{Proof of Theorem~\ref {thm:main}}{equation.A.19}{}}
\newlabel{eq:firsteq}{{20}{9}{Proof of Theorem~\ref {thm:main}}{equation.A.20}{}}
\newlabel{eq:afterarrange}{{21}{10}{Proof of Theorem~\ref {thm:main}}{equation.A.21}{}}
\newlabel{eq:prebdd}{{23}{10}{Proof of Theorem~\ref {thm:main}}{equation.A.23}{}}
\newlabel{eq:mkcal}{{25}{10}{Proof of Theorem~\ref {thm:main}}{equation.A.25}{}}
\newlabel{eq:gksur}{{26}{11}{Proof of Theorem~\ref {thm:main}}{equation.A.26}{}}
\newlabel{eq:gmbd}{{27}{11}{Proof of Theorem~\ref {thm:main}}{equation.A.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem\nobreakspace  {}\ref  {thm:mainasymp}}{12}{subsection.A.2}}
\newlabel{lemmars}{{1}{12}{}{Lemma.1}{}}
\newlabel{eq:dvk}{{29}{12}{Proof of Theorem~\ref {thm:mainasymp}}{equation.A.29}{}}
\newlabel{eq:dxk}{{30}{12}{Proof of Theorem~\ref {thm:mainasymp}}{equation.A.30}{}}
\newlabel{eq:dek}{{31}{12}{Proof of Theorem~\ref {thm:mainasymp}}{equation.A.31}{}}
\newlabel{eq:esur}{{32}{13}{Proof of Theorem~\ref {thm:mainasymp}}{equation.A.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of Lemma\nobreakspace  {}\ref  {lemmars}}{13}{subsection.A.3}}
\newlabel{appendix:lemma}{{A.3}{13}{Proof of Lemma~\ref {lemmars}}{subsection.A.3}{}}
\citation{meyn2012markov}
\newlabel{chik}{{36}{14}{Proof of Lemma~\ref {lemmars}}{equation.A.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Practical Details for the Binary Logistic Regression on the Traumabase}{14}{appendix.B}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Traumabase dataset quantitative variables}{14}{subsection.B.1}}
\newlabel{appendix:variables}{{B.1}{14}{Traumabase dataset quantitative variables}{subsection.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Metropolis-Hastings algorithm}{15}{subsection.B.2}}
\newlabel{app:trauma_mh}{{B.2}{15}{Metropolis-Hastings algorithm}{subsection.B.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces MH aglorithm\relax }}{15}{algorithm.3}}
\newlabel{alg:mh}{{3}{15}{MH aglorithm\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}MISSO Update}{15}{subsection.B.3}}
\newlabel{app:update_logistic}{{B.3}{15}{MISSO Update}{subsection.B.3}{}}
\newlabel{eq:surrogatedet}{{41}{15}{MISSO Update}{equation.B.41}{}}
\newlabel{ass:log1}{{1}{16}{}{assumptionL.1}{}}
\newlabel{eq:surrogatelogit}{{42}{16}{MISSO Update}{equation.B.42}{}}
\newlabel{eq:mixedsurrogate}{{43}{16}{MISSO Update}{equation.B.43}{}}
\newlabel{eq:msteplog}{{44}{17}{MISSO Update}{equation.B.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Wall clock time}{17}{subsection.B.4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Logistic Regression with missing values: running time in seconds for $10$ epochs.\relax }}{17}{table.caption.4}}
\newlabel{tab:tablelogisitc}{{1}{17}{Logistic Regression with missing values: running time in seconds for $10$ epochs.\relax }{table.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Convergence of parameters ${\bm  {\delta }}$ and ${\bm  {\beta }}$ for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against time elapsed (in seconds).\relax }}{17}{figure.caption.5}}
\newlabel{fig:misso_trauma_wallclock}{{3}{17}{Convergence of parameters ${\bm \delta }$ and ${\bm \beta }$ for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against time elapsed (in seconds).\relax }{figure.caption.5}{}}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Plots against epochs elapsed}{18}{subsection.B.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Convergence of parameters ${\bm  {\delta }}$ and ${\bm  {\beta }}$ for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against time elapsed (in seconds).\relax }}{18}{figure.caption.6}}
\newlabel{fig:misso_trauma_wallclock}{{4}{18}{Convergence of parameters ${\bm \delta }$ and ${\bm \beta }$ for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against time elapsed (in seconds).\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Practical Details for the Incremental Variational Inference}{18}{appendix.C}}
\newlabel{appendix:vimisso}{{C}{18}{Practical Details for the Incremental Variational Inference}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Neural Networks Architecture}{18}{subsection.C.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces LeNet-5 architecture\relax }}{18}{table.caption.7}}
\newlabel{table:lenet}{{2}{18}{LeNet-5 architecture\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces ResNet-18 architecture\relax }}{18}{table.caption.8}}
\newlabel{table:resnet}{{3}{18}{ResNet-18 architecture\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Algorithms updates}{19}{subsection.C.2}}
\newlabel{bnn:updates}{{C.2}{19}{Algorithms updates}{subsection.C.2}{}}
\newlabel{eq:missoupdate}{{45}{19}{Algorithms updates}{equation.C.45}{}}
\newlabel{eq:drifts}{{46}{19}{Algorithms updates}{equation.C.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Wall clock time}{20}{subsection.C.3}}
\@writefile{toc}{\contentsline {paragraph}{LeNet-5 on MNIST}{20}{section*.9}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Bayesian Deep Neural Network: running time in seconds for $100$ epochs.\relax }}{20}{table.caption.10}}
\newlabel{tab:tablebnn}{{4}{20}{Bayesian Deep Neural Network: running time in seconds for $100$ epochs.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Negated ELBO versus wall clock time for fitting a Bayesian LeNet-5 on MNIST. Plotted on the average of the 5 repetitions.\relax }}{20}{figure.caption.11}}
\newlabel{fig:lenetopt_wallclock}{{5}{20}{Negated ELBO versus wall clock time for fitting a Bayesian LeNet-5 on MNIST. Plotted on the average of the 5 repetitions.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {LeNet-5 on MNIST}}}{20}{subfigure.5.1}}
\@writefile{toc}{\contentsline {paragraph}{ResNet-18 on CIFAR10}{20}{section*.12}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces ResNet-18: running time in seconds for $20$ epochs.\relax }}{20}{table.caption.13}}
\newlabel{tab:tableresnet}{{5}{20}{ResNet-18: running time in seconds for $20$ epochs.\relax }{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Negated ELBO versus wall clock time for fitting a Bayesian ResNet-18 on CIFAR10. Plotted on the average of the 5 repetitions.\relax }}{21}{figure.caption.14}}
\newlabel{fig:resnetopt_wallclock}{{6}{21}{Negated ELBO versus wall clock time for fitting a Bayesian ResNet-18 on CIFAR10. Plotted on the average of the 5 repetitions.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ResNet-18 on CIFAR10}}}{21}{subfigure.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Plots against the epochs elapsed}{21}{subsection.C.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Negated ELBO versus epochs elapsed for fitting (a) Bayesian LeNet-5 on MNIST and (b) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods, and the shaded area represents the standard deviation.\relax }}{21}{figure.caption.15}}
\newlabel{fig:lenetopt}{{7}{21}{Negated ELBO versus epochs elapsed for fitting (a) Bayesian LeNet-5 on MNIST and (b) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods, and the shaded area represents the standard deviation.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {LeNet-5: MNIST}}}{21}{subfigure.7.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ResNet-18: CIFAR-10}}}{21}{subfigure.7.2}}
