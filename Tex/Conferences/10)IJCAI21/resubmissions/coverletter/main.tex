\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}

\input{shortcuts}




\begin{document}



\title{Cover Letter for\\
MISSO: Minimization by Incremental Stochastic Surrogate Optimization for Large Scale Nonconvex and Nonsmooth Problems}


\date{}
\maketitle



A growing and important task while learning models on observed data, is the ability to train the latter over a large number of clients which could either be devices or distinct entities.
In the paradigm of Federated Learning (FL)~\citep{konevcny2016federated,mcmahan2017communication}, the focus of our paper, a central server orchestrates the optimization over those clients under the constraint that the data can neither be centralized nor shared among the clients.

\clearpage

\bibliographystyle{plain}
\bibliography{ref}


\end{document} 