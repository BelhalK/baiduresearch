\begin{thebibliography}{10}

\bibitem{alistarh2017qsgd}
{\sc D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic}, {\em Qsgd:
  Communication-efficient sgd via gradient quantization and encoding}, in
  Advances in Neural Information Processing Systems, 2017, pp.~1709--1720.

\bibitem{basu2019qsparse}
{\sc D.~Basu, D.~Data, C.~Karakus, and S.~Diggavi}, {\em Qsparse-local-sgd:
  Distributed sgd with quantization, sparsification and local computations}, in
  Advances in Neural Information Processing Systems, 2019, pp.~14695--14706.

\bibitem{bernstein2018signsgd}
{\sc J.~Bernstein, Y.-X. Wang, K.~Azizzadenesheli, and A.~Anandkumar}, {\em
  signsgd: Compressed optimisation for non-convex problems}, arXiv preprint
  arXiv:1802.04434,  (2018).

\bibitem{bottou2008tradeoffs}
{\sc L.~Bottou and O.~Bousquet}, {\em The tradeoffs of large scale learning},
  in Advances in neural information processing systems, 2008, pp.~161--168.

\bibitem{cormode2005improved}
{\sc G.~Cormode and S.~Muthukrishnan}, {\em An improved data stream summary:
  the count-min sketch and its applications}, Journal of Algorithms, 55 (2005),
  pp.~58--75.

\bibitem{haddadpour2020federated}
{\sc F.~Haddadpour, M.~M. Kamani, A.~Mokhtari, and M.~Mahdavi}, {\em Federated
  learning with compression: Unified analysis and sharp guarantees}, arXiv
  preprint arXiv:2007.01154,  (2020).

\bibitem{horvath2020better}
{\sc S.~Horv{\'a}th and P.~Richt{\'a}rik}, {\em A better alternative to error
  feedback for communication-efficient distributed learning}, arXiv preprint
  arXiv:2006.11077,  (2020).

\bibitem{ivkin2019communication}
{\sc N.~Ivkin, D.~Rothchild, E.~Ullah, I.~Stoica, R.~Arora, et~al.}, {\em
  Communication-efficient distributed sgd with sketching}, in Advances in
  Neural Information Processing Systems, 2019, pp.~13144--13154.

\bibitem{karimireddy2019scaffold}
{\sc S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~J. Reddi, S.~U. Stich, and A.~T.
  Suresh}, {\em Scaffold: Stochastic controlled averaging for on-device
  federated learning}, arXiv preprint arXiv:1910.06378,  (2019).

\bibitem{kleinberg2003bursty}
{\sc J.~Kleinberg}, {\em Bursty and hierarchical structure in streams}, Data
  Mining and Knowledge Discovery, 7 (2003), pp.~373--397.

\bibitem{li2019privacy}
{\sc T.~Li, Z.~Liu, V.~Sekar, and V.~Smith}, {\em Privacy for free:
  Communication-efficient learning with differential privacy using sketches},
  arXiv preprint arXiv:1911.00972,  (2019).

\bibitem{liang2019variance}
{\sc X.~Liang, S.~Shen, J.~Liu, Z.~Pan, E.~Chen, and Y.~Cheng}, {\em Variance
  reduced local sgd with lower communication complexity}, arXiv preprint
  arXiv:1912.12844,  (2019).

\bibitem{lin2017deep}
{\sc Y.~Lin, S.~Han, H.~Mao, Y.~Wang, and W.~J. Dally}, {\em Deep gradient
  compression: Reducing the communication bandwidth for distributed training},
  arXiv preprint arXiv:1712.01887,  (2017).

\bibitem{reisizadeh2019fedpaq}
{\sc A.~Reisizadeh, A.~Mokhtari, H.~Hassani, A.~Jadbabaie, and R.~Pedarsani},
  {\em Fedpaq: A communication-efficient federated learning method with
  periodic averaging and quantization}, arXiv preprint arXiv:1909.13014,
  (2019).

\bibitem{robbins1951stochastic}
{\sc H.~Robbins and S.~Monro}, {\em A stochastic approximation method}, The
  annals of mathematical statistics,  (1951), pp.~400--407.

\bibitem{stich2018sparsified}
{\sc S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi}, {\em Sparsified sgd with
  memory}, in Advances in Neural Information Processing Systems, 2018,
  pp.~4447--4458.

\bibitem{wang2018cooperative}
{\sc J.~Wang and G.~Joshi}, {\em Cooperative sgd: A unified framework for the
  design and analysis of communication-efficient sgd algorithms}, arXiv
  preprint arXiv:1808.07576,  (2018).

\bibitem{wangni2018gradient}
{\sc J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang}, {\em Gradient sparsification
  for communication-efficient distributed optimization}, in Advances in Neural
  Information Processing Systems, 2018, pp.~1299--1309.

\end{thebibliography}
