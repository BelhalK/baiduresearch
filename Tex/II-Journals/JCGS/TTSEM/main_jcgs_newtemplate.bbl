\begin{thebibliography}{}

\bibitem[Allassonni{\`e}re et~al., 2007]{allassonniere2007towards}
Allassonni{\`e}re, S., Amit, Y., and Trouv{\'e}, A. (2007).
\newblock Towards a coherent statistical framework for dense deformable
  template estimation.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 69(1):3--29.

\bibitem[Allassonni{\`e}re et~al., 2013]{allassonniere2013statistical}
Allassonni{\`e}re, S., Bigot, J., Glaun{\`e}s, J.~A., Maire, F., and Richard,
  F.~J. (2013).
\newblock Statistical models for deformable templates in image and shape
  analysis.
\newblock {\em Annales math{\'e}matiques Blaise Pascal}, 20(1):1--35.

\bibitem[Allassonni{\`e}re et~al., 2010]{allassonniere2010construction}
Allassonni{\`e}re, S., Kuhn, E., and Trouv{\'e}, A. (2010).
\newblock Construction of bayesian deformable models via a stochastic
  approximation algorithm: a convergence study.
\newblock {\em Bernoulli}, 16(3):641--678.

\bibitem[Baey et~al., 2016]{baey2016nonlinear}
Baey, C., Trevezas, S., and Courn{\`e}de, P.-H. (2016).
\newblock A non linear mixed effects model of plant growth and estimation via
  stochastic variants of the {EM} algorithm.
\newblock {\em Communications in Statistics-Theory and Methods},
  45(6):1643--1669.

\bibitem[Blei et~al., 2017]{BleiVariational2017}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D. ({2017}).
\newblock {Variational Inference: A Review for Statisticians}.
\newblock {\em {Journal of the American statistical Association}},
  {112}({518}):{859--877}.

\bibitem[Brooks et~al., 2011]{brooks2011handbook}
Brooks, S., Gelman, A., Jones, G., and Meng, X.-L. (2011).
\newblock {\em Handbook of markov chain monte carlo}.
\newblock CRC press.

\bibitem[Capp{\'e}, 2011]{cappe2011online}
Capp{\'e}, O. (2011).
\newblock Online {EM} algorithm for hidden markov models.
\newblock {\em Journal of Computational and Graphical Statistics},
  20(3):728--749.

\bibitem[Capp{\'e} and Moulines, 2009]{cappe2009line}
Capp{\'e}, O. and Moulines, E. (2009).
\newblock On-line expectation--maximization algorithm for latent data models.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 71(3):593--613.

\bibitem[Carlin and Chib, 1995]{carlin1995bayesian}
Carlin, B.~P. and Chib, S. (1995).
\newblock Bayesian model choice via markov chain monte carlo methods.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 57(3):473--484.

\bibitem[Chakraborty and Das, 2010]{das2010Inferences}
Chakraborty, A. and Das, K. (2010).
\newblock Inferences for joint modelling of repeated ordinal scores and time to
  event data.
\newblock {\em Computational and mathematical methods in medicine},
  11(3):281--295.

\bibitem[Chen et~al., 2018]{chen2018stochastic}
Chen, J., Zhu, J., Teh, Y.~W., and Zhang, T. (2018).
\newblock Stochastic expectation maximization with variance reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7978--7988.

\bibitem[Delyon et~al., 1999]{delyon1999}
Delyon, B., Lavielle, M., and Moulines, {\'E}. (1999).
\newblock Convergence of a stochastic approximation version of the {EM}
  algorithm.
\newblock {\em Ann. Statist.}, 27(1):94--128.

\bibitem[Dempster et~al., 1977]{dempster1977Maximum}
Dempster, A.~P., Laird, N.~M., and Rubin, D.~B. (1977).
\newblock Maximum likelihood from incomplete data via the {EM} algorithm.
\newblock {\em Journal of the royal statistical society. Series B
  (methodological)}, pages 1--38.

\bibitem[Efron, 1975]{efron1975defining}
Efron, B. (1975).
\newblock Defining the curvature of a statistical problem (with applications to
  second order efficiency).
\newblock {\em The Annals of Statistics}, 3(6):1189--1242.

\bibitem[Fort et~al., 2020]{fortem2020}
Fort, G., Moulines, {\'E}., and Wai, H.-T. (2020).
\newblock A stochastic path integral differential estimator expectation
  maximization algorithm.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H., editors, {\em Advances in Neural Information Processing Systems},
  volume~33, pages 16972--16982. Curran Associates, Inc.

\bibitem[Fort et~al., 2021]{fort2021geom}
Fort, G., Moulines, {\'E}., and Wai, H.-T. (2021).
\newblock Geom-spider-em: Faster variance reduced stochastic expectation
  maximization for nonconvex finite-sum optimization.
\newblock In {\em ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3135--3139. IEEE.

\bibitem[Ghadimi and Lan, 2013]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G. (2013).
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368.

\bibitem[Hughes, 1999]{hughes1999mixed}
Hughes, J.~P. (1999).
\newblock Mixed effects models with censored data with application to hiv rna
  levels.
\newblock {\em Biometrics}, 55(2):625--629.

\bibitem[Hull, 1994]{hull1994database}
Hull, J.~J. (1994).
\newblock A database for handwritten text recognition research.
\newblock {\em IEEE Transactions on pattern analysis and machine intelligence},
  16(5):550--554.

\bibitem[Jain and Kar, 2017]{jain2017non}
Jain, P. and Kar, P. (2017).
\newblock Non-convex optimization for machine learning.
\newblock {\em Found. Trends Mach. Learn.}, 10(3-4):142--336.

\bibitem[Johnson and Zhang, 2013]{johnson:zhang:2013}
Johnson, R. and Zhang, T. (2013).
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in neural information processing systems}, pages
  315--323.

\bibitem[Karimi et~al., 2019]{karimi2019global}
Karimi, B., Wai, H.-T., Moulines, {\'E}., and Lavielle, M. (2019).
\newblock On the global convergence of (fast) incremental expectation
  maximization methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2833--2843.

\bibitem[Kuhn and Lavielle, 2004]{kuhn2004coupling}
Kuhn, E. and Lavielle, M. (2004).
\newblock Coupling a stochastic approximation version of {EM} with an mcmc
  procedure.
\newblock {\em ESAIM: Probability and Statistics}, 8:115--131.

\bibitem[Kuhn et~al., 2020]{kuhn2019properties}
Kuhn, E., Matias, C., and Rebafka, T. (2020).
\newblock Properties of the stochastic approximation {EM} algorithm with
  mini-batch sampling.
\newblock {\em Stat. Comput.}, 30(6):1725--1739.

\bibitem[Liang and Klein, 2009]{liang2009online}
Liang, P. and Klein, D. (2009).
\newblock Online {EM} for unsupervised models.
\newblock In {\em Proceedings of Human Language Technologies: Conference of the
  North American Chapter of the Association of Computational Linguistics
  (HLT-NAACL)}, pages 611--619, Boulder, CO.

\bibitem[Maire et~al., 2017]{maire2016online}
Maire, F., Moulines, {\'E}., and Lefebvre, S. (2017).
\newblock Online {EM} for functional data.
\newblock {\em Comput. Stat. Data Anal.}, 111:27--47.

\bibitem[McCulloch, 1997]{mcculloch1997maximum}
McCulloch, C.~E. (1997).
\newblock Maximum likelihood algorithms for generalized linear mixed models.
\newblock {\em Journal of the American statistical Association},
  92(437):162--170.

\bibitem[McLachlan and Krishnan, 2007]{mclachlan2007algorithm}
McLachlan, G. and Krishnan, T. (2007).
\newblock {\em The {EM} algorithm and extensions}, volume 382.
\newblock John Wiley \& Sons.

\bibitem[Meyn and Tweedie, 2012]{meyn2012markov}
Meyn, S.~P. and Tweedie, R.~L. (2012).
\newblock {\em Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media.

\bibitem[Neal and Hinton, 1998]{neal1998view}
Neal, R.~M. and Hinton, G.~E. (1998).
\newblock A view of the {EM} algorithm that justifies incremental, sparse, and
  other variants.
\newblock In {\em Learning in graphical models}, pages 355--368. Springer.

\bibitem[Ng and McLachlan, 2003]{ngChoice2003}
Ng, S. and McLachlan, G.~J. (2003).
\newblock On the choice of the number of blocks with the incremental {EM}
  algorithm for the fitting of normal mixtures.
\newblock {\em Stat. Comput.}, 13(1):45--55.

\bibitem[Nguyen et~al., 2020]{nguyen2020mini}
Nguyen, H.~D., Forbes, F., and McLachlan, G.~J. (2020).
\newblock Mini-batch learning of exponential family finite mixture models.
\newblock {\em Stat. Comput.}, 30(4):731--748.

\bibitem[Reddi et~al., 2016]{reddi2016fast}
Reddi, S.~J., Sra, S., P{\'{o}}czos, B., and Smola, A.~J. (2016).
\newblock Fast incremental method for smooth nonconvex optimization.
\newblock In {\em Proceedings of the 55th {IEEE} Conference on Decision and
  Control (CDC)}, pages 1971--1977, Las Vegas, NV.

\bibitem[Robbins and Monro, 1951]{robbins1951stochastic}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407.

\bibitem[Wei and Tanner, 1990]{wei1990monte}
Wei, G.~C. and Tanner, M.~A. (1990).
\newblock A monte carlo implementation of the {EM} algorithm and the poor man's
  data augmentation algorithms.
\newblock {\em Journal of the American statistical Association},
  85(411):699--704.

\bibitem[Wu, 1983]{wu1983convergence}
Wu, C.~J. (1983).
\newblock On the convergence properties of the {EM} algorithm.
\newblock {\em The Annals of statistics}, pages 95--103.

\bibitem[Zhu et~al., 2017]{zhu2017high}
Zhu, R., Wang, L., Zhai, C., and Gu, Q. (2017).
\newblock High-dimensional variance-reduced stochastic gradient
  expectation-maximization algorithm.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pages 4180--4188, Sydney, Australia.

\end{thebibliography}
