\documentclass{article}

\usepackage{neurips_2020_author_response}
% \usepackage{fullpage}
\setlength\parindent{0pt}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{color}

\begin{document}
We would like to thank the four reviewers for their valuable feedback. Please find below the corresponding replies.

\textbf{Reviewer 1.}
\textit{Q1: More explanations on notations:} 
Notations will be explained and simplified in the revised paper.

\textit{Q2: Better presentation of line 41-45:} Line 41-45 simply highlights that our setting is different from  [Reddi et al., 2019], not arguing that their approach is incorrect. We will revise this part to avoid confusion.

\textit{Q3: Assumption A2 is strong:}
A2 is necessary for the analysis of adaptive gradient methods and is standard in the literature.
In the decentralized literature, this assumption might be viewed as strong since only the convergence of SGD-like algorithms has been dealt with so far.
% However, the convergence analyses of adaptive gradient methods are just established a few years ago and A2 is commonly assumed in the literature. 
Relaxing A2 is interesting but it is out of the scope of this work. 

\textit{Q4: Similar ideas on consensus of step-size:}
Thanks for providing the relevant references. 
[2] averages the predefined stepsize sequence across iterations to make it more tolerant to staleness in asynchronous updates.  
[3] does not explicitly apply consensus on stepsize but rather allows the stepsize on different nodes to be different (the maximum difference depends on the graph structure) for deterministic strongly convex problems. 
Our learning rate consensus is \emph{across workers} instead of across iterations and we allow the adaptive learning sequence on different nodes to be completely different. 
Our technique and motivation are thus different from these works. 
A discussion about this will be added.

\textit{Q5: More experiments:}
Experiments with larger datasets and complex models are under production.

\textbf{Reviewer 2.}
\textit{Q1: Connection to counter example in [Reddi et al., 2019]:}
Both our example and the one in [Reddi et al., 2019] use the idea that sample dependent learning rate can lead to non-convergence. 
Yet, in decentralized setting, the sample dependent learning rate is caused by different nodes having different adaptive learning rate sequences, while in [Reddi et al., 2019], the non-convergence is caused by over-adaptivity of adaptive learning rate of Adam. 

\textit{Q2: Highlight the novelty of the algorithm design:}
The novelty of our design is twofold.
First, we aim at bridging the realms of decentralized optimization and adaptive gradient methods. 
The study of adaptive and decentralized methods are conducted independently in the literature. 
To the best of our knowledge, this is the first success application (with rigorous convergence guarantee) of adaptive methods in decentralized optimization. 
Second, our gossip technique is not the direct average consensus mechanism used in the extensively studied DGD.
% The particular consensus technique we adopt was just used in optimization a few years ago. 
We will add more discussion on why the direct average consensus mechanism in DGD cannot be used in our case.

\textbf{Reviewer 3.}
\textit{Q1: More rigorous proof for Theorem 1:}
{\color{red} TO COMPLETE}

\textit{Q2: More discussion of Theorem 2 and Algorithm 2/3:}
We will add more interpretation of the theoretical results and algorithms to improve clarity. 

\textit{Q3: Tuning $\epsilon$ for different algorithms:}
We will include this as a tunable hyperparameter in future experiments. 
% This will indeed make the experiments more fair even though DADAM is not convergent over heterogeneous data.

\textbf{Reviewer 4.}
\textit{Q1: Is Theorem 1 stepsize dependent?:}
{\color{red} TO COMPLETE}

\textit{Q2:  Clarify line 164:}
 [Nazari et al., 2019] claims that DADAM achieves $O(\sqrt{T})$ regret as an online algorithm, but with a non-standard regret for online optimization. 
 We prove that DADAM can fail to converge which is in some sense contradicting their convergent result. The reason might be that the convergence measure defined in [Nazari et al., 2019] can hide this non-convergence issue.

\textit{Q3: A large N leads to high communication cost:}
Indeed, there will be a trade-off between communication and computation in practice.
Discussion on this will be added.
The optimal $N$ depends on the ratio between the speed of computation and communication.   

\textbf{Reviewer 6.}
\textit{Q1:Bounded gradient assumption is strong:}
This assumption is commonly assumed in the literature of adaptive gradient methods since the analyses for these algorithms are way more complicated than that for SGD. 
Relaxing this assumption is an interesting question but it will be out of the scope of this paper.

\textit{Q2: Advantages over SGD in numerical experiments:}
Our experiments in the main paper aim at showing the advantages over DADAM. 
The advantages over SGD are highlighted comparing Figure 3 and Figure 4 in Appendix D where we note that the proposed algorithm is less sensitive to the learning rate, which is one advantage of adaptive methods.

\textit{Q3: Theorem 1 violates bounded gradient assumption:}
{\color{red} TO COMPLETE}

\textit{Q4: The work seems incremental given the framework of DADAM and existing consensus algorithms:}
The main contribution of this work is the rigorous convergence analysis of adaptive gradient methods in decentralized setting and the proposed convergent algorithm \texttt{Decentralized AMSGrad}. 
In the literature, the study of decentralized optimization and adaptive gradient methods are usually independent. 
Given the non convergence of DADAM, we believe provides the first decentralized adaptive method.
% Given the fact that DADAM is not a convergent algorithm and is not noticed by the community of decentralized optimization. We believe our contribution is timely and important. 


\end{document}
