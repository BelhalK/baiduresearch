\documentclass{article}

\usepackage{neurips_2020_author_response}
% \usepackage{fullpage}
\setlength\parindent{0pt}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{color}

\begin{document}
We thank the five reviewers for their valuable feedback. We first discuss concerns shared by some reviewers:\vspace{-5pt}

\textbf{Reviewer 3, Reviewer 4 and Reviewer 6.} \textit{On the proof of Theorem 1:} We give a more rigorous proof for Theorem 1, using another counter example that satisfies all the assumptions in the paper. Consider a two-node setting with objective function $f(x) =1/2 \sum_{i=1}^2 f_i(x)$ and $f_1(x) =  \min(2x^2, 4|x|-2)$, $f_2(x) =  \min((x-1)^2, 2|x-1|-1)$, $W = [0.5,0.5;0.5,0.5]$. The optimal solution is $x^* = 1/3$ Both $f_1$ and $f_2$ are smooth and convex with bounded gradient norm 4 and 2, respectively. We also have Lipschitz smoothness $L = 4$ in A1. 
If we initialize with $x_{1,2} = x_{1,2} = -1$ and run DADAM with $\beta_1 = \beta_2 =\beta_3 = 0$ and $\epsilon \leq 1$, we will get $\hat v_{1,1} = 16$ and $\hat v_{1,2} = 4$. Since we have $|g_{t,1}| \leq 4, |g_{t,2}| \leq 2$ due to bounded gradient, and $\hat v_{t,1}$ and $\hat v_{t,2}$ are non-decreasing, we know $\hat v_{t,1} = 16, \hat v_{t,2}=4, \forall t \geq 1$. Thus, after $t=1$, DADAM is equivalent to running DGD with a re-scaled $f_1$ and $f_2$. I.e. running DGD on  
$f'(x) = \sum_{i=1}^2 f_i'(x)$ with $f_1'(x) =  0.25\min(2x^2, 4|x|-2)$ and $f_2'(x) = 0.5  \min((x-1)^2, 2|x-1|-1)$, which has unique optimal $x'=0.5$. 
Define $\bar x_t = (x_{t,1}+x_{t,2})/2$, then by Th. 2 in [1], we have $\alpha < 1/4$, $f'(\bar x_t) - f(x') = O(1/(\alpha t))$. 
Since $f'$ has a unique optima, the above bound implies $\bar x_t$ is converging to 0.5 which has non-zero gradient $\nabla f(0.5) = 0.5$.\vspace{-5pt}

[1]  Kun Yuan, Qing Ling, and Wotao Yin. "On the convergence of decentralized gradient descent." \textit{SIAM Journal on Optimization 26.3 (2016): 1835-1854.}\vspace{-5pt}

%\textbf{R2 and R6.} \textit{Novelty of the paper:}
%The novelty of our design is twofold.
%First, we aim at bridging the realms of decentralized optimization and adaptive gradient methods. 
%The study of adaptive and decentralized methods are conducted independently in the literature. 
%To the best of our knowledge, this is the first success application (with rigorous convergence guarantee) of adaptive methods in decentralized optimization. 
%Second, our gossip technique is not the direct average consensus mechanism used in the extensively studied DGD.
%% The particular consensus technique we adopt was just used in optimization a few years ago. 
%We will add more discussion on why the direct average consensus mechanism in DGD cannot be used in our case.
%Given the non convergence of DADAM, we believe our paper provides the first decentralized adaptive method.\vspace{-5pt}

\textbf{Reviewer 1.}
\textit{Q1: More explanations on notations:} 
Notations will be explained and simplified in the revised paper.\vspace{-5pt}

\textit{Q2: Better presentation of line 41-45:} Line 41-45 simply highlights that our setting is different from  [Reddi et al., 2019], not arguing that their approach is incorrect. We will revise this part to avoid confusion.\vspace{-5pt}

\textit{Q3: Assumption A2 is strong:}
A2 is necessary for the analysis of adaptive gradient methods and is standard in the literature.
In the decentralized literature, this assumption might be viewed as strong since only the convergence of SGD-like algorithms has been dealt with so far.
% However, the convergence analyses of adaptive gradient methods are just established a few years ago and A2 is commonly assumed in the literature. 
Relaxing A2 is interesting but it is out of the scope of this work. \vspace{-5pt}

\textit{Q4: Similar ideas on consensus of step-size:}
Thank you for providing the relevant references. 
[2] averages the predefined stepsizes across iterations to make it more tolerant to staleness in asynchronous updates.  
[3] does not explicitly apply consensus on stepsize but rather allows the stepsize on different nodes to be different (the maximum difference depends on the graph structure) for deterministic strongly convex problems. 
Our learning rate consensus is \emph{across workers} instead of across iterations and we allow the adaptive learning sequence on different nodes to be completely different. 
Our technique and motivation are thus different from these works. 
A discussion about this will be added.\vspace{-5pt}

%\textit{Q5: More experiments:}
%Experiments with larger datasets and complex models are under production.\vspace{-5pt}

\textbf{Reviewer 2.}
\textit{Q1: Connection to counter example in [Reddi et al., 2019]:}
Both our example and the one in [Reddi et al., 2019] use the idea that sample dependent learning rate can lead to non-convergence. 
Yet, in decentralized setting, the sample dependent learning rate is caused by different nodes having different adaptive learning rate sequences, while in [Reddi et al., 2019], the non-convergence is caused by over-adaptivity of adaptive learning rate of Adam. \vspace{-5pt}

\textit{Q2: Highlight the novelty of the algorithm design:}
The novelty of our design is twofold.
First, we aim at bridging the realms of decentralized optimization and adaptive gradient methods. 
The study of adaptive and decentralized methods are conducted independently in the literature. 
To the best of our knowledge, this is the first success application (with rigorous convergence guarantee) of adaptive methods in decentralized optimization. 
Second, our gossip technique is not the direct average consensus mechanism used in the extensively studied DGD.
% The particular consensus technique we adopt was just used in optimization a few years ago. 
We will add more discussion on why the direct average consensus mechanism in Decentralized Gradient Descent cannot be used in our case.\vspace{-5pt}

\textbf{Reviewer 3.}
\textit{Q1: More rigorous proof for Theorem 1:}
 See an updated proof above in "On the proof of Theorem 1".\vspace{-5pt}

\textit{Q2: More discussion of Th. 2 and Alg. 2/3:}
%We will add more interpretation of the theoretical results and algorithms to improve the clarity of the paper.
We will add more interpretations on this to improve the clarity of the paper.\vspace{-5pt}

\textit{Q3: Tuning $\epsilon$ for different algorithms:}
We will include this as a tunable hyperparameter in the future experiments. \vspace{-5pt}
% This will indeed make the experiments more fair even though DADAM is not convergent over heterogeneous data.

\textbf{Reviewer 4.}
\textit{Q1: Is Theorem 1 stepsize dependent?:} It is actually not, see "On the proof of Theorem 1" for an update.\vspace{-5pt}

\textit{Q2:  Clarify line 164:}
 [Nazari et al., 2019] claims that DADAM achieves $O(\sqrt{T})$ regret, but with a non-standard regret for online optimization. 
 We prove that DADAM can fail to converge which contradicts their convergent result. 
 The reason is that the convergence measure defined in [Nazari et al., 2019] may hide this non-convergence issue.\vspace{-5pt}

\textit{Q3: A large N leads to high communication cost:}
Indeed, there is a trade-off between communication and computation in practice.
%Discussion will be added.
The optimal $N$ depends on the ratio between the speed of computation and communication.   \vspace{-5pt}

\textbf{Reviewer 6.}
\textit{Q1:Bounded gradient assumption is strong:}
This assumption is commonly assumed in the literature of adaptive gradient methods since the analyses for these algorithms are way more complicated than that for SGD. 
Relaxing this assumption is an interesting question but it will be out of the scope of this paper.\vspace{-5pt}

\textit{Q2: Advantages over SGD in numerical experiments:}
Our experiments in the main paper aim at showing the advantages over DADAM. 
The advantages over SGD are highlighted comparing Figure 3 and Figure 4 in Appendix D where we note that the proposed algorithm is less sensitive to the learning rate, which is one advantage of adaptive methods.\vspace{-5pt}

\textit{Q3: Theorem 1 violates bounded gradient assumption:} We have an updated proof using a counter example satisfying all assumptions, see "On the proof of Theorem 1".\vspace{-5pt}

\textit{Q4: The work seems incremental given [...] existing consensus algorithms:}
The main contribution of this work is the rigorous convergence analysis of adaptive gradient methods in decentralized setting and the proposed convergent algorithm \texttt{Decentralized AMSGrad}. 
In the literature, the study of decentralized optimization and adaptive gradient methods are usually independent. 
Given the non convergence of DADAM, we believe our paper provides the first decentralized adaptive method.


% Given the fact that DADAM is not a convergent algorithm and is not noticed by the community of decentralized optimization. We believe our contribution is timely and important. 


\end{document}
