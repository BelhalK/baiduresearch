\documentclass{article}
\usepackage{neurips_2020_author_response,xcolor,bm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{lipsum}


\begin{document}

We would like to thank four reviewers for their feedback. Upon acceptance, we will include in the final version \emph{{\sf (a)} a clearer presentation of the numerical results} and \emph{{\sf (b)} missing references}. 
We first discuss a common concern shared by \textbf{\color{blue}reviewer 1}, \textbf{\color{red} reviewer 2}, \textbf{\color{green!50!black}reviewer 4}.

${\color{blue}\bullet}\!~{\color{red}\bullet}\!~{\color{green!50!black}\bullet}$ \textbf{Novelty of The Contribution}: 
We want to stress on the generality of our incremental framework, which tackles a \emph{constrained}, \emph{non-convex} and \emph{non-smooth} optimization problem. 
The main contribution of this paper is to propose a \emph{unifying} framework for the analysis of a large class of optimization algorithms which indeed includes well-known but not so well-studied algorithms.
The major goal here is to relax the class of surrogate functions used in MISO [Mairal, 2015] and replace that by the respective Monte-Carlo approximations.
We provide a general algorithm and global convergence analysis under mild assumptions on the model and show that two examples, MLE for general latent data models and Variational Inference, are its special instances.
Working at the crossroads of \emph{Optimization} and \emph{Sampling} constitues what we believe to be the novelty and the technicality of our theoretical results.


\textbf{\textcolor{blue}{Reviewer 1:}} We thank the reviewer for valuable comments and references. We would like to make the following clarification regarding the difference with MISO:

\textbf{Originality:} The main contribution of the paper is to extend the MISO algorithm when the surrogate functions are not tractable. 
We motivate the need for dealing with intractable surrogate functions when nonconvex latent data models are being trained. 
In this case, the surrogate functions can be written as an expectation due to the latent structure of the problem and the nonconvexity yields a generally intractable expectation to compute. 
The only option is to build a stochastic surrogate function based on a Monte Carlo approximation.


\textbf{\textcolor{red}{Reviewer 2:}} We thank the reviewer for the useful comments. Our point-to-point response is as follows:

\textbf{Numerical Plots:} Due to space constraints, we only presented several dimensions for the logistic parameters and the mean of the latent variable. 
As the reviewer mentioned, we also learn the variance of those latent variables and the convergence plots of those variances will be added to the rebuttal version.
For all experiments, we made sure that the estimated parameters for each method converge to the same value. 
This was our main criteria to claim that a method is faster than the other. 
Then, the problem being (highly) nonconvex, the estimations can indeed get trapped in various local minima. 
Regardless of generalization properties of the output vector of estimated parameters, our focus through those numerical examples was to highlight faster convergence, in iteration, of our method.

\textbf{Wallclock Time}:
Wallclock time per iteration is comparable for each method. Indeed the methods always only involve first order computation. Yet, we acknowledge that MISSO can present some memory bottlenecks since it requires to store $n$ gradients through the run. This has not been a problem for the presented numerical examples.

\textbf{Parameter Tuning:}
The baseline methods were tuned and presented to the best of their performances both with regards to their stepsize (grid search) and minibatch size.
We believe your remark refers to the first numerical example (logistic regression with missing values): Regarding the stepsize, as MCEM does not have one, we indeed tuned the stepsize of SAEM. Rather than $c/k$, common practice is to tune a parameter $\alpha$ such that $\gamma_k = 1/\gamma^{\alpha}$. We report results for SAEM with the best $\alpha$ ($\alpha = 0.6$). Regarding batch size, for SAEM and MCEM both are full batch methods and the idea here is to compare different values of minibatch size for the MISSO method to see its influence on the performances.



\textbf{\textcolor{yellow!80!black}{Reviewer 3:}} We thank the reviewer for valuable comments and references. We clarify the following point:

\textbf{Verification of the Assumptions:} {\color{red} TODO }



\textbf{\textcolor{green!50!black}{Reviewer 4:}} We thank the reviewer for valuable comments and the numerous related references. Our point-to-point response is as follows:

\textbf{Comparison to [Murray+, 2012] and [Tran+, 2017]:} We agree that the literature of the Bayesian Inference is rich and could be more explicit in our introduction. Though, our focus is not on a method for posterior sampling purposes but rather on the interlink between nonconvex optimization and sampling techniques. While our analysis cover direct, and MCMC, sampling, we agree that future research directions analyzing complex bayesian inference techniques and nonconvex optimization can interesting.

\textbf{Comparison to [Kang+, 2015]:} [Kang+, 2015] is indeed a relevant reference and will be added to the introduction along the MISO [Marial, 2015] reference.
Nevertheless, [Kang+, 2015] solely focus on MM scheme when the surrogate functions are deterministic, \textit{i.e.} can be computed exactly. 
While the objective function is nonconvex as in our work, the construction of their surrogate functions does not imply any latent structure, inherent to the problem, and thus are easily computed and characterized for convergence purposes. Also, their analysis requires \emph{strong convexity} of the gap between the convex surrogate and the nonconvex objective function while our analysis only requires a \emph{smoothness-like} assumption, see \textbf{H2}.

%\textbf{Comparison with MC-ADAM Figure 2:}



\end{document}

