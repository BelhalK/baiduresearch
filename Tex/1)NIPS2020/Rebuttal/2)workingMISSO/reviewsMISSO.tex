\documentclass{article}
\usepackage{neurips_2019_author_response,xcolor,bm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{lipsum}


\begin{document}

We would like to thank three reviewers for their feedback. Upon acceptance, we will include in the final version \emph{{\sf (a)} new local linear convergence results for fiEM method}, \emph{{\sf (b)} an improved presentation of main results} and \emph{{\sf (c)} missing references}. We first discuss a few common concerns shared by \textbf{\color{blue}reviewer 1}, \textbf{\color{red} reviewer 2}, \textbf{\color{green!50!black}reviewer 3}.

${\color{blue}\bullet}\!~{\color{red}\bullet}$ \textbf{Local Linear Convergence of fiEM}: As observed by the reviewers, empirically fiEM shows a local linear convergence similar to sEM-VR. We found that fiEM has local linear convergence \textbf{in theory}, too. The new analysis requires same assumptions as [Thm.1,Chen+2018] and adopts proof of [Defazio+2014]. We show $\mathbb{E} \| \hat{\bf s}^{(k)} - {\bf s}^\star \|^2 \leq (1-\delta)^k \| \hat{\bf s}^{(0)} - {\bf s}^\star\|^2$ for $k \geq 0$ with $\delta = \Theta(1/n)$, where  ${\bf s}^\star$ is a stationary point to (19). %It supports our empirical results. 

${\color{blue}\bullet}\!~{\color{red}\bullet}$ \textbf{Satisfaction of Assumptions}:  \underline{\emph{All}} assumptions H1-H5 are verified rigorously in the   GMM, pLSA applications presented, as proven in Appendix G. They are mild even though should be checked on a case-by-case basis. Reviewers are referred to [McLachlan\&Krishnan 2007] which shows satisfaction of similar assumptions on a variety of applications.

${\color{red}\bullet}\!~{\color{green!50!black}\bullet}$ \textbf{Clarity:} We admit it is a challenging task to present all technical results within the page limit, but we will try our best to improve in the final version, viz.~using a running example to illustrate the assumptions used and implementation of algorithms.  
%Second, we will mention the idea of MISO that is presented in (15) to (18). 
We will also clarify about the expectation operators in theorems and correct typos.

\textbf{\textcolor{blue}{Reviewer 1:}} We thank the reviewer for valuable comments and references. Our point-to-point response is as follows:

\textbf{Related work:} The paper [Karimi+2019] is relevant and will be included. Thank you for bringing it to our attention. Karimi+[2019] focused on a biased stochastic approximation scheme and gave a global convergence rate for sEM. In this case, their analysis shares similar scaled gradient interpretation as fiEM and sEM-VR, yet w/o variance reduction.
%In this sense, our paper shows the fast global convergence of the latter.

\textbf{iEM's Rate}: You are right as the rate of iEM is comparable to GD. Yet, iEM is a popular method without a previously known global rate. Indeed, the  comparison of iEM to fiEM, sEM-VR (theoretical \& empirical) is our main contribution.

%\textbf{Linear convergence rates:} The reviewer is correct that the empirical convergence appears to be linear, which is better than the sublinear rate predicted by our analysis. We suspect that fiEM also enjoys a local linear convergence rate similar to sEM-VR as shown by Chen et al. (2018), which explains the empirical observations. Nevertheless, with a lower target accuracy, Fig. 1(right) shows that our analyzed dependence on data size $n$ is tight.

\textbf{Comparison to [Chen+2018]}:
Our assumptions are more practical and less restrictive.
%Under our notations, a major assumption made in [Chen+2018] is that the sufficient statistics ${\bf s}_i( \overline{\bm{\theta}} ( {\bm s}' ) )$ is $L_s$-Lipschitz continuous in ${\bm s}'$ for each $i$. This is, however, a blanket assumption that can be difficult to verify. On the other hand, in Lemma 4 of our paper, we show that  H1-H5 implies the said Lipschitz continuous condition. Our H1-H5 are more practical as the latter are \emph{verifiable} assumptions.
%To prove local linear convergence, [Thm.1, Chen+2018] assumes $\| \hat{\bf s}^{(k)} - {\bf s}^\star\|$ is bounded in a radius of ${\cal O}(1/L_s)$. Our global rate does not depend on such condition. That said, we have also shown fiEM converges linearly locally (see above).
Global convergence to stationary point for sEM-VR in [Thm.2, Chen+2018] assumes i) the sufficient statistics ${\bf s}_i( \overline{\bm{\theta}} ( {\bm s}' ) )$ is $L_s$-Lipschitz continuous in ${\bm s}'$,  $\forall i$ -- this is implied by our H1-H5 via Lemma 4; ii) the complete log-likelihood is strongly concave -- this is slightly relaxed in our H4 which only requires a unique global minimizer for the complete log-likelihood.
Besides, H1-H5 are \textbf{directly verifiable} (as explained above) and we provide the rate towards a stationary point.
Lastly, local convergence in [Thm.1, Chen+2018] requires $\| \hat{\bf s}^{(k)} - {\bf s}^\star\|$ to be in a ball of radius ${\cal O}(1/L_s)$ for \textbf{any} $k\geq1$. This is a strong assumption which is not directly verifiable even if $\hat{\bf s}^{(0)} \approx {\bf s}^\star$ is known a-priori (as [Chen+2018] shows convergence in expectation).

\textbf{\textcolor{red}{Reviewer 2:}} We thank the reviewer for useful comments. Please find the comparison of fiEM, sEM-VR below:

\textbf{Comparing fiEM to sEM-VR:} This comparison is analogous to comparing SAGA to SVRG for finite sum optimization, and there is no clear ordering. In short, it depends on the trade-off of memory imprint and computation complexity. sEM-VR requires ${\cal O}({\rm dim}({\sf S}))$ space to store $\overline{\bf s}^{(\ell(k))}$, yet a \emph{full pass} on the data set is needed at each epoch, resulting in higher complexity; meanwhile,  fiEM only processes the data set \emph{incrementally}, but it requires ${\cal O}(n {\rm dim}({\sf S}))$ to store the variables involved.
%Besides, we showed that fiEM converges linearly locally as sEM-VR (see above).
We remark that the global rate for sEM-VR is \textbf{not proven} in [Chen+2018].
In Fig.~2 we show fiEM outperformed sEM-VR in one dataset (a bigger one) but was outperformed by sEM-VR in the other (a smaller one).
%This is similar to the empirical performance comparisons between SAGA and SVRG, where there is no clear ordering.
%This is normal since we did not claim fiEM to be always superior to sEM-VR.
%, however, we emphasize that fiEM has better performance on the large dataset.

%\textbf{Relaxing compactness assumption:} We agree with the concern about assumption H1 on the compactness of the sets $S$ and $Z$. Both our examples satisfy the compactness of S and Z, as shown in the supplementary material, but indeed is not necessarily satisfied for every problem. One interesting perspective is not relax this assumption to obtain non-asymptotic bounds.





\textbf{\textcolor{green!50!black}{Reviewer 3:}} We thank the reviewer for the comments. We clarify that \emph{in addition} to analyzing iEM using the MISO framework (which will be mentioned explicitly), we analyzed fiEM, sEM-VR with a \textbf{completely different} framework w/ \textbf{scaled gradient}, the latter constitutes our main contribution of fast global convergence rates; see p.2 of our paper. 

\textbf{Global Convergence \& H4:} We emphasize H4 \textbf{does not} imply that every stationary point of (1) is global minima, as having a  unique global minimizer \emph{does not} imply \emph{any first order critical point is global minima}.
%, as an elementary example, $f(x) = -\exp(-x^2) - (1/2) \exp(-(x-5)^2)$ has two first-order critical points: $x \approx 0$ is unique global optimal, and $x \approx 5$ is sub-optimal.
Also, H4 refers to \emph{complete log-likelihood} $L({\bm s}, \bm{\theta})$ with fixed ${\bm s}$, instead of incomplete log-likelihood $\overline{\cal L}(\bm{\theta})$ in (1). It holds for most exponential family models where  EM is useful [McLachlan\&Krishnan 2007]. Mind that $\overline{\cal L}(\bm{\theta})$ is  non-convex and our convergence is \emph{global} in the sense that it does not restrict the initialization, a common assumption for analysis of EM.

%The global convergence is in the sense of  paper proves the \emph{global convergence rate} to a stationary point of  $\overline{\cal L}(\bm{\theta})$.


%Assumption H4 is made on the complete log likelihood which is a convex surrogate function of the statistics $s$ that can efficiently be optimized at each iteration.

\textbf{Bounds in theorems:} 
The current presentation style of theorems, which evaluates the gradient norm of a randomly terminated stochastic EM solution, is common in \textbf{stochastic non-convex} optimization e.g., [Ghadimi\&Lan 2013,Reddi+2016a/b]. Part of the reason is that it results in a practical solution. 
While picking the best iterate leads to the same sublinear rate as ours, doing so  involves a full pass on the data ($\nabla \overline{\cal L}$) and computing the incomplete likelihood, both are \textbf{difficult} tasks avoided in stochastic EM methods. 
Besides, as the reviewer mentioned, both random termination and best iterate schemes lead to a quantity upper bounded by $\sum_k \mathbb{E} \| \nabla \overline{\cal L}( \bm{\theta}^{(k)} )\|^2 / K_{\sf max}$. This quantity is not equal to the \emph{averaged iterate}, and upper bounding it by ${\cal O}(1/K_{\sf max})$ is a non-trivial task -- and is precisely our main contribution.

%It is a very common technique in \emph{stochastic} non-convex optimization analysis, .
%First, we note that the bounds in our theorems 




%While picking the best iterate leads to the same result as ours, using the averaged iterates $\sum_k \bm{\theta}^{(k)} / K_{\sf max}$ does not lead to the desirable result. The theorems stated assumes a random termination scheme via an r.v.~$K$ [see Algo.~1]. As derived by the reviewer, the random quantity is upper bounded by $\sum_k \mathbb{E} \| \nabla \overline{\cal L}( \bm{\theta}^{(k)} )\|^2 / K_{\sf max}$. Bounding the latter is however non-trivial which is where we spent most of efforts on.
%Indeed, random termination is common in \emph{stochastic} non-convex optimization analysis, e.g., [Ghadimi\&Lan 2013]. It has a practical advantage over the best iterate scheme which involves a full pass on the data ($\nabla \overline{\cal L}$) and computing the incomplete likelihood, both are difficult tasks avoided in stochastic EM methods.

%The best iterate / random termination (as stated in the theorems) give the same upper bound $\mathcal{O}(1/K_{max})$ result. However, picking the best iterate is not computationally feasible as that involves evaluating the gradient for $L$ - involving a full pass on the data and computing the incomplete likelihood, both are difficult tasks avoided in stochastic EM methods. The current bound is thus written with an estimator used in real life. As we remarked in the paper, this technique is common in non-convex stochastic optimization, see [Ghadimi and Lan, 2013].

%\textbf{Clarity and Typos:}

\textbf{Theorem 1 of Paper 1613:} Indeed, Theorem 1 for iEM is  a special case of [Thm.~1, 1613]. We will cite the latter properly. Our main contribution here lies on \emph{fast convergence} of fiEM, sEM-VR shown by a different framework, see Theorem 2. Detailed comments about the difference between this paper and 1613 has been sent to the AC.

\end{document}

