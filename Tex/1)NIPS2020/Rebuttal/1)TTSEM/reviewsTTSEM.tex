\documentclass{article}
\usepackage{neurips_2020_author_response,xcolor,bm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{lipsum}


\begin{document}

We would like to thank the five reviewers for their feedback. Upon acceptance, we will include in the final version \emph{{\sf (a)} improved notations}, \emph{{\sf (b)} an improved presentation of related work} and \emph{{\sf (c)} missing references}. 
We first discuss a few common concerns shared by \textbf{\color{blue}R1}, \textbf{\color{red} R2}, \textbf{\color{green!50!black}R3},\textbf{\color{yellow!50!black}R4} and \textbf{\color{purple}R5}.

${\color{blue}\bullet}\!~{\color{red}\bullet}\!~{\color{green!50!black}\bullet}\!~{\color{yellow!50!black}\bullet}\!~{\color{purple}\bullet}$ \textbf{Notations Issue}: 
We acknowledge the cumbersome notations of our paper and will modify them in order to reflect the reviewers remarks. 
Deterministic and Stochastic quantities will be clearly identified in their notations and some less important abstractions will be dismissed in the revised paper.


${\color{blue}\bullet}\!~{\color{red}\bullet}$ \textbf{Novelty:}: 
We agree with the reviewers that our contribution stands as a combination of variance reduction ([11,19]), EM ([20,22]) and SA ([12,32]). 
The synthesis of these contributions into a single framework constitues the originality of this paper on the algorithmic and theoretical plans.
Adding a layer of Monte Carlo (MC) approximation and the stepsize $\gamma_k$ to reduce its variance introduce some new technicalities and challenfes that need careful considerations.\vspace{-0.05in}


%${\color{green!50!black}\bullet}\!~{\color{purple}\bullet}$ \textbf{Importance of the Assumptions}: 



\textbf{\textcolor{blue}{R1:}} We thank the reviewer for valuable comments. We would like to clarify the following points:\vspace{-0.05in}

%\textbf{Potential Applications:} 
%We admit it is a challenging task to present all technical results and obvious applications within the page limit, but we will try our best to improve in the final version, using a running example to illustrate the potential applications. 
%For instance, the deformable template analysis or the pharmacokinetics example (which can be found in the Appendix) will be presented throughout the paper with clear motivation for using our scheme.

\textbf{Exponential Family:} 
The curved exponential family is a classical one in the EM-related literature and holds for most models where  EM is useful [McLachlan\&Krishnan 2007] . 
While remaining general, the advantage of such family is to write the algorithm updates only with respect to the sufficient statistics and not in the space of parameters $\theta$. 
%The M-step is thus in general expressed in \emph{closed-form} and not as a black-box optimization ($\arg\max$ operation).
%Yet, we would like to clarify to the reviewer that exponential family does not imply tractable posterior. 
%The intractability of this posterior sampling step is, in our case, due to the nonconvexity of the loss function. 
Yet, we would like to clarify that due to Bayes rule and the intractable normalizing constant, a complete likelihood that belongs to the exponential family does not imply a tractable posterior distribution.\vspace{-0.05in}


\textbf{\textcolor{red}{R2:}} We thank the reviewer for the comments and typos. We add the following remarks:\vspace{-0.05in}

\textbf{Comparison with [Karimi+, 2019]:} 
%Their work cannot be directly compared to ours since the models tackled are different. 
While both of these papers are dealing with nonconvex functions, the added layer of randomness, due to the sampling step in our method, makes it a practically and theoretically different approach.
Yet, as pointed by the \textcolor{red}{R2}, Lemmas 1 and 2 are needed to characterize the deterministic part of those models. 
The stochastic part (posterior sampling) is new and is the object of our paper.

\textbf{Comparison with gradient-based EM algorithms:} 
Gradient-based methods have been developed and analyzed in [Zhu+, 2017] but remain out of the scope as they tackle the high-dimensionality issue. 
%Gradient-EM are also relevant when the M-step can only be solved through a gradient descent method. 
The exponential family allows to leverage the sufficient statistics and a max. function $ \overline{\theta}( \overline{\textbf{s}}(\theta) )$ updating $\theta$ without an inner iterative process (eg. GD).\vspace{-0.05in}


\textbf{\textcolor{green!50!black}{R3:}} We thank the reviewer for insightful comments and typos. Our point-to-point response is as follows:\vspace{-0.05in}

\textbf{Compacity assumption:}  
%We agree with the reviewer on the need for random projections in order to stay in a compact set.
For our analysis, we assume that the statistics always remain in a defined compact subset of $\mathbb{R}^d$.
While this assumption holds for the GMM example, it is not the case for the deformable template analysis one.
We implemented the \emph{Truncation on random boundaries} techniques found in [Allassonniere+, 2010] based on restart.

\textbf{Comparison of proxies (Table 1):} 
The advantage between the incremental proxy and the two variance reduction yields from their sublinear convergence rate (see Theorems 2 and 3).
The vrTTEM requires the tuning of the epoch length $m$ but stores one vector of $n+1$ quantities while the fiTTEM requires storing two vectors of params without any tuning.\vspace{-0.05in}

\textbf{\textcolor{yellow!50!black}{R4:}} We thank the reviewer for valuable comments and references. Our point-to-point response is as follows:\vspace{-0.05in}

\textbf{Various questions:} $\bullet$ $t_i^k$ is not empty by construction since it stores the iteration at which index $i$ was last drawn. 
They are initialized after a single pass over all indices.
$\bullet$ We are not aware of similar algorithms mixing optimization and sampling techniques. 
Neither SAEM nor MCEM have been studied non asymptotically.
$\bullet$ The random stopping criterion $K_m$ is common in non-convex optimization, see [Ghadimi \& Lan,2013] and is needed for theoretical purposes.\vspace{-0.05in}


\textbf{\textcolor{purple}{R5:}} We thank the reviewer for valuable comments and references. We make the following precision:\vspace{-0.05in}

\textbf{Comparison to EM theory papers:}
%We thank the reviewer for the comprehensive list of references.
After careful consideration, the listed references are either related to deterministic EM methods, where no sampling is required since the expectations are always tractable, or to gradient EM method which has been dealt with above (see \textbf{\textcolor{red}{R2:}}).
%They also focus on mixture models which is an instance of the exponential family we are tackling.
We agree that more specific studies depending on the model (such as mixture) would lead to different analysis, yet we would lose the generality of our paper.


\textbf{Response to Additional Feedbacks :}
%In Eq. (9), $\rho_k$ is crucial for the incremental update (as in [Karimi+, 2019], it performs the variance reduction).
%$\gamma_k$ is important for the variance reduction of the MC approximation. 
$\bullet$ The two-timescale update is crucial for the following reason: the \emph{noise induced by sampling a single index} is tempered by $\rho_k$ (Eq. (9)) while the \emph{noise induced by sampling the latent variables} is tempered by $\gamma_k$. 
Initial runs without $\gamma_k$ showed poor convergence properties due to the large variance of the posterior sampling.
Of course the downside will be the tuning of two stepsizes. 
In practice, using a decreasing stepsize as $\gamma_k = 1/k^{\alpha}$ and constant $\rho \propto n^{2/3}$ works well.
Using a non constant $\rho_k$ is an interesting open question.
$\bullet$ Notations and paragraph on stepsizes will be clarified in the revised version.
$\bullet$ Assumptions: \textbf{A1-A4} are necessary for the non asymptotic analysis and are easily satisfied for exponential family model such as GMM as thoroughly described in [Karimi+, 2019].
\textbf{A5} is a classical result which we realized that it needs to be developed in greater detail. For the case of i.i.d.~samples, using Example 19.7, Lemma 19.36 from `Asymptotic Statistics' by van der Vaart (2000), it can be shown that the MC noise $\eta_i$ is is sublinear in $p$. Meanwhile, the cases for Markov samples are not as obvious, though comparable results can be found for $\beta$-mixing processes [Thm.2, Doukhan+1995]. 
$\bullet$ The term \emph{Global} is employed in the sense that it does not restrict the initialization, a common assumption for the analysis of EM.
$\bullet$ All the methods converge to a reasonable precision (y axis from 1 to $10^{-3}$).
The GMM example illustrates how the proposed framework achieves to reduce the variance of baseline method in order to reach a higher precision ($10^{-3}$ in this example).


\end{document}

