\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2020)Chen, Karimi, Zhao, and Li]{chen2020decent}
X.~Chen, B.~Karimi, W.~Zhao, and P.~Li.
\newblock Convergent adaptive gradient methods in decentralized optimization.
\newblock In \emph{Submitted}, 2020.

\bibitem[Haddadpour et~al.(2020)Haddadpour, Karimi, Li, and Li]{had2020}
F.~Haddadpour, B.~Karimi, P.~Li, and X.~Li.
\newblock Fedsketch: Communication-efficient federated learning via sketching.
\newblock In \emph{Submitted}, 2020.

\bibitem[Karimi and Lavielle(2018)]{karimi2018eff}
B.~Karimi and M.~Lavielle.
\newblock {Efficient Metropolis-Hastings sampling for nonlinear mixed effects
  models}.
\newblock \emph{{Proceedings of BAYSM 2018}}, 2018.

\bibitem[Karimi and Li(2020{\natexlab{a}})]{karimi2020hwa}
B.~Karimi and P.~Li.
\newblock Hwa: Hyperparameters weight averaging bayesian neural networks.
\newblock In \emph{Submitted}, 2020{\natexlab{a}}.

\bibitem[Karimi and Li(2020{\natexlab{b}})]{karimi2020tts}
B.~Karimi and P.~Li.
\newblock Two timescale stochastic em algorithms.
\newblock In \emph{Submitted}, 2020{\natexlab{b}}.

\bibitem[Karimi et~al.(2018)Karimi, Lavielle, and Moulines]{karimi2018fsaem}
B.~Karimi, M.~Lavielle, and E.~Moulines.
\newblock f-saem: A fast stochastic approximation of the {EM} algorithm for
  nonlinear mixed effects models.
\newblock \emph{Computational Statistics and Data Analysis, CSDA}, 2018.

\bibitem[Karimi et~al.(2019{\natexlab{a}})Karimi, Lavielle, and
  Moulines]{karimi2019convergence}
B.~Karimi, M.~Lavielle, and {\'E}.~Moulines.
\newblock On the convergence properties of the mini-batch em and mcem
  algorithms.
\newblock 2019{\natexlab{a}}.

\bibitem[Karimi et~al.(2019{\natexlab{b}})Karimi, Miasojedow, Moulines, and
  Wai]{karimi2019non}
B.~Karimi, B.~Miasojedow, E.~Moulines, and H.-T. Wai.
\newblock Non-asymptotic analysis of biased stochastic approximation scheme.
\newblock In A.~Beygelzimer and D.~Hsu, editors, \emph{Proceedings of the
  Thirty-Second Conference on Learning Theory}, volume~99 of \emph{Proceedings
  of Machine Learning Research}, pages 1944--1974, Phoenix, USA, 25--28 Jun
  2019{\natexlab{b}}. PMLR.

\bibitem[Karimi et~al.(2019{\natexlab{c}})Karimi, Wai, and
  Moulines]{karimi2019misso}
B.~Karimi, H.-T. Wai, and E.~Moulines.
\newblock A doubly stochastic surrogate optimization scheme for non-convex
  finite-sum problems.
\newblock \emph{Submitted paper}, 2019{\natexlab{c}}.

\bibitem[Karimi et~al.(2019{\natexlab{d}})Karimi, Wai, Moulines, and
  Lavielle]{karimi2019global}
B.~Karimi, H.-T. Wai, E.~Moulines, and M.~Lavielle.
\newblock On the global convergence of (fast) incremental expectation
  maximization methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2837--2847, 2019{\natexlab{d}}.

\bibitem[Karimi et~al.(2020)Karimi, Wai, Moulines, and Li]{karimi2020misso}
B.~Karimi, H.-T. Wai, E.~Moulines, and P.~Li.
\newblock Misso: Minimization by incremental stochastic surrogate optimization
  for large scale nonconvex and nonsmooth problems.
\newblock In \emph{Submitted}, 2020.

\bibitem[Ren et~al.(2020)Ren, Zhao, Karimi, and Li]{ren2020vfg}
S.~Ren, Y.~Zhao, B.~Karimi, and P.~Li.
\newblock Vfg: Variational flow graphical model with hierarchical latent
  structure.
\newblock In \emph{Submitted}, 2020.

\bibitem[Wang et~al.(2020)Wang, Karimi, Li, and Li]{kun2020}
J.-K. Wang, B.~Karimi, X.~Li, and P.~Li.
\newblock An optimistic acceleration of amsgrad for nonconvex optimization.
\newblock In \emph{Submitted}, 2020.

\bibitem[Zhou et~al.(2020)Zhou, Karimi, Yu, Xu, and Li]{zhou2020towards}
Y.~Zhou, B.~Karimi, J.~Yu, Z.~Xu, and P.~Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1--10, 2020.

\end{thebibliography}
