\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2020)Chen, \textbf{B. Karimi}, Zhao, and
  Li]{chen2020decent}
X.~Chen, \textbf{B. Karimi}, W.~Zhao, and P.~Li.
\newblock Convergent adaptive gradient methods in decentralized optimization.
\newblock In \emph{Submitted}, 2020.

\bibitem[Haddadpour et~al.(2020)Haddadpour, \textbf{B. Karimi}, Li, and
  Li]{had2020}
F.~Haddadpour, \textbf{B. Karimi}, P.~Li, and X.~Li.
\newblock Fedsketch: Communication-efficient federated learning via sketching.
\newblock In \emph{Submitted}, 2020.

\bibitem[Ren et~al.(2020)Ren, Zhao, \textbf{B. Karimi}, and Li]{ren2020vfg}
S.~Ren, Y.~Zhao, \textbf{B. Karimi}, and P.~Li.
\newblock Vfg: Variational flow graphical model with hierarchical latent
  structure.
\newblock In \emph{Submitted}, 2020.

\bibitem[\textbf{B. Karimi} and Lavielle(2018)]{karimi2018eff}
\textbf{B. Karimi} and M.~Lavielle.
\newblock {Efficient Metropolis-Hastings sampling for nonlinear mixed effects
  models}.
\newblock \emph{{Proceedings of BAYSM 2018}}, 2018.

\bibitem[\textbf{B. Karimi} and Li(2020{\natexlab{a}})]{karimi2020hwa}
\textbf{B. Karimi} and P.~Li.
\newblock Hwa: Hyperparameters weight averaging bayesian neural networks.
\newblock In \emph{Submitted}, 2020{\natexlab{a}}.

\bibitem[\textbf{B. Karimi} and Li(2020{\natexlab{b}})]{karimi2020tts}
\textbf{B. Karimi} and P.~Li.
\newblock Two timescale stochastic em algorithms.
\newblock In \emph{Submitted}, 2020{\natexlab{b}}.

\bibitem[\textbf{B. Karimi} et~al.(2018)\textbf{B. Karimi}, Lavielle, and
  Moulines]{karimi2018fsaem}
\textbf{B. Karimi}, M.~Lavielle, and E.~Moulines.
\newblock f-saem: A fast stochastic approximation of the {EM} algorithm for
  nonlinear mixed effects models.
\newblock \emph{Computational Statistics and Data Analysis, CSDA}, 2018.

\bibitem[\textbf{B. Karimi} et~al.(2019{\natexlab{a}})\textbf{B. Karimi},
  Lavielle, and Moulines]{karimi2019convergence}
\textbf{B. Karimi}, M.~Lavielle, and {\'E}.~Moulines.
\newblock On the convergence properties of the mini-batch em and mcem
  algorithms.
\newblock 2019{\natexlab{a}}.

\bibitem[\textbf{B. Karimi} et~al.(2019{\natexlab{b}})\textbf{B. Karimi},
  Miasojedow, Moulines, and Wai]{karimi2019non}
\textbf{B. Karimi}, B.~Miasojedow, E.~Moulines, and H.-T. Wai.
\newblock Non-asymptotic analysis of biased stochastic approximation scheme.
\newblock In A.~Beygelzimer and D.~Hsu, editors, \emph{Proceedings of the
  Thirty-Second Conference on Learning Theory}, volume~99 of \emph{Proceedings
  of Machine Learning Research}, pages 1944--1974, Phoenix, USA, 25--28 Jun
  2019{\natexlab{b}}. PMLR.

\bibitem[\textbf{B. Karimi} et~al.(2019{\natexlab{c}})\textbf{B. Karimi}, Wai,
  and Moulines]{karimi2019misso}
\textbf{B. Karimi}, H.-T. Wai, and E.~Moulines.
\newblock A doubly stochastic surrogate optimization scheme for non-convex
  finite-sum problems.
\newblock \emph{Submitted paper}, 2019{\natexlab{c}}.

\bibitem[\textbf{B. Karimi} et~al.(2019{\natexlab{d}})\textbf{B. Karimi}, Wai,
  Moulines, and Lavielle]{karimi2019global}
\textbf{B. Karimi}, H.-T. Wai, E.~Moulines, and M.~Lavielle.
\newblock On the global convergence of (fast) incremental expectation
  maximization methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2837--2847, 2019{\natexlab{d}}.

\bibitem[\textbf{B. Karimi} et~al.(2020)\textbf{B. Karimi}, Wai, Moulines, and
  Li]{karimi2020misso}
\textbf{B. Karimi}, H.-T. Wai, E.~Moulines, and P.~Li.
\newblock Misso: Minimization by incremental stochastic surrogate optimization
  for large scale nonconvex and nonsmooth problems.
\newblock In \emph{Submitted}, 2020.

\bibitem[Wang et~al.(2020)Wang, \textbf{B. Karimi}, Li, and Li]{kun2020}
J.-K. Wang, \textbf{B. Karimi}, X.~Li, and P.~Li.
\newblock An optimistic acceleration of amsgrad for nonconvex optimization.
\newblock In \emph{Submitted}, 2020.

\bibitem[Zhou et~al.(2020)Zhou, \textbf{B. Karimi}, Yu, Xu, and
  Li]{zhou2020towards}
Y.~Zhou, \textbf{B. Karimi}, J.~Yu, Z.~Xu, and P.~Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1--10, 2020.

\end{thebibliography}
