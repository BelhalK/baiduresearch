\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % for professional tables
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newtheorem{definition}{Definition}
\newcommand{\algo}{\textsc{eff-EBM}}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\def\code#1{\texttt{#1}}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}   


\input{shortcuts.tex}





\begin{document}



\title{Weekly Report KARIMI-2021-11-12}


\date{}
\maketitle

\vspace{-0.5in}


My work this week has mainly been towards
\begin{enumerate}
\item Distributed and Private EBM
\item AAAI22 rebuttal (STANLEY paper)
\end{enumerate}

\section{Distributed and Private EBM}
Focus on running experiments for this project.
Talked to Jianwen several times to narrow down the project.

\begin{algorithm}[H]
\DontPrintSemicolon
  
  \KwInput{Total number of iterations $T$, number of MCMC transitions $K$ and of samples $M$, sequence of global learning rate $\{\eta_t\}_{t >0}$,  sequence of MCMC stepsizes ${\gamma_k}_{k >0}$, initial value $\theta_0$, MCMC initialization $\{ z_{0}^m \}_{m=1}^M$. Set of selected devices $\mathcal{D}^t$.}
  \KwOutput{Vector of fitted parameters $\theta_{T+1}$.}
  \KwData{ $\{ x^p_{i} \}_{i=1}^{n_p}$, $n_p$ number of observations on device $p$. $n = \sum_{p=1}^P n_p$ total.}
\hrulefill

\For{$t=1$ to $T$}
{	
	\tcc{Happening on distributed devices}
	
	\For{For device $p \in \mathcal{D}^t$} 
	{
		{Draw $M$ negative samples $\{ z_{K}^{p,m} \}_{m=1}^M$} \tcp*{local langevin diffusion}
			\For{$k=1$ to $K$}
			{
			\beq\notag
			z_{k}^{p,m} = z_{k-1}^{p,m} + \gamma_k/2\nabla_z f_{\theta_t}( z_{k-1}^{p,m})  ^{p,m}+ \sqrt{\gamma_k} \mathsf{B}_k^p \eqsp,
			\eeq
			where $\mathsf{B}_k^p$ denotes the Brownian motion (Gaussian noise).
			}
		{Assign $\{ z_{t}^{p,m} \}_{m=1}^M \leftarrow \{ z_{K}^{p,m} \}_{m=1}^M$.}
		
		{Sample $M$ positive observations $\{ x^p_{i} \}_{i=1}^M$ from the empirical data distribution.}
		
		{Compute the gradient of the empirical log-EBM} \tcp*{local - and + gradients}
		{
		$$\delta^p = \frac{1}{M} \sum_{i=1}^{M} \nabla_{\theta} f_{\theta_t}\left(x^p_{i}\right)- \frac{1}{M} 	\sum_{m=1}^{M} \nabla_{\theta} f_{\theta_t}\left(z_K^{p,m}\right)$$
		}
		{Use black box compression operators}
		{
		$$\Delta^p = \mathcal{C}(\delta^p )$$
		}
		{Devices broadcast $\Delta^p$ to Server} 
	}
	
	  \tcc{Happening on the central server}
	  
	{Aggregation of devices gradients: $\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.$}
%	{
%	\beq\notag
%	\begin{split}
%	\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.
%	\end{split}
%	\eeq
%	}

	{Update the vector of global parameters of the EBM: $\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t)$}
%	{
%	\beq\notag
%	\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t) \eqsp.
%	\eeq
%	}
}
\caption{Distributed and private EBM}
\end{algorithm}

\section{ AAAI22 rebuttal (STANLEY paper)}
Wrote the rebuttal and added some complexity comparison for our method.

\noindent \textbf{* Complexity Analysis:} We provide the running times of our method and the baselines in Table~\ref{tab:runtimes} on CIFAR-10 and Celeb-A datasets with a batchsize of $100$.
We would like to stress on the similar computational complexity between the vanilla Langevin and our method STANLEY since our newly introduced stepsize uses the already computed gradient vector. 
On the contrary, the HMC method has recourse to both the gradient and the Hessian of the target distribution, resulting in longer computation time as reported on Table~\ref{tab:runtimes}.

\begin{table}[h]
\small
\caption{ Runtime (in s) for training our EBM during 1 epoch.}\label{tab:runtimes}
	\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllllll}
\toprule[1pt]
& {Vanilla Langevin} & {HMC} & {GD} & {\textbf{STANLEY}}  \\ \hline
{CIFAR-10 Dataset}  & {$232.5$} & {$698.4$} & {$211.3$} & {$265.2$}   \\ 
\toprule[1pt]
{Celeb-A dataset}  & {$376.3$} & {$640.1$} & {$345.2$} & {$414.8$}  \\ 
\toprule[1pt]
\end{tabular}
}\vspace{-0.1in}
\end{table}


\bibliographystyle{plain}
\bibliography{ref}


\end{document} 