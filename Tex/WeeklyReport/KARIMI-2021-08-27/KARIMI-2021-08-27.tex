\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\def\code#1{\texttt{#1}}


\usepackage{algorithm,algpseudocode}

\algnewcommand{\algorithmicforeach}{\textbf{for each}}
\algdef{SE}[FOR]{ForEach}{EndForEach}[1]
  {\algorithmicforeach\ #1\ \algorithmicdo}% \ForEach{#1}
  {\algorithmicend\ \algorithmicforeach}% \EndForEach




\begin{document}



\title{Weekly Report KARIMI 2021-08-27}


\date{}
\maketitle

\vspace{-0.5in}

My work this week has mainly been towards
\begin{enumerate}
\item ACML21 Rebuttal for OPT-AMS paper
\item Federated and Distributed EM paper
\item AAAI22 Submission
\item Compression and Generative Modeling (discussion with Xiaoyun)
\end{enumerate}



\section{ACML paper}

Grades are 2 Accept and 1 weak reject.
The third reviewer had mainly concerns on some steps of the proofs which I clarified in the rebuttal.


\section{Federated EM paper}

Mostly working on the experiments now.
I am encountering some bottleneck to make it work.


\textbf{TODO:}
\begin{itemize}
\item Plots for a PK model (on oxford boys dataset and warfarin dataset).
\end{itemize}


\section{STANLEY paper}

The paper is almost ready for submission to AAAI22.
Jianwen and I talked about it this week and his concerns are on the inpainting experiments where the Vanilla Langevin baseline is not that good.
It might be tuning or something else.
I will share my code with him for help.

\textbf{TODO:}
\begin{itemize}
\item Share codebase with Jianwen to check it.
\end{itemize}


\section{Compression and/or Distributed Generative Models}

This is a new topic I am interested in developing and which I believe is important.

\textcolor{red}{Compression for memory saving:} At the sampling step, there might be a need for more memory to store a lot of negative samples.
If this pain point is verified, then compression can be used in the MCMC to sample more negative samples while using less memory storage. 
The result is that the approximation of the gradient will hence be more accurate (because we use more samples) without increasing the memory storage.


\textcolor{red}{Compression for distributed computing:} 
Xiaoyun raised another point which is that if EBM has a need for distributed computing (and maybe federated), then compression can be used to alleviate the low bandwidth of the workers.
This could be a project but I have to verify that there is a need for distributing the EBM training.


\textbf{TODO:}
\begin{itemize}
\item Check with Jianwen if memory during the sampling step is an issue. 
\item Check literature about distributed and federated EBM.
\end{itemize}



\bibliographystyle{plain}
\bibliography{ref}


\end{document} 