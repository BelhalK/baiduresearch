\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\def\code#1{\texttt{#1}}


\usepackage{algorithm,algpseudocode}

\algnewcommand{\algorithmicforeach}{\textbf{for each}}
\algdef{SE}[FOR]{ForEach}{EndForEach}[1]
  {\algorithmicforeach\ #1\ \algorithmicdo}% \ForEach{#1}
  {\algorithmicend\ \algorithmicforeach}% \EndForEach




\begin{document}



\title{Weekly Report KARIMI 2021-08-27}


\date{}
\maketitle

\vspace{-0.5in}

My work this week has mainly been towards
\begin{enumerate}
\item Arxiv papers + Paddle Paddle
\item Federated and Distributed EM paper
\item AAAI22 Submission
\item Compression and Generative Modeling
\end{enumerate}



\section{Arxiv papers + Paddle Paddle}
Worked on the Fed-LAMB and Spars-AMS codebase in paddle paddle.
And the arxiv paper fro Fed-LAMB.

Arxiv for Spars-AMS will be also a priority once we finish to \textbf{add 1-bit Adam paper (ICML21) as a baseline} and \textbf{add new experiment on imagenet or tiny imagenet}.

\section{Federated EM paper}

Added an experiment in the paper.

Most of my progress is on overleaf. The algorithms are written as their final versions (because they work in practice).

However, the theoretical assumptions are not final since I have not finished the theory part yet.


\section{STANLEY paper}
Final touches for AAAI22 submission.

Currently debugging a problem on celeba dataset in order to improve the current results.


\section{Compression and Distributed Generative Models}


\textcolor{red}{Compression for memory saving:} At the sampling step, there might be a need for more memory to store a lot of negative samples.
If this pain point is verified, then compression can be used in the MCMC to sample more negative samples while using less memory storage. 
The result is that the approximation of the gradient will hence be more accurate (because we use more samples) without increasing the memory storage.


\textcolor{red}{Compression for distributed computing:} 
Xiaoyun raised another point which is that if EBM has a need for distributed computing (and maybe federated), then compression can be used to alleviate the low bandwidth of the workers.
This could be a project but I have to verify that there is a need for distributing the EBM training.


\textbf{The most urgent question to answer for this is:}

\begin{itemize}
\item How to construct a Markov Chain with compressed states (the idea is to obtain compressed negative samples and hence never store any high resolution chain states)
\item Once chains are built with only compressed states, then the user will be able to increase drastically the number of chains to run in parallel (no more memory issue) and hence have a larger MC batch, of compressed images
\item The approximation of the gradient is hence better. Yet gradient vectors need to be computed, which can occupy a lot of memory. We have to think about this crucial point.
\end{itemize}

\bibliographystyle{plain}
\bibliography{ref}


\end{document} 