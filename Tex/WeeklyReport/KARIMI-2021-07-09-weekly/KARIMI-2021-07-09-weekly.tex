\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}

\input{shortcuts}




\begin{document}



\title{Weekly Report KARIMI 2021 07-09}


\date{}
\maketitle

For this week's report I will recall the main axis of research I am working on for the summer until next important deadlines like AAAI (21/08), ICLR (29/09) or AISTATS (08/10).
Beginning next week's report I will only detail what I have done in the past 5 days and so on.

It follows two main domains:

\section{MCMC based EBM training}


\subsection{Moreau-Yosida based proposal for MCMC within EBM training}
I am working on a novel proposal for the MCMC within the EBM that is based on a regularization of the target distribution that would smooth any nonsmooth parts of the Gibbs potential: this is particularly appealing to Convnet-Based EBM or when any nonsmooth activations functions are employed.
The algorithm is as follows:

\begin{algorithm}[H]
\caption{\algo\ for Energy-Based model} \label{alg:anila}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: Total number of iterations $T$, number of MCMC transitions $K$ and of samples $M$, sequence of global learning rate $\{\eta_t\}_{t >0}$,  sequence of MCMC stepsizes ${\gamma_k}_{k >0}$, initial value $\theta_0$, MCMC initialization $\{ z_{0}^m \}_{m=1}^M$ and observations $\{ x_{i} \}_{i=1}^n$. Moreau Yosida decreasing sequence of parameters $\{\lambda_t\}$
\FOR{$t=1$ to $T$}
\STATE Draw $M$ samples $\{ z_{t}^m \}_{m=1}^M$ from the objective potential via Langevin diffusion:\label{line:langevin}
\FOR{$k=1$ to $K$}
\STATE Construct the Markov Chain as follows:
\beq\label{eq:anila}
z_{k}^{m} = z_{k-1}^m + \gamma_k/2  \left[\nabla f_{\theta_t}(z_{k-1}^m) + \lambda_t^{-1}\left(z_{k-1}^m-\operatorname{prox}_{g}^{\lambda}(z_{k-1}^m)\right) \right] + \sqrt{\gamma_k} \mathsf{B}_k \eqsp,
\eeq
where $\mathsf{B}_t$ denotes the Brownian motion (Gaussian noise).
\ENDFOR
\STATE Assign $\{ z_{t}^m \}_{m=1}^M \leftarrow \{ z_{K}^m \}_{m=1}^M$.
\STATE Sample $m$ positive observations $\{ x_{i} \}_{i=1}^m$ from the empirical data distribution.
\STATE Compute the gradient of the empirical log-EBM:
\beq\notag
\begin{split}
\nabla \log p(\theta_t) 
 =& \mathbb{E}_{p_{\text {data }}}\left[\nabla_{\theta} f_{\theta_t}(x)\right]-\mathbb{E}_{p_{\theta}}\left[\nabla_{\theta_t} f_{\theta}(z_t)\right]\\
 &\hspace{-0.6in}\approx  \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} f_{\theta_t}\left(x_{i}\right)-\frac{1}{m} \sum_{i=1}^{m} \nabla_{\theta} f_{\theta_t}\left(z_t^m\right)\eqsp.
\end{split}
\eeq
\STATE Update the vector of global parameters of the EBM:\label{line:gradient}
\beq\notag
\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t) \eqsp.
\eeq
\ENDFOR
\STATE \textbf{Output:} Vector of fitted parameters $\theta_{T+1}$.
\end{algorithmic}
\end{algorithm}

This is in the same line as one of Jianwen's intern that works on smoothing the activation function RELU (with a smoothRELU function).



\subsection{EBM for Motion Planning}

We work on this with Weifu Wang.
It is really hard to get it started since we are not sure of the best settings for integrating EBM in the planning procedure.
Weifu is looking at how MDP could be used in his daily planning challenges. And I am looking at how EBM could be used to infer the trainsition probabilities better in the MDP.
The mix of both could help obtain a better State transition probability density and thus improve the planning task.



\section{(stochastic) EM algorithms under Federated Learning settings}

\subsection{Stochastic EM under Federated Learning settings}

I am working on those two algorithms right now.
Mainly doing numerical runs.
Once this is done with preliminary results, Guanhua and I already talked about interesting theoretical properties that we could show.

The EM has never been studied under the Federated nor the distributed settings so I think that this line of work is interesting and important.
I am still wondering which one of those following algorithms will work the best but I will detail more at my Monday's talk.


\textbf{Periodic averaging of the local models}

\begin{algorithm}[H]
\caption{FL-SAEM with parameter averaging} \label{alg:flsaem}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: .
\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
\FOR{$r=1$ to $R$}
\FOR{parallel for device $i \in D^{r}$}
\STATE Set $\hat{\theta}^{(0,k)}_i = \hat{\theta}^{(k)}$.
\FOR{$t=1$ to $T$}
\STATE Draw M samples $\{z_{i,m}^{(t,k)}\}_{m=1}^{M}$ under model $\hat{\theta}^{(t,k)}_i$
\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(t,k+1)}$
\STATE Update local model:
$$
\hat{\theta}^{(t,k+1)}_i = \overline{\theta}( \tilde{S}_i^{(t,k+1)}) 
$$
\ENDFOR
\STATE Devices send $\hat{\theta}^{(T,k+1)}_i$ to server.
\ENDFOR
\STATE Server computes \textbf{the average of the local models}:
$$
\hat{\theta}^{(k+1)} = \frac{1}{n} \sum_{i=1}^n \hat{\theta}^{(T,k+1)}_i
$$ 
and send global model back to the devices. \label{line:final}
\ENDFOR
\end{algorithmic}
\end{algorithm}


\textbf{Periodic averaging of the local statistics}

\begin{algorithm}[H]
\caption{FL-SAEM with statistics averaging} \label{alg:flsaem2}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: .
\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
\FOR{$r=1$ to $R$}
\FOR{parallel for device $i \in D^{r}$}
\STATE Set $\hat{\theta}^{(0,k)}_i = \hat{\theta}^{(k)}$.
\FOR{$t=1$ to $T$}
\STATE \textcolor{red}{ Here one local iteration, $T=1$}
\STATE Draw M samples $z_{i,m}^{(k)}$ under model $\hat{\theta}^{(t,k)}_i$
\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(t,k+1)}$
\ENDFOR
\STATE Devices send local statistics $\tilde{S}_{i}^{(t,k+1)}$ to server.
\ENDFOR
\STATE Server computes \textbf{global model using the aggregated statistics}:
$$
\hat{\theta}^{(k+1)} = \overline{\theta}( \tilde{S}^{(t,k+1)}) 
$$
where $\tilde{S}^{(t,k+1)} = (\tilde{S}_i^{(t,k+1)}, i \in D_r)$  and send global model back to the devices. 
\ENDFOR
\end{algorithmic}
\end{algorithm}




\end{document} 