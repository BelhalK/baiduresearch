\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % for professional tables
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newtheorem{definition}{Definition}
\newcommand{\algo}{\textsc{eff-EBM}}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\def\code#1{\texttt{#1}}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}   


\input{shortcuts.tex}





\begin{document}



\title{Weekly Report KARIMI-2021-11-19}


\date{}
\maketitle




My work this week has mainly been towards
\begin{enumerate}
\item Fed-LAMB (more experiments)
\item ICLR22 Rebuttal (Lowest score questions)
\item Distributed and Private EBM
\end{enumerate}

\section{Fed-LAMB (more experiments)}
We included the Adaptive Federated Optimization of \citep{reddi2020adaptive} to our several experiments.
Done for single GPU and currently implementing the new baseline in the distributed settings.

\section{ICLR22 Rebuttal (Lowest score questions)}
Please refer to the Overleaf projects for the rebuttal

\section{Distributed and Private EBM}
The distributed aspect of the algorithm is not an issue, either practically (since in practice we actually always train EBM on several GPUs).
Yet, the compressed component introduces some issues.

\textbf{Practical:} In practice, the compression obviously implies that natural images are no longer visually appealling. Hence for the training part it does not matter since we only use those negative samples to compute a gradient but for the testing phase, the evaluation of FID of Inception scores is falsified.

\textbf{Theoretical:} In theory, we must assume ergodicity of the MCMC used in Algorithm~\ref{algebm}. An issue that is not easy to deal with is that with Jianwen we concluded that using CD-1 (Contrastive divergence with only one iteration, i.e. $K=1$ in Algorithm~\ref{algebm}) is the most interesting. Yet, assuming ergodicity and mixing of the chain with CD-1 is a bit farfetched.
So either I do assume it, or I don't have recourse to CD-1.

\begin{algorithm}[H]
\label{algebm}
\DontPrintSemicolon
  
  \KwInput{Total number of iterations $T$, number of MCMC transitions $K$ and of samples $M$, sequence of global learning rate $\{\eta_t\}_{t >0}$,  sequence of MCMC stepsizes ${\gamma_k}_{k >0}$, initial value $\theta_0$, MCMC initialization $\{ z_{0}^m \}_{m=1}^M$. Set of selected devices $\mathcal{D}^t$.}
  \KwOutput{Vector of fitted parameters $\theta_{T+1}$.}
  \KwData{ $\{ x^p_{i} \}_{i=1}^{n_p}$, $n_p$ number of observations on device $p$. $n = \sum_{p=1}^P n_p$ total.}
\hrulefill

\For{$t=1$ to $T$}
{	
	\tcc{Happening on distributed devices}
	
	\For{For device $p \in \mathcal{D}^t$} 
	{
		{Draw $M$ negative samples $\{ z_{K}^{p,m} \}_{m=1}^M$} \tcp*{local langevin diffusion}
			\For{$k=1$ to $K$}
			{
			\beq\notag
			z_{k}^{p,m} = z_{k-1}^{p,m} + \gamma_k/2\nabla_z f_{\theta_t}( z_{k-1}^{p,m})  ^{p,m}+ \sqrt{\gamma_k} \mathsf{B}_k^p \eqsp,
			\eeq
			where $\mathsf{B}_k^p$ denotes the Brownian motion (Gaussian noise).
			}
		{Assign $\{ z_{t}^{p,m} \}_{m=1}^M \leftarrow \{ z_{K}^{p,m} \}_{m=1}^M$.}
		
		{Sample $M$ positive observations $\{ x^p_{i} \}_{i=1}^M$ from the empirical data distribution.}
		
		{Compute the gradient of the empirical log-EBM} \tcp*{local - and + gradients}
		{
		$$\delta^p = \frac{1}{M} \sum_{i=1}^{M} \nabla_{\theta} f_{\theta_t}\left(x^p_{i}\right)- \frac{1}{M} 	\sum_{m=1}^{M} \nabla_{\theta} f_{\theta_t}\left(z_K^{p,m}\right)$$
		}
		{Use black box compression operators}
		{
		$$\Delta^p = \mathcal{C}(\delta^p )$$
		}
		{Devices broadcast $\Delta^p$ to Server} 
	}
	
	  \tcc{Happening on the central server}
	  
	{Aggregation of devices gradients: $\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.$}
%	{
%	\beq\notag
%	\begin{split}
%	\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.
%	\end{split}
%	\eeq
%	}

	{Update the vector of global parameters of the EBM: $\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t)$}
%	{
%	\beq\notag
%	\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t) \eqsp.
%	\eeq
%	}
}
\caption{Distributed and private EBM}
\end{algorithm}



\bibliographystyle{plain}
\bibliography{ref}


\end{document} 