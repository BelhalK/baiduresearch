\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % for professional tables
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newtheorem{definition}{Definition}
\newcommand{\algo}{\textsc{eff-EBM}}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\def\code#1{\texttt{#1}}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}   


\input{shortcuts.tex}





\begin{document}



\title{Weekly Report KARIMI-2021-11-19}


\date{}
\maketitle




My work this week has mainly been towards
\begin{enumerate}
\item Fed-LAMB (more experiments)
\item ICLR22 Rebuttal (Lowest score questions)
\item Distributed and Private EBM
\end{enumerate}

\section{Fed-LAMB (more experiments)}
We included the Adaptive Federated Optimization of \citep{reddi2020adaptive} to our several experiments.
Done for single GPU and currently implementing the new baseline in the distributed settings.

\section{ICLR22 Rebuttal (Lowest score questions)}
Please refer to the Overleaf projects for the rebuttal

\section{Distributed and Private EBM}
Focus on running experiments for this project.
Talked to Jianwen several times to narrow down the project.

\begin{algorithm}[H]
\DontPrintSemicolon
  
  \KwInput{Total number of iterations $T$, number of MCMC transitions $K$ and of samples $M$, sequence of global learning rate $\{\eta_t\}_{t >0}$,  sequence of MCMC stepsizes ${\gamma_k}_{k >0}$, initial value $\theta_0$, MCMC initialization $\{ z_{0}^m \}_{m=1}^M$. Set of selected devices $\mathcal{D}^t$.}
  \KwOutput{Vector of fitted parameters $\theta_{T+1}$.}
  \KwData{ $\{ x^p_{i} \}_{i=1}^{n_p}$, $n_p$ number of observations on device $p$. $n = \sum_{p=1}^P n_p$ total.}
\hrulefill

\For{$t=1$ to $T$}
{	
	\tcc{Happening on distributed devices}
	
	\For{For device $p \in \mathcal{D}^t$} 
	{
		{Draw $M$ negative samples $\{ z_{K}^{p,m} \}_{m=1}^M$} \tcp*{local langevin diffusion}
			\For{$k=1$ to $K$}
			{
			\beq\notag
			z_{k}^{p,m} = z_{k-1}^{p,m} + \gamma_k/2\nabla_z f_{\theta_t}( z_{k-1}^{p,m})  ^{p,m}+ \sqrt{\gamma_k} \mathsf{B}_k^p \eqsp,
			\eeq
			where $\mathsf{B}_k^p$ denotes the Brownian motion (Gaussian noise).
			}
		{Assign $\{ z_{t}^{p,m} \}_{m=1}^M \leftarrow \{ z_{K}^{p,m} \}_{m=1}^M$.}
		
		{Sample $M$ positive observations $\{ x^p_{i} \}_{i=1}^M$ from the empirical data distribution.}
		
		{Compute the gradient of the empirical log-EBM} \tcp*{local - and + gradients}
		{
		$$\delta^p = \frac{1}{M} \sum_{i=1}^{M} \nabla_{\theta} f_{\theta_t}\left(x^p_{i}\right)- \frac{1}{M} 	\sum_{m=1}^{M} \nabla_{\theta} f_{\theta_t}\left(z_K^{p,m}\right)$$
		}
		{Use black box compression operators}
		{
		$$\Delta^p = \mathcal{C}(\delta^p )$$
		}
		{Devices broadcast $\Delta^p$ to Server} 
	}
	
	  \tcc{Happening on the central server}
	  
	{Aggregation of devices gradients: $\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.$}
%	{
%	\beq\notag
%	\begin{split}
%	\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.
%	\end{split}
%	\eeq
%	}

	{Update the vector of global parameters of the EBM: $\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t)$}
%	{
%	\beq\notag
%	\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t) \eqsp.
%	\eeq
%	}
}
\caption{Distributed and private EBM}
\end{algorithm}



\bibliographystyle{plain}
\bibliography{ref}


\end{document} 