\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2019efficient}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 102--110, Long Beach, CA, 2019.

\bibitem[Brezinski and Zaglia(2013)]{brezinski2013extrapolation}
Claude Brezinski and Redivo Zaglia.
\newblock \emph{Extrapolation methods: theory and practice}.
\newblock Elsevier, 2013.

\bibitem[Cabay and Jackson(1976)]{cabay1976polynomial}
Stan Cabay and LW~Jackson.
\newblock A polynomial extrapolation method for finding limits and antilimits
  of vector sequences.
\newblock \emph{SIAM Journal on Numerical Analysis}, 13\penalty0 (5):\penalty0
  734--752, 1976.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Liu, Sun, and
  Hong]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2020)Chen, Li, and Li]{chen2020toward}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In \emph{Proceedings of the {ACM-IMS} Foundations of Data Science
  Conference (FODS)}, pages 119--128, Virtual Event, USA, 2020.

\bibitem[Chen et~al.(2021)Chen, Karimi, Zhao, and Li]{chen2021convergent}
Xiangyi Chen, Belhal Karimi, Weijie Zhao, and Ping Li.
\newblock On the convergence of decentralized adaptive gradient methods.
\newblock \emph{arXiv preprint arXiv:2109.03194}, 2021.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Yuan, Yi, Zhou, Chen, and
  Yang]{chen2019universal}
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao
  Yang.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019{\natexlab{b}}.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and Zhu]{CJ12}
Chao{-}Kai Chiang, Tianbao Yang, Chia{-}Jung Lee, Mehrdad Mahdavi, Chi{-}Jen
  Lu, Rong Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock In \emph{Proceedings of the 25th Annual Conference on Learning Theory
  (COLT)}, pages 6.1--6.20, Edinburgh, Scotland, UK, 2012.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and
  Zeng]{daskalakis2018training}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020convergence}
Alexandre D{\'e}fossez, L{\'e}on Bottou, Francis Bach, and Nicolas Usunier.
\newblock On the convergence of adam and adagrad.
\newblock \emph{arXiv e-prints}, pages arXiv--2003, 2020.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2121--2159, 2011.

\bibitem[Gers et~al.(2000)Gers, Schmidhuber, and Cummins]{gers1999learning}
Felix~A. Gers, J{\"{u}}rgen Schmidhuber, and Fred~A. Cummins.
\newblock Learning to forget: Continual prediction with {LSTM}.
\newblock \emph{Neural Comput.}, 12\penalty0 (10):\penalty0 2451--2471, 2000.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{{SIAM} J. Optim.}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget{-}Abadie, Mirza, Xu,
  Warde{-}Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian~J. Goodfellow, Jean Pouget{-}Abadie, Mehdi Mirza, Bing Xu, David
  Warde{-}Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2672--2680, Montreal, Canada, 2014.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{GMH13}
Alex Graves, Abdel{-}rahman Mohamed, and Geoffrey~E. Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{Proceedings of the {IEEE} International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 6645--6649,
  Vancouver, Canada, 2013.

\bibitem[Hazan(2019)]{hazan2019introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{arXiv preprint arXiv:1909.05207}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the 2016 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 770--778, Las Vegas, NV, 2016.

\bibitem[Karimi et~al.(2021)Karimi, Li, and Li]{karimi2021fed}
Belhal Karimi, Xiaoyun Li, and Ping Li.
\newblock Fed-lamb: Layerwise and dimensionwise locally adaptive optimization
  algorithm.
\newblock \emph{arXiv preprint arXiv:2110.00532}, 2021.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem[Larochelle et~al.(2007)Larochelle, Erhan, Courville, Bergstra, and
  Bengio]{Proc:Larochelle_ICML07}
Hugo Larochelle, Dumitru Erhan, Aaron~C. Courville, James Bergstra, and Yoshua
  Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock In \emph{Proceedings of the Twenty-Fourth International Conference on
  Machine Learning (ICML)}, pages 473--480, Corvalis, Oregon, 2007.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{J. Mach. Learn. Res.}, 17:\penalty0 39:1--39:40, 2016.

\bibitem[Li(2010)]{Proc:ABC_UAI10}
Ping Li.
\newblock Robust logitboost and adaptive base class (abc) logitboost.
\newblock In \emph{Proceedings of the Twenty-Sixth Conference Annual Conference
  on Uncertainty in Artificial Intelligence (UAI)}, pages 302--311, Catalina
  Island, CA, 2010.

\bibitem[Li(2018)]{arXiv:Li_2018several}
Ping Li.
\newblock Several tunable {GMM} kernels.
\newblock \emph{arXiv preprint arXiv:1805.02830}, 2018.

\bibitem[Li and Orabona(2019)]{li2019convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 983--992, Naha,
  Japan, 2019.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mohri and Yang(2016)]{mohri2015accelerating}
Mehryar Mohri and Scott Yang.
\newblock Accelerating online convex optimization via adaptive prediction.
\newblock In \emph{Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 848--856, Cadiz,
  Spain, 2016.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Ussr computational mathematics and mathematical physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Rakhlin and Sridharan(2013)]{RS13b}
Alexander Rakhlin and Karthik Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 3066--3074, Lake Tahoe, NV, 2013.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {Adam} and beyond.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[Scieur et~al.(2018)Scieur, Oyallon, dAspremont, and Bach]{Scieur18}
Damien Scieur, Edouard Oyallon, Alexandre dAspremont, and Francis Bach.
\newblock Nonlinear acceleration of deep neural networks.
\newblock \emph{CoRR}, abs/1805.09639, 2018.

\bibitem[Scieur et~al.(2020)Scieur, d'Aspremont, and
  Bach]{scieur2020regularized}
Damien Scieur, Alexandre d'Aspremont, and Francis~R. Bach.
\newblock Regularized nonlinear acceleration.
\newblock \emph{Math. Program.}, 179\penalty0 (1):\penalty0 47--83, 2020.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and Schapire]{SALS15}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2989--2997, Montreal, Canada, 2015.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012rmsprop}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude. coursera: Neural networks for machine learning.
\newblock \emph{COURSERA Neural Networks Mach. Learn}, 2012.

\bibitem[Tseng(2008)]{tseng2008accelerated}
Paul Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock \emph{submitted to SIAM Journal on Optimization}, 2\penalty0 (3),
  2008.

\bibitem[Walker and Ni(2011)]{walker2011anderson}
Homer~F. Walker and Peng Ni.
\newblock Anderson acceleration for fixed-point iterations.
\newblock \emph{{SIAM} J. Numer. Anal.}, 49\penalty0 (4):\penalty0 1715--1735,
  2011.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
Rachel Ward, Xiaoxia Wu, and L{\'{e}}on Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 6677--6686, Long Beach, CA, 2019.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence (IJCAI)}, pages 2955--2961, Stockholm,
  Sweden, 2018.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank~J. Reddi, Devendra~Singh Sachan, Satyen Kale, and Sanjiv
  Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9815--9825, Montr{\'{e}}al, Canada, 2018.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zhou et~al.(2018)Zhou, Chen, Cao, Tang, Yang, and
  Gu]{zhou2018convergence}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Karimi, Yu, Xu, and Li]{zhou2020towards}
Yingxue Zhou, Belhal Karimi, Jinxing Yu, Zhiqiang Xu, and Ping Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\bibitem[Zou and Shen(2018)]{zou2018convergence}
Fangyu Zou and Li~Shen.
\newblock On the convergence of adagrad with momentum for training deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1808.03408}, 2\penalty0 (3):\penalty0 5,
  2018.

\end{thebibliography}
