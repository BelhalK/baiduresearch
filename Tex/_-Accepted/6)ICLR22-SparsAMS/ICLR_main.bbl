\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2018)Agarwal, Suresh, Yu, Kumar, and
  McMahan]{Proc:Agrawal_NIPS19}
Naman Agarwal, Ananda~Theertha Suresh, Felix~X. Yu, Sanjiv Kumar, and Brendan
  McMahan.
\newblock cpsgd: Communication-efficient and differentially-private distributed
  {SGD}.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, pp.\  7575--7586, 2018.

\bibitem[Ajalloeian \& Stich(2020)Ajalloeian and Stich]{ajalloeian2020analysis}
Ahmad Ajalloeian and Sebastian~U Stich.
\newblock Analysis of sgd with biased gradient estimators.
\newblock \emph{arXiv preprint arXiv:2008.00051}, 2020.

\bibitem[Aji \& Heafield(2017)Aji and Heafield]{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock \emph{arXiv preprint arXiv:1704.05021}, 2017.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1709--1720, 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Khirirat,
  Konstantinov, and Renggli]{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
  Konstantinov, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock \emph{arXiv preprint arXiv:1809.10505}, 2018.

\bibitem[Basu et~al.(2019)Basu, Data, Karakus, and Diggavi]{Proc:Basu_NIPS19}
Debraj Basu, Deepesh Data, Can Karakus, and Suhas~N. Diggavi.
\newblock Qsparse-local-sgd: Distributed {SGD} with quantization,
  sparsification and local computations.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  14668--14679, 2019.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  560--569. PMLR, 2018.

\bibitem[Bernstein et~al.(2019)Bernstein, Zhao, Azizzadenesheli, and
  Anandkumar]{Proc:Bernstein_ICLR19}
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock signsgd with majority vote is communication efficient and fault
  tolerant.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Horv{\'{a}}th, Richt{\'{a}}rik, and
  Safaryan]{Arxiv:Beznosikov20}
Aleksandr Beznosikov, Samuel Horv{\'{a}}th, Peter Richt{\'{a}}rik, and Mher
  Safaryan.
\newblock On biased compression for distributed learning.
\newblock \emph{CoRR}, abs/2002.12410, 2020.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, Eckstein,
  et~al.]{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Chang et~al.(2018)Chang, Balachandar, Lam, Yi, Brown, Beers, Rosen,
  Rubin, and Kalpathy{-}Cramer]{Proc:Chang18}
Ken Chang, Niranjan Balachandar, Carson~K. Lam, Darvin Yi, James~M. Brown,
  Andrew Beers, Bruce~R. Rosen, Daniel~L. Rubin, and Jayashree
  Kalpathy{-}Cramer.
\newblock Distributed deep learning networks among institutions for medical
  imaging.
\newblock \emph{J. Am. Medical Informatics Assoc.}, 25\penalty0 (8):\penalty0
  945--954, 2018.

\bibitem[Charikar et~al.(2002)Charikar, Chen, and
  Farach{-}Colton]{Proc:Charikar_ICALP02}
Moses Charikar, Kevin~C. Chen, and Martin Farach{-}Colton.
\newblock Finding frequent items in data streams.
\newblock In \emph{Automata, Languages and Programming, 29th International
  Colloquium, {ICALP} 2002, Malaga, Spain, July 8-13, 2002, Proceedings},
  volume 2380 of \emph{Lecture Notes in Computer Science}, pp.\  693--703.
  Springer, 2002.

\bibitem[Chen et~al.(2020)Chen, Shen, Huang, Wu, and Liu]{Arxiv:QAdam}
Congliang Chen, Li~Shen, Haozhi Huang, Qi~Wu, and Wei Liu.
\newblock Quantized adam with error feedback.
\newblock \emph{arXiv preprint arXiv:2004.14180}, 2020.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{Proc:Chen_ICLR19}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{Symposium on Operating Systems Design and Implementation},
  pp.\  571--582, 2014.

\bibitem[Choi et~al.(2019)Choi, Shallue, Nado, Lee, Maddison, and
  Dahl]{Arxiv:Choi_2019}
Dami Choi, Christopher~J. Shallue, Zachary Nado, Jaehoon Lee, Chris~J.
  Maddison, and George~E. Dahl.
\newblock On empirical comparisons of optimizers for deep learning.
\newblock \emph{CoRR}, abs/1910.05446, 2019.

\bibitem[Covington et~al.(2016)Covington, Adams, and
  Sargin]{Proc:Covington_2016}
Paul Covington, Jay Adams, and Emre Sargin.
\newblock Deep neural networks for youtube recommendations.
\newblock In \emph{Proceedings of the 10th {ACM} Conference on Recommender
  Systems, Boston, MA, USA, September 15-19, 2016}, pp.\  191--198. {ACM},
  2016.

\bibitem[De~Sa et~al.(2017)De~Sa, Feldman, R{\'e}, and
  Olukotun]{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In \emph{Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pp.\  561--574, 2017.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Le, Mao, Ranzato,
  Senior, Tucker, Yang, and Ng]{Proc:Dean_NIPS12}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc~V. Le,
  Mark~Z. Mao, Marc'Aurelio Ranzato, Andrew~W. Senior, Paul~A. Tucker, Ke~Yang,
  and Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems 25: 26th
  Annual Conference on Neural Information Processing Systems 2012. Proceedings
  of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States},
  pp.\  1232--1240, 2012.

\bibitem[Dettmers(2016)]{Proc:8-bit_ICLR16}
Tim Dettmers.
\newblock 8-bit approximations for parallelism in deep learning.
\newblock In Yoshua Bengio and Yann LeCun (eds.), \emph{4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Proc:BERT}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
  1 (Long and Short Papers)}, pp.\  4171--4186. Association for Computational
  Linguistics, 2019.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{Duchi10-adagrad}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In \emph{{COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010}, pp.\  257--269, 2010.

\bibitem[Duchi et~al.(2011)Duchi, Agarwal, and Wainwright]{duchi2011dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic control}, 57\penalty0
  (3):\penalty0 592--606, 2011.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget{-}Abadie, Mirza, Xu,
  Warde{-}Farley, Ozair, Courville, and Bengio]{Proc:GAN_NIPS14}
Ian~J. Goodfellow, Jean Pouget{-}Abadie, Mehdi Mirza, Bing Xu, David
  Warde{-}Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems 27: Annual
  Conference on Neural Information Processing Systems 2014, December 8-13 2014,
  Montreal, Quebec, Canada}, pp.\  2672--2680, 2014.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'{a}}r, Girshick, Noordhuis,
  Wesolowski, Kyrola, Tulloch, Jia, and He]{Arxiv:Goyal17}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock \emph{CoRR}, abs/1706.02677, 2017.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{Proc:Graves_ICASSP13}
Alex Graves, Abdel{-}rahman Mohamed, and Geoffrey~E. Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP} 2013, Vancouver, BC, Canada, May 26-31, 2013},
  pp.\  6645--6649. {IEEE}, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Proc:Resnet_CVPR16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pp.\
  770--778. {IEEE} Computer Society, 2016.

\bibitem[Hong et~al.(2017)Hong, Hajinezhad, and Zhao]{hong2017prox}
Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1529--1538, 2017.

\bibitem[Ivkin et~al.(2019)Ivkin, Rothchild, Ullah, Braverman, Stoica, and
  Arora]{Proc:Ivkin_NIPS19}
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica,
  and Raman Arora.
\newblock Communication-efficient distributed {SGD} with sketching.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  13144--13154, 2019.

\bibitem[Jiang et~al.(2018)Jiang, Fu, Yang, and Cui]{Proc:Jiang_SIGMOD18}
Jiawei Jiang, Fangcheng Fu, Tong Yang, and Bin Cui.
\newblock Sketchml: Accelerating distributed machine learning with data
  sketches.
\newblock In \emph{Proceedings of the 2018 International Conference on
  Management of Data, {SIGMOD} Conference 2018, Houston, TX, USA, June 10-15,
  2018}, pp.\  1269--1284. {ACM}, 2018.

\bibitem[Jiang \& Agrawal(2018)Jiang and Agrawal]{jiang2018linear}
Peng Jiang and Gagan Agrawal.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  2530--2541, 2018.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock \emph{arXiv preprint arXiv:1901.09847}, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3478--3487, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{mnist}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{Proc:Lin_ICLR18}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and Bill Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.

\bibitem[Lu et~al.(2019)Lu, Zhang, Sun, and Hong]{lu2019gnsd}
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong.
\newblock Gnsd: A gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In \emph{2019 IEEE Data Science Workshop (DSW)}, pp.\  315--321,
  2019.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{imdb}
Andrew Maas, Raymond~E Daly, Peter~T Pham, Dan Huang, Andrew~Y Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th annual meeting of the association
  for computational linguistics: Human language technologies}, pp.\  142--150,
  2011.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Mikami et~al.(2018)Mikami, Suganuma, Tanaka, Kageyama,
  et~al.]{mikami2018massively}
Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, Yuichi Kageyama, et~al.
\newblock Massively distributed sgd: Imagenet/resnet-50 training in a flash.
\newblock \emph{arXiv preprint arXiv:1811.05233}, 2018.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{Arxiv:MnihKSGAWR13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{CoRR}, abs/1312.5602, 2013.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{arXiv preprint arXiv:1901.09109}, 2019.

\bibitem[Nedic \& Ozdaglar(2009)Nedic and Ozdaglar]{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48, 2009.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{Proc:Seide14}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{{INTERSPEECH} 2014, 15th Annual Conference of the
  International Speech Communication Association, Singapore, September 14-18,
  2014}, pp.\  1058--1062. {ISCA}, 2014.

\bibitem[Shen et~al.(2018)Shen, Mokhtari, Zhou, Zhao, and
  Qian]{Proc:Shen_ICML18}
Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, and Hui Qian.
\newblock Towards more efficient stochastic decentralized learning: Faster
  convergence and sparse communication.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\
  4631--4640. {PMLR}, 2018.

\bibitem[Shi et~al.(2019)Shi, Zhao, Wang, Tang, and Chu]{shi2019convergence}
Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu.
\newblock A convergence analysis of distributed sgd with
  communication-efficient gradient sparsification.
\newblock In \emph{IJCAI}, pp.\  3411--3417, 2019.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{AlphaGo_17}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  Yutian Chen, Timothy~P. Lillicrap, Fan Hui, Laurent Sifre, George van~den
  Driessche, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nat.}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Stich \& Karimireddy(2019)Stich and
  Karimireddy]{Article:Stich_arxiv19}
Sebastian~U. Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock \emph{CoRR}, abs/1909.05350, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.05350}.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4447--4458, 2018.

\bibitem[Tang et~al.(2021)Tang, Gan, Awan, Rajbhandari, Li, Lian, Liu, Zhang,
  and He]{Proc:1bitAdam}
Hanlin Tang, Shaoduo Gan, Ammar~Ahmad Awan, Samyam Rajbhandari, Conglong Li,
  Xiangru Lian, Ji~Liu, Ce~Zhang, and Yuxiong He.
\newblock 1-bit adam: Communication efficient large-scale training with adam's
  convergence speed.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10118--10129. {PMLR},
  2021.

\bibitem[Voulodimos et~al.(2018)Voulodimos, Doulamis, Doulamis, and
  Protopapadakis]{CV_review18}
Athanasios Voulodimos, Nikolaos Doulamis, Anastasios~D. Doulamis, and Eftychios
  Protopapadakis.
\newblock Deep learning for computer vision: {A} brief review.
\newblock \emph{Comput. Intell. Neurosci.}, 2018:\penalty0
  7068349:1--7068349:13, 2018.

\bibitem[Wang et~al.(2019)Wang, Li, Karimi, and Li]{wang2019optimistic}
Jun-Kun Wang, Xiaoyun Li, Belhal Karimi, and Ping Li.
\newblock An optimistic acceleration of amsgrad for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1903.01435}, 2019.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1299--1309, 2018.

\bibitem[Wei et~al.(2017)Wei, He, Chen, Zhou, and Tang]{Article:Wei_2017}
Jian Wei, Jianhua He, Kai Chen, Yi~Zhou, and Zuoyin Tang.
\newblock Collaborative filtering and deep learning based recommendation system
  for cold start items.
\newblock \emph{Expert Systems with Applications}, 69:\penalty0 29--39, 2017.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock \emph{arXiv preprint arXiv:1705.07878}, 2017.

\bibitem[Wu et~al.(2018)Wu, Huang, Huang, and Zhang]{Proc:Wu_ICML18}
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.
\newblock Error compensated quantized {SGD} and its applications to large-scale
  distributed optimization.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\
  5321--5329. {PMLR}, 2018.

\bibitem[Yang et~al.(2019)Yang, Zhang, Kirichenko, Bai, Wilson, and
  De~Sa]{yang2019swalp}
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew~Gordon
  Wilson, and Chris De~Sa.
\newblock Swalp: Stochastic weight averaging in low precision training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7015--7024. PMLR, 2019.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{Proc:LAMB_ICLR20}
Yang You, Jing Li, Sashank~J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho{-}Jui Hsieh.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Young et~al.(2018)Young, Hazarika, Poria, and Cambria]{NLP_review18}
Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria.
\newblock Recent trends in deep learning based natural language processing
  [review article].
\newblock \emph{{IEEE} Comput. Intell. Mag.}, 13\penalty0 (3):\penalty0 55--75,
  2018.

\bibitem[Yu et~al.(2019{\natexlab{a}})Yu, Jin, and Yang]{Proc:Yu_ICML19}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
  7184--7193. {PMLR}, 2019{\natexlab{a}}.

\bibitem[Yu et~al.(2019{\natexlab{b}})Yu, Wu, and Huang]{Proc:Yu_AISTATS19}
Yue Yu, Jiaxiang Wu, and Junzhou Huang.
\newblock Exploring fast and communication-efficient algorithms in large-scale
  distributed networks.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2019, 16-18 April 2019, Naha, Okinawa, Japan},
  volume~89 of \emph{Proceedings of Machine Learning Research}, pp.\  674--683.
  {PMLR}, 2019{\natexlab{b}}.

\bibitem[Zeiler(2012)]{Proc:adadelta}
Matthew~D. Zeiler.
\newblock {ADADELTA:} an adaptive learning rate method.
\newblock \emph{CoRR}, abs/1212.5701, 2012.

\bibitem[Zhang et~al.(2017)Zhang, Li, Kara, Alistarh, Liu, and
  Zhang]{Proc:Zhang_ICML17}
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji~Liu, and Ce~Zhang.
\newblock Zipml: Training linear models with end-to-end low precision, and a
  little bit of deep learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, volume~70
  of \emph{Proceedings of Machine Learning Research}, pp.\  4035--4043. {PMLR},
  2017.

\bibitem[Zhang et~al.(2018)Zhang, Wang, and Liu]{sentiment_review18}
Lei Zhang, Shuai Wang, and Bing Liu.
\newblock Deep learning for sentiment analysis: {A} survey.
\newblock \emph{Wiley Interdiscip. Rev. Data Min. Knowl. Discov.}, 8\penalty0
  (4), 2018.

\bibitem[Zhang et~al.(2020)Zhang, Wu, Katiyar, Weinberger, and
  Artzi]{Arxiv:Zhang_ICLR21}
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian~Q. Weinberger, and Yoav Artzi.
\newblock Revisiting few-sample {BERT} fine-tuning.
\newblock \emph{CoRR}, abs/2006.05987, 2020.

\bibitem[Zhao et~al.(2020)Zhao, Xie, Jia, Qian, Ding, Sun, and
  Li]{Proc:Zhao_MLsys20}
Weijie Zhao, Deping Xie, Ronglai Jia, Yulei Qian, Ruiquan Ding, Mingming Sun,
  and Ping Li.
\newblock Distributed hierarchical {GPU} parameter server for massive scale
  deep learning ads systems.
\newblock In \emph{Proceedings of Machine Learning and Systems 2020, MLSys
  2020, Austin, TX, USA, March 2-4, 2020}. mlsys.org, 2020.

\bibitem[Zheng et~al.(2019)Zheng, Huang, and Kwok]{Proc:Zheng_NIPS19}
Shuai Zheng, Ziyue Huang, and James~T. Kwok.
\newblock Communication-efficient distributed blockwise momentum {SGD} with
  error-feedback.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  11446--11456, 2019.

\bibitem[Zhou et~al.(2018)Zhou, Tang, Yang, Cao, and Gu]{Arxiv:Zhou_18}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{CoRR}, abs/1808.05671, 2018.

\end{thebibliography}
