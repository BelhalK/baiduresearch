\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{koller2007graphical}
\citation{bilmes2005graphical}
\citation{shwe1990probabilistic}
\citation{jordan1999graphical}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{sanner2012symbolic,kahle2008junction}
\citation{jordan1999introduction,hoffman2013stochastic,kingma2013auto,liu2016stein}
\citation{xing2012generalized}
\citation{bishop2003vibes,winn2005variational}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kingma2013auto}
\citation{kingma2018glow,rezende2015variational}
\citation{tabak2010density}
\citation{Dinh2016DensityEU,rippel2013high}
\citation{rezende2015variational}
\citation{Dinh2016DensityEU,dinh2014nice,de2020block,ho2019flow++,papamakarios2019normalizing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\newlabel{eq:vae_recon}{{1}{2}{Preliminaries}{equation.2.1}{}}
\citation{rezende2015variational}
\citation{rezende2015variational,berg2018sylvester}
\citation{kingma2013auto,rezende2014stochastic}
\newlabel{eq:flow}{{2}{3}{Preliminaries}{equation.2.2}{}}
\newlabel{eq:flow2}{{3}{3}{Preliminaries}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variational Flow Graphical Model}{3}{section.3}}
\newlabel{sec:main}{{3}{3}{Variational Flow Graphical Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evidence Lower Bound of Variational Flow Graphical Models}{3}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ (Left) Node $\mathbf  {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf  {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation\nobreakspace  {}nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. Thin lines are identity functions, and thick lines are flow functions. \relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tree}{{1}{3}{\small (Left) Node $\mathbf {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation~nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. Thin lines are identity functions, and thick lines are flow functions. \relax }{figure.caption.2}{}}
\citation{kingma2013auto}
\newlabel{eq:posterior}{{4}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.4}{}}
\newlabel{eq:elbo}{{5}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.5}{}}
\newlabel{eq:kl}{{6}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.6}{}}
\newlabel{eq:elbo_dag}{{7}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Forward and backward message passing to generate each node's hidden variable. Forward message passing approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions. \relax }}{4}{figure.caption.3}}
\newlabel{fig:tree_message}{{2}{4}{Forward and backward message passing to generate each node's hidden variable. Forward message passing approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ELBO Calculation}{4}{subsection.3.2}}
\newlabel{eq:post_smp}{{8}{5}{ELBO Calculation}{equation.3.8}{}}
\newlabel{eq:prior_smp}{{9}{5}{ELBO Calculation}{equation.3.9}{}}
\newlabel{eq:KL_l}{{10}{5}{ELBO Calculation}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3} Aggregation Nodes}{5}{subsection.3.3}}
\newlabel{sec:node_aggr}{{3.3}{5}{Aggregation Nodes}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (Left) Aggregation node $\mathbf  {h}^{l+1,1}$ has three children, $\mathbf  {h}^{l,1}$, $\mathbf  {h}^{l,2}$, and $\mathbf  {h}^{l,3}$. (Right) A VFG model with one aggregation node, $\mathbf  {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }}{5}{figure.caption.4}}
\newlabel{fig:node_aggre}{{3}{5}{(Left) Aggregation node $\mathbf {h}^{l+1,1}$ has three children, $\mathbf {h}^{l,1}$, $\mathbf {h}^{l,2}$, and $\mathbf {h}^{l,3}$. (Right) A VFG model with one aggregation node, $\mathbf {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }{figure.caption.4}{}}
\newlabel{eq:one_agg_node}{{11}{6}{Aggregation Nodes}{equation.3.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inference model parameters with forward and backward message propagation\relax }}{6}{figure.caption.5}}
\newlabel{alg:main}{{1}{6}{Inference model parameters with forward and backward message propagation\relax }{figure.caption.5}{}}
\newlabel{line:for2}{{4}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.4}{}}
\newlabel{line:forward}{{6}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.6}{}}
\newlabel{line:backward}{{11}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.11}{}}
\newlabel{line:update}{{15}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Algorithms and Implementation}{6}{section.4}}
\newlabel{sec:algrithm}{{4}{6}{Algorithms and Implementation}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Layer-wise Training}{6}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }}{6}{figure.caption.6}}
\newlabel{fig:two_layer_infer}{{4}{6}{{\small (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Random Masking}{7}{subsection.4.2}}
\newlabel{eq:elbo_tree_mask}{{13}{7}{Random Masking}{equation.4.13}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Inference model parameters with random masking\relax }}{7}{figure.caption.7}}
\newlabel{alg:rand_mask}{{2}{7}{Inference model parameters with random masking\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Inference on VFG Models }{7}{section.5}}
\newlabel{sec:infer}{{5}{7}{Inference on VFG Models}{section.5}{}}
\newlabel{eq:aggr_obs_ch}{{14}{7}{Inference on VFG Models}{equation.5.14}{}}
\newlabel{lm:apprx}{{1}{7}{}{lemma.1}{}}
\newlabel{rmk:apprx_mul}{{1}{7}{}{remark.1}{}}
\citation{bengio2013representation}
\citation{Dinh2016DensityEU}
\citation{berg2018sylvester}
\citation{berg2018sylvester}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Experiments}{8}{section.6}}
\newlabel{sec:numerical}{{6}{8}{Numerical Experiments}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Evaluation on Inference with Missing Entries Imputation}{8}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }}{8}{figure.caption.8}}
\newlabel{fig:sim}{{5}{8}{Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ELBO and Likelihood}{8}{subsection.6.2}}
\newlabel{sec:exp:elbo}{{6.2}{8}{ELBO and Likelihood}{subsection.6.2}{}}
\citation{kingma2013auto}
\citation{rezende2015variational}
\citation{kingma2016improving}
\citation{berg2018sylvester}
\citation{Lecunmnist2010}
\citation{Sorrenson2020}
\citation{maaten2008visualizing}
\bibdata{ref}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }}{9}{table.1}}
\newlabel{tab:elbo}{{1}{9}{Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Latent Representation Learning on MNIST}{9}{subsection.6.3}}
\newlabel{sec:exp:mnist}{{6.3}{9}{Latent Representation Learning on MNIST}{subsection.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }}{9}{figure.caption.9}}
\newlabel{fig:reconst}{{6}{9}{(Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The tree structure for MNIST.\relax }}{9}{figure.caption.10}}
\newlabel{fig:struct}{{7}{9}{The tree structure for MNIST.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }}{9}{figure.caption.11}}
\newlabel{fig:z_tsne}{{8}{9}{MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{9}{section.7}}
\newlabel{sec:conclusion}{{7}{9}{Conclusion}{section.7}{}}
\bibcite{bengio2013representation}{{1}{}{{}}{{}}}
\bibcite{berg2018sylvester}{{2}{}{{}}{{}}}
\bibcite{bilmes2005graphical}{{3}{}{{}}{{}}}
\bibcite{bishop2003vibes}{{4}{}{{}}{{}}}
\bibcite{de2020block}{{5}{}{{}}{{}}}
\bibcite{dinh2014nice}{{6}{}{{}}{{}}}
\bibcite{Dinh2016DensityEU}{{7}{}{{}}{{}}}
\bibcite{efron1975defining}{{8}{}{{}}{{}}}
\bibcite{ho2019flow++}{{9}{}{{}}{{}}}
\bibcite{hoffman2013stochastic}{{10}{}{{}}{{}}}
\bibcite{hruschka2007bayesian}{{11}{}{{}}{{}}}
\bibcite{jordan1999graphical}{{12}{}{{}}{{}}}
\bibcite{jordan1999introduction}{{13}{}{{}}{{}}}
\bibcite{kahle2008junction}{{14}{}{{}}{{}}}
\bibcite{Khemakhem20a}{{15}{}{{}}{{}}}
\bibcite{kingma2018glow}{{16}{}{{}}{{}}}
\bibcite{kingma2016improving}{{17}{}{{}}{{}}}
\bibcite{kingma2013auto}{{18}{}{{}}{{}}}
\bibcite{koller2007graphical}{{19}{}{{}}{{}}}
\bibcite{Lecunmnist2010}{{20}{}{{}}{{}}}
\bibcite{liu2016stein}{{21}{}{{}}{{}}}
\bibcite{maaten2008visualizing}{{22}{}{{}}{{}}}
\bibcite{madigan1995bayesian}{{23}{}{{}}{{}}}
\bibcite{papamakarios2019normalizing}{{24}{}{{}}{{}}}
\bibcite{rezende2015variational}{{25}{}{{}}{{}}}
\bibcite{rezende2014stochastic}{{26}{}{{}}{{}}}
\bibcite{rippel2013high}{{27}{}{{}}{{}}}
\bibcite{sanner2012symbolic}{{28}{}{{}}{{}}}
\bibcite{shwe1990probabilistic}{{29}{}{{}}{{}}}
\bibcite{Sorrenson2020}{{30}{}{{}}{{}}}
\bibcite{tabak2010density}{{31}{}{{}}{{}}}
\bibcite{winn2005variational}{{32}{}{{}}{{}}}
\bibcite{xing2012generalized}{{33}{}{{}}{{}}}
\bibstyle{abbrv}
\citation{Dinh2016DensityEU}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Numerical Experiments}{13}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}California Housing Dataset}{13}{subsection.A.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }}{13}{table.caption.14}}
\newlabel{tab:imp_arrhytmia}{{2}{13}{California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Representation Learning with MNIST}{13}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.1}Latent Representation Learning on MNIST}{13}{subsubsection.A.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }}{14}{figure.caption.15}}
\newlabel{fig:z_no_Y}{{9}{14}{MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.2}Disentanglement on MNIST}{14}{subsubsection.A.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces MNIST: Increasing each latent variable from a small value to a larger one.\relax }}{14}{figure.caption.16}}
\newlabel{fig:mnist_dis}{{10}{14}{MNIST: Increasing each latent variable from a small value to a larger one.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}ELBO Calculation}{15}{appendix.B}}
\newlabel{sec:vfg_inference}{{B}{15}{ELBO Calculation}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  The recognition model consists of froward message from data to approximate the posterior distributions; the generative model is realized by backward message from the root node. \relax }}{15}{figure.caption.17}}
\newlabel{fig:tree_message2}{{11}{15}{The recognition model consists of froward message from data to approximate the posterior distributions; the generative model is realized by backward message from the root node. \relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Distributions of Latent Variables}{15}{subsection.B.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Generative Model}{15}{subsubsection.B.1.1}}
\newlabel{sec:generative}{{B.1.1}{15}{Generative Model}{subsubsection.B.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}Recognition Model}{16}{subsubsection.B.1.2}}
\newlabel{eq:posteriorapp}{{15}{16}{Recognition Model}{equation.B.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}$\mathbf  {KL}$ Term}{16}{subsection.B.2}}
\newlabel{eq:KL_lapp}{{16}{16}{$\mathbf {KL}$ Term}{equation.B.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Reconstruction Term}{16}{subsection.B.3}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Aggregation Node}{16}{appendix.C}}
\newlabel{eq:child_avg}{{17}{16}{Aggregation Node}{equation.C.17}{}}
\newlabel{eq:parent_avg}{{18}{17}{Aggregation Node}{equation.C.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Aggregation node on a DAG.}\relax }}{17}{figure.caption.18}}
\newlabel{fig:dag_aggr}{{12}{17}{{\small Aggregation node on a DAG.}\relax }{figure.caption.18}{}}
\newlabel{eq:i_child}{{19}{17}{Aggregation Node}{equation.C.19}{}}
\newlabel{eq:i_parent}{{20}{17}{Aggregation Node}{equation.C.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}More Details on Inference}{18}{appendix.D}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Derivation of the ELBOs for Trees and DAGs }{18}{appendix.E}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}ELBO of Tree Models}{18}{subsection.E.1}}
\newlabel{appd:tree_elbo}{{E.1}{18}{ELBO of Tree Models}{subsection.E.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A tree VFG with $L=5$ and three aggregation nodes.\relax }}{18}{figure.caption.19}}
\newlabel{fig:tree_vfg}{{13}{18}{A tree VFG with $L=5$ and three aggregation nodes.\relax }{figure.caption.19}{}}
\newlabel{eq:prior}{{21}{18}{ELBO of Tree Models}{equation.E.21}{}}
\newlabel{eq:posterior2}{{22}{18}{ELBO of Tree Models}{equation.E.22}{}}
\newlabel{eq:chain_post}{{23}{19}{ELBO of Tree Models}{equation.E.23}{}}
\newlabel{eq:chain_prior}{{24}{19}{ELBO of Tree Models}{equation.E.24}{}}
\newlabel{eq:elbo12L}{{25}{19}{ELBO of Tree Models}{equation.E.25}{}}
\newlabel{eq:kl_lL}{{26}{19}{ELBO of Tree Models}{equation.E.26}{}}
\newlabel{eq:elbo0}{{27}{19}{ELBO of Tree Models}{equation.E.27}{}}
\newlabel{eq:kl_l}{{28}{19}{ELBO of Tree Models}{equation.E.28}{}}
\citation{rezende2015variational,kingma2016improving,berg2018sylvester}
\citation{rezende2015variational,kingma2016improving,berg2018sylvester}
\newlabel{eq:elbo1}{{29}{20}{ELBO of Tree Models}{equation.E.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Improve ELBO Estimation with Flows}{20}{subsection.E.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}ELBO of DAG Models}{20}{subsection.E.3}}
\newlabel{appd:dag_elbo}{{E.3}{20}{ELBO of DAG Models}{subsection.E.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces A DAG with inverse topology order {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \{\vcenter to\@ne \big@size {}\right .$}\box \z@ } \{1,2,3\}, \{4,5\}, \{6\}, \{7\} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \}\vcenter to\@ne \big@size {}\right .$}\box \z@ }, and they correspond to layers 0 to 3. \relax }}{20}{figure.caption.20}}
\newlabel{fig:dag}{{14}{20}{A DAG with inverse topology order \big \{ \{1,2,3\}, \{4,5\}, \{6\}, \{7\} \big \}, and they correspond to layers 0 to 3. \relax }{figure.caption.20}{}}
\newlabel{eq:dag_elbo}{{30}{20}{ELBO of DAG Models}{equation.E.30}{}}
\citation{Khemakhem20a,Sorrenson2020}
\citation{efron1975defining}
\newlabel{eq:dag_chain_q}{{31}{21}{ELBO of DAG Models}{equation.E.31}{}}
\newlabel{eq:dag_chain_p}{{32}{21}{ELBO of DAG Models}{equation.E.32}{}}
\newlabel{eq:dag_kl_lL}{{33}{21}{ELBO of DAG Models}{equation.E.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Theoretical Justifications for Latent Representation Learning}{21}{appendix.F}}
\newlabel{sec:theory}{{F}{21}{Theoretical Justifications for Latent Representation Learning}{appendix.F}{}}
\newlabel{eq:exp_h}{{34}{21}{Theoretical Justifications for Latent Representation Learning}{equation.F.34}{}}
\newlabel{eq:xt_gen}{{35}{21}{Theoretical Justifications for Latent Representation Learning}{equation.F.35}{}}
\citation{Khemakhem20a}
\newlabel{eq:u_diff}{{36}{22}{Theoretical Justifications for Latent Representation Learning}{equation.F.36}{}}
\newlabel{eq:A_sim}{{37}{22}{Theoretical Justifications for Latent Representation Learning}{equation.F.37}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
