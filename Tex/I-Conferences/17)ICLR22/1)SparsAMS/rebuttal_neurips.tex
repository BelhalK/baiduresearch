\documentclass{article}
\usepackage{xcolor,bm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{lipsum}


\begin{document}




\section{Reviewer uGfq (4;4):}


On the positive side, the submission addresses an interesting topic, on which we still know fairly little. To the extent that I was able to check them (see below), I believe the results check out.

Unfortunately, there are a few significant shorcomings of the current version, which prevent me from advocating for acceptance. I list them in arbitrary order:

The novelty claims are somewhat tenuous, as there is indeed concurrent work which addresses the problem of compression for adaptive optimization, in particular https://arxiv.org/abs/2102.02888 (in ICML) and [13].
The writing of the paper, specifically with respect to the technical parts, is below the standard I would expect for a paper that could be accepted to NeurIPS. To give just one example, the parameter epsilon has multiple uses throughout the paper, but is never formally introduced (e.g. is the eps in algorithm 1 the same as the one in algorithm 2, and the same as the one in Theorem 1?). This may seem like a trifle, but notice that this renders the paper's main result (Theorem 1) difficult to interpret (by the way, eps is not defined in this Theorem, which is unfortunate since one might expect the main result to be self-contained).
The theoretical analysis should be improved for this to be a top-tier paper. In particular, the authors provide the 'standard' analysis only for the unrealistic single-node case (why would we run gradient compression on a single node?), and require additional assumptions on the LR sequence (which are non-standard for adaptive algorithms) in the multi-node case.
The experiments are conducted on a tiny setup, on which no-one would actually need distributed learning. While the experimental data does corroborate the analysis to some extent, it is executed on tiny datasets and models, on which adaptive optimizers don't actually give state-of-the-art results (relative to e.g. SGD). This is unfortunate, since there actually are state-of-the-art settings (in particular, Transformer models) for which adaptive optimizers are state-of-the-art, and for which distributed training is the norm. I believe the experiments have to be significantly improved in this respect.
General comments / Typos

single-machine -> single machine
'distributed learning framework has been'
the claim that 'gradient averaging has not been considered for adaptive optimization' seems a bit too strong due to e.g. https://arxiv.org/abs/2102.02888 and [13]
1/sqrt(n) is technically not linear speedup, since it's not 1/n. This is the best achievable is this setting, but the formulation could be strange for someone who is not familiar with this. Please revise
element-wisely -> elementwise
Algorithm 1: what is epsilon? a small constant?
lines 166-167: formulation is weird, please rephrase
Theorem 1: why is epsilon not defined here?! Notice you already have ane epsilon defined in line 16 of Algorithm 2?
l255: 'irrelevant quantities' -> one could disagree that all the quantities you choose to ignore are irrelevant. In particular, it would be good to have a discussion of the quantities you ignore here.
moreover, your discussion of Corrollary 2 appears a bit 'optimistic' to me. In particular, you have the $n * sigma^2$ terms there, which are going to affect you fairly significantly in the regime where n is very large. I don't think this is the case for momentum SGD, which is what you claim.


\textcolor{purple}{Our reply:}

Thanks for your valuable comments and suggestions.

1. Thanks for your reference to the very recent related works on this topic. Since the ICML proceeding and the arxiv were both released after the NeurIPS deadline, we were not aware of it at that time. The '1-bit Adam' uses a trick to freeze the v in the update. Our work provides a fundamental and general framework where $m$ and $v$ are both updating; Their method can be considered as a extended special case. 
Based on Comp-AMS, we may indeed develop more variants using other tricks in the next steps. We will add more comparison with this very recent paper in the revision. Thanks again.

2. We will make the formulation of Algorithm 1 and Algorithm 2 more consistent, and clarify $\epsilon$ in Theorem 1. Yes, $\epsilon$ is the initial value of $v$, same for both algorithms and the theorems. 

3. Our analysis on the single-node case is inspired by [33] Error feedback fixes sign-sgd and other gradient compression schemes, where single-node compressed SGD with error feedback is precisely studied. We aim to show that for adaptive AMSGrad, the same property holds. 
The decreasing learning rate has also been used in several works analyzing adaptive methods, e.g. [48] On the convergence of adam and beyond.; [71] On the convergence of adaptive gradient methods for nonconvex optimization.

4. We will include more experiments on larger models and datasets in the future version. Thanks for the suggestion.

5. Corollary 2: Corollary 2 has same dependence of $n\sigma^2$ as momentum SGD. Please see e.g. [63] On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization, equation (11) therein.

Thanks also for other suggestions on improving the quality and presentation of our paper.



\section{Reviewer YLji (6;4):}

The paper is generally well-written and structured clearly. As the authors mentioned, the proposed algorithm, COMP-AMS with error feedback (Alg. 2), is a straightforward extension of the classical distributed SGD that uses the AMSGrad step-sizes and compression in the update rule. The results are not surprising, and the assumptions used for the theoretical results are the same with several papers analyzing methods in the distributed non-convex setting. However, to the best of my knowledge and as the authors also mentioned, there is no analysis of adaptive method with error feedback under the same assumptions. The presentation of Theorem 1 and Corollary 1 is clear, and the discussion of what they mean is very informative and easy to follow. To the best of my knowledge, Corollary 1 is the first result showing that compressed adaptive methods with EF converge as fast as their standard counterpart.

Even if the assumptions and proof techniques are very similar to previous papers, the proposed scheme could be a step towards designing and analyzing more efficient adaptive and distributed algorithms. I ve spot-checked the proofs of the supplementary material, and the results seem correct.

Limitations:

Figures 2 and some plots of Figure 3 should be improved to be accessible for a color-blind audience. At the moment, there is no clear distinction between the lines.

This is not a limitation of the paper rather a suggestion for extension. I would like to see the convergence of the method for the strongly convex case. In this setting, there are several analyses of other distributed compressed algorithms for which one can guarantee faster convergence.

\textcolor{purple}{Our reply:}

Thanks for your positive feedback and recognizing our contributions. We will improve the figures for color-blind readers. Thanks also for the nice suggestion on additionally studying the convex case. We will consider this setting in the revision. Thank you.


\section{Reviewer D7iz (3;5):}


The authors claimed in Lines 37-39 that 'Burdensome gradient transmission would slow down the whole training system, or even be impossible because of the limited bandwidth in some applications. 'I do not necessarily agree with that. The authors assume a data center setting, Lines 26-29, and in theory, what they claim in Lines 37-39 is correct! In the phase 2016-2019, people were claiming this and got away. Unfortunately, many recent dedicated works on gradient compression under various settings (with varying network speeds) have shown in practice; compression/decompression has high computational overhead, so high that we barely see a benefit when the bandwidth approaches 1Gbps. See [GRACE: A compressed communication framework for distributed machine learning, in IEEE ICDCS 2021] and [Agarwal et al. in On the Utility of Gradient Compression in Distributed Training Systems, 2021, ArXiv]. For bandwidths above 10 Gbps (even I will argue at 1 Gbps bandwidth, which is not uncommon in data centers, new Nvidia A100 servers come with 200 Gbps NICs), the network overhead is negligible, so compression does not pay off. Therefore, compression would not be beneficial in any setting of distributed training but could be in federated learning where clients are geographically remote and network bandwidths are low.

What do you mean by 'In general, larger bias and variance of the compressed gradients usually bring more significant performance downgrade in terms of convergence [53, 2]?' This statement is too general and might be misleading. Therefore, I do not essentially agree with this statement as well. If you mean to say 'compression downgrade in terms of convergence,' I will humbly mention it does not. In the non-convex setting, compressed (irrespective of quantization, sparsification, error feedback) SGD has the same asymptotic convergence rate as uncompressed baseline SGD. Moreover, in terms of test accuracy of DNN training (which is more important than the loss function convergence), compressed SGD outperforms its uncompressed counterpart in many cases. Please see again [GRACE: A compressed communication framework for distributed machine learning, in IEEE ICDCS 2021].

Please check the paper's Notation [On the Convergence of Adam and Beyond, by Reddi et al. in ICLR 2018]. The authors strictly mention the abuse of notation in presenting a non-standard way of writing a 'vector dividing by another vector.' However, you never mentioned that anywhere in your manuscript, and suddenly, out of the blue, Algorithm 1 appears, making me think how could one divide a vector by another vector. This is a bad practice.

I disagree with 'This is also the first result in the literature regarding the linear speedup property of distributed adaptive learning under gradient compression.' Moreover, the authors claim, 'Furthermore, adopting gradient compression in adaptive methods has also been rarely studied in the literature,' is imprecise. Quickly refer to the paper: [13] Quantized Adam with Error Feedback by Chen et al. in 2020. Although there is some difference (desired) of this manuscript with [13], these strong claims are misleading. However, I thank the authors for discussing the paper later. Also, can you clarify your claim in Lines 203-204? You cite Theorem 1 (although wrongly) from [13]? After quick checking, I also think this is not true.

Please see [1-bit Adam: Communication Efficient Large-Scale Training with Adam s Convergence Speed, by Tang et al. 2021, ArXiv] that claims ADAM does not converge with error feedback. Therefore, I am a little curious about how your claims stand with the error feedback?

Uniform bound of Stochastic gradients as in Assumption 3 is too strong. In practice, it should hold when f(x) is bounded, which is never the case for DNN training. Therefore, I will encourage you to use Assumption 3 in [54], see Page 7.

Why do you need to use the complex form of Youngs inequality (To be precise, it is Peter-Paul's inequality) in arriving equation (9) in Appendix? The inequality you used with  is a fancy version of the inequality with . The philosophy behind using the one with  is that if one quantity has an extremely large norm, then  can balance that. Otherwise, it is wiser to use the simpler version of the inequality with . However, I note that you used the simple one in Lines 696+3.

The inequality just above line 707 is not correct. There should be a factor  multiplied with . I also do not understand what 707 mean.

What is upper case N in Line 674?

You chose  in line 699. This essentially, makes  blow up. Does not it? Now, you also chose 
 
, and  is a function of  and  as well. Although I did not carefully check the polynomial order, I anticipate there is a discrepancy. Please correct me if I am wrong. Also, kindly justify these choices.

The numerical experiments in this paper are sub-standard. You may think of using the GRACE framework and deploy your COMP-AMS algorithm. Another suggestion is to compare baseline SGD with and without compression to highlight why one should use the COMP-AMS algorithm. In the present numerical experiment, this motivation is totally missing.

How do you determine if one compressor outperforms the other? If they send different data volumes, the claim 'one compressor outperforms the other' is meaningless. For example, please see Figure 3. If I understand correctly, in CIFAR-10 n=16, CP-AMS TopK-0.01 is sending the least number of bits, and the accuracy suffers the most; similar observations can be made for other figures. Therefore, one suggestion is to set the parameters to send the same average data volumes and then compare their performance.

What does this line mean? Line 158: 'Note that, larger q indicates important an compression while smaller q implies better approximation of the true gradient.' I did not understand.


\textcolor{purple}{Our reply:}

We appreciate your detailed comments and suggestions on notations and assumptions for us to improve the paper. We would like to further clarify the following questions.

1. Thanks for your very recent references and remarks on the practical side of gradient compression. For your mentioned recent Arxiv paper, on page 2 the authors report that compression can bring significant practical benefit with limited bandwidth (<5 Gbps), and the ICDCS paper also experimented with 1 Gbps setting. 
Therefore, we believe that limited bandwidth among devices is also a meaningful setting, considering e.g. economic expenditures. We will add some discussion on this more practical aspects. Thank you.

2. We mean that directly using compressed gradients would lead to downgrade, illustrated by our Figure 1 and other related works. For a theoretical investigation, please see [2] Analysis of sgd with biased gradient estimators, 2020, where the authors showed that if the compressor has larger bias, SGD would have worse convergence in non-convex optimization (e.g. their Theorem 4).

4. At the time of our submission, we were not aware of similar result on adaptive methods. The authors of [13] updated arxiv after our submission. Yet, note that now their result relies on a stronger assumption (see A.4). We will update our discussion accordingly.

5. We believe that our result does not contradict with the "1-bit Adam" paper, which was released online after our submission. Their discussion is that it is hard to get a estimation of the variance term $v_t$ when using EF with Adam. Indeed, this is described similarly in our Section 5.4 as "and extra error in the variance estimation", which may lead to some generalization downgrade. And this is in fact the future direction where various tricks can be applied to our fundamental SPARS-AMS as variants. The 1-bit Adam is, if we understood correctly, one such possible variants that freeze the $v_t$ after several iterations. 

Regarding the convergence, firstly, the use of $\hat v_t$ in AMSGrad would make this variance term more stable. Secondly, under our assumptions, the variance term $\hat v_t$ is bounded. This would ensure the theoretical convergence in the squared gradient norm of our method. 

7. We use flexible $\rho$ in (9) since we need to make the constant in front of $\Vert e_{t,i}\Vert^2$ less than 1, such that the geometric series summation in the recursion can be well controlled. If we use $\rho=1$, then the $\sigma^2$ term Lemma 2 would be linearly dependent on $T$.

8 \& 9. Thanks for pointing our the typos. We missed the $\cdot/n$ at line 707. This will not affect our main claims. And $N$ should read $n$. We have corrected them. Thank you.

10. Our choice of $\rho$ would not make the term explode because the properly chosen range on the step size $\eta$. We have double-checked the calculation and believe it is correct. The $\rho$ constant on line 699 is not the same one as the $\rho$ on line 714. We will use another notation to distinct them.

11. In our experiments we focused on comparing with the compressed adaptive methods. We agree that the SGD counterparts would be a good to compare with. We will add the results in future version. Thanks for the nice suggestion.

12. The criteria of our evaluation is: a method is better if it uses less communication to achieve (nearly) the same learning accuracy as the full-precision distributed AMSGrad in a adequate number of epochs. This is because in practice, in many cases we train the models for some fixed epochs. We agree that we can add a comparison under e.g. same communication. Thanks for the suggestion.

13. It should be "a heavier compression". We have corrected this typo.


\section{Reviewer uKmS (6;3):}


In general, the paper is clear and well-written. The ingredients of the proposed method (AMSGrad and gradient compression) have already appeared elsewhere and have been thoroughly studied, however their combination is -- at the best of my knowledge -- novel. Thus, the paper has some novelty. The main weakness of this contribution is that QAdam still provides superior generalization performance, for both CIFAR-10 and IMDB (MNIST is probably just too easy so QAdam offers no improvement). The superior generalization of QAdam comes at the cost of a larger communication complexity (1-2 orders of magnitudes).

A few more detailed comments are below:

(1) Have the authors experimented also on other datasets and found out that QAdam again performs better?

(2) As for compression methods, the authors discuss Top-K and Block-Sign. How would quantisation-based methods (e.g., QSGD) perform here?

(3) Is the dependence on  tight in Theorem 1? Does the scaling in  of the theoretical results match experimental evidence?

(4) Typos/imprecisions: (i) L. 157, 'important an compression' --> 'an important compression'; (ii) L. 163, mention that the sign function is applied component wise; (iii) L. 215, is the bound uniform in ?; (iv) L. 259,  should be 



\textcolor{purple}{Our reply:}

We sincerely thank you for your valuable feedback. 

1. We haven't tested more datasets and we will present more in future version. As we discussed in Section 5.4, we think that the error brought by estimating the $v_t$ term on the denominator could be the main cause of its slightly worse generalisation performance. We believe that based on this work and our observations, researchers can develop various tricks in the next steps under this framework.

2. We did not test QSGD here since both BkSign and TopK-0.01 have approached the accuracy of full-precision method, and the compression ratio of QSGD would be smaller than these two methods. We conjecture that QSGD should be quite similar to Block-Sign. We will add QSGD runs in the revision. Thanks for the suggestion.

3. Precisely measuring the dependence of compression parameter $q$ 
and the convergence in practice is a quite challenging task that has not been done in literature (as fat as we are aware of). We believe one reason is that the dependence of the convergence rate on $q$ is always quite complicated and highly non-linear. Yet, we agree that this could be an interesting investigation worth trying in the future. 

4. Thanks for pointing our the typos. the bound on line 215 is uniform for all $\theta_t$.


\end{document}

