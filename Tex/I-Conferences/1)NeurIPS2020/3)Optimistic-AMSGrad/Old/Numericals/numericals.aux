\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{RKK18}
\citation{RKK18}
\citation{KB15}
\citation{MNIST07}
\citation{CNN15}
\citation{Rnet16}
\@writefile{toc}{\contentsline {section}{\numberline {1}Experiments}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training loss vs. Number of iterations. The first row are results with fully-connected neural network.}}{2}{figure.1}}
\newlabel{train_loss}{{1}{2}{Training loss vs. Number of iterations. The first row are results with fully-connected neural network}{figure.1}{}}
\newlabel{train_loss@cref}{{[figure][1][]1}{[1][2][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textit  {MNIST-back-image} + CNN, \textit  {CIFAR10} + Res-18 and \textit  {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy. We observe that \textsc  {Optimistic-AMSGrad} consistently improves the two baselines.}}{3}{figure.2}}
\newlabel{figs:CIFAR10_new3}{{2}{3}{\textit {MNIST-back-image} + CNN, \textit {CIFAR10} + Res-18 and \textit {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss, training accuracy, testing loss, and testing accuracy. We observe that \textsc {Optimistic-AMSGrad} consistently improves the two baselines}{figure.2}{}}
\newlabel{figs:CIFAR10_new3@cref}{{[figure][2][]2}{[1][2][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Choice of parameter $r$}{3}{subsection.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The training loss of \textsc  {OPTIMISTIC-AMSGrad} with different $r$.}}{3}{figure.3}}
\newlabel{fig:compare-r}{{3}{3}{The training loss of \textsc {OPTIMISTIC-AMSGrad} with different $r$}{figure.3}{}}
\newlabel{fig:compare-r@cref}{{[figure][3][]3}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Concluding Remarks}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Discussion on the iteration cost}{3}{subsection.2.1}}
\citation{MG15}
\bibdata{reference}
\bibstyle{plain}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Conclusion}{4}{subsection.2.2}}
