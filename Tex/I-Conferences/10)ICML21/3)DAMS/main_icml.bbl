\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2019efficient}
Agarwal, N., Bullins, B., Chen, X., Hazan, E., Singh, K., Zhang, C., and Zhang,
  Y.
\newblock Efficient full-matrix adaptive regularization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  102--110, 2019.

\bibitem[Aji \& Heafield(2017)Aji and Heafield]{aji2017sparse}
Aji, A.~F. and Heafield, K.
\newblock Sparse communication for distributed gradient descent.
\newblock In \emph{Empirical Methods in Natural Language Processing}, pp.\
  440--445, 2017.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1709--1720, 2017.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and
  Rabbat]{assran2019stochastic}
Assran, M., Loizou, N., Ballas, N., and Rabbat, M.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  344--353, 2019.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, Eckstein,
  et~al.]{boyd2011distributed}
Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{chen2018convergence}
Chen, X., Liu, S., Sun, R., and Hong, M.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock In \emph{International Conference for Learning Representations},
  2019.

\bibitem[Chen et~al.(2010)Chen, Guan, and Wang]{chen2010approximate}
Chen, Y., Guan, T., and Wang, C.
\newblock Approximate nearest neighbor search by residual vector quantization.
\newblock \emph{Sensors}, 10\penalty0 (12):\penalty0 11259--11273, 2010.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
Chilimbi, T., Suzue, Y., Apacible, J., and Kalyanaraman, K.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{Symposium on Operating Systems Design and Implementation},
  pp.\  571--582, 2014.

\bibitem[Di~Lorenzo \& Scutari(2016)Di~Lorenzo and Scutari]{di2016next}
Di~Lorenzo, P. and Scutari, G.
\newblock Next: In-network nonconvex optimization.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 2\penalty0 (2):\penalty0 120--136, 2016.

\bibitem[Duchi et~al.(2011{\natexlab{a}})Duchi, Hazan, and
  Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011{\natexlab{a}}.

\bibitem[Duchi et~al.(2011{\natexlab{b}})Duchi, Agarwal, and
  Wainwright]{duchi2011dual}
Duchi, J.~C., Agarwal, A., and Wainwright, M.~J.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic control}, 57\penalty0
  (3):\penalty0 592--606, 2011{\natexlab{b}}.

\bibitem[Ge et~al.(2013)Ge, He, Ke, and Sun]{ge2013optimized}
Ge, T., He, K., Ke, Q., and Sun, J.
\newblock Optimized product quantization for approximate nearest neighbor
  search.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pp.\  2946--2953, 2013.

\bibitem[Hong et~al.(2017)Hong, Hajinezhad, and Zhao]{hong2017prox}
Hong, M., Hajinezhad, D., and Zhao, M.-M.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1529--1538, 2017.

\bibitem[Jegou et~al.(2010)Jegou, Douze, and Schmid]{jegou2010product}
Jegou, H., Douze, M., and Schmid, C.
\newblock Product quantization for nearest neighbor search.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 33\penalty0 (1):\penalty0 117--128, 2010.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Koloskova, A., Stich, S.~U., and Jaggi, M.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3478--3487, 2019.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Li \& Orabona(2019)Li and Orabona]{li2019convergence}
Li, X. and Orabona, F.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  983--992, 2019.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5330--5340, 2017.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, W.~J.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lu et~al.(2019)Lu, Zhang, Sun, and Hong]{lu2019gnsd}
Lu, S., Zhang, X., Sun, H., and Hong, M.
\newblock Gnsd: A gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In \emph{2019 IEEE Data Science Workshop (DSW)}, pp.\  315--321,
  2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adaptive}
Luo, L., Xiong, Y., Liu, Y., and Sun, X.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{International Conference for Learning Representations},
  2019.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Nazari, P., Tarzanagh, D.~A., and Michailidis, G.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{arXiv preprint arXiv:1901.09109}, 2019.

\bibitem[Nedic \& Ozdaglar(2009)Nedic and Ozdaglar]{nedic2009distributed}
Nedic, A. and Ozdaglar, A.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48, 2009.

\bibitem[Reddi et~al.(2020)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\`y}, Kumar, and McMahan]{reddi2020adaptive}
Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Kone{\v{c}}n{\`y},
  J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock \emph{arXiv preprint arXiv:2003.00295}, 2020.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Shi, W., Ling, Q., Wu, G., and Yin, W.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Stich, S.~U., Cordonnier, J.-B., and Jaggi, M.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4447--4458, 2018.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d}
Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J.
\newblock $\text{D}^ 2$: Decentralized training over decentralized data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4848--4856, 2018.

\bibitem[Tang et~al.(2019)Tang, Yu, Lian, Zhang, and
  Liu]{tang2019doublesqueeze}
Tang, H., Yu, C., Lian, X., Zhang, T., and Liu, J.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6155--6165, 2019.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{wang2018atomo}
Wang, H., Sievert, S., Liu, S., Charles, Z., Papailiopoulos, D., and Wright, S.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9850--9861, 2018.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Wangni, J., Wang, J., Liu, J., and Zhang, T.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1299--1309, 2018.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
Ward, R., Wu, X., and Bottou, L.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6677--6686, 2019.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Yan, Y., Yang, T., Li, Z., Lin, Q., and Yang, Y.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  pp.\  2955--2961, 2018.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
Yuan, K., Ling, Q., and Yin, W.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (3):\penalty0
  1835--1854, 2016.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9793--9803, 2018.

\bibitem[Zou \& Shen(2018)Zou and Shen]{zou2018convergence}
Zou, F. and Shen, L.
\newblock On the convergence of weighted adagrad with momentum for training
  deep neural networks.
\newblock \emph{arXiv preprint arXiv:1808.03408}, 2018.

\end{thebibliography}
