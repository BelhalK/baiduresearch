
\documentclass{article} % For LaTeX2e




\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{color}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{xcolor} 
\usepackage{pdfpages}
\usepackage{natbib}

\usepackage{enumerate}
\usepackage{enumitem}
%\usepackage[nonatbib]{neurips_2020}
\usepackage{icml2021_author_response}


  





\title{Convergent Adaptive Gradient Methods in Decentralized Optimization\vspace{-0.15in}}
% \title{On the Convergence of Decentralized Adaptive Gradient Methods}

\allowdisplaybreaks[2]



\begin{document}

\maketitle


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



%We would like to thank the four reviewers for their feedback. 
%Upon acceptance, we will include in the final version \emph{{\sf (a)} improved comparison with prior work}, and \emph{{\sf (b)} missing references}. 
We first discuss some shared questions  by the reviewers.
%\textcolor{blue}{\textbf{R1}},  \textcolor{blue}{\textbf{R5}},  \textcolor{blue}{\textbf{R6}}, and \textcolor{blue}{\textbf{R8}}:
\vspace{-2pt}

 \textbf{-- Numerical Experiments (to \textcolor{blue}{\textbf{R1}},  \textcolor{blue}{\textbf{R5}},  \textcolor{blue}{\textbf{R6}}, and \textcolor{blue}{\textbf{R8}}):} 
 Our experiments serve as a support of our theory that decentralized AMSGrad can converge while DADAM cannot.
%Our experiments in the main paper aim at showing the advantages over DADAM, a decentralized variant of Adam method, developed in [Nazari et. al., 2019].
We recall that the purpose of this paper is to provide both an \emph{algorithmic} and {theoretical} framework for decentralized variants of adaptive gradient methods. 
%Hence, single-server Adam method does not constitute a baseline for our method, rather its decentralized version DADAM does. 
Though our experiments showed some advantages of decentralized AMSGrad over  D-PSGD of [Lian et. al., 2017], this is not our main purpose. %The experiments serves as a support of our theory that decentralized AMSGrad can converge while DADAM cannot. 
 Figure 1 shows the divergence issue of DADAM is negligible on homogeneous data but can be a big problem on heterogeneous data, highlighting the need for convergent algorithms in practice. Exploring more benefits of the proposed algorithms is indeed an important and interesting question for practitioners. However, to be honest, the authors currently do not have enough  resources to scale up the experiments since it requires setting up a distributed computation environment on more machines and common free computation resources do not support it. To the best of our knowledge, adaptive gradient methods are never used in decentralization with rigorous guarantees, we sincerely hope the reviewers can evaluate our contribution base more on our algorithmic framework and theoretical analyses.
%Though highlight the advantages over SGD comparing Figure 2, 3 and Figure 4 in section F of the appendix where our proposed algorithm is less sensitive to the lr, which is one edge of adaptive methods.
%While we acknowledge that the numerical experiments can be improved by adding runs on larger datasets (which we plan on doing for the revised paper), we stress on the fact that the current experiments support our theory. 
%The current experiments we are displaying in our paper are informative on how our newly proposed decentralized framework behaves with respect to baseline methods. 
%In Figure 1 (b), we show a very bad convergence behavior of DADAM on heterogeneous data, in echo of the theoretical divergence that we claim in the paper. 
%While DADAM shows divergence (Fig. 1), our decentralized framework, using AMSGrad as a prototype, and D-PSGD of [Lian et. al., 2017] are exhibiting great convergence. 
%Our framework is similar and sometimes better than D-PSGD. 
%Figure 1 is convincing on the need for a convergent decentralized adaptive method, thus fixing the divergence issue of DADAM (shown both theoretically and empirically through Figure 1).
%The revised paper will include runs on larger datasets.

\textbf{-- Discussion on the matrix $W$ (to \textcolor{blue}{\textbf{R6}} and \textcolor{blue}{\textbf{R8}}):}
The way to set $W$ is not unique, a common choice for undirected graph is the maximum-degree method in [Boyd et. al. "Fastest mixing Markov chain on a graph.", 2004] (denote $d_i$ as degree of vertex $i$ and $d_{\max} = \max_i d_i$, this method sets $W_{i,i} = 1-d_i/d_{\max}$, $W_{i,j} = 1/d_{\max}$ if $i\neq j$ and $(i,j)$ is an edge,  and $W_{i,j} = 0$ otherwise, a variant is $\gamma I + (1-\gamma) W$ for some $\gamma \in [0,1)$). This $W$ ensures assumption A4 for many common connected graph types.  
A more refined choice of $W$ coupled with a comprehensive discussion on $\lambda$ in our Th. 2 can be found in [Boyd et. al. "Fastest mixing Markov chain on graphs with symmetries.", 2009], e.g., $1-\lambda =O(1/N^2)$ for cycle graphs, $1-\lambda =O(1/\log(N))$ for hypercube graphs, $\lambda = 0$ for fully connected graph. 
Intuitively, $\lambda$ can be close to 1 for sparse graphs and to 0 for dense graphs.
This is consistent with the bound in Th. 2, which is large for $\lambda$ close to 1 and small for $\lambda $ close to 0 since average consensus on sparse graphs takes longer.


\vspace{-2pt}
\textcolor{blue}{\textbf{R1:}}
We thank the reviewer for the remarks. \vspace{-4pt}

\textbf{-- Comparison with [Chen et. al, 2020] ([C20]):} 
[C20] is one of a few recent attempts to use adaptive gradient methods with the periodic model averaging technique in federated learning. 
%Essentially, the divergence issue in both [C20] and our paper are caused by asynchronous adaptive lr. 
[C20] use the parameter server to maintain a synchronized adaptive learning rate (lr) sequence to ensure convergence, leading to local AMSGrad (LAMS). 
Our setting is different since \emph{a central server is not available}, thus we use an average consensus mechanism to gradually synchronize adaptive lr. 
Since both decentralized AMSGrad (DAMS) and LAMS use AMSGrad as the prototype, they reduce to similar ones if local iterations $k=1$ in LAMS and the graph is fully connected in DAMS. 
%With differences being the $\hat v_{t,i}$ is maintained by each worker and the extra $\epsilon$ in line 10 of DAMS. 
The key difference is we study how to use adaptive gradient methods in decentralized optimization \textbf{without} a parameter server, rather than under federated learning settings. 
As asked by the reviewer, it is indeed possible to extend the periodical averaging technique used in [C20] to our decentralized setting. 
The resulting algorithm will execute line 7,8,11 every $k$ iterations and $\tilde {u}_{t,i}$ will not be updated in local iterations. 
We expect our result to have a similar dependency on $k$ as in [C20], i.e., the big-O rate will not be affected for $k \leq O(T^{1/4})$ and applies to our framework. 
%However, this represents future work.
%In such a case, there are two tiny difference, one is the the extra parameter $\epsilon$ in line 10 of decentralized AMSGrad, the other one being the the max operation in update rule of $\hat v_{t,i}$ (line 7) is executed by individual workers using $v_{t,i}$ in decentralized AMSGrad while the max operation is executed only by the parameter server in local AMSGrad using average of $v_{t,i}$.

 \vspace{-2pt}
\textbf{-- Bias in second moment estimation:} We will not have [mean of square of gradients] vs [square of mean of gradients] issue in most cases. 
Using the same AdaGrad example with $\hat v_{t,i} = \frac{1}{t}\sum_{k=1}^t g_{k,i}^2$, when $t$ is large and $\epsilon$ is small, the adaptive lr $\tilde{u}_{t,i}$ is close to its tracking target $\frac{1}{N} \sum_{i=1}^N\hat v_{t,i} = \frac{1}{Nt} \sum_{i=1}^N\sum_{k=1}^t g_{k,i}^2$, which is the mean of square of stochastic gradients. This estimation is unbiased 
if we want to estimate second moment of stochastic gradients over the optimization trajectory. 
It could also be biased if we want to estimate the second moment at recent iterations, as the distribution of stochastic gradients could change with $t$. 
The effect of the bias on the training is usually problem-dependent. 
Killing the bias is possible by drawing fresh samples of stochastic gradients to estimate the adaptive lr with extra computation cost. %The average consensus mechanism will also induce a small bias due to the time lag in average consensus of $\tilde{u}_{t,i}$. 

\vspace{-2pt}
\textcolor{blue}{\textbf{R5:}}
We thank you for the valuable comments. \vspace{-4pt}




\textbf{-- Comparison with [Chen et. al, 2019] ([C19]) and [Zhou et al., 2018]] ([Z18]):}
We compare Th. 3.1 of [C19] with our Th. 2. 
The term multiplied by $C_1$ in our Th. 2 have similar a source as the terms multiplied by $C_1$ and $C_4$ in Th. 3.1 of [C19]. 
The terms multiplied by $C_4$ and $C_5$ in our Th. 2 have a similar source as the terms multiplied by $C_2$ and $C_3$ in [C19]. 
The other terms in our Th. 2 are caused by \emph{consensus errors} of variable and adaptive lr. We also compare Th. 5.1 in [Z18] with our Th. 3. The $C_1'$ terms in Th. 3 are counterparts of $M_1$ and $M_3$ terms in [Z18], the $C_4'$ term corresponds to the $M_2$ term in [Z18]. 
[Z18] can show an extra improved rate assuming sparse gradients. 
We will add  more detailed comparisons in our paper. 


\vspace{-2pt}
\textcolor{blue}{\textbf{R6:}}
Thank you for the thorough analysis. \vspace{-4pt}

\textbf{-- Dimension dependency:}
Our stepsize and convergence rate depends on dimension because of assumption A3, which implies the variance of the gradient estimator is linear in $d$. Under such an assumption, even the best rate of SGD is $O(\sqrt{d}/\sqrt{T})$ with stepsize being $O(1/\sqrt{Td})$, similar to our results. It is possible to improve the dependency on $d$ by assuming the total variance of gradient is independent of $d$, which will lead to $O(1/\sqrt{T})$ rate with $O(1/\sqrt{T})$ stepsize.
%As rightly mentioned by the reviewer, the stepsize is in order $\alpha_t = 1/\sqrt{T}$. 
%The dependence in $d$ leads to a small lr in the presence of large networks but our Th. states that the rate would then be as fast as we present it. 
%Hence, our bound prevails over the intuition that the convergence will be slow due to a small lr.

%\textbf{-- Discussion on W:} We thank you for your co feedback.


\vspace{-2pt}
\textcolor{blue}{\textbf{R8:}}
We thank the reviewer for the constructive feedback and interest in our paper. We provide more discussions on the numerical experiments and choice of $W$ on the left page, we hope they could address the reviewer's questions.


%[1]. Boyd, Stephen, Persi Diaconis, and Lin Xiao. "Fastest mixing Markov chain on a graph." SIAM review 46.4 (2004): 667-689.
%[2]. Boyd, Stephen, et al. "Fastest mixing Markov chain on graphs with symmetries." SIAM Journal on Optimization 20.2 (2009): 792-819.



\end{document}
