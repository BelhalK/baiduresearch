\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amssymb}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{comment}
\usepackage{color}
\usepackage{array}
\usepackage{graphicx}
\usepackage{booktabs}

% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.


\begin{document}

We sincerely thank the five reviewers for their valuable feedback. 
We list below our replies to your concerns.

\vspace{-0.1in}
\section{Reviewer 1}

% Thank you for the comments and suggestions.



\noindent \textbf{* Benefit of Anisotropic stepsize:} 
We develop STANLey in order to more efficiently sample from the Gibbs potential. H
ence, our goal is not to reach an optimal and high resolution generated image, but rather to decrease the number of kernel transitions need at each EBM iteration in order to obtain relatively good samples.
Drastically reducing this number would have a great impact on the energy consumption and speed of the whole training process.
Besides, we stress on the important theoretical contribution that is presented along our algorithm. 
To the best of our knowledge, EBM methods are presented mainly using empirical insights on their respective contribution.
In this paper, we wanted to show the benefits of using adaptive stepsize for learning a convent-based EBM where the energy landscape is highly nonconvex, not only via experiments but with a rigorous non-asymptotic convergence analysis.
This also echoes with R4's remark on "how the approach fits within the broader landscape of energy-based modeling approaches".
Indeed, we specifically design STANLey update to take into account the curvature of the nonconvex energy landscape by embedding a dimension gradient informed stepsize.

\textit{The goal is to propose an efficient algorithm with low computational cost and with theoretical guaranties of relevance.
These are particularly hard tasks when dealing with high dimensional data and nonconvex models such as CNNs.
Hence, we propose an optimized Langevin update and prove that this new sampler leads to a geometrically uniformly ergodic Markov chain.}

\noindent \textbf{* Visual Checks:} 
We will improve the resolution of Figure 6 for the sake of clarity.

\vspace{-0.1in}
\section{Reviewer 2}

% We thank the reviewer for the thorough analysis.

\noindent \textbf{* Performance of HMC:}
The slightly better performance of HMC in terms of FID can naturally be explained by its Hamiltonian dynamics that exploit first and second order moments of the target distribution to explore the sampling space.
HMC has been for a long time the state of the art method for sampling high dimensional posterior distribution.
Yet, the second-order information computation brings heavy computational burden exhibited Table~\ref{tab:runtimes}.
For that reason, reaching similar performances using our cheaper method is a good sign.

\vspace{-0.1in}
\section{Reviewer 3}

% We thank the reviewer for the valuable feedback.

\noindent \textbf{* Quantitative results:}
We would like to point out that Figure 2 presents a visual comparison between our method and the vanilla Langevin, Figure 3 displays how STANLEY and three other baselines compare when generating synthetic natural images and Figure 6 corresponds to the visual comparison of STANLEY and the vanilla Langevin on the celeb-A dataset. Those visual plots aim at getting a visual perspective of the benefits of our method.
We add that Figure 4 and 5 compare all baselines in terms of FID which is also a visual comparison of the generated images.

\noindent \textbf{* Complexity of STANLEY:}
Running times are now reported on Table~\ref{tab:runtimes}.
Our method is relatively comparable to the vanilla method in terms of computation complexity since line 3 of Algorithm 1 in the main paper uses the already computed gradient vector (no added computation unlike HMC method).
In terms of memory, we shift from a constant scalar learning in the vanilla Langevin to a vector of stepsizes depending on the dimension index $d$. No memory issue were to be reported during our extensive experiments.

\vspace{-0.1in}
\section{Reviewer 5}

% Thanks for reading through our paper and your remarks. 


\noindent \textbf{* FID plots:} We would like to invite the reviewer to re-assess this remark on the FID curves. 
Indeed an FID equal to 0 corresponds to two sets of exactly identical images, which would be absurd when dealing with generated synthetic images.
As a result, plotting FID=0 is out of context here. 



\noindent \textbf{* Complexity Analysis:} We provide the running times of our method and the baselines in Table~\ref{tab:runtimes} on CIFAR-10 and Celeb-A datasets with a batchsize of $100$.
We would like to stress on the similar computational complexity between the vanilla Langevin and our method STANLEY since our newly introduced stepsize uses the already computed gradient vector. 
On the contrary, the HMC method has recourse to both the gradient and the Hessian of the target distribution, resulting in longer computation time as reported on Table~\ref{tab:runtimes}.

\begin{table}[h]
\small
\caption{ Runtime (in s) for training our EBM during 1 epoch.}\label{tab:runtimes}
	\resizebox{\columnwidth}{!}{%
\begin{tabular}{lllllll}
\toprule[1pt]
& {Vanilla Langevin} & {HMC} & {GD} & {\textbf{STANLEY}}  \\ \hline
{CIFAR-10 Dataset}  & {$232.5$} & {$698.4$} & {$211.3$} & {$265.2$}   \\ 
\toprule[1pt]
{Celeb-A dataset}  & {$376.3$} & {$640.1$} & {$345.2$} & {$414.8$}  \\ 
\toprule[1pt]
\end{tabular}
}\vspace{-0.1in}
\end{table}
We run each of the method, including ours, on a single TitanXx8 GPU for our experiments.

In terms of memory complexity, we acknowledge that STANLEY requires to store a gradient-informed stepsize larger than a constant one for the vanilla Langevin. Though, in none of our runs we encountered any memory issue. The benfits of using a gradient-informed in terms of convergence outweighs its memory constraint.

\noindent \textbf{* Reproducibility:} As the reviewer as evaluated the reproducibility of our work as fair, we would like to highlight that the proofs of our results can be found in the supplementary material, the data is open source (CIFAR, Flowers and celeb-A) and the code can be requested.

\noindent \textbf{* Conclusion:} Except the FID curves concern, which was out of context, we addressed the main concern of Reviewer 5. For that reason we would like the reviewer to consider increasing its score on our contribution.

\vspace{-0.1in}
\section{Reviewer 6}

% We thank the reviewer for the valuable comments.

\noindent \textbf{* Originality of our contribution:} 
See Reviewer 1 reply.

\noindent \textbf{* Comparison with GANs and Flow models:} 

Comparing to flow models and GANs is out of the scope of our contribution.
Indeed, we introduce a novel MCMC sampling method for the sole purpose of training an energy based model.
Since EBM are based on the learning of an energy function through negative sampling (obtaining samples from the conditional distribution is the main challenge), we do not believe that including discriminator-based architecture is irrelevant. 


%
%Dear Senior Program Chair,
%
%We are writing about several reviews we received on our submission 10124.
%In particular, the quality of reviewers R5 and R6, while showing good ?confidence in their evaluation, are rather poor in our opinion.
%
%Regarding Reviewer 5, mentioning a null FID is showing a lack of expertise in the scope of our paper and would like to ask the PC to replace such a review with another one from an additional expert.
%
%Regarding Reviewer 6, we stress on the fact that the majority of R6 review is based on the comparison with GAN and flow models, which in our opinion, is completely out of the scope of the paper.
%Indeed, we suggest an improved MCMC sampling method for Energy-based models, while those two models mentioned above do not embed any sampling method.
%
%We appreciate your attention and thank you again for handling the review of our paper. 
%
%Best Regards,
%Authors of 10124.



\end{document}
