\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Allassonniere and Kuhn(2015)}]{allassonniere2015convergent}
Allassonniere, S.; and Kuhn, E. 2015.
\newblock Convergent Stochastic Expectation Maximization algorithm with
  efficient sampling in high dimension. Application to deformable template
  model estimation.
\newblock \emph{Computational Statistics \& Data Analysis}, 91: 4--19.

\bibitem[{Atchad{\'e}(2006)}]{atchade2006adaptive}
Atchad{\'e}, Y.~F. 2006.
\newblock An adaptive version for the Metropolis adjusted Langevin algorithm
  with a truncated drift.
\newblock \emph{Methodology and Computing in applied Probability}, 8(2):
  235--254.

\bibitem[{Bottou and Bousquet(2008)}]{bottou2008}
Bottou, L.; and Bousquet, O. 2008.
\newblock The Tradeoffs of Large Scale Learning.
\newblock In Platt, J.~C.; Koller, D.; Singer, Y.; and Roweis, S.~T., eds.,
  \emph{Advances in Neural Information Processing Systems 20}, 161--168. Curran
  Associates, Inc.

\bibitem[{Brosse et~al.(2017)Brosse, Durmus, Moulines, and
  Sabanis}]{brosse2017tamed}
Brosse, N.; Durmus, A.; Moulines, {\'E}.; and Sabanis, S. 2017.
\newblock The Tamed Unadjusted Langevin Algorithm.
\newblock \emph{arXiv preprint arXiv:1710.05559}.

\bibitem[{Cotter et~al.(2013)Cotter, Roberts, Stuart, and
  White}]{cotter2013mcmc}
Cotter, S.~L.; Roberts, G.~O.; Stuart, A.~M.; and White, D. 2013.
\newblock MCMC methods for functions: modifying old algorithms to make them
  faster.
\newblock \emph{Statistical Science}, 424--446.

\bibitem[{de~Freitas et~al.(2001)de~Freitas, H{\o}jen-S{\o}rensen, Jordan, and
  Russell}]{freitas}
de~Freitas, N.; H{\o}jen-S{\o}rensen, P.; Jordan, M.~I.; and Russell, S. 2001.
\newblock Variational MCMC.
\newblock \emph{Proceedings of the Seventeenth Conference on Uncertainty in
  Artificial Intelligence}, 120--127.

\bibitem[{Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and
  Ranzato}]{deng2020residual}
Deng, Y.; Bakhtin, A.; Ott, M.; Szlam, A.; and Ranzato, M. 2020.
\newblock Residual energy-based models for text generation.
\newblock \emph{arXiv preprint arXiv:2004.11714}.

\bibitem[{Doucet, Godsill, and Andrieu(2000)}]{doucet2000sequential}
Doucet, A.; Godsill, S.; and Andrieu, C. 2000.
\newblock On sequential Monte Carlo sampling methods for Bayesian filtering.
\newblock \emph{Statistics and computing}, 10(3): 197--208.

\bibitem[{Du et~al.(2020{\natexlab{a}})Du, Li, Tenenbaum, and
  Mordatch}]{du2020improved}
Du, Y.; Li, S.; Tenenbaum, J.; and Mordatch, I. 2020{\natexlab{a}}.
\newblock Improved Contrastive Divergence Training of Energy Based Models.
\newblock \emph{arXiv preprint arXiv:2012.01316}.

\bibitem[{Du et~al.(2019)Du, Meier, Ma, Fergus, and Rives}]{du2019energy}
Du, Y.; Meier, J.; Ma, J.; Fergus, R.; and Rives, A. 2019.
\newblock Energy-based models for atomic-resolution protein conformations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Du et~al.(2020{\natexlab{b}})Du, Meier, Ma, Fergus, and
  Rives}]{du2020energy}
Du, Y.; Meier, J.; Ma, J.; Fergus, R.; and Rives, A. 2020{\natexlab{b}}.
\newblock Energy-based models for atomic-resolution protein conformations.
\newblock \emph{arXiv preprint arXiv:2004.13167}.

\bibitem[{Du and Mordatch(2019)}]{du2019implicit}
Du, Y.; and Mordatch, I. 2019.
\newblock Implicit Generation and Modeling with Energy Based Models.
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d~Alche-Buc, F.;
  Fox, E.; and Garnett, R., eds., \emph{Advances in Neural Information
  Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[{Durmus et~al.(2017)Durmus, Roberts, Vilmart, and
  Zygalakis}]{durmus2017fast}
Durmus, A.; Roberts, G.~O.; Vilmart, G.; and Zygalakis, K.~C. 2017.
\newblock Fast Langevin based algorithm for MCMC in high dimensions.
\newblock \emph{Ann. Appl. Probab.}, 27(4): 2195--2237.

\bibitem[{Gao et~al.(2018)Gao, Lu, Zhou, Zhu, and Wu}]{gao2018learning}
Gao, R.; Lu, Y.; Zhou, J.; Zhu, S.-C.; and Wu, Y.~N. 2018.
\newblock Learning generative convnets via multi-grid modeling and sampling.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 9155--9164.

\bibitem[{Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu}]{gao2020flow}
Gao, R.; Nijkamp, E.; Kingma, D.~P.; Xu, Z.; Dai, A.~M.; and Wu, Y.~N. 2020.
\newblock Flow contrastive estimation of energy-based models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 7518--7528.

\bibitem[{Girolami and Calderhead(2011)}]{girolami2011riemann}
Girolami, M.; and Calderhead, B. 2011.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73(2): 123--214.

\bibitem[{Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio}]{goodfellow2014generative}
Goodfellow, I.~J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.;
  Ozair, S.; Courville, A.; and Bengio, Y. 2014.
\newblock Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1406.2661}.

\bibitem[{Grenander and Miller(1994)}]{grenander1994representations}
Grenander, U.; and Miller, M.~I. 1994.
\newblock Representations of knowledge in complex systems.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 56(4): 549--581.

\bibitem[{Gustafsson et~al.(2020)Gustafsson, Danelljan, Bhat, and
  Sch{\"o}n}]{gustafsson2020energy}
Gustafsson, F.~K.; Danelljan, M.; Bhat, G.; and Sch{\"o}n, T.~B. 2020.
\newblock Energy-based models for deep probabilistic regression.
\newblock In \emph{European Conference on Computer Vision}, 325--343. Springer.

\bibitem[{Gutmann and Hyv{\"a}rinen(2010)}]{gutmann2010noise}
Gutmann, M.; and Hyv{\"a}rinen, A. 2010.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, 297--304.

\bibitem[{Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine}]{haarnoja2017reinforcement}
Haarnoja, T.; Tang, H.; Abbeel, P.; and Levine, S. 2017.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, 1352--1361.
  PMLR.

\bibitem[{Han et~al.(2019)Han, Nijkamp, Fang, Hill, Zhu, and
  Wu}]{han2019divergence}
Han, T.; Nijkamp, E.; Fang, X.; Hill, M.; Zhu, S.-C.; and Wu, Y.~N. 2019.
\newblock Divergence triangle for joint training of generator model,
  energy-based model, and inferential model.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 8670--8679.

\bibitem[{Han et~al.(2020)Han, Nijkamp, Zhou, Pang, Zhu, and Wu}]{han2020joint}
Han, T.; Nijkamp, E.; Zhou, L.; Pang, B.; Zhu, S.-C.; and Wu, Y.~N. 2020.
\newblock Joint training of variational auto-encoder and latent energy-based
  model.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 7978--7987.

\bibitem[{Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter}]{heusel2017gans}
Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S.
  2017.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{arXiv preprint arXiv:1706.08500}.

\bibitem[{Hinton(2002)}]{hinton2002training}
Hinton, G.~E. 2002.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14(8): 1771--1800.

\bibitem[{Ingraham et~al.(2018)Ingraham, Riesselman, Sander, and
  Marks}]{ingraham2018learning}
Ingraham, J.; Riesselman, A.; Sander, C.; and Marks, D. 2018.
\newblock Learning protein structure with a differentiable simulator.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}.

\bibitem[{Jacob, O~Leary, and Atchad{\'e}(2020)}]{jacob2020unbiased}
Jacob, P.~E.; O~Leary, J.; and Atchad{\'e}, Y.~F. 2020.
\newblock Unbiased Markov chain Monte Carlo methods with couplings.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82(3): 543--600.

\bibitem[{Jarner and Hansen(2000)}]{jarner2000geometric}
Jarner, S.~F.; and Hansen, E. 2000.
\newblock Geometric ergodicity of Metropolis algorithms.
\newblock \emph{Stochastic processes and their applications}, 85(2): 341--361.

\bibitem[{Jin, Lazarow, and Tu(2017)}]{jin2017introspective}
Jin, L.; Lazarow, J.; and Tu, Z. 2017.
\newblock Introspective classification with convolutional nets.
\newblock In \emph{Advances in Neural Information Processing Systems},
  823--833.

\bibitem[{Kingma and Ba(2015)}]{KB15}
Kingma, D.~P.; and Ba, J. 2015.
\newblock Adam: A Method for Stochastic Optimization.
\newblock \emph{ICLR}.

\bibitem[{Kingma and Welling(2013)}]{kingma2013auto}
Kingma, D.~P.; and Welling, M. 2013.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}.

\bibitem[{Krizhevsky and Hinton(2009)}]{krizhevsky2009learning}
Krizhevsky, A.; and Hinton, G. 2009.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}.

\bibitem[{Lazarow, Jin, and Tu(2017)}]{lazarow2017introspective}
Lazarow, J.; Jin, L.; and Tu, Z. 2017.
\newblock Introspective neural networks for generative modeling.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, 2774--2783.

\bibitem[{LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang}]{lecun2006tutorial}
LeCun, Y.; Chopra, S.; Hadsell, R.; Ranzato, M.; and Huang, F. 2006.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting structured data}, 1(0).

\bibitem[{Lee et~al.(2018)Lee, Xu, Fan, and Tu}]{lee2018wasserstein}
Lee, K.; Xu, W.; Fan, F.; and Tu, Z. 2018.
\newblock Wasserstein introspective neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 3702--3711.

\bibitem[{Liu et~al.(2020)Liu, Wang, Owens, and Li}]{ebmood2020}
Liu, W.; Wang, X.; Owens, J.~D.; and Li, Y. 2020.
\newblock Energy-based Out-of-distribution Detection.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}.

\bibitem[{Liu et~al.(2015)Liu, Luo, Wang, and Tang}]{liu2015deep}
Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, 3730--3738.

\bibitem[{Marshall and Roberts(2012)}]{marshall2012adaptive}
Marshall, T.; and Roberts, G. 2012.
\newblock An adaptive approach to Langevin MCMC.
\newblock \emph{Statistics and Computing}, 22(5): 1041--1057.

\bibitem[{Meyn and Tweedie(2012)}]{meyn2012markov}
Meyn, S.~P.; and Tweedie, R.~L. 2012.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media.

\bibitem[{Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and
  Dean}]{mikolov2013distributed}
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.; and Dean, J. 2013.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock \emph{arXiv preprint arXiv:1310.4546}.

\bibitem[{Neal et~al.(2011)}]{neal2011mcmc}
Neal, R.~M.; et~al. 2011.
\newblock MCMC using Hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2(11): 2.

\bibitem[{Ngiam et~al.(2011)Ngiam, Chen, Koh, and Ng}]{ngiam2011learning}
Ngiam, J.; Chen, Z.; Koh, P.~W.; and Ng, A.~Y. 2011.
\newblock Learning deep energy models.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, 1105--1112.

\bibitem[{Nijkamp et~al.(2020)Nijkamp, Hill, Han, Zhu, and
  Wu}]{nijkamp2020anatomy}
Nijkamp, E.; Hill, M.; Han, T.; Zhu, S.; and Wu, Y.~N. 2020.
\newblock On the Anatomy of MCMC-Based Maximum Likelihood Learning of
  Energy-Based Models.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, 5272--5280. {AAAI} Press.

\bibitem[{Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu}]{nijkamp2019learning}
Nijkamp, E.; Hill, M.; Zhu, S.-C.; and Wu, Y.~N. 2019.
\newblock Learning non-convergent non-persistent short-run MCMC toward
  energy-based model.
\newblock \emph{arXiv preprint arXiv:1904.09770}.

\bibitem[{Nilsback and Zisserman(2008)}]{nilsback2008automated}
Nilsback, M.-E.; and Zisserman, A. 2008.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, 722--729. IEEE.

\bibitem[{Qiu, Zhang, and Wang(2019)}]{qiu2019unbiased}
Qiu, Y.; Zhang, L.; and Wang, X. 2019.
\newblock Unbiased contrastive divergence algorithm for training energy-based
  latent variable models.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Robbins and Monro(1951)}]{robbins1951A}
Robbins, H.; and Monro, S. 1951.
\newblock A stochastic approximation method.
\newblock \emph{Annals of Mathematical Statistics}, 22: 400--407.

\bibitem[{Robert and Casella(2010)}]{mh:robert}
Robert, C.~P.; and Casella, G. 2010.
\newblock \emph{Metropolis--Hastings Algorithms}, 167--197.
\newblock New York, NY: Springer New York.
\newblock ISBN 978-1-4419-1576-4.

\bibitem[{Roberts and Rosenthal(1997)}]{roberts}
Roberts, G.~O.; and Rosenthal, J.~S. 1997.
\newblock Optimal scaling of discrete approximations to Langevin diffusions.
\newblock \emph{J. R. Statist. Soc. B}, 60: 255--268.

\bibitem[{Roberts, Rosenthal et~al.(2004)}]{roberts2004general}
Roberts, G.~O.; Rosenthal, J.~S.; et~al. 2004.
\newblock General state space Markov chains and MCMC algorithms.
\newblock \emph{Probability surveys}, 1.

\bibitem[{Roberts and Tweedie(1996)}]{robertsmala}
Roberts, G.~O.; and Tweedie, R.~L. 1996.
\newblock Exponential convergence of Langevin distributions and their discrete
  approximations.
\newblock \emph{Bernoulli}, 2(4): 341--363.

\bibitem[{Roberts, Tweedie et~al.(1996)}]{roberts1996exponential}
Roberts, G.~O.; Tweedie, R.~L.; et~al. 1996.
\newblock Exponential convergence of Langevin distributions and their discrete
  approximations.
\newblock \emph{Bernoulli}, 2(4): 341--363.

\bibitem[{Rue, Martino, and Chopin(2009)}]{rue2009approximate}
Rue, H.; Martino, S.; and Chopin, N. 2009.
\newblock Approximate Bayesian inference for latent Gaussian models by using
  integrated nested Laplace approximations.
\newblock \emph{Journal of the royal statistical society: Series b (statistical
  methodology)}, 71(2): 319--392.

\bibitem[{Song et~al.(2020)Song, Garg, Shi, and Ermon}]{song2020sliced}
Song, Y.; Garg, S.; Shi, J.; and Ermon, S. 2020.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 574--584. PMLR.

\bibitem[{Tieleman(2008)}]{tieleman2008training}
Tieleman, T. 2008.
\newblock Training restricted Boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, 1064--1071.

\bibitem[{Wainwright and Jordan(2008)}]{jordanvi}
Wainwright, M.~J.; and Jordan, M.~I. 2008.
\newblock Graphical Models, Exponential Families, and Variational Inference.
\newblock \emph{Found. Trends Mach. Learn.}, 1(1-2): 1--305.

\bibitem[{Welling and Hinton(2002)}]{welling2002new}
Welling, M.; and Hinton, G.~E. 2002.
\newblock A new learning algorithm for mean field Boltzmann machines.
\newblock In \emph{International Conference on Artificial Neural Networks},
  351--357. Springer.

\bibitem[{Welling and Teh(2011)}]{welling2011bayesian}
Welling, M.; and Teh, Y.~W. 2011.
\newblock Bayesian learning via stochastic gradient Langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, 681--688.

\bibitem[{Wenliang et~al.(2019)Wenliang, Sutherland, Strathmann, and
  Gretton}]{wenliang2019learning}
Wenliang, L.; Sutherland, D.; Strathmann, H.; and Gretton, A. 2019.
\newblock Learning deep kernels for exponential family densities.
\newblock In \emph{International Conference on Machine Learning}, 6737--6746.
  PMLR.

\bibitem[{Wolfinger(1993)}]{wolfinger}
Wolfinger, R. 1993.
\newblock Laplace's approximation for nonlinear mixed models.
\newblock \emph{Biometrika}, 80(4): 791--795.

\bibitem[{Xie et~al.(2018{\natexlab{a}})Xie, Lu, Gao, Zhu, and
  Wu}]{xie2018cooperative}
Xie, J.; Lu, Y.; Gao, R.; Zhu, S.-C.; and Wu, Y.~N. 2018{\natexlab{a}}.
\newblock Cooperative training of descriptor and generator networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 42(1): 27--45.

\bibitem[{Xie et~al.(2016)Xie, Lu, Zhu, and Wu}]{xie2016theory}
Xie, J.; Lu, Y.; Zhu, S.-C.; and Wu, Y. 2016.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning}, 2635--2644.
  PMLR.

\bibitem[{Xie et~al.(2021{\natexlab{a}})Xie, Xu, Zheng, Zhu, and
  Wu}]{xie2021GPointNet}
Xie, J.; Xu, Y.; Zheng, Z.; Zhu, S.; and Wu, Y.~N. 2021{\natexlab{a}}.
\newblock Generative PointNet: energy-based learning on unordered point sets
  for 3D generation, reconstruction and classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}.

\bibitem[{Xie et~al.(2021{\natexlab{b}})Xie, Zheng, Fang, Zhu, and
  Wu}]{xie2021cooperative}
Xie, J.; Zheng, Z.; Fang, X.; Zhu, S.-C.; and Wu, Y.~N. 2021{\natexlab{b}}.
\newblock Cooperative Training of Fast Thinking Initializer and Slow Thinking
  Solver for Conditional Learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}.

\bibitem[{Xie et~al.(2021{\natexlab{c}})Xie, Zheng, Fang, Zhu, and
  Wu}]{xie2021cycleCoopNets}
Xie, J.; Zheng, Z.; Fang, X.; Zhu, S.-C.; and Wu, Y.~N. 2021{\natexlab{c}}.
\newblock Learning cycle-consistent cooperative networks via alternating MCMC
  teaching for unsupervised cross-domain translation.
\newblock In \emph{Proceedings of The Thirty-Fifth AAAI Conference on
  Artificial Intelligence (AAAI)}.

\bibitem[{Xie et~al.(2018{\natexlab{b}})Xie, Zheng, Gao, Wang, Zhu, and
  Nian~Wu}]{xie2018learning}
Xie, J.; Zheng, Z.; Gao, R.; Wang, W.; Zhu, S.-C.; and Nian~Wu, Y.
  2018{\natexlab{b}}.
\newblock Learning descriptor networks for 3D shape synthesis and analysis.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 8629--8638.

\bibitem[{Xie et~al.(2020)Xie, Zheng, Gao, Wang, Zhu, and
  Wu}]{xie2020generative}
Xie, J.; Zheng, Z.; Gao, R.; Wang, W.; Zhu, S.-C.; and Wu, Y.~N. 2020.
\newblock Generative VoxelNet: Learning Energy-Based Models for 3D Shape
  Synthesis and Analysis.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}.

\bibitem[{Xie, Zheng, and Li(2021)}]{xie2020learning}
Xie, J.; Zheng, Z.; and Li, P. 2021.
\newblock Learning Energy-Based Model with Variational Auto-Encoder as
  Amortized Sampler.
\newblock In \emph{The Thirty-Fifth AAAI Conference on Artificial Intelligence
  (AAAI)}.

\bibitem[{Xie, Zhu, and Wu(2017)}]{XieCVPR17}
Xie, J.; Zhu, S.-C.; and Wu, Y.~N. 2017.
\newblock Synthesizing dynamic patterns by spatial-temporal generative ConvNet.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 7093--7101.

\bibitem[{Xie, Zhu, and Wu(2019)}]{xie2019learning}
Xie, J.; Zhu, S.-C.; and Wu, Y.~N. 2019.
\newblock Learning energy-based spatial-temporal generative convnets for
  dynamic patterns.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}.

\bibitem[{Xu et~al.(2019)Xu, Xie, Zhao, Baker, Zhao, and Wu}]{xu2019energy}
Xu, Y.; Xie, J.; Zhao, T.; Baker, C.; Zhao, Y.; and Wu, Y.~N. 2019.
\newblock Energy-based continuous inverse optimal control.
\newblock \emph{arXiv preprint arXiv:1904.05453}.

\bibitem[{Yu et~al.(2020)Yu, Song, Song, and Ermon}]{yu2020training}
Yu, L.; Song, Y.; Song, J.; and Ermon, S. 2020.
\newblock Training deep energy-based models with f-divergence minimization.
\newblock In \emph{International Conference on Machine Learning}, 10957--10967.

\bibitem[{Zhu, Wu, and Mumford(1998)}]{zhu1998filters}
Zhu, S.~C.; Wu, Y.; and Mumford, D. 1998.
\newblock Filters, random fields and maximum entropy (FRAME): Towards a unified
  theory for texture modeling.
\newblock \emph{International Journal of Computer Vision}, 27(2): 107--126.

\end{thebibliography}
