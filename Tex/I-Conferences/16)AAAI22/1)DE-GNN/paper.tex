\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[table]{xcolor}
\usepackage{makecell}
\usepackage{floatrow}
\floatsetup[table]{capposition=top}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Dual Energy-Flow Enhanced Graph Neural Network\\ for Visual Question Answering}
% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar, \textsuperscript{\rm 2}
%     % J. Scott Penberthy, \textsuperscript{\rm 3}
%     % George Ferguson,\textsuperscript{\rm 4}
%     % Hans Guesgen, \textsuperscript{\rm 5}.
%     % Note that the comma should be placed BEFORE the superscript for optimum readability

%     2275 East Bayshore Road, Suite 160\\
%     Palo Alto, California 94303\\
%     % email address must be in roman text type, not monospace or sans serif
%     publications22@aaai.org
% %
% % See more examples next
% }

% %Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi

% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name,\textsuperscript{\rm 1}
%     Second Author Name, \textsuperscript{\rm 2}
%     Third Author Name \textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1} Affiliation 1\\
%     \textsuperscript{\rm 2} Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Scene Graphs (SG), as a structural abstraction of natural images, contain massive detailed information. 
Modeling visual reasoning through SG can significantly improve the ability and strengthen the interpretability of reasoning. 
However, existing models often fail to neither \emph{jointly} exploit objects, relations, and attributes information in SG, nor balance the importance of objects and relations. 
In this paper, we introduce a novel Dual Energy-Flow Enhanced Graph Neural Network (DE-GNN), which learns a comprehensive representation by encoding full-scale scene graphs information from objects, attributes, and relations.
Specifically, two types of SG structures are employed in the encoder: 
(i) \textit{Object-significant graphs} which absorb attribute and relation information into nodes' representations. 
(ii) \textit{Relation-significant graphs} which intensify the model's perception of relation features. 
In addition, we design an \textit{energy-flow mechanism} to enhance the information transfer from edges and adjacent nodes to the updating nodes. 
We conduct extensive experiments on public GQA and Visual Genome datasets and achieve new state-of-the art performances highlighting the benefits of our method.\footnote{our code is available at anonymous github}
% Evaluated on public GQA and Visual Genome datasets, our method is able to outperform previous state-of-the-art models on these two datasets.
% \footnote{\href{https://anonymous.4open.science/r/dual_ggnn-for-vqa-CEDC/}{Our code is available at anonymous github}}
\end{abstract}

\section{Introduction}
Recent developments in deep learning have accelerated the research progress in Computer Vision (CV) and Natural Language Processing (NLP) areas. 
Multimodal fusion tasks between image and text have attracted a growing attention, such as image captioning and visual question answering (VQA) tasks. 
In particular, the task of VQA requires a model to answer a free-form natural language question using visual information from an image. 
VQA has proven to be a crucial multimodal task with a large scope of applications such as AI assistants, multimodal customer service dialogue and image-based search to name a few.

\begin{figure}[ht] 
    % \vspace{-0.1in}
    \centering 
    \includegraphics[scale=0.5]{./pic/intro.pdf} 
    % \vspace{-0.1in}
    \caption{Visualization of a scene graph with objects, attributes, and relations information} 
    \label{scene-graph} 
    % \vspace{-0.2in}
\end{figure}

Scene graph (SG) reasoning is an important branch of VQA tasks, see~\cite{DBLP:journals/corr/abs-2007-01072}. 
To generate the scene graph, the model extracts objects' names, attributes and relationships from the images and constructs them into graph representation as illustrated in Figure~\ref{scene-graph}. 
SG representation modeling displays several virtues over classical techniques leveraging object features extracted from images by, for eg., convolutional neural network (CNN) since in SG (a) the features are presented in plain and free text form~\cite{DBLP:journals/corr/abs-2101-05479}, (b) it makes use of graph structures which have better interpretability~\cite{DBLP:conf/bmvc/ZhangCX19}.
In this contribution, we propose two reasoning methods on scene graphs: (i) Consider scene graphs as probabilistic graphs and iteratively update nodes' probabilities using soft instructions extracted from questions such as Neural State Machine (NSM)~\cite{DBLP:conf/nips/HudsonM19,DBLP:conf/ijcnn/LeLV020}; (ii) Apply Graph Neural Network (GNN) into scene graphs~\cite{inproceedings,DBLP:conf/iccv/LiGCL19} to learn a joint representation of the nodes and their relations, and then feed the representation into a predictor to generate the answer.

\begin{figure*}[ht] 
    % \vspace{-0.5in}
    \centering 
    \includegraphics[width=1.0\textwidth]{./pic/DE-GNN2.pdf} 
    % \vspace{-0.1in}
    \caption{Model structure of the proposed Dual Energy-Flow enhanced Graph Neural Networks. EF stands for the energy-flow module. Images are transformed into scene graphs by the scene graph generator. The object-significant form and relation-significant form of the scene graph are injected into the object encoder and the relation encoder. Nodes' representations are generated from the sum of energy-flow modules. The representations are then be fused with question representation to predict an answer.} 
    \label{fig2} 
    % \vspace{-0.2in}
\end{figure*}


Scene graph reasoning frameworks have proven to be useful in VQA tasks, see for instance \cite{johnson2015image,DBLP:journals/corr/abs-2007-01072,yang2020prior}. 
However, none of the existing methods fully utilize the scene graph information, including objects, attributes, and relations.
Besides, they fail at generating comprehensive representations for objects using features from their neighbors and their attributes. 
Generally, information from objects and relations connected to them are reconstructed into object features in GNN-based methods.
% \textcolor{red}{Some references would help for those methods that uses object features in GNN \cite{xu2019spatial}}
However, these encoding methods lack information from objects' attributes and objects on the other side of the edges. 
The NSM methods use attention mechanisms to update answer possibilities of objects, attributes, and relations, but they cannot learn the joint representation of all three types of information. 
Additionally, GNN-based models and NSM models are more focused towards object features, and considering relation features as references. 
Empirically, we demonstrate that a correct relation representation is crucial to the VQA task and enables to alleviate the bottlenecks of VQA implied by inefficient usages of scene graph information described above.

Therefore, as a fix to current ineffective strategies, we propose the Dual Energy-Flow Enhanced Graph Neural Network (DE-GNN) for VQA, introducing a novel scene graph reasoning model that extracts full-scale feature maps from objects, attributes, and relations information in scene graphs. 
Concretely, as shown in Figure~\ref{fig2}, our DE-GNN model contains a scene graph generator, a question encoder, dual graph encoders, and a fusion module. 
Essentially, the scene graph generator extracts graphs out of images. 
Besides, to preserve integrated information in the encoding process, we transform scene graphs into a relation-significant modality, in which nodes represent relations and edges represent objects, and an object-significant modality, in which nodes represent objects and edges represent relations. Lastly, after receiving scene graphs in two modalities, dual graph encoders can produce feature maps focusing on both relations and objects.

Furthermore, to learn a node's joint representation from its attributes, edges, and adjacent nodes, we modify the gated graph neural network (GGNN) structure in our proposed DE-GNN by adding the energy-flow module. 
It is a bidirectional GRU that guides the internal information flow.
The encoder can capture information from nodes, edges, and adjacent nodes that connect to them. 
The outputs of the encoder pass through multi-head attention layers using question features extracted from the question encoder, see Figure~\ref{fig2}. 
Hence, the model can dynamically focus on the critical parts of the questions and use the most similar part of the scene graph as the most adequate answer.

In summary, our main contributions are as follows:%\vspace{-0.05in}
\begin{itemize}
\setlength{\itemsep}{5pt}
\setlength{\parsep}{5pt}
\setlength{\parskip}{5pt}
\item We propose a novel DE-GNN model to learn a comprehensive representation of scene graphs by encoding graphs' object-significant modality and relation-significant modality.\vspace{-0.06in}

\item Our energy-flow module is more suitable for processing graphs with meaningful edges and nodes with internal attributes.\vspace{-0.06in}

\item We conduct experiments on GQA and Visual Genome and experimental results demonstrate the effectiveness of DE-GNN which can improve the reasoning accuracy on semantically complicated questions.
% \item We show that our method outperforms other SG-based models in four scene graph datasets, which are generated from the VG dataset by four scene graph generation methods. Extensive experiments demonstrate the effectiveness of DE-GNN.
\end{itemize}\vspace{-0.06in}

\section{Related Works}
\noindent\textbf{Visual Question Answering.}
Most VQA approaches utilize a question encoder architecture that can learn complex temporal dynamics using a sequence of hidden states. To encode the image, most VQA approaches employ CNN-based pretrained models like Mask-RCNN or Faster-RCNN~\cite{DBLP:conf/cvpr/FanZ18,DBLP:conf/cvpr/PatroN18,DBLP:conf/cvpr/NamHK17}. The image encoder and question encoder then pass through a multimodal fusion part and the output fusion vector pass through an answer predictor.

To learn image representations that more focused on questions, many attention-based models are proposed such as BUTD~\cite{DBLP:conf/cvpr/00010BT0GZ18}, SAT~\cite{DBLP:conf/cvpr/YangHGDS16}, question-guided spatial attention~\cite{DBLP:conf/eccv/XuS16}. In~\citet{DBLP:conf/nips/LuYBP16}, the authors proposed a hierarchical co-attention model that jointly implements both image-guided question attention and question-guided visual attention.  MacNet~\cite{DBLP:conf/iclr/HudsonM18}uses Mac-cells to combine attention and encoding function. However, there still exists a significant semantic gap between image and natural language. Transformer-based models such as Unicoder-VL~\cite{DBLP:conf/aaai/LiDFGJ20} can achieve outstanding performance on VQA tasks, but these models need complicated pretrain strategies and extra datasets. The pretraining tasks are time-consuming and hard to update under the changeable environment. To solve the existing problems in attention-based and transformer-based VQA models, we apply scene graphs as our reasoning model base.

\vspace{0.05in}
\noindent\textbf{Scene Graph Generation and Reasoning.}
Most scene graph generation (SGG) methods use object detection methods like mask-rcnn or faster-rcnn to extract region proposals from images~\cite{DBLP:conf/cvpr/XuZCF17,DBLP:conf/eccv/YangLLBP18,DBLP:conf/cvpr/ZellersYTC18,DBLP:conf/nips/WooKCK18,DBLP:conf/cvpr/DaiZL17,DBLP:conf/iccv/LiOZWW17,DBLP:conf/eccv/YinSLYWSL18}. Methods to reduce the SGG training bias has been put forward~\cite{DBLP:conf/cvpr/TangNHSZ20}. Due to the graph hierarchy extracted from images, SG can promotes explainable reasoning for downstream multimodal tasks such as VQA~\cite{DBLP:conf/bmvc/ZhangCX19}. In our work, scene graph generation methods are used to transform VQA datasets into scene graph datasets. Our model is then tested in datasets generated by different SGG methods.

In typical scene graph reasoning models, neural state machine~\cite{DBLP:conf/nips/HudsonM19} first predicts a scene graph that represents its underlying semantics and serves as a structured model of the world. 
Then it performs sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. 
But, as we describe below, state machine-based models can not effectively capture complicated scene graph features.
For instance, FSTT~\cite{inproceedings} uses GGNN based model to encode scene graphs but it neglects vital information from edges and attributes. Relation-aware Graph Attention Network~\cite{DBLP:conf/iccv/LiGCL19} encodes each image into a graph and models multitype inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. However, neither can fully use attribute information nor can it learn the comprehensive representation of scene graphs using graph attention network. 
Our model uses GGNN structure to learn more comprehensive scene graph representations. 

\vspace{0.05in}
\noindent\textbf{Graph Neural Network.}
GNN~\cite{DBLP:journals/tnn/ScarselliGTHM09} are a class of traditional neural networks methods designed to infer on data described by graphs models. 
A group of graph-based models~\cite{DBLP:conf/aaai/0001RFHLRG19,DBLP:conf/aaai/LiuCLZLSQ19} were proposed for different graph tasks including graph representation learning. 
Inspired by convolution neural network, graph convolutional network (GCN)~\cite{DBLP:conf/iclr/KipfW17} improves GNNs efficiency with fast approximated spectral operations. 
GAT~\cite{DBLP:conf/iclr/VelickovicCCRLB18} introduces the attention mechanism to GNN, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. GGNN~\cite{DBLP:journals/corr/LiTBZ15} uses gated recurrent units (GRU) to accelerate the training speed and gain favorable inductive biases on large-scaled graphs.
Similar to scene graphs, \cite{DBLP:conf/cncl/WangGCL16,DBLP:conf/aaai/WangCGL18,DBLP:conf/aistats/SunL19} apply GNN-based models on knowledge graphs.
However, existing GNN-based models cannot effectively process graphs with node attributes and complicated labels. Our DE-GNN model can learn a comprehensive representation using full-scale scene graph information from objects, attributes, and relations to overcome these problems.

\section{DE-GNN Methodology}

Beforehand, we define the VQA task. 
It is a classification task that, given a text question about an image, output an answer. 
Formally, given question \emph{q} and image \emph{m}, the model aims to maximizing a conditional distribution over candidate answers \emph{a}:
\begin{equation}
    \hat{a} = \mathop{\arg\max}_{a \in A}p_\theta(a|q, m)
\end{equation}
where \emph{A} is the set of all possible answers, $p_\theta$ represents the VQA model with the trainable vector of parameters $\theta$ and $\hat{a}$ denotes the final answer.

Our proposed architecture designed for the VQA task is illustrated in Figure~\ref{fig2}. 
Our model contains a scene graph generator, a question encoder, dual graph encoders and a fusion module. For the scene graph generator, we follow the method detailed in \cite{tang2020sggcode} and other three baselines referred in this work, which we will describe in the experiment section \ref{sec:experiments}. 
For the question encoder, semantic questions are first projected into an embedding space using GLOVE pretrained word embedding model~\cite{pennington-etal-2014-glove}. 
After adding a positional encoding matrix into questions, we use long short-term memory (LSTM) networks to generate questions embedding $q \in R^{dim}$. 
We introduce our dual GGNN encoders in the following subsection.

\subsection{Object/Relation-Significant Graph}
We organize scene graphs into object-significant and relation-significant modalities. 

\medskip
\textbf{Object-Significant Graph.} We define the object-significant modality as $\emph{G}_{obj}$, every nodes represent objects in the image and every edges represent relations between two objects. Define $\emph{N}$ as the node set and $\emph{E}$ as the edge set. For $n_i, n_j \in \emph{N}$, $e_k \in \emph{E}$, < $n_i$ - $e_k$ - $n_j$ > denotes the relation tuple that represents the relation $e_k$ from object $n_i$ to object $n_j$. Noted that relation tuples are not symmetrical: if $<n_i - e_k - n_j>$ is a valid relation tuple,  $<n_j - e_k - n_i>$ may not exist. Also, $n_i$ and $n_j$ may have several relations. 

\medskip
\textbf{Relation-Significant Graph.} We define relation-significant modality as $\emph{G}_{rel}$, every nodes represent relations appear between objects in the image and every edges represent objects, which is completely opposed to the object-significant modality. For $e_i, e_j \in \emph{E}$, $n_k \in \emph{N}$, $<e_i - n_k - e_j>$ denotes the relation tuple that represents the relation $e_i$ and $e_j$ have a shared object $n_k$. Noted that relation tuples are also not symmetrical.

\medskip
\textbf{Attribute types.} Define \emph{L} as attribute types (such as material, color, etc). 
For each node $n_i\in\emph{N}$ that corresponds to an object in the image, we define a set of $\emph{L}+1$ property variables ${\left\{ n_i^j\right\}}_{l=0}^L$, where $n_i$ represents the object's name embedding and $n^l$ represents the embedding of node's $l^{th}$ attribute.

\subsection{Dual Encoders}

In our DE-GNN model, every input scene graph is transformed into an information tuple ($\emph{N}, \emph{E}, \emph{A}_{in}, \emph{A}_{out}$) where:
\begin{itemize}
    \item \emph{N} is a collection of node embeddings.
    \item \emph{E} is a collection of directed edges that specify valid relation between nodes.
    \item $\emph{A}_{in}$ is the adjacency matrix of incident edges.
    \item $\emph{A}_{out}$ is the adjacency matrix of output edges.
\end{itemize}

Let $h_i^t$ is the hidden state of node $n_i$ in GGNN at timestep $\emph{t}$, then at $\emph{t}=0$, we initialize $h_i^0$ as the GLOVE embedding of $n_i$ with appropriate zero padding:
\begin{equation}
    h_i^0 = [n_i^T, 0]^T  \, .
\end{equation}

The incident and output edges are retrieved in the respective adjacency matrices $\emph{A}_{in}$ and $\emph{A}_{out}$. 

\medskip
\subsubsection{Energy-Flow Module}
To enhance the information transfer from edges and adjacent nodes to the updating nodes, we use the Energy-Flow module (EF).
EF module comes as a replacement of the fully-connected layers from the original GGNN model. 
% In the energy-flow module, consider, for instance the tuple, $<{n}_i, {e}_k, {n}_j>$ where the edge ${e}_k$'s embedding state $e_k$ and neighbor node ${n}_j$'s hidden state $h_j$ are injected into a bidirectional GRU network as input sequence while the node ${n}_i$'s hidden state $h_i$ is injected as GRU's initial hidden state. 
Take a tuple $<{n}_i, {e}_k, {n}_j>$ as the processing sample of the energy-flow module. 
The embedding state, noted $\textit{e}_k$, of the edge ${e}_k$ and neighbor node ${n}_j$'s hidden state $\textit{h}_j$ are injected into a bidirectional GRU network as input sequence while the node ${n}_i$'s hidden state $\textit{h}_i$ is injected as the GRU's initial hidden state. 
The output of the GRU represents the updating information for hidden state $\textit{h}_i$, which corresponds to the key information from edge ${e}_k$ and node ${n}_j$ that is related to node ${n}_i$. 
The sum of every GRU output is ${n}_i$'s total information gain from ${n}_i$'s adjacent nodes and edges. 
We detail the complete energy-flow module formula as follows:
\begin{gather}\notag
    EF_i(A_{in}) = \sum\limits_{k,j}^{<n_i,e_k,n_j>\in A_{in}} \text{GRU}([\textit{e}_k, \textit{h}_j], \textit{h}_i) \, , \\ \notag
    EF_i(A_{out}) = \sum\limits_{k,j}^{<{n}_j,{e}_k,{n}_i>\in A_{out}} \text{GRU}([\textit{e}_k, \textit{h}_j], \textit{h}_i) \, ,\notag
\end{gather}
where $EF_i(A_{in})$ is ${n}_i$'s incident information gain, and $EF_i(A_{out})$ is ${n}_i$'s output information gain.


\medskip
\subsubsection{Propagation Model}

At timestep $t$, the hidden states of all nodes are updated by the following gated propagator module:
\begin{gather}
    k_{i}^t = [EF_i^t(A_{in}), EF_i^t(A_{out})] \, ,
\end{gather}
where $k_{i}^t$ represents the node ${n}_i$'s representation from all its incident edges, output edges and adjacent nodes.

% The remaining are 
Then, we adopt GRU-like updates to incorporate information from adjacent nodes and from the previous timestep leading to an update of each node's hidden state:
\begin{gather}\notag
    % c_i^t = W_i^T[h_i^{(t-1) T}, k_{i}^{(t-1) T}]^T + b\\
    c_i^t = [h_i^{(t-1)}, k_{i}^{(t-1)}] W + b \, ,\\
    z_i^t = \sigma(U^z c_i^t) \, , \\\notag
    r_i^t = \sigma(U^r c_i^t)\, , \notag
\end{gather}
where $W, U^z$ and $U^r$ are referred to as the trainable weight matrices and $b$ as a bias term.
At timestep $t$, we denote by $z_i^t$ and $r_i^t$ the update and reset gates, respectively.
\begin{gather}
    \tilde{h}_i^t = \text{tanh}(U_1 k_{i}^{(t-1)} + U_2(r_u^t \odot h_i^{(t-1)})) \, ,\\
    h_i^t = (1-z_i^t)\odot h_i^{(t-1)} + z_i^t\odot \tilde{h}_i^t \, .
\end{gather}
Here, $U_1$ and $U_2$ denote the trainable parameters of the linear layers, the operator $\odot$ is the element-wise multiplication.
% and $h_i^t$ designates the updated hidden state for node ${n}_i$. 
After \emph{T} steps, the GGNN encoder generates the final hidden state map $G$ of the graph. 
Finally, we compute the graph embedding $g_i \in G$ for node ${n}_i$ as follows:
\begin{equation}
    g_i = \sigma( \emph{f}(h_i^T, n_i)) \, ,
\end{equation}
where $\emph{f}(\cdot, n_i)$ is the multi-layer perceptron (MLP) layer which receives the concatenation of $h_i^T$ and $n_i$, then generates the final representation of node ${n}_i$. 

\subsection{Fusion Module and Answer Predictor}

Once the dual encoders embedded in our model output the node and relation features, we first fuse the attributes into feature maps. For node feature map $G^N$ and relation feature map $G^E$, the fusion feature map $F^N$ and $F^E$ are defined as

{\small \begin{equation}
F_i^N= \begin{cases}
[g_i^N, n_i^0]  \\
\cdots, \\
[g_i^N, n_i^L]
\end{cases} 
,\, F_j^E = [g_j^E, e_j] , \,
    F = [F^N, F^E]  ,
\end{equation}
}

where $F_i^N$ indicates the fusion features of node $i$ and $g_i^N$ is node $i$'s representation from the GGNN encoder. 
We denote by the vector $(n_i^0, \cdots,n_i^L)$ the embeddings attributes of node $i$. 
$F_j^E$ corresponds to the fusion feature of edge $j$. $g_j^E$ is edge $j$'s representation from the GGNN encoder. 
$e_j$ is edge $j$'s original embedding. 
The full-scale feature map, noted $F$, is the concatenation of $F^N$ and $F^E$.

Then, the question embedding $\emph{q}$ generated from the LSTM encoder and the full-scale feature map $F$ are fed into a multi-head attention layer, where the query is stored in $F$ and the key and values are stored in $\emph{q}$.
% Scores for every features are calculated. 
The reasoning vector, noted $\emph{r}$, and which stems from the graph and the question, is computed using a weighted sum of the feature map using the scores output from the attention layer; i.e.:
\begin{gather}
    r = \text{Attention}(F, q) \, .
\end{gather}

Regarding the answer predictor module, we adopt a two-layer MLP noted by $f(\cdot)$. 
This MLP can be viewed as a classifier over the set of candidate answers. 
The input of the answer predictor is the concatenation vector $(\emph{q},\emph{r})$. 
This type of classifier has been applied in many VQA models such as NSM~\cite{DBLP:conf/nips/HudsonM19} and MacNet~\cite{DBLP:conf/nips/LuYBP16}.
Formally, the output answer reads:
\begin{gather}
    \hat{a} = \mathop{\arg\max}(\text{softmax}(f((\emph{q},\emph{r}))))\, .
\end{gather}



\section{Numerical Experiments}\label{sec:experiments}

\begin{table*}[ht] 
\centering
    \begin{tabular}{l|lllllllll}
    \hline
    \textbf{Question type}&\textbf{What}&\textbf{Color}&\textbf{Where}&\textbf{How}&\textbf{Who}&\textbf{When}&\textbf{Why}&\textbf{Overall}\\
    \hline
     Percentage &(54\%) &(14\%) &(17\%) &(3\%) &(5\%) &(4\%) &(3\%) &(100\%)\\
    \hline
    \multicolumn{9}{c}{\bf VG-GT} \cr\hline %& & & &\makecell[c]{\textbf{VG-GT}}& & & &\\
    % \hline
     NSM~\cite{DBLP:conf/nips/HudsonM19} &33.1 &52.4 &51.0 &52.9 &49.8 &77.9 &12.3 &45.1\\
     MLP~\cite{DBLP:conf/eccv/JabriJM16} &- &- &- &- &- &- &- &58.5\\
     F-GN~\cite{DBLP:conf/bmvc/ZhangCX19}&60.9 &53.6 &62.0 &46.2 &63.3 &\textbf{83.7} &50.9 &60.1\\
     U-GN~\cite{DBLP:conf/bmvc/ZhangCX19}&61.6 &54.0 &62.4 &45.9 &63.9 & 83.2 &50.3 &60.5\\
     SAN~\cite{DBLP:conf/cvpr/YangHGDS16} &- &- &- &- &- &- &- &62.6\\
     FSTT~\cite{inproceedings} &65.5 &45.6 &70.1 &47.8 &68.3 &82.1 &91.5 &65.6\\
     ReGAT~\cite{DBLP:conf/iccv/LiGCL19} &72.1 &\textbf{70.8} &64.4 &\textbf{68.9} &72.7 &65.0 &92.3 &71.2\\
     DEGNN (ours) &\textbf{75.9} &64.9 &\textbf{73.1} &66.8 &\textbf{82.6} &81.4 &\textbf{98.8} &\textbf{75.4}\\
    \hline
     \multicolumn{9}{c}{\bf Motif} \cr\hline
    % \hline
     NSM~\cite{DBLP:conf/nips/HudsonM19} &31.8 &62.4 &53.1 &51.4 &47.6 &83.3 &10.9 &43.1\\
     FSTT~\cite{inproceedings} &48.8 &40.4 &49.2 &40.1 &40.6 &54.5 &70.3 &48.1\\
     F-GN~\cite{DBLP:conf/bmvc/ZhangCX19} &58.7 &60.8 &60.4 &47.2 &61.8 &84.8 &49.0 &60.0\\
     U-GNN~\cite{DBLP:conf/bmvc/ZhangCX19} &59.4 &58.2 &60.3 &54.3 &66.6 &\textbf{85.3} &48.1 &60.5\\
     ReGATT~\cite{DBLP:conf/iccv/LiGCL19} &75.4 &\textbf{69.2} &57.6 &\textbf{69.9} &69.1 &57.4 &91.8 &69.9\\
     DEGNN (ours) &\textbf{79.4} &67.6 &\textbf{62.7} &65.3 &\textbf{72.8} &63.0 &\textbf{96.1} &\textbf{72.9}\\
    \hline
    \end{tabular}
\caption{\label{VG-detail}
Visual QA accuracy(\%) on different question types. VG-GT: Visual Genome with ground truth scene graphs. Motif: Visual Genome with motif scene graphs.
}
\end{table*}


\subsection{Datasets}

\quad The \textbf{Visual Genome} dataset contains $108 \, 077$ images with comprehensively annotated objects, attributes, and relations. To enrich the scene graph annotation in Visual Genome, we use a scene graph generation method, motif~\cite{DBLP:conf/cvpr/ZellersYTC18} to generate a new scene graph dataset. 
Compared with the Visual Genome dataset, the new one has the same images and questions-answers tuples, but has scene graph annotations with different qualities and biases. 
We split both datasets into train, valid, and test sets using a $7:1:2$ ratio. 

\quad The \textbf{GQA} dataset~\cite{DBLP:conf/cvpr/HudsonM19} focuses on real-world reasoning, scene understanding and compositional question answering. It consists of 113k images and 22M questions of assorted types and varying compositionality degrees, measuring performance on an array of reasoning skills such as object and attribute recognition, transitive relation tracking, spatial reasoning, logical inference and comparisons.



\subsection{Implementation Details}

We use 50-dimensional GLOVE word embeddings model ~\cite{pennington-etal-2014-glove} to embed words in the scene graph and questions. In order to record the questions' position information, we set up the positional encoding matrix $\textrm{PE}$:
\begin{gather}\notag
    \textrm{PE}_{\texttt{pos}=2\emph{i}} = \sin(\texttt{pos}/10000^{2i/d_{\textrm{m}}}) \, ,\\
    \textrm{PE}_{\texttt{pos}=2\emph{i}+1} = \cos(\texttt{pos}/10000^{2i/d_{\textrm{m}}}) \, , \notag
\end{gather}
where $\texttt{pos}$ is the position of the word in the question sequence. 
If $\texttt{pos}$ is odd, the position information is generated by a $\sin$ function, else, it is generated by a $\cos$ function. 
We also let model dimension equal to $d_{\textrm{m}}=50$.
After adding position information, the question embeddings are injected into a single-directional GRU network. 
The dimension of the hidden layers of the GRU is 100, and the dropout rate is 0.2.

In our energy-flow enhanced GGNN encoder, the propagator time step is 5, and we use a bidirectional GRU as our energy-flow module. 
Here, we set the dimension of the single GRU hidden layer to 50. 

In the fusion module, we apply a multi-head attention layer with 5 heads and no dropout. 
Regarding the answer predictor, we select the top-2000 answer candidates and use a 2-layer MLP as the output classifier.

We use Adam~\cite{kingma2014adam} as the optimizer, and Cross Entropy Loss as the loss function during the training of our model. 
For motif dataset, we set the batch size to 512. For Visual Genome ground truth dataset and GQA dataset, we set the batch size to 16 due to their abundant scene graph annotations.

The learning rate is decaying depending on the epoch number. We initialize the learning rate to be $1e^{-3}$, and when 30\% epochs finish, the learning rate drops to $2e^{-4}$. When 60\% epochs finish, the learning rate drops to $4e^{-5}$ and it becomes $8e^{-6}$ after 80\% epochs finish. We train our model and other baselines on a single V100 GPU.

% \begin{figure}[h] 
% \mbox{ \hspace{-0.12in}
%     \includegraphics[width=1.6in]{fig/fig5-left.eps} \hspace{-0.12in}
%     \includegraphics[width=1.6in]{fig/fig5-right.eps} 
% }
%     \caption{The validation accuracy curves for ablation experiments} 
%     \label{ablation-figure} 
% \end{figure}

\begin{table*}[htbp]
    \begin{floatrow}
    \capbtabbox{
     \begin{tabular}{lll}
      \hline
      \textbf{Models}&\textbf{Acc.}\\
      \hline
      \textbf{Base} & 35.4\% \\
      \hline
       +\emph{EF} & -\% \\
       +\emph{Oj} & -\% \\
       +\emph{Oj}+\emph{EF}& 39.3\% \\
       +\emph{Re} & -\% \\
       +\emph{Re}+\emph{EF}& 38.8\% \\
       +\emph{Oj}+\emph{Re} &-\%\\
      \hline
       + (w/o \emph{QF}) & 54.9\% \\
       + (w/o \emph{attr}) & 71.6\% \\
      \hline
       DEGNN(ours) & 75.2\%\\
      \hline
    \end{tabular}
    }{
     \caption{Ablation study}
     \label{model-without-fusion}
    }
    \capbtabbox{
     %\rowcolors{1}{blue!10}{white}
     \begin{tabular}{llllll}
       \hline
       \textbf{Models}&\textbf{Binary$\uparrow$}&\textbf{Open$\uparrow$}&\textbf{Validity$\uparrow$}&\textbf{Distribution$\downarrow$}&\textbf{Accuracy$\uparrow$}\\
    \hline
     Human &91.20 &87.40 &98.90 &- &89.30\\
     BottomUp &66.64 &34.83 &96.18 &5.98 &49.74\\
     MAC &71.23 &38.91 &96.16 &5.34 &54.06\\
     SK T-Brain &77.42 &43.10 &96.26 &7.54 &59.19\\
     PVR &77.69 &43.01 &96.45 &5.80 &59.27\\
     GRN &77.53 &43.35 &96.18 &6.06 &59.37\\
     Dream &77.84 &43.72 &96.38 &8.40 &59.72\\
     LXRT &77.76 &44.97 &96.30 &8.31 &60.34\\
     NSM &78.94 &49.25 &\textbf{96.41} &\textbf{3.71} &63.17\\
     ReGAT &\textbf{83.57} &62.58 &92.70 &9.32 &70.50\\
     DEGNN(ours) &69.79 &\textbf{72.21} &93.80 &3.78 &\textbf{71.21}\\
    \hline
    \hiderowcolors
    \end{tabular}
    }{
     \caption{Performance on the GQA dataset.}
     \label{GQA}
    }
    \end{floatrow}
\end{table*}

\subsection{Empirical Results}
In this subsection, we provide the experimental results on various datasets mentioned above. 
The different baselines compared in our experiments all use various methods to generate the scene graphs for images. 
In order to ensure general fairness across the methods, we implement them from scratch, removing their scene graph generation parts to eliminate the interference of different generation methods.

Table~\ref{VG-detail} reports the results on the test sets of the VG ground truth datasets and the motif dataset generated from the VG dataset. Compared to the baseline models, we can observe that our DE-GNN model outperforms the others at $3\%$-$4\%$. 

In addition, we provide detailed results on the VG dataset with different question types. Compared to the other scene graph based VQA models, our model perform well in "what", "where", "who" and "why" type. Specially, our model have $6\%$ accuracy improvement in "why" type questions, which highly require VQA models' reasoning ability.

% In addition, the stability of our model is conspicuous. Our model can achieve consistent and stable performance under different scene graph qualities. Our model's performances under four different scene graph datasets only suffer 3\% fluctuation,
% while FSTT suffers from a 4.7\% fluctuation and Re-GAT suffers from a 26.9\% fluctuation. 
% Figure~\ref{four-graph} is the intuitive validation curves on four datasets. Our model in the blue line can steadily converge at nearly 15 epochs under four different datasets with various scene graph qualities. 

\begin{table}
    \begin{tabular}{lllll}
    \hline
     \textbf{Models}& NSM & FSTT & ReGAT & DE-GNN\\
    \hline
     Relation & 2656 & 3638 & 3531 & 2605\\
     Object & 4177 &2588 & 2089 & 1113\\
     Attribute & 8410 & 7687 & 3531 & 2605\\
    \hline
    \end{tabular}
\caption{\label{badcase}
Badcase Analysis on Motif dataset. 
}
\end{table} 

We report on Table~\ref{GQA} the detailed results on the test sets of the GQA dataset. Compared to the baseline models, our DE-GNN model achieves state-of-the-art accuracy performance. 

We also evaluate our model and other baselines across GQA dataset's various metrics, where "Binary" represents binary-answer questions, "Open" represents open domain questions and "Distribution" represents the distance between prediction distribution and standard answer distribution. In open domain questions which are difficult for reasoning, our model outperforms the others at $10\%$. In distribution metric, our model alse achieves 2nd lowest score compared to other baselines.

To demonstrate that our dual encoders structure can intensify the model's perception of relation features and learn a comprehensive representation from nodes, attributes, and relations information, we make badcase analysis for baselines and our model on Motif dataset.

To classify the badcase answer category, we generate three dictionaries to record objects, attributes and relations that appear in scene graphs.

For each answer, we first extract potential relations and match them up with the relation dictionary. Then we split the objects and attributes in the answer and search objects and attributes dictionaries. As for some evasive answers containing both objects and its' attributes, we add them in both object and attribute categories. 

We present Table~\ref{badcase} the results for our badcase analysis. Our DE-GNN model surpasses all baselines in the terms of objects detection. Also, our model reduces nearly half of the wrong answers in FSTT, Re-GAT, and NSM in the attribute aspect. Finally, our model does well in relation retrieval, outperforming GNN based FSTT and Re-GAT.
% \begin{figure}[H]
%     % \vspace{-0.75in}
%     \includegraphics[width=1.0\textwidth]{./pic/badcase3.pdf} 
%     % \vspace{-0.1in}
%     \caption{badcase analysis on Motif dataset.} 
%     \label{badcase} 
%     % \vspace{-0.2in}
% \end{figure}

The question fusion module, which concatenates the question vector with the reasoning vector before entering into the answer predictor module, is a common module in VQA models, including NSM, FSTT, and our DE-GNN model. This method has been shown to improve the accuracy of VQA models. However, from a cognitive point of view, the question fusion module lacks of interpretability since the question features are included in the reasoning vector of the model. 
Also, the addition of the question fusion module may lead the reasoning model to only \emph{guess answers} from questions, which negatively influences the reasoning itself. 
We retrain our model and other baselines without the question fusion module to evaluate the reasoning ability without the influence of outer question information. 
Table~\ref{model-without-fusion} shows the results of models without question fusion module. 
Note that there is no question fusion module in the Re-GAT baseline, so the Re-GAT result is the same as Table~\ref{citation-guide}.
After reducing the concatenation of the question vector and reasoning vector, FSTT, NSM, and our DE-GNN model suffer accuracy recessions. Without question fusion, our model still outperforms other baselines.

\subsection{Ablation Study}

We compare three ablated forms of DE-GNN with our complete one. 
The accuracy results are reported in Table~\ref{ablation-accuracy} and are obtained using the Motif dataset. 
The Object-EF model corresponds to the object encoder part of our DE-GNN model, which contains one energy-flow enhanced GGNN network to encode \emph{object-significant} graphs. 
On the other hand, the relation-EF model is the other half of our DE-GNN model, which contains one energy-flow enhanced GGNN network to encode \emph{relation-significant} graphs. 
We use the original GGNN network as the baseline model to encode object-significant graphs.

First, we validate the effectiveness of applying dual structure to balance the importance of relations and objects by splitting our DE-GNN into an object-single model and a relation-single model. Table~\ref{ablation-accuracy} shows that both object-EF model and relation-EF model perform poorly, at about 39\%. 
It also shows that both relations and objects are vital to VQA performance. Lack of any of them leads to severe accuracy recession. Combining the object-single model and the relation-single model leads to an empirical gain of approximately 35\% accuracy upward, which shows that the dual structure is significant in balancing relation and object information. 

Then, we validate the effectiveness of applying energy-flow structure to learn a more comprehensive representation for scene graphs than the original GGNN structure, which represents the baseline in Table~\ref{ablation-accuracy}. We compare the object-EF model and the baseline model, which both learn representations from object-significant graphs, and note that after adding the energy-flow structure, there is an accuracy improvement of around 3\%. 
This latter point shows that energy-flow structure can successfully improve the representation quality of scene graphs. 

% To better understand how our dual energy-flow enhanced GNN model inferences and answers questions, we further visualize and compare the attention maps learned by the ablated models in the following subsection.

\subsection{Visualization}
To better illustrate the effectiveness of the dual encoder structure and the energy-flow module in our DE-GNN model, we compare the attention scores learned by DE-GNN model with those learned by our baseline, object-EF, and relation-EF models. 
Figure~\ref{visual} exhibits the detail of the visualization results. 
The top row corresponds to three typical input images along with their associated questions. 

Comparing the first graph in row 2 with row 3 shows that energy-flow enhanced GGNN encoder can more correctly focus on crucial objects than the original GGNN encoder. 
For the objects mentioned in the questions, the attention scores of "bus" increase by 0.29 with the addition of the energy-flow module.
As for objects unrelated to questions, the attention scores of "car" and "tree" decrease by 0.12 and 0.08. 
The original GGNN encoder can not fuse object and attribute information, which leads to the wrong answer. Our object-EF model learns a jointly representation from objects and attributes. 
The top-3 candidate attributes for the question are "red", "steel", and "driving" while the object-EF model correctly answers "red".

Comparing the first and third graphs in row 3 with row 4 shows that both object-EF and relation-EF can capture related information, but these models have a significant bias on objects and relations. Relation-EF shows the acute perception on relations, but the model can not capture object information or score objects correctly, which leads to relation-EF's failure on the first graph. The object-EF model can not capture "on" relation in graph 3, while relation-EF model can easily capture the correct answer.

\begin{figure}[h] 
    \flushleft
    % \vspace{-0.1in}
    \centering 
    \includegraphics[scale=0.32]{./pic/visual2.pdf} 
    \caption{Visualization of attention scores learned by baseline, object-EF, relation-EF, and DE-GNN.} 
    \label{visual}
    % \vspace{-0.2in}
\end{figure}

Our DE-GNN model, presented row 5 of Figure~\ref{visual}, does not only balance the importance of objects and relations, but also learn jointly representations from objects, attributes, and relations using the energy-flow module. Comparing the second graph in row 3 and row 5, our model correctly capture "horse" object and "behind" relation. This leads to the correct attention score of "old man", which is 0.61 higher than the score from object-EF. 
Note from Figure~\ref{visual} that our model answers all three questions correctly unlike baselines.


\section{Conclusion}
In this work, we study classical scene graph reasoning methods such as GNN-based models and Neural State Machine for Visual Question Answering tasks. 
We observe that existing methods fail to \emph{jointly} exploit nodes, relations, and attributes information in scene graphs. 
To address this problem, we propose the Dual Energy-Flow Enhanced Graph Neural Network (DE-GNN), which encodes each scene graph into feature representations via an object encoder and a relation encoder generating full-scale feature maps using nodes, attributes, and relations information. 
We demonstrate the effectiveness of our method on the various datasets achieving significant improvement and state-of-the-art performances.
% We have presented a novel Dual Energy-Flow Enhanced Graph Neural Network (DE-GNN), which encodes each scene graph into feature representations via two energy-flow enhanced GNN structures. Two types of SG structures are injected into the encoder: (i) object-significant graphs with object nodes and relation edges, which absorb attribute and relation information into node representations. (ii) relation-significant graphs with relation nodes and object edges, which intensifies the model's perception of relation features.
% We conduct extensive experiments on the VG dataset and achieve new state-of-the-art performances. 

In future work, we will test our DE-GNN model on more VQA datasets, such as VQA-CP2. 
We hope that our work can further enhance the effect of scene graph for reasoning modeling.


\newpage

% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper
% \nobibliography{custom}

\clearpage
\bibliography{custom}

\end{document}
