\documentclass[11pt]{article}
%\usepackage{fullpage,graphicx,algorithm,algorithmic,bm,amsmath,amsthm,amssymb,color,hyperref,cite,natbib}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers}{natbib}
\usepackage{natbib}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{wrapfig,lipsum}
\usepackage[textwidth=1cm,textsize=footnotesize]{todonotes}

% ready for submission
\usepackage{neurips_2020}

\usepackage[colorlinks=true,
linkcolor=red,
urlcolor=blue,
citecolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{cleveref}

\setlength{\parskip}{.2cm}

\newtheorem{Fact}{Fact}
\newtheorem{Lemma}{Lemma}
\newtheorem{Prop}{Proposition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Conjecture}{Conjecture}
\newtheorem{Property}{Property}
\newtheorem{Observation}{Observation}
%\theorembodyfont{\rmfamily}
\newtheorem{Exa}{Example}
\newtheorem{assumption}{H\!\!}
\newtheorem{assumptionA}{S\!\!}
\newtheorem{assumptionL}{L\!\!}
\newtheorem{Remark}{Remark}
\newtheorem*{Lemma*}{Lemma}
\newtheorem*{Theorem*}{Theorem}
 \makeatletter
\renewenvironment{proof}[1][\proofname]{%
   \par\pushQED{\qed}\normalfont%
   \topsep6\p@\@plus6\p@\relax
   \trivlist\item[\hskip\labelsep\bfseries#1]%
   \ignorespaces
}{%
   \popQED\endtrivlist\@endpefalse
}
\makeatother

%%%%%%%%%%% Stuffs for Tikz %%%%%%%%%%%%%%%%%%
\usepackage{pgfplots}
\usepackage{xargs}
\usepackage{stmaryrd}
\usetikzlibrary{arrows,shapes,calc,tikzmark,backgrounds,matrix,decorations.markings}
\usepgfplotslibrary{fillbetween}

\pgfplotsset{compat=1.3}

\usepackage{relsize}
\tikzset{fontscale/.style = {font=\relsize{#1}}
    }

\definecolor{lavander}{cmyk}{0,0.48,0,0}
\definecolor{violet}{cmyk}{0.79,0.88,0,0}
\definecolor{burntorange}{cmyk}{0,0.52,1,0}

\def\lav{lavander!90}
\def\oran{orange!30}

\definecolor{asuorange}{rgb}{1,0.699,0.0625}
\definecolor{asured}{rgb}{0.598,0,0.199}
\definecolor{asuborder}{rgb}{0.953,0.484,0}
\definecolor{asugrey}{rgb}{0.309,0.332,0.340}
\definecolor{asublue}{rgb}{0,0.555,0.836}
\definecolor{asugold}{rgb}{1,0.777,0.008}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{shortcuts_OPT}

%\renewcommand{\textwidth}{5.5in}

% Here's the definition of Sb, stolen from amstex
    \makeatletter
    \def\multilimits@{\bgroup
  \Let@
  \restore@math@cr
  \default@tag
 \baselineskip\fontdimen10 \scriptfont\tw@
 \advance\baselineskip\fontdimen12 \scriptfont\tw@
 \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
 \lineskiplimit\lineskip
 \vbox\bgroup\ialign\bgroup\hfil$\m@th\scriptstyle{##}$\hfil\crcr}
    \def\Sb{_\multilimits@}
    \def\endSb{\crcr\egroup\egroup\egroup}
\makeatother

\newtheoremstyle{t}         %name
    {\baselineskip}{2\topsep}      %space above and below
    {\rm}                   %Body font
    {0pt}{\bfseries}  %Heading indent and font
    {}                      %after heading
    { }                      %head after space
    {\thmname{#1}\thmnumber{#2}.}

\theoremstyle{t}
\newtheorem{q}{Q}
\parindent=0pt

%\newcommand{\eric}[1]{\todo[color=red!20]{{\bf EM:} #1}}
%\newcommand{\erici}[1]{\todo[color=red!20,inline]{{\bf EM:} #1}}
%\newcommand{\belhal}[1]{\todo[color=green!20]{{\bf BK:} #1}}
%\newcommand{\belhali}[1]{\todo[color=green!20,inline]{{\bf BK:} #1}}
%\newcommand{\toco}[1]{\todo[color=yellow!20]{{\bf To:} #1}}



\makeatletter
\DeclareRobustCommand*\cal{\@fontswitch\relax\mathcal}
\makeatother

\begin{document}
\title{Distributed and Private Stochastic EM Methods\\
via Quantized and Compressed MCMC}
%\author{}
\date{\today}

\maketitle

\begin{abstract}
To be completed
\end{abstract}


\section{Introduction}

We consider the distributed minimization of the following negated log incomplete data likelihood 
\begin{align} \label{eq:em_motivate}
\begin{split} 
 \min_{ \theta \in \Theta }~ \overline{L} ( \theta ) \eqdef  L ( \theta ) + r (\theta) \quad \text{with}~~L(\theta) = \frac{1}{n} \sum_{i=1}^n L_i( \theta) \eqdef  \frac{1}{n} \sum_{i=1}^n \big\{ - \log g( y_i ; \theta ) \big\}\eqs,
\end{split} 
\end{align}
where $n$ denotes the number of workers, $\{y_i\}_{i=1}^n$ are observations, $\Param \subset \rset^d$ is the parameters set and $\Pen : \Param \rightarrow \rset$ is a smooth regularizer.

The objective  $L( \theta )$ is possibly {nonconvex} and is assumed to be lower bounded. 
In the latent data model, the likelihood $g(y_i ; \param)$, is the marginal distribution of the complete~data likelihood, noted $f(z_i,y_i; \param)$, such that 
\begin{align}
g(y_i; \param) = \int_{\Zset} f (z_i,y_i;\param) \mu(\rmd z_i),    
\end{align}
where $\{ z_i \}_{i=1}^n$ are the vectors of latent variables associated to the observations $\{y_i\}_{i=1}^n$.

We also consider a special case of that problem since the complete likelihood pertains to the curved exponential family:
\beq \label{eq:exp}
f(z_i,y_i; \theta) = h  (z_i,y_i) \textrm{exp} ( \pscal{S(z_i,y_i)}{\phi(\theta)} - \psi(\theta) )\eqs,
\eeq
where $\psi(\theta)$, $h(z_i,y_i)$ are scalar functions, $\phi(theta) \in \rset^k$ is a vector function, and $\{S(z_i,y_i) \in \rset^k\}_{i=1}^n$ is the vector of sufficient statistics.
We refer the readers to \citep{efron1975defining} for details on this subclass of problems which is of high interest given the broad range of problems that fall under this assumption.



%\subsection{Periodic averaging of the local models}
%
%\begin{algorithm}[H]
%\caption{FL-SAEM with parameter averaging} \label{alg:flsaem}
%\begin{algorithmic}[1]
%%\small
%\STATE \textbf{Input}: .
%\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
%\FOR{$r=1$ to $R$}
%\FOR{parallel for device $i \in D^{r}$}
%\STATE Set $\hat{\theta}^{(0,k)}_i = \hat{\theta}^{(k)}$.
%\FOR{$t=1$ to $T$}
%\STATE Draw M samples $\{z_{i,m}^{(t,k)}\}_{m=1}^{M}$ under model $\hat{\theta}^{(t,k)}_i$
%\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(t,k+1)}$
%\STATE Update local model:
%$$
%\hat{\theta}^{(t,k+1)}_i = \overline{\theta}( \tilde{S}_i^{(t,k+1)}) 
%$$
%\ENDFOR
%\STATE Devices send $\hat{\theta}^{(T,k+1)}_i$ to server.
%\ENDFOR
%\STATE Server computes \textbf{the average of the local models}:
%$$
%\hat{\theta}^{(k+1)} = \frac{1}{n} \sum_{i=1}^n \hat{\theta}^{(T,k+1)}_i
%$$ 
%and send global model back to the devices. \label{line:final}
%\ENDFOR
%\end{algorithmic}
%\end{algorithm}
%
%
%\subsection{Periodic averaging of the local statistics}
%
%\begin{algorithm}[H]
%\caption{FL-SAEM with statistics averaging} \label{alg:flsaem2}
%\begin{algorithmic}[1]
%%\small
%\STATE \textbf{Input}: .
%\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
%\FOR{$r=1$ to $R$}
%\FOR{parallel for device $i \in D^{r}$}
%\STATE Set $\hat{\theta}^{(0,k)}_i = \hat{\theta}^{(k)}$.
%\FOR{$t=1$ to $T$}
%\STATE \textcolor{red}{ Here one local iteration, $T=1$}
%\STATE Draw M samples $z_{i,m}^{(k)}$ under model $\hat{\theta}^{(t,k)}_i$
%\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(t,k+1)}$
%\ENDFOR
%\STATE Devices send local statistics $\tilde{S}_{i}^{(t,k+1)}$ to server.
%\ENDFOR
%\STATE Server computes \textbf{global model using the aggregated statistics}:
%$$
%\hat{\theta}^{(k+1)} = \overline{\theta}( \tilde{S}^{(t,k+1)}) 
%$$
%where $\tilde{S}^{(t,k+1)} = (\tilde{S}_i^{(t,k+1)}, i \in D_r)$  and send global model back to the devices. 
%\ENDFOR
%\end{algorithmic}
%\end{algorithm}


\clearpage
\section{Algorithms}

For computational purposes and privacy enhanced matter, I have chosen to study and develop the second algorithms that I proposed in my last week's report.
In that algorithm, one does not compute a periodic averaging of the local models (this would requires performing as many M-steps as there are workers).
Rather, workers compute local statistics and send them to the central server for a periodic averaging of those vectors and the latter computes one M-step to update the global model.

\begin{algorithm}[H]
\caption{FL-SAEM with Periodic Statistics Averaging} \label{alg:flsaemreg}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: \textcolor{red}{TO COMPLETE}
\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
\FOR{$r=1$ to $R$}
\FOR{parallel for device $i \in D^{r}$}
\STATE Set $\hat{\theta}^{(0,r)}_i = \hat{\theta}^{(r)}$.
\STATE Draw M samples $z_{i,m}^{(r)}$ under model $\hat{\theta}^{(r)}_i$ \label{line:samplingreg}
\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(r+1)}$ \label{line:computereg}
\STATE Workers send local statistics $\tilde{S}_{i}^{(k+1)}$ to server.
\ENDFOR
\STATE Server computes \textbf{global model using the aggregated statistics}:
$$
\hat{\theta}^{(r+1)} = \overline{\theta}( \tilde{S}^{(r+1)}) 
$$
where $\tilde{S}^{(r+1)} = (\tilde{S}_i^{(r+1)}, i \in D_r)$  and send global model back to the devices. 
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Challenges with Algorithm~\ref{alg:flsaemreg}}
While Algorithm~\ref{alg:flsaemreg} is a distributed variant of the SAEM, it is neither (a) \emph private nor (b) \emph communication-efficient.

\textbf{Privacy:} Indeed, we remark that broadcasting the vector of statistics are a potential breach to the data observations as their expression is related $y$ and the latent data $z$. With a simple knowledge of the model used, the data could be retrieved if one extracts those statistics.

\textbf{Communication bottlenecks:} Also regarding (b), the broadcast of $n$ vector of statistics $S(y_i,z_i)$ can be cumbersome when the size of the latent space and the parameter space of the model are huge.

\subsection{Algorithmic solutions}

\textbf{Line~\ref{line:samplingreg} -- Quantization:} 
The first step is to quantize the gradient in the Stochastic Langevin Dynamics step used in our sampling scheme Line~\ref{line:samplingreg} of Algorithm~\ref{alg:flsaemreg}.
Inspired by \citep{alistarh2017qsgd}, we use an extension of the QSGD algorithm for our latent samples.
Define the quantization operator as follows:

\beq\label{eq:operator}
\mathsf{C}_{j}^{(\ell)}\left(g, \xi_{j}\right)=\|v\| \cdot \textrm{sign}\left(g_{j}\right) \cdot\left(\left\lfloor \ell \left|g_{j}\right| /\|v\|\right\rfloor+\mathbf{1}\left\{\xi_{j} \leq \ell \left|g_{j}\right| /\|v\|-\left\lfloor \ell \left|g_{j}\right| /\|v\|\right\rfloor\right\}\right) /\ell
\eeq
where $\ell$ is the level of quantization and $j \in [d]$ denotes the dimension of the gradient.

Hence, for the sampling step, Line~\ref{line:samplingreg}, we use the modified SGLD below, to be compliant with the privacy of our method.
\begin{algorithm}[H]
\caption{Langevin Dynamics with Quantization for worker $i$} \label{alg:quant}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: Current local model $\hat{\theta}^{(r)}_i$ for worker $i \in \inter$.

\STATE Draw $M$ samples $\{ z_{i}^{(r,m} \}_{m=1}^M$ from the posterior distribution $p(z_i| y_i; \hat{\theta}^{(k)}_i)$ via Langevin diffusion with a quantized gradient:\label{line:langevin}
\FOR{$k=1$ to $K$}
\STATE Compute the quantized gradient of $\nabla \log p(z_i| y_i; \hat{\theta}^{(k)}_i)$:
\beq\label{eq:grad}
g_i{(k,m)} = \mathsf{C}_{j}^{(\ell)}\left(\nabla_j f_{\theta_t}(z_i^{(k-1,m)}), \xi^{(k)}_{j}\right)
\eeq
where $\xi^{(k)}_{j}$ is a realization of a uniform random variable.
\STATE Sample the latent data using the following chain:
\beq\label{eq:lang}
z_i^{(k,m)} = z_i^{(k-1,m)} + \frac{\gamma_k}{2}  g_i{(k,m)} + \sqrt{\gamma_k}  \mathsf{B}_k \eqsp,
\eeq
where $\mathsf{B}_t$ denotes the Brownian motion and $m \in [M]$ denotes the MC sample.
\ENDFOR
\STATE Assign $\{ z_{i}^{(r,m} \}_{m=1}^M \leftarrow \{ z_i^{(K,m)} \}_{m=1}^M$.
\STATE \textbf{Output:} latent data $z_{i,m}^{(k)}$ under model $\hat{\theta}^{(t,k)}_i$ 
\end{algorithmic}
\end{algorithm}



\noindent \textbf{Line~\ref{line:computereg} -- Compression MCMC output:}
We use the notorious \textbf{Top-$k$} operator that we define as $\mathcal C(x)_i=x_i$, if $i\in \mathcal S$; $\mathcal C(x)_i=0$ otherwise and where $\mathcal S$ is defined as the size-$k$ set of $i\in[p]$.
Recall that after Line~\ref{line:samplingreg} we compute the local statistics $\tilde{S}_{i}^{(k+1)}$ using the output latent variables from Algorithm~\ref{alg:quant}.
We now use those statistics and compress them using Algorithm~\ref{alg:spars} as follows:

\begin{algorithm}[H]
\caption{Sparsified Statistics with \textbf{Top-$k$}} \label{alg:spars}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: Current local statistics $\tilde{S}_{i}^{(k+1)}$ for worker $i \in \inter$. Sparsification level $k$.
\STATE Apply \textbf{Top-$k$}:
\beq\label{eq:topkstats}
\ddot{S}_{i}^{(k+1)} = \mathcal C \left( \tilde{S}_{i}^{(k+1)}\right)
\eeq
\STATE \textbf{Output:} Compressed local statistics for worker $i$ denoted $\ddot{S}_{i}^{(k+1)}$.
\end{algorithmic}
\end{algorithm}


\section{FL and Distributed Algorithmic Contributions}
Final method:

\begin{algorithm}[H]
\caption{Quantized and Compressed FL-SAEM with Periodic Statistics Averaging} \label{alg:flsaem2}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Compression operator $\mathcal C(\cdot)$, number of rounds $R$, initial parameter $\theta_{0}$.
	\FOR{$r=1$ to $R$}%
		\FOR{parallel for device $i \in D^{r}$}
		\STATE Set $\hat{\theta}^{(0,r)}_i = \hat{\theta}^{(r)}$. \algorithmiccomment{\textcolor{blue}{Initialize each worker with current global model}}
		\STATE Draw M samples $z_{i,m}^{(r)}$ under model $\hat{\theta}^{(r)}_i$ via Quantized LD: \algorithmiccomment{\textcolor{blue}{Local Quantized MCMC step}}
			\FOR{$k=1$ to $K$}
			\STATE Compute the quantized gradient of $\nabla \log p(z_i| y_i; \hat{\theta}^{(k)}_i)$: 
			$$g_i{(k,m)} = \mathsf{C}_{j}^{(\ell)}\left(\nabla_j f_{\theta_t}(z_i^{(k-1,m)}), \xi^{(k)}_{j}\right) \quad \textrm{where} \quad \xi^{(k)}_{j} \sim \mathcal{U}_{[a,b]} $$
			\STATE Sample the latent data using the following chain:
			\begin{equation}\notag
			z_i^{(k,m)} = z_i^{(k-1,m)} + \frac{\gamma_k}{2}  g_i{(k,m)} + \sqrt{\gamma_k}  \mathsf{B}_k ,
			\end{equation}
			\qquad\qquad\quad  where $\mathsf{B}_t$ denotes the Brownian motion and $m \in [M]$ denotes the MC sample.
			\ENDFOR
		\STATE Assign $\{ z_{i}^{(r,m)} \}_{m=1}^M \leftarrow \{ z_i^{(K,m)} \}_{m=1}^M$.
		\STATE Compute $\tilde{S}_{i}^{(r+1)}$ and its \textbf{Top-$k$} variant $\ddot{S}_{i}^{(r+1)} = \mathcal C \left( \tilde{S}_{i}^{(r+1)}\right)$. \label{line:compute} \algorithmiccomment{\textcolor{blue}{Compressed local statistics}}
		\STATE Worker send local statistics $\tilde{S}_{i}^{(r+1)}$ to server. \algorithmiccomment{\textcolor{blue}{Single round of communication}}
          \ENDFOR
          \STATE Server computes \textbf{global model}: \algorithmiccomment{\textcolor{blue}{(Global) M-Step using aggregated statistics}}
$$
\hat{\theta}^{(r+1)} = \overline{\theta}( \ddot{S}^{(r+1)}) 
$$
\qquad where $\ddot{S}^{(r+1)} = (\ddot{S}_i^{(r+1)}, i \in D_r)$  and send global model back to the devices. 

    \ENDFOR
  \end{algorithmic}
\end{algorithm}







We can also consider the plain distributed version of the sEM which does not tackle any privacy or communication bottlenecks.
It goes as follows:

\begin{algorithm}[H]
\caption{Distributed SAEM with Periodic Locals Models Averaging} \label{alg:distsaem}
\begin{algorithmic}[1]
\STATE \textbf{Input}: Compression operator $\mathcal C(\cdot)$, number of rounds $R$, initial parameter $\theta_{0}$.
	\FOR{$r=1$ to $R$}%
		\FOR{parallel for device $i \in D^{r}$}
		\STATE Set $\hat{\theta}^{(r)}_i = \hat{\theta}^{(r)}$. \algorithmiccomment{\textcolor{blue}{Initialize each worker with current global model}}
		\STATE Draw M samples $z_{i,m}^{(r+1)}$ under model $\hat{\theta}^{(r)}_i$ via MCMC: \algorithmiccomment{\textcolor{blue}{Local MCMC step}}
		\STATE Compute the local statistics $\tilde{S}_{i}^{(r+1)} = S(z_{i,m}^{(r+1)})$. \label{line:computedist} \algorithmiccomment{\textcolor{blue}{Local statistics}}
		\STATE Worker computes \textbf{local model}: \algorithmiccomment{\textcolor{blue}{(Local) M-Step using local statistics}}
		$$
		\hat{\theta}^{(r+1)}_i = \overline{\theta}( \tilde{S}_{i}^{(r+1)}) 
		$$
		\STATE Worker sends local model $\hat{\theta}^{(r+1)}_i$ to server.
          \ENDFOR
          \STATE Server computes \textbf{global model} by periodic averaging \algorithmiccomment{\textcolor{blue}{Local model averaging}}
          $$
	\hat{\theta}^{(r+1)} \eqdef \frac{1}{n} \sum_{i=1}^n	\hat{\theta}^{(r+1)}_i
	$$
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\clearpage
\section{Theoretical Findings}



\clearpage
\section{Numerical Experiments}


\subsection{Nonlinear Mixed Models under Distributed Settings}

\textcolor{red}{Compare SAEM, MCEM, dist-SAEM and maybe one distributed Gradient Descent as baseline}

\textcolor{red}{Same for Private settings with Sketched SGD or another good baseline}


\paragraph{Fitting a linear mixed model on Oxford boys dataset \citep{pinheiro2006mixed}}

\paragraph{Fitting a nonlinear mixed model on Warfarin dataset \citep{international2009estimation}}


\subsection{Probabilistic Latent Dirichlet Allocation}



\subsection{Bi-factor models under the Federated Learning settings}

\newpage

\bibliographystyle{abbrvnat}
\bibliography{ref}



%-----------------------------------------------------------------------------
%\vspace{0.4cm}

\end{document} 
