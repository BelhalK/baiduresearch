\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhu1998filters,lecun2006tutorial}
\citation{ngiam2011learning,xie2016theory,xie2020generative,du2019implicit}
\citation{mikolov2013distributed,deng2020residual}
\citation{wenliang2019learning,song2020sliced}
\citation{haarnoja2017reinforcement}
\citation{xie2016theory}
\citation{song2021train}
\citation{song2021train}
\citation{meyn2012markov}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{nijkamp2019learning}
\citation{hinton2002training}
\citation{tieleman2008training}
\citation{welling2002new,gao2018learning,du2019implicit}
\citation{meyn2012markov}
\citation{qiu2019unbiased,jacob2020unbiased}
\citation{du2020improved}
\@writefile{toc}{\contentsline {section}{\numberline {2}On MCMC based Energy Based Models}{2}{section.2}}
\newlabel{sec:mcmc}{{2}{2}{On MCMC based Energy Based Models}{section.2}{}}
\newlabel{eq:ebm}{{1}{2}{On MCMC based Energy Based Models}{equation.2.1}{}}
\citation{robbins1951A,bottou2008}
\citation{grenander1994representations,roberts1996exponential}
\citation{neal2011mcmc}
\citation{lecun2006tutorial,ngiam2011learning}
\citation{kingma2013auto}
\citation{goodfellow2014generative}
\citation{du2019implicit}
\citation{song2020score}
\citation{gao2020flow}
\citation{welling2011bayesian}
\newlabel{eq:mle}{{2}{3}{On MCMC based Energy Based Models}{equation.2.2}{}}
\newlabel{eq:mcapprox}{{4}{3}{On MCMC based Energy Based Models}{equation.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Energy Based Models: }{3}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{MCMC procedures: }{3}{section*.2}}
\citation{atchade2006adaptive,marshall2012adaptive}
\citation{atchade2006adaptive,marshall2012adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Informed Langevin Diffusion}{4}{section.3}}
\newlabel{sec:main}{{3}{4}{Gradient Informed Langevin Diffusion}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminaries and Bottlenecks of Langevin MCMC based EBM}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Curvature informed MCMC}{4}{subsection.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {StAnLey}\ for Energy-Based Model}}{4}{algorithm.1}}
\newlabel{alg:anila}{{1}{4}{Curvature informed MCMC}{algorithm.1}{}}
\newlabel{line:step}{{3}{4}{Curvature informed MCMC}{ALC@unique.3}{}}
\newlabel{eq:step}{{5}{4}{Curvature informed MCMC}{equation.3.5}{}}
\newlabel{line:langevin}{{4}{4}{Curvature informed MCMC}{ALC@unique.4}{}}
\newlabel{eq:anila}{{6}{4}{Curvature informed MCMC}{equation.3.6}{}}
\citation{girolami2011riemann}
\@writefile{toc}{\contentsline {section}{\numberline {4}Geometric ergodicity of \textsc  {StAnLey}sampler}{5}{section.4}}
\newlabel{sec:theory}{{4}{5}{Geometric ergodicity of \algo sampler}{section.4}{}}
\newlabel{ass:bounded}{{1}{5}{}{assumption.1}{}}
\newlabel{ass:contlogpi}{{2}{5}{}{assumption.2}{}}
\citation{meyn2012markov}
\newlabel{eq:driftfunction}{{12}{6}{Geometric ergodicity of \algo sampler}{equation.4.12}{}}
\newlabel{eq:vfunctions}{{13}{6}{Geometric ergodicity of \algo sampler}{equation.4.13}{}}
\newlabel{ass:V2}{{3}{6}{}{assumption.3}{}}
\newlabel{thm:thm1}{{1}{6}{}{theo.1}{}}
\newlabel{thm:main1}{{15}{6}{}{equation.4.15}{}}
\newlabel{thm:main2}{{16}{6}{}{equation.4.16}{}}
\newlabel{eq:twogauss}{{18}{6}{Geometric ergodicity of \algo sampler}{equation.4.18}{}}
\citation{jarner2000geometric}
\newlabel{eq:delta}{{19}{7}{Geometric ergodicity of \algo sampler}{equation.4.19}{}}
\newlabel{mainproof}{{21}{7}{Geometric ergodicity of \algo sampler}{equation.4.21}{}}
\newlabel{eq:main1}{{22}{7}{Geometric ergodicity of \algo sampler}{equation.4.22}{}}
\newlabel{eq:comp}{{24}{7}{Geometric ergodicity of \algo sampler}{equation.4.24}{}}
\newlabel{eq:lowandup}{{26}{7}{Geometric ergodicity of \algo sampler}{equation.4.26}{}}
\newlabel{eq:defcone}{{33}{8}{Geometric ergodicity of \algo sampler}{equation.4.33}{}}
\newlabel{eq:constant}{{35}{9}{Geometric ergodicity of \algo sampler}{equation.4.35}{}}
\newlabel{eq:driftvtheta}{{36}{9}{Geometric ergodicity of \algo sampler}{equation.4.36}{}}
\newlabel{eq:defv}{{37}{9}{Geometric ergodicity of \algo sampler}{equation.4.37}{}}
\newlabel{eq:uniform1}{{38}{9}{Geometric ergodicity of \algo sampler}{equation.4.38}{}}
\newlabel{coro:coro1}{{1}{10}{}{coro.1}{}}
\newlabel{coro:main}{{44}{10}{}{equation.4.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Intermediary Lemmas}{10}{subsection.4.1}}
\newlabel{lem:cone}{{1}{10}{}{lemm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{11}{section.5}}
\newlabel{sec:numericals}{{5}{11}{Numerical Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Application on Toy Example: Gaussian Mixture Model}{11}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (Rings Toy Dataset) }}{11}{figure.1}}
\newlabel{fig:results}{{1}{11}{(Rings Toy Dataset)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Flowers Dataset}{11}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (Flowers Dataset). Left: Langevin Method. Right: \textsc  {StAnLey}method. After 100k iterations.}}{11}{figure.2}}
\newlabel{fig:flowers}{{2}{11}{(Flowers Dataset). Left: Langevin Method. Right: \algo method. After 100k iterations}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}CIFAR Dataset}{12}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (CIFAR Dataset). Left: Langevin Method. Right: \textsc  {StAnLey}method. After 100k iterations.}}{12}{figure.3}}
\newlabel{fig:cifar}{{3}{12}{(CIFAR Dataset). Left: Langevin Method. Right: \algo method. After 100k iterations}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{12}{section.6}}
\newlabel{sec:conclusion}{{6}{12}{Conclusion}{section.6}{}}
\bibstyle{plainnat}
\bibdata{ref}
\bibcite{atchade2006adaptive}{{1}{2006}{{Atchad{\'e}}}{{}}}
\bibcite{bottou2008}{{2}{2008}{{Bottou and Bousquet}}{{}}}
\bibcite{deng2020residual}{{3}{2020}{{Deng et~al.}}{{Deng, Bakhtin, Ott, Szlam, and Ranzato}}}
\bibcite{du2019implicit}{{4}{2019}{{Du and Mordatch}}{{}}}
\bibcite{du2020improved}{{5}{2020}{{Du et~al.}}{{Du, Li, Tenenbaum, and Mordatch}}}
\bibcite{gao2018learning}{{6}{2018}{{Gao et~al.}}{{Gao, Lu, Zhou, Zhu, and Wu}}}
\bibcite{gao2020flow}{{7}{2020}{{Gao et~al.}}{{Gao, Nijkamp, Kingma, Xu, Dai, and Wu}}}
\bibcite{girolami2011riemann}{{8}{2011}{{Girolami and Calderhead}}{{}}}
\bibcite{goodfellow2014generative}{{9}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{grenander1994representations}{{10}{1994}{{Grenander and Miller}}{{}}}
\bibcite{haarnoja2017reinforcement}{{11}{2017}{{Haarnoja et~al.}}{{Haarnoja, Tang, Abbeel, and Levine}}}
\bibcite{hinton2002training}{{12}{2002}{{Hinton}}{{}}}
\bibcite{jacob2020unbiased}{{13}{2020}{{Jacob et~al.}}{{Jacob, O~Leary, and Atchad{\'e}}}}
\bibcite{jarner2000geometric}{{14}{2000}{{Jarner and Hansen}}{{}}}
\bibcite{kingma2013auto}{{15}{2013}{{Kingma and Welling}}{{}}}
\bibcite{lecun2006tutorial}{{16}{2006}{{LeCun et~al.}}{{LeCun, Chopra, Hadsell, Ranzato, and Huang}}}
\bibcite{marshall2012adaptive}{{17}{2012}{{Marshall and Roberts}}{{}}}
\bibcite{meyn2012markov}{{18}{2012}{{Meyn and Tweedie}}{{}}}
\bibcite{mikolov2013distributed}{{19}{2013}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{neal2011mcmc}{{20}{2011}{{Neal et~al.}}{{}}}
\bibcite{ngiam2011learning}{{21}{2011}{{Ngiam et~al.}}{{Ngiam, Chen, Koh, and Ng}}}
\bibcite{nijkamp2019learning}{{22}{2019}{{Nijkamp et~al.}}{{Nijkamp, Hill, Zhu, and Wu}}}
\bibcite{qiu2019unbiased}{{23}{2019}{{Qiu et~al.}}{{Qiu, Zhang, and Wang}}}
\bibcite{robbins1951A}{{24}{1951}{{Robbins and Monro}}{{}}}
\bibcite{roberts1996exponential}{{25}{1996}{{Roberts et~al.}}{{Roberts, Tweedie, et~al.}}}
\bibcite{song2021train}{{26}{2021}{{Song and Kingma}}{{}}}
\bibcite{song2020sliced}{{27}{2020{a}}{{Song et~al.}}{{Song, Garg, Shi, and Ermon}}}
\bibcite{song2020score}{{28}{2020{b}}{{Song et~al.}}{{Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole}}}
\bibcite{tieleman2008training}{{29}{2008}{{Tieleman}}{{}}}
\bibcite{welling2002new}{{30}{2002}{{Welling and Hinton}}{{}}}
\bibcite{welling2011bayesian}{{31}{2011}{{Welling and Teh}}{{}}}
\bibcite{wenliang2019learning}{{32}{2019}{{Wenliang et~al.}}{{Wenliang, Sutherland, Strathmann, and Gretton}}}
\bibcite{xie2016theory}{{33}{2016}{{Xie et~al.}}{{Xie, Lu, Zhu, and Wu}}}
\bibcite{xie2020generative}{{34}{2020}{{Xie et~al.}}{{Xie, Zheng, Gao, Wang, Zhu, and Wu}}}
\bibcite{zhu1998filters}{{35}{1998}{{Zhu et~al.}}{{Zhu, Wu, and Mumford}}}
