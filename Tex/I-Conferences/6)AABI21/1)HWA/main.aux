\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{neal2012bayesian}
\citation{guo2017calibration,kendall2017uncertainties}
\citation{blundell2015weight,kingma2015variational}
\citation{neal2012bayesian}
\citation{graves2011practical,hoffman2013stochastic}
\citation{blundell2015weight}
\citation{polyak1992acceleration}
\providecommand \oddpage@label [2]{}
\jmlr@title{Hyperparameters Weight Averaging in BNNs}{HWA: Hyperparameters Weight Averaging in Bayesian Neural Networks}
\jmlr@author{\Name {Anonymous Authors}\\ \addr Anonymous Institution}{\Name {Anonymous Authors}\\ \addr Anonymous Institution}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\citation{neal2011mcmc}
\citation{hastings1970monte}
\citation{ma2015complete}
\citation{graves2011practical}
\citation{blundell2015weight}
\citation{kingma2015variational,blundell2015weight,molchanov2017variational}
\citation{louizos2017multiplicative}
\citation{wu2018deterministic}
\citation{gal2016dropout}
\citation{polyak1990sa}
\citation{ruppert1988efficient}
\citation{zhou2017convergence}
\citation{izmailov2018averaging}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\citation{garipov2018loss}
\citation{fort2019deep}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.0.2}}
\newlabel{sec:related}{{2}{2}{Related Work}{section.0.2}{}}
\citation{blei2017variational}
\citation{bottou2008tradeoffs}
\citation{kucukelbir2017automatic}
\citation{izmailov2018averaging}
\citation{garipov2018loss}
\citation{he2019asymmetric}
\citation{keskar2016large}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hyperparameters Averaging in Bayesian Neural Networks}{3}{section.0.3}}
\newlabel{sec:main}{{3}{3}{Hyperparameters Averaging in Bayesian Neural Networks}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Neural Networks and ELBO Maximization}{3}{subsection.0.3.1}}
\newlabel{sec:modelbnn}{{3.1}{3}{Bayesian Neural Networks and ELBO Maximization}{subsection.0.3.1}{}}
\newlabel{eq:vi}{{1}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.0.3.1}{}}
\newlabel{eq:variationalobjective}{{2}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.0.3.2}{}}
\citation{kirkpatrick2017overcoming,blundell2015weight}
\citation{maddox2019simple}
\citation{hoffman2013stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Averaging model snapshots through hyperparameters loss landscapes}{4}{subsection.0.3.2}}
\newlabel{eq:hwa_updates}{{3}{4}{Averaging model snapshots through hyperparameters loss landscapes}{equation.0.3.3}{}}
\newlabel{line:svi}{{4}{4}{Averaging model snapshots through hyperparameters loss landscapes}{ALC@unique.4}{}}
\newlabel{line:svisigma}{{5}{4}{Averaging model snapshots through hyperparameters loss landscapes}{ALC@unique.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces HWA: Hyperparameters Weight Averaging}}{4}{algorithm.1}}
\newlabel{alg:hwa}{{1}{4}{Averaging model snapshots through hyperparameters loss landscapes}{algorithm.1}{}}
\citation{maddox2019simple}
\citation{zhang2019cyclical}
\citation{blundell2015weight}
\citation{welling2011bayesian}
\citation{zhang2019cyclical}
\citation{lecun1998mnist}
\citation{krizhevsky2009learning}
\citation{lecun1998gradient}
\citation{simonyan2014very}
\citation{wen2018flipout}
\newlabel{eq:lowrankcov}{{4}{5}{Averaging model snapshots through hyperparameters loss landscapes}{equation.0.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{5}{section.0.4}}
\newlabel{sec:numerical}{{4}{5}{Numerical Experiments}{section.0.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison for Bayesian LeNet CNN architecture on MNIST dataset (top) and Bayesian VGG architecture on CIFAR-10 dataset (bottom). The plots are averaged over 5 repetitions.}}{6}{figure.1}}
\newlabel{fig:all}{{1}{6}{Comparison for Bayesian LeNet CNN architecture on MNIST dataset (top) and Bayesian VGG architecture on CIFAR-10 dataset (bottom). The plots are averaged over 5 repetitions}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.0.5}}
\newlabel{sec:conclusion}{{5}{6}{Conclusion}{section.0.5}{}}
\bibdata{ref}
\bibcite{blei2017variational}{{1}{2017}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{blundell2015weight}{{2}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{bottou2008tradeoffs}{{3}{2008}{{Bottou and Bousquet}}{{}}}
\bibcite{defazio2014saga}{{4}{2014}{{Defazio et~al.}}{{Defazio, Bach, and Lacoste-Julien}}}
\bibcite{fort2019deep}{{5}{2019}{{Fort et~al.}}{{Fort, Hu, and Lakshminarayanan}}}
\bibcite{gal2016dropout}{{6}{2016}{{Gal and Ghahramani}}{{}}}
\bibcite{gal2017concrete}{{7}{2017}{{Gal et~al.}}{{Gal, Hron, and Kendall}}}
\bibcite{garipov2018loss}{{8}{2018}{{Garipov et~al.}}{{Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson}}}
\bibcite{graves2011practical}{{9}{2011}{{Graves}}{{}}}
\bibcite{guo2017calibration}{{10}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{hastings1970monte}{{11}{1970}{{Hastings}}{{}}}
\bibcite{he2019asymmetric}{{12}{2019}{{He et~al.}}{{He, Huang, and Yuan}}}
\bibcite{hoffman2013stochastic}{{13}{2013}{{Hoffman et~al.}}{{Hoffman, Blei, Wang, and Paisley}}}
\bibcite{izmailov2018averaging}{{14}{2018}{{Izmailov et~al.}}{{Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson}}}
\bibcite{kendall2017uncertainties}{{15}{2017}{{Kendall and Gal}}{{}}}
\bibcite{keskar2016large}{{16}{2016}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2015variational}{{17}{2015}{{Kingma et~al.}}{{Kingma, Salimans, and Welling}}}
\bibcite{kirkpatrick2017overcoming}{{18}{2017}{{Kirkpatrick et~al.}}{{Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.}}}
\bibcite{krizhevsky2009learning}{{19}{2009}{{Krizhevsky and Hinton}}{{}}}
\bibcite{kucukelbir2017automatic}{{20}{2017}{{Kucukelbir et~al.}}{{Kucukelbir, Tran, Ranganath, Gelman, and Blei}}}
\bibcite{lecun1998mnist}{{21}{1998}{{LeCun}}{{}}}
\bibcite{lecun1998gradient}{{22}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, Haffner, et~al.}}}
\bibcite{louizos2017multiplicative}{{23}{2017}{{Louizos and Welling}}{{}}}
\bibcite{ma2015complete}{{24}{2015}{{Ma et~al.}}{{Ma, Chen, and Fox}}}
\bibcite{maddox2019simple}{{25}{2019}{{Maddox et~al.}}{{Maddox, Izmailov, Garipov, Vetrov, and Wilson}}}
\bibcite{mairal2015incremental}{{26}{2015}{{Mairal}}{{}}}
\bibcite{molchanov2017variational}{{27}{2017}{{Molchanov et~al.}}{{Molchanov, Ashukha, and Vetrov}}}
\bibcite{neal2012bayesian}{{28}{2012}{{Neal}}{{}}}
\bibcite{neal2011mcmc}{{29}{2011}{{Neal et~al.}}{{}}}
\bibcite{polyak1990sa}{{30}{1990}{{Polyak}}{{}}}
\bibcite{polyak1992acceleration}{{31}{1992}{{Polyak and Juditsky}}{{}}}
\bibcite{ruppert1988efficient}{{32}{1988}{{Ruppert}}{{}}}
\bibcite{schmidt2017minimizing}{{33}{2017}{{Schmidt et~al.}}{{Schmidt, Le~Roux, and Bach}}}
\bibcite{simonyan2014very}{{34}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{welling2011bayesian}{{35}{2011}{{Welling and Teh}}{{}}}
\bibcite{wen2018flipout}{{36}{2018}{{Wen et~al.}}{{Wen, Vicol, Ba, Tran, and Grosse}}}
\bibcite{wu2018deterministic}{{37}{2018}{{Wu et~al.}}{{Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and Gaunt}}}
\bibcite{zhang2019cyclical}{{38}{2019}{{Zhang et~al.}}{{Zhang, Li, Zhang, Chen, and Wilson}}}
\bibcite{zhou2017convergence}{{39}{2017}{{Zhou and Cong}}{{}}}
\citation{zhou2017convergence}
\citation{schmidt2017minimizing}
\citation{defazio2014saga}
\citation{mairal2015incremental}
\citation{mairal2015incremental}
\citation{maddox2019simple}
\@writefile{toc}{\let \numberline \appendixnumberline }
\@writefile{toc}{\contentsline {section}{\numberline {A}Comparison with other classical averaging procedures in nonconvex optimization}{10}{section.0.A}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Embedding HWA in Variational Inference}{10}{section.0.B}}
\newlabel{app:viandhwa}{{B}{10}{Embedding HWA in Variational Inference}{section.0.B}{}}
\citation{gal2016dropout}
\citation{gal2017concrete}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Variational Inference with HWA for BNNs}}{11}{algorithm.2}}
\newlabel{alg:trainingbnn}{{2}{11}{Embedding HWA in Variational Inference}{algorithm.2}{}}
\newlabel{jmlrend}{{B}{11}{end of Hyperparameters Weight Averaging in BNNs}{section*.2}{}}
