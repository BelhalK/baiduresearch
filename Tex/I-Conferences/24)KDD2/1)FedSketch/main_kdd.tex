%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%\documentclass[sigconf]{acmart}
\documentclass[sigconf, anonymous, review]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



%\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage{natbib}
\usepackage{comment}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{booktabs} % for professional tables



\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{multirow}
\usepackage{makecell}
\usepackage{pifont}
\usepackage{mathtools}
\usepackage{balance}

\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{tikzmark,calc}

\newcommand\DrawBoxhom[3][]{%
  \begin{tikzpicture}[remember picture,overlay]
    \draw[overlay,fill=gray!30,#1] 
    ([xshift=-0.5em,yshift=-0.1ex]{pic cs:#2}) 
    rectangle 
    ([xshift=35pt,yshift=3.1ex]pic cs:#3);
  \end{tikzpicture}%
}


\newcommand\DrawBoxxhom[3][]{%
  \begin{tikzpicture}[remember picture,overlay]
    \draw[overlay,fill=gray!30,#1] 
    ([xshift=-5.9em,yshift=-1.1ex]{pic cs:#2}) 
    rectangle 
    ([xshift=80pt,yshift=-1.5ex]pic cs:#3);
  \end{tikzpicture}%
}


\newcommand\DrawBox[3][]{%
  \begin{tikzpicture}[remember picture,overlay]
    \draw[overlay,fill=gray!30,#1] 
    ([xshift=10em,yshift=-0.4ex]{pic cs:#2}) 
    rectangle 
    ([xshift=-4pt,yshift=1.1ex]pic cs:#3);
  \end{tikzpicture}%
}


\newcommand\DrawBoxx[3][]{%
  \begin{tikzpicture}[remember picture,overlay]
    \draw[overlay,fill=gray!30,#1] 
    ([xshift=-8.0em,yshift=-1.0ex]{pic cs:#2}) 
    rectangle 
    ([xshift=50pt,yshift=-1.2ex]pic cs:#3);
  \end{tikzpicture}%
}

%\algnewcommand\algorithmicinput{\textbf{Input:}}
%\algnewcommand\INPUT{\item[\algorithmicinput]}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\newcommand{\pl}{Polyak-\L{}ojasiewicz}
\newcommand{\todoM}[1]{\textcolor{blue}{ToDo (Farzin): #1}}
\newcommand{\todo}[1]{\textcolor{red}{ToDo:~#1}}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\E}{\mathrm{E}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{property}{Property}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{claim}[theorem]{\bf Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{example}[theorem]{Example}


\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%



%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


\allowdisplaybreaks




% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:


\begin{document}

\title{\texttt{FedSKETCH}: Communication-Efficient Federated Learning via Sketching}
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Communication complexity and data privacy are the two key challenges in Federated Learning (FL) where the goal is to perform a distributed learning through a large volume of devices. In this work, we introduce two new algorithms, namely \texttt{FedSKETCH} and \texttt{FedSKETCHGATE}, to address jointly both challenges and which are, respectively, intended to be used for homogeneous and heterogeneous data distribution settings. Our algorithms are based on a key and novel sketching technique, called \texttt{HEAPRIX} that is unbiased, compresses the accumulation of local gradients using count sketch, and exhibits communication-efficiency properties leveraging low-dimensional sketches. We provide sharp convergence guarantees of our algorithms and validate our theoretical findings with various sets of experiments. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle




\section{Introduction}
\vspace{-0.05in}
Federated Learning (FL) is an emerging framework for distributed large scale machine learning problems.
In FL, data is distributed across devices~\citep{konevcny2016federated,mcmahan2016communication} and users are only allowed to communicate with the parameter server.
Formally, the optimization problem across $p$ distributed devices is defined as follows:
\begin{align}\label{eq:main}
   \min_{\boldsymbol{x}\in \mathbb{R}^{d},\: \sum_{j=1}^pq_j=1} f(\boldsymbol{x})\triangleq \sum_{j=1}^{p}q_jf_j(\boldsymbol{x}) \, , 
\end{align}
where for device $j\in \{1,\ldots,p\}$, $f_j(\boldsymbol{x})=\mathbb{E}_{\xi\in\mathcal{D}_j}\left[L_j\left(\boldsymbol{x},\xi\right)\right]$, $L_j$ is a loss function that measures the performance of model $\boldsymbol{x}$, $\xi$ is a random variable distributed according to probability distribution $\mathcal{D}_j$, $q_j\triangleq\frac{n_j}{n}$ indicates the portion of data samples, $n_j$ is the number of data shards and $n=\sum_{j=1}^pn_j$ is the total number of data samples. Note that contrary to the homogeneous setting where we assume $\{\mathcal{D}_j\}_{j=1}^p$ have the same distribution across devices and $L_i=L_j \ , \ 1 \leq (i,j) \leq p$, in the heterogeneous setting these distributions and loss functions $L_j$ can vary from a device to another. 
% The parameter server orchestrates optimization among devices by aggregating gradient-related information of devices and broadcasts the average of received vectors. 
% Besides, moving data across the devices during the learning a global model can be costly and could violate the privacy of user devices~\citep{carlini2019secret,mcmahan2017learning}. 


There are several challenges that need to be addressed in FL in order to efficiently learn a global model that performs well in average for all devices:

-- \emph{Communication-efficiency}: There are often many devices communicating with the server, thus incurring immense communication overhead. 
One approach to reduce the number of communication rounds is using \emph{local SGD with periodic averaging}~\citep{zhou2018convergence,stich2019local,yu2019parallel,wang2018cooperative} which periodically averages local models after a few local updates, contrary to baseline SGD~\citep{bottou-bousquet-2008} where gradient averaging is performed at each iteration.
Local SGD has been proposed in~\citep{mcmahan2016communication,konevcny2016federated} under the FL setting and its convergence analysis is studied in~\citep{stich2019local,wang2018cooperative,zhou2018convergence,yu2019parallel}, later on improved in the followup references~\citep{basu2019qsparse,haddadpour2019convergence,bayoumi2020tighter,stich2019error} for homogeneous setting. 
It is further extended to heterogeneous setting~\citep{sahu2018convergence,haddadpour2019convergence,karimireddy2019scaffold,yu2019linear,li2019convergence,liang2019variance}. The second approach dealing with communication cost aims at reducing the size of communicated message per communication round, such as gradient quantization~\citep{alistarh2017qsgd,bernstein2018signsgd,tang2018communication,wen2017terngrad,wu2018error} or sparsification~\citep{stich2018sparsified,alistarh2018convergence,lin2017deep,stich2019error}. 
%Additionally, similar Local SGD with adaptive gradient methods can be found in~\citep{reddi2020adaptive,chen2020toward}.


--\emph{Data heterogeneity}: 
Since locally generated data in each device may come from different distribution, local computations involved in FL setting can lead to poor convergence error in practice~\citep{li2019federated,liang2019variance}. 
To mitigate the negative impact of data heterogeneity,~\citep{horvath2019stochastic,liang2019variance,karimireddy2019scaffold,haddadpour2020federated} suggest applying variance reduction or gradient tracking techniques along local computations.

 --\emph{Privacy}~\citep{geyer2017differentially,hardy2017private}: Privacy has been widely addressed by injecting an additional layer of randomness to respect differential-privacy property~\citep{mcmahan2017learning} or using cryptography-based approaches under secure multi-party computation~\citep{bonawitz2017practical}. 
  Further study related to FL setting can be found in recent surveys~\citep{li2019federated} and~\citep{kairouz2019advances}.
%One approach to deal with communication cost is the idea of \emph{local SGD with periodic averaging}~\citep{zhou2018convergence,stich2019local,yu2019parallel,wang2018cooperative} which asserts that instead of taking the average within each iteration, like baseline SGD~\citep{bottou-bousquet-2008}, one may take the average periodically and performs local update, see local SGD~\citep{lin2019don}. 
%It is shown that local SGD with periodic averaging benefits from the same convergence rate as baseline SGD, while requiring less communication rounds. 
%The second approach to deal with communication cost aims at reducing the size of communicated message per communication round. 
%Available methods reduce the size of the message by communicating compressed local gradients or models to parameter server via quantization~\citep{alistarh2017qsgd,bernstein2018signsgd,tang2018communication,wen2017terngrad,wu2018error}, sparsification~\citep{alistarh2018convergence,lin2017deep,stich2018sparsified,stich2019error}.%, or combination of both~\citep{basu2019qsparse}.
%A large amount of research efforts, such as~\citep{liang2019variance,karimireddy2019scaffold,horvath2019stochastic,haddadpour2020federated} aim at mitigating the effect of data heterogeneity by exploiting variance reduction or gradient tracking techniques in distributed optimization settings where data distribution is non-iid.
%Solving the privacy issue has been widely performed by injecting an additional layer of random noise in order to respect differential-privacy property of the method~\citep{mcmahan2017learning} or using cryptography-based approaches under secure multi-party computation~\citep{bonawitz2017practical} framework.
%Almost all of the previous studies consider addressing the aforementioned challenges separately. 


To jointly tackle the aforementioned challenges in FL, sketching based algorithms~\citep{DBLP:journals/tcs/CharikarCF04,cormode2005improved,kleinberg2003bursty,Proc:Li_Church_Hastie_NIPS08} 
are promising methods. 
For instance, to reduce the communication cost,~\citep{ivkin2019communication} develops a distributed SGD algorithm using sketching, provides its convergence analysis in the homogeneous setting, and establishes a communication complexity of order $\mathcal{O}(\log(d))$ per round, where $d$ is the dimension of the gradient vector compared to $\mathcal{O}(d)$ complexity per round of baseline mini-batch SGD. Nonetheless, the proposed sketching scheme in~\citep{ivkin2019communication}, built from a communication-efficiency perspective, is based on a deterministic procedure which requires access to the exact information of the gradients, thus not meeting the  privacy-preserving criteria.
This systemic issue is partially addressed in~\citep{rothchild2020fetchsgd}. 


Focusing on privacy,~\citep{li2019privacy} derives a single framework in order to address these issues and introduces \texttt{DiffSketch} algorithm, based on the Count Sketch operator, yet does not provide its convergence analysis.
Besides, the estimation error of \texttt{DiffSketch} is higher than the sketching scheme in~\citep{ivkin2019communication} which could lead to poor convergence. 
%~\citep{rothchild2020fetchsgd} considers using sketching technique in heterogeneous setting from a communication-efficiency perspective.
%They introduce \texttt{Sketched-SGD} and establish a communication complexity of order $\mathcal{O}(\log(d))$ (per round) where $d$ is the dimension of the vector of parameters, i.e., the dimension of the gradient.
%Compression methods such as quantized gradients are developed in~\citep{alistarh2017qsgd,lin2017deep,stich2018sparsified,horvath2019stochastic,horvath2020better}. 
 
 In this paper, we propose new methods to tackle the communication bottleneck of FL baselines.
 Our main contributions are summarized as follows:
\begin{itemize}
    \item We provide a new algorithm -- \texttt{HEAPRIX} -- and theoretically show that it reduces the cost of communication, based on unbiased sketching without requiring the broadcast of exact values of gradients to the server. 
    Based on \texttt{HEAPRIX}, we develop general algorithms for communication-efficient and sketch-based FL, namely \texttt{FedSKETCH} and \texttt{FedSKETCHGATE} for  homogeneous and heterogeneous data distribution settings respectively.
    \item We establish non-asymptotic convergence bounds for \pl\:(PL), convex and non-convex functions in Theorems~\ref{thm:homog_case} and~\ref{thm:hetreg_case} in both homogeneous and heterogeneous cases, and highlight an improvement in the number of iterations to reach a stationary point.
We also provide \emph{sharper} convergence analysis for the \texttt{PRIVIX}/\texttt{DiffSketch}\footnote{We use \texttt{PRIVIX} and \texttt{DiffSketch} \citep{li2019privacy} interchangeably throughout the paper.} algorithm proposed in~\citep{li2019privacy}.
    \item We illustrate the benefits of \texttt{FedSKETCH} and \texttt{FedSKETCHGATE} over baseline methods through a number of experiments. 
    The latter shows the advantages of the \texttt{HEAPRIX} compression method achieving comparable test accuracy as Federated SGD (\texttt{FedSGD}) while compressing the information exchanged between devices and server.
\end{itemize}


\noindent\textbf{Notation:} 
We denote the number of communication rounds and bits per round and per device by $R$ and $B$ respectively. 
The count sketch of vector $\boldsymbol{x}$ is designated by $\mathbf{S}(\boldsymbol{x})$. $[p]$ denotes~the~set~$\{1,\dots,p\}$.

\vspace{-0.05in}
\section{Compression using Count Sketch}\label{sec:compression}
\vspace{-0.05in}

Throughout the paper, we employ the commonly used \texttt{Count Sketch} ~\citep{DBLP:journals/tcs/CharikarCF04} as building component of our algorithms. 
Please refer to the Appendix for the detailed Count Sketch algorithm. 

There are various types of sketching algorithms which are developed based on count sketching that we develop in the following subsections. 




\vspace{-0.05in}
\subsection{Sketching based Unbiased Compressor}
\vspace{-0.05in}



We define an unbiased compressor as follows:
\begin{definition}[Unbiased compressor]\label{def:unbiased}
We call the randomized function $\mathcal{C}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$ an unbiased compression operator with $\Delta\geq 1$, if
\begin{align}\notag
\mathbb{E}\left[\mathcal{C}(\boldsymbol{x})\right]&=\boldsymbol{x}\nonumber \quad \textrm{and} \quad    \mathbb{E}\left[\left\|\mathcal{C}(\boldsymbol{x})\right\|^2_2\right] \leq \Delta\left\|\boldsymbol{x}\right\|^2_2\notag \, .
\end{align}
We denote this class of compressors by $\mathbb{U}(\Delta)$.
\end{definition}
This definition leads to the following property 
\begin{align}\notag
    \mathbb{E}\left[\left\|\mathcal{C}(\boldsymbol{x})-\boldsymbol{x}\right\|^2_2\right]&\leq \left(\Delta-1\right)\left\|\boldsymbol{x}\right\|^2_2\, .
\end{align}
This property allows us to control the noise of the compression.
Note that if we let $\Delta=1$, then our algorithm reduces to the case of no compression. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\textit{An Example of Unbiased Compressor via Sketching:}
For instance,\texttt{PRIVIX} is an unbiased compressor which obtains an estimate of input $\boldsymbol{x}$ from a count sketch noted $\boldsymbol{S}(\boldsymbol{x})$. 
In this algorithm, to query the quantity $x_i$, the $i$-th element of the vector $\boldsymbol{x}$, we compute the median of $t$ approximated values specified by the indices of $h_j(i)$ for $1\leq j\leq t$, see~\citep{li2019privacy}, or Algorithm~\ref{Alg:privix} in the Appendix (for more details).
%\begin{algorithm}[t]
%\caption{\texttt{PRIVIX}~\citep{li2019privacy}: Unbiased compressor based on sketching. }\label{Alg:privix}
%\begin{algorithmic}[1]
%\STATE\textbf{Inputs:} $\boldsymbol{x}\in\mathbb{R}^{d}, t, m, \mathbf{S}_{m\times t}, h_j (1\leq i\leq t), sign_j (1\leq i\leq t)$
%\STATE\textbf{Query} $\tilde{\boldsymbol{x}}\in\mathbb{R}^d$ \textbf{from $\mathbf{S(\boldsymbol{x})}$:}
%\STATE\textbf{for} $i=1,\ldots,d$ \textbf{do}
%\STATE\quad\quad ${\tilde{\boldsymbol{x}}}[i]=\text{Median}\{\text{sign}_j(i).\mathbf{S}[j][h_j(i)]:1\leq j\leq t\}$ 
%\STATE\textbf{end for}
%\STATE\textbf{Output:} ${\tilde{\boldsymbol{x}}}$
%\vspace{- 0.1cm}
%\end{algorithmic}
%\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Next, we review a few properties of \texttt{PRIVIX}: 
% \vspace{0.1in}\noindent\textbf{Estimation errors:}
We give below a useful property of Count Sketch for our theoretical analysis.
%\begin{property}[\citep{li2019privacy}]
%For any $\mathbf{x}\in \mathbb{R}^{d}$, $\texttt{PRIVIX}$ is unbiased: $\mathbb{E}_{\mathbf{S}}\left[\texttt{PRIVIX}\left[\mathbf{S}\left(\mathbf{x}\right)\right]\right]=\mathbf{x}$ and has \emph{bounded variance}: $\mathbb{E}_{\mathbf{S}}[\|\texttt{PRIVIX}[\mathbf{S}(\mathbf{x})]-\mathbf{x}\|_2^2]\leq \mu^2 d\|\mathbf{x}\|_2^2, \: \textrm{w.p.} \: 1-\delta$, if $m=\mathcal{O}\left(\frac{e}{\mu^2}\right)$, $t=\mathcal{O}\left(\ln \left(\frac{d}{\delta}\right)\right)$.
%\end{property}
\begin{property}[\citep{li2019privacy}]
For any $\boldsymbol{x}\in \mathbb{R}^{d}$:

\textit{Unbiased estimation}: As in~\citep{li2019privacy}, we have $ \mathbb{E}_{\mathbf{S}}\left[\texttt{PRIVIX}\left[\mathbf{S}\left(\boldsymbol{x}\right)\right]\right]=\boldsymbol{x}\, $.

\textit{Bounded variance:} For the given $m<d$, $t=\mathcal{O}\left(\ln \left(\frac{d}{\delta}\right)\right)$ with probability $1-\delta$ we have:
    \begin{align}\notag
        \mathbb{E}_{\mathbf{S}}\left[\left\|\texttt{PRIVIX}\left[\mathbf{S}\left(\boldsymbol{x}\right)\right]-\boldsymbol{x}\right\|_2^2\right] \leq c\frac{d}{m}\left\|\boldsymbol{x}\right\|_2^2\ , 
    \end{align}
 where $c$ ($e\leq c<m$) is a positive constant independent of the dimension of the input, $d$.
\end{property}
 We note that this bounded variance assumption does not necessary mean that compression is happening since dimension $d$ may be relatively large. 
Thus, with probability $1-\delta$, we obtain $\texttt{PRIVIX}\in \mathbb{U}(1+c\frac{d}{m})$.
 $\Delta=1+c\frac{d}{m}$ implies that if $m\rightarrow d$, then $\Delta\rightarrow 1+c$, indicates a noisy reconstruction. 
 \citep{li2019privacy} shows that if the data is normally distributed, \texttt{PRIVIX} is differentially private~\citep{DBLP:conf/icalp/Dwork06}, up to additional assumptions and algorithmic design choices.
%  that this aspect of the sketching is beyond the scope of this paper.  


\vspace{-0.05in}
\subsection{Sketching based Biased Compressor}
\vspace{-0.05in}

A biased compressor is defined as follows:
\begin{definition}[Biased compressor]
A (randomized) function,  ${\mathcal{C}}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$ belongs to $\mathbb{C}(\Delta,\alpha)$, a class of compression operators with $\alpha>0$ and $\Delta\geq 1$, if
\begin{align}\notag
    \mathbb{E}\left[\left\|\alpha\boldsymbol{x}-{\mathcal{C}}(\boldsymbol{x})\right\|^2_2\right]\leq \left(1-\frac{1}{\Delta}\right)\left\|\boldsymbol{x}\right\|^2_2\, .
\end{align}
%We denote this class of biased operators by $\mathbb{C}(\Delta,\alpha)$. 
\end{definition}
It is proven in~\citep{horvath2020better} that $\mathbb{U}(\Delta)\subset\mathbb{C}(\Delta, \alpha)$.
%The following Lemma links these two definitions:
%\begin{lemma}[\citep{horvath2020better}]
%We have $\mathbb{U}(\Delta)\subset\mathbb{C}(\Delta, \alpha)$.
%\end{lemma}
An example of biased compression using sketching methods and using a top$_m$ operator is provided below:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\caption{\texttt{HEAVYMIX} (Modified \citep{ivkin2019communication})}\label{alg:heavymix}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\mathbf{S}({\mathbf{g}})$; parameter $m$
\STATE {Query the vector $\tilde{\mathbf{g}}\in\mathbb{R}^{d}$ from $\mathbf{S}\left(\mathbf{g}\right)$:}
\STATE Query $\hat{\ell}_2^2=\left(1\pm 0.5\right)\left\|\mathbf{g}\right\|^2$ from sketch~$\mathbf{S}(\mathbf{g})$
\STATE $\forall j$ query $\hat{\mathbf{g}}_j^2=\hat{\mathbf{g}}_j^2\pm \frac{1}{2m}\left\|\mathbf{g}\right\|^2$ from sketch $\mathbf{S}(\mathbf{g})$
\STATE $H=\{j|\hat{\mathbf{g}}_j\geq \frac{\hat{\ell}_2^2}{m}\}$ and $NH=\{j|\hat{\mathbf{g}}_j<\frac{\hat{\ell}_2^2}{m}\}$
\STATE Top$_m=H\cup \text{rand}_\ell(NH)$, where $\ell=m-\left|H\right|$
\STATE Get exact values of Top$_m$ 
\STATE \textbf{Output:} $\tilde{\mathbf{g}}:\forall j\in\text{Top}_m:\tilde{\mathbf{g}}_{i}=\mathbf{g}_{i}$ else $ \mathbf{g}_{i}=0$
%\vspace{- 0.1cm}
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Following~\citep{ivkin2019communication}, \texttt{HEAVYMIX} with sketch size $\Theta\left(m\log\left(\frac{d}{\delta}\right)\right)$ is a biased compressor with $\alpha=1$ and  $\Delta=d/m$ with probability $\geq1-\delta$.
% Indeed, it reconstructs the vector $\tilde{\mathbf{g}}$ from the input $\mathbf{g}$. 
In other words, with probability $1-\delta$, $\texttt{HEAVYMIX}\in \mathcal{C}(\frac{d}{m},1)$ . 
%\begin{lemma}[\citep{ivkin2019communication}]
%\texttt{HEAVYMIX}, with sketch size $\Theta\left(m\log\left(\frac{d}{\delta}\right)\right)$ is a biased compressor with $\alpha=1$ and  $\Delta=d/m$ with probability $\geq1-\delta$. In other words, with probability $1-\delta$, $\texttt{HEAVYMIX}\in \mathcal{C}(\frac{d}{m},1)$. 
%\end{lemma}
Note that Algorithm~\ref{alg:heavymix} is a variant of the sketching algorithm developed in~\citep{ivkin2019communication} with the distinction that \texttt{HEAVYMIX} does not require any second round of communication to obtain the exact values of top$_m$.
This is mainly because in \texttt{SKETCHED-SGD}~\citep{ivkin2019communication}, the server receives the exact values of \emph{the average of the sketches} while \texttt{HEAVYMIX} obtains exact local values.
% , hence \emph{does not require a second round of communication at the same time does not violate user privacy}. 
Additionally, while \texttt{HEAVYMIX} has a smaller estimation error compared to \texttt{PRIVIX}, in \texttt{PRIVIX} the central server does need to have access to the exact values of local gradient providing user privacy as underlined in~\cite{li2019privacy}.

In the following, we introduce \texttt{HEAPRIX} which is built upon \texttt{HEAVYMIX} and \texttt{PRIVIX} methods.

\vspace{-0.05in}
\subsection{Sketching based Induced Compressor}
\vspace{-0.05in}

From Theorem~$3$ in~\citep{horvath2020better}, stating that a biased compressor can be converted into an unbiased one such that, for $\mathcal{C}_1\in \mathbb{C}(\Delta_1)$ with $\alpha=1$, if one chooses $\mathcal{C}_2\in \mathbb{U}(\Delta_2)$, then the induced compressor $\mathcal{C}: x \mapsto \mathcal{C}_1(\mathbf{x})+\mathcal{C}_2\left(\mathbf{x}-\mathcal{C}_1\left(\mathbf{x}\right)\right)$ belongs to $\mathbb{U}(\Delta)$ with $\Delta=\Delta_2+\frac{1-\Delta_2}{\Delta_1}$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
\caption{\texttt{HEAPRIX} }\label{alg:heaprix}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\boldsymbol{x}\in\mathbb{R}^{d}, t, m, \mathbf{S}_{m\times t}, h_j (1\leq i\leq t), \text{sign}_j (1\leq i\leq t)$, parameter $m$
\STATE {Approximate $\mathbf{S}(\boldsymbol{x})$ using \texttt{HEAVYMIX} }
\STATE {Approximate} $\mathbf{S}\left(\boldsymbol{x} - \texttt{HEAVYMIX}[\mathbf{S}(\boldsymbol{x})]\right)$ with \texttt{PRIVIX} 
\STATE \textbf{Output:}
\vspace{- 0.1cm}
\[ \Resize{7.4cm}{\texttt{HEAVYMIX}\left[\mathbf{S}\left(\boldsymbol{x}\right)\right]+\texttt{PRIVIX}\left[\mathbf{S}\left(\boldsymbol{x}-\texttt{HEAVYMIX}\left[\mathbf{S}\left(\boldsymbol{x}\right)\right]\right)\right]}\ .\]
%\vspace{- 0.1cm}
\end{algorithmic}
\end{algorithm}
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Based on this notion, Algorithm~\ref{alg:heaprix} proposes an induced sketching algorithm by utilizing \texttt{HEAVYMIX} and \texttt{PRIVIX} for $\mathcal{C}_1$ and $\mathcal{C}_2$ respectively where the reconstruction of input $\mathbf{x}$ is performed using hash table $\mathbf{S}$ and $\mathbf{x}$, similar to \texttt{PRIVIX} and \texttt{HEAVYMIX}.
Note that if $m\rightarrow d$, then $\mathcal{C}(\boldsymbol{x})\rightarrow \boldsymbol{x}$, implying that the convergence rate  can be improved by decreasing the size of compression $m$. 
\begin{corollary}\label{cor:small}
Based on~Theorem 3 of~\citep{horvath2020better}, \texttt{HEAPRIX} in Algorithm~\ref{alg:heaprix} satisfies $\mathcal{C}(\boldsymbol{x})\in \mathbb{U}(c \frac{d}{m})$. 
\end{corollary}

\vspace{-0.05in}
\section{\texttt{FedSKETCH} and \texttt{FedSKETCHGATE}}\label{sec:algos}
\vspace{-0.05in}
%We define general frameworks for different sketching algorithms for homogeneous and heterogeneous settings.
We introduce two new algorithms for both homogeneous and heterogeneous settings.



\vspace{-0.1in}
\subsection{Homogeneous Setting}
\vspace{-0.05in}

%The proposed algorithms for FL leverage sketching techniques to reduce communication costs.
In \texttt{FedSKETCH}, the number of local updates, between two consecutive communication rounds, at device $j$ is denoted by $\tau$.
 Unlike~\citep{haddadpour2020federated}, the server does not store any global model, rather, device $j$ has two models: $\boldsymbol{x}^{(r)}$ and $\boldsymbol{x}^{(\ell,r)}_j$, which are respectively the global and local models. 
%At communication round $r$ and device $j$, the local model $\boldsymbol{x}^{(\ell,r)}_j$ is updated using the rule $$\boldsymbol{x}_j^{(\ell+1,r)}=\boldsymbol{x}_j^{(\ell,r)}-\eta \tilde{\mathbf{g}}_j^{(\ell,r)} \qquad \text{for}\quad\ell=0,\ldots,\tau-1\, , $$
%where $\tilde{\mathbf{g}}_j^{(\ell,r)}\triangleq\nabla{f}_j(\boldsymbol{x}_j^{(\ell,r)},\Xi_j^{(\ell,r)})\triangleq\frac{1}{b}\sum_{\xi\in\Xi_j^{(\ell,r)}}\nabla{L}_j(\boldsymbol{x}_j^{(\ell,r)},\xi)$ is a stochastic gradient of $f_j$ evaluated using the mini-batch $\Xi_j^{(\ell,r)}=\{\xi^{(\ell,r)}_{j,1},\ldots,\xi^{(\ell,r)}_{j,b_j} \}$ of size $b_j$ and $\eta$ is the local learning rate. After $\tau$ local updates, model at device $j$ and communication round $r$ is indicated by $\boldsymbol{x}_j^{(\tau,r)}$. The next step of our algorithm is that device $j$ sends the count sketch $\mathbf{S}_j^{(r)}\triangleq\mathbf{S}_j\left(\boldsymbol{x}_j^{(\tau,r)}-\boldsymbol{x}_j^{(0,r)}\right)$ back to the server. We note that
% \begin{equation}
% \begin{split}
% \mathbf{S}_j^{(r)} & \triangleq\mathbf{S}_j\left(\boldsymbol{x}_j^{(\tau,r)}-\boldsymbol{x}_j^{(0,r)}\right)\\
% & =\mathbf{S}_j\left(\eta\sum_{\ell=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(\ell,r)}\right)=\eta\mathbf{S}_j\left(\sum_{\ell=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(\ell,r)}\right)\, ,
%\end{split}
%  \end{equation}
% which is the aggregation of the consecutive stochastic gradients multiplied with local updates $\eta$.
%
%Upon receiving all $\mathbf{S}_j^{(r)}$ from sampled devices, the server computes \begin{align}\mathbf{S}^{(r)}=\frac{1}{k}\sum_{j\in\mathcal{K}^{(r)}}\mathbf{S}_j^{(r)}\label{eq:average-skestching}
%\end{align} and broadcasts it to all devices. 
%Devices after receiving $\mathbf{S}^{(r)}$ from server update global model $\boldsymbol{x}^{(r)}$ using rule $$\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma \texttt{PRIVIX}\left[\mathbf{S}^{(r-1)}\right]\, .$$
We develop \texttt{FedSKETCH} in Algorithm~\ref{Alg:PFLHom} with a variant of this algorithm implementing \texttt{HEAPRIX}. 
For this variant, we need to have an additional communication round between the server and worker $j$ to aggregate $\mathbf{\delta}_j^{(r)}\triangleq \mathbf{S}_j\left[\texttt{HEAVYMIX}(\mathbf{S}^{(r)})\right]$ (Lines~\ref{line:heaprix1} and~\ref{line:heaprix2}) to compute $ {\mathbf{S}}^{(r)}=\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbf{S}^{(r)}_{j}$. The main difference between  \texttt{FedSKETCH} and \texttt{DiffSketch}  in~\citep{li2019privacy} is that we use distinct local and global learning rates. Furthermore, unlike~\citep{li2019privacy}, we do not add local Gaussian noise \emph{as privacy is not the main focus of this paper}.



\begin{algorithm}[t]
\DrawBoxhom[draw=white,fill=blue!6]{i}{j}
\DrawBoxxhom[draw=white,fill=blue!6]{k}{l}
\caption{\texttt{FedSKETCH}($R$, $\tau, \eta, \gamma$) }\label{Alg:PFLHom}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\boldsymbol{x}^{(0)}$: initial  model shared by local devices, global and local learning rates $\gamma$ and $\eta$, respectively
%\STATE Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\STATE \textbf{for $r=0, \ldots, R-1$ do}
\STATE \textbf{parallel for device $j\in \mathcal{K}^{(r)}$ do}:
\STATE \quad \textbf{if PRIVIX variant:} 
{\small $${\mathbf{\Phi}}^{(r)}\triangleq  {\texttt{PRIVIX}}\left[{\mathbf{S}}^{(r-1)}\right]$$}
\tikzmark{i} 
\STATE \quad \textbf{if HEAPRIX variant:} \label{line:heaprix1}
{\small $${\mathbf{\Phi}}^{(r)}\triangleq \texttt{HEAVYMIX}\left[{\mathbf{S}}^{(r-1)}\right]+\texttt{PRIVIX}\left[{\mathbf{S}}^{(r-1)}- \tilde{\mathbf{S}}^{(r-1)}\right]$$}
\STATE \quad Set $\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma{\mathbf{\Phi}}^{(r)}$ and $\boldsymbol{x}_j^{(0,r)}=\boldsymbol{x}^{(r)}$ 
\tikzmark{j}  
\STATE \quad\textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}
\STATE \qquad Sample a mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}$
\STATE \qquad Update $\boldsymbol{x}^{(\ell+1,r)}_{j}=\boldsymbol{x}^{(\ell,r)}_j-\eta~ \tilde{\mathbf{g}}_{j}^{(\ell,r)}$ \label{eq:update-rule-alg}
\STATE \quad \textbf{end for}
\STATE \quad Device $j$ broadcasts $\mathbf{S}^{(r)}_{j}\triangleq\mathbf{S}_{j}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right)$.
\STATE \quad Server \textbf{computes} $ {\mathbf{S}}^{(r)}=\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbf{S}^{(r)}_{j}$ .
\STATE \quad Server \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to devices in randomly drawn devices $\mathcal{K}^{(r)}$.
\vspace{0.1cm}
\tikzmark{k}  
\STATE \quad \textbf{if HEAPRIX variant:} \label{line:heaprix2}
\STATE \qquad Second round of communication for computing $\mathbf{\delta}_j^{(r)} :=  \mathbf{S}_j\left[\texttt{HEAVYMIX}(\mathbf{S}^{(r)})\right]$ and broadcasts $\tilde{\mathbf{S}}^{(r)}\triangleq\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbf{\delta}_j^{(r)}$ to devices in set $\mathcal{K}^{(r)}$
\tikzmark{l}  
\vspace{0.1cm}
\STATE \textbf{end parallel for}
\STATE \textbf{end}
\STATE \textbf{Output:} ${\boldsymbol{x}}^{(R-1)}$
\vspace{- 0.1cm}
\end{algorithmic}
\end{algorithm}

\vspace{0.05in}
\noindent\textbf{Algorithmic comparison  with~\citep{haddadpour2020federated}}
An important feature of our algorithm is that due to a lower dimension of the count sketch, the resulting averaged quantities ($\mathbf{S}^{(r)}$ and  $\tilde{\mathbf{S}}^{(r)}$) received by the server are also of lower dimension. 
Therefore, our algorithms exploit a bidirectional compression during the communication phases between server and devices. 
As a result, for the case of large quantization error $\omega=\theta(\frac{d}{m})$ as shown in~\citep{haddadpour2020federated}, our algorithms can outperform those developed in~\citep{haddadpour2020federated} if sufficiently large hash tables are used and the uplink communication cost is high. 
Furthermore, while in~\citep{haddadpour2020federated}, server stores a global model and aggregates the partial gradients from devices which can enable the server to extract some information regarding the device's data, in contrast, in our algorithms server does not store the global model and only broadcasts the average sketches. 
% Thus, sketching-based communication FL algorithms such as ours do not reveal the exact values of the inputs.

\begin{remark}
As discussed in~\citep{horvath2020better}, while induced compressors transform a biased compressor into an unbiased one at the cost of doubling the communication cost since the devices need to send $\mathcal{C}_1(\boldsymbol{x})$ and $\mathcal{C}_2\left(\boldsymbol{x}-\mathcal{C}_1\left(\boldsymbol{x}\right)\right)$ separately, we emphasize that with \texttt{HEAPRIX}, due to the use of sketching, the extra communication round cost is compensated with lower number of bits per round thanks to the lower dimension of the sketches.  
\end{remark}

\paragraph{Benefits of \texttt{HEAPRIX} based algorithms:} Corollary~\ref{cor:small} states that, unlike \texttt{PRIVIX}, \texttt{HEAPRIX} compression noise can be made as small as possible using larger hash size. 
In the distributed setting, contrary to \texttt{SKETCHED-SGD}~\citep{ivkin2019communication} where decompressing is happening in the server, \texttt{HEAPRIX} does not require to have access to exact $\text{top}_m$ values of the input. This is because \texttt{HEAPRIX} uses \texttt{HEAVYMIX} \emph{ where decompressing is performed at each device locally, thus not requiring server to have exact values of gradients of each device}.
In other words, \texttt{HEAPRIX}-based algorithm leverages the best of both: the \emph{unbiasedness} of \texttt{PRIVIX} while using \emph{heavy hitters}~as~in~\texttt{HEAVYMIX}. 


\begin{algorithm}[t]
\DrawBox[draw=white,fill=blue!6]{a}{b}
\DrawBoxx[draw=white,fill=blue!6]{c}{d}
\caption{\texttt{FedSKETCHGATE}($R$, $\tau, \eta, \gamma$)}\label{Alg:PFLHet}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\boldsymbol{x}^{(0)}=\boldsymbol{x}^{(0)}_j$ shared by all local devices, global and local learning rates $\gamma$ and $\eta$.
\STATE \textbf{for $r=0, \ldots, R-1$ do}
\STATE   \textbf{parallel for device $j=1,\ldots,p$ do}:
\STATE  \quad \textbf{if PRIVIX variant:} \label{line:cj_privix}
{\small$$\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left[{\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}\right)-{\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}_{j}\right)\right]$$} {\small where ${\mathbf{\Phi}}^{(r)}\triangleq \texttt{PRIVIX}(\mathbf{S}^{(r-1)})$}
\vspace{0.1cm}
\tikzmark{a}  
\STATE  \quad \textbf{if HEAPRIX variant:} 
{\small $$\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left(\mathbf{\Phi}^{(r)}-\mathbf{\Phi}^{(r)}_j\right)$$}
\tikzmark{b}
\STATE  \quad Set $\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma\mathbf{\Phi}^{(r)}$ and $\boldsymbol{x}_j^{(0,r)}=\boldsymbol{x}^{(r)}$ 
\STATE \quad \textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}
\STATE  \qquad Sample mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}$
\STATE \qquad $\boldsymbol{x}^{(\ell+1,r)}_{j}=\boldsymbol{x}^{(\ell,r)}_j-\eta \left( \tilde{\mathbf{g}}_{j}^{(\ell,r)}-\mathbf{c}_j^{(r)}\right)$ \label{eq:update-rule-alg-heter1}
\STATE \quad \textbf{end for}
\STATE \quad Device $j$ broadcasts $\mathbf{S}^{(r)}_{j}\triangleq\mathbf{S}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right)$.
\STATE \quad Server \textbf{computes} $ {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}\mathbf{S}^{(r)}_{j}$ and  \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.
\vspace{0.1cm}
\tikzmark{c}  
\STATE  \quad \textbf{if HEAPRIX variant:} 
\STATE  \qquad Device $j$ computes $\mathbf{\Phi}^{(r)}_j\triangleq \texttt{HEAPRIX}[\mathbf{S}_j^{(r)}]$.
\STATE  \qquad Second round of communication to obtain $\mathbf{\delta}_j^{(r)} :=  \mathbf{S}_j\left(\texttt{HEAVYMIX}[\mathbf{S}^{(r)}]\right)$.  \label{line:tildeS}
\STATE  \qquad Broadcasts $\tilde{\mathbf{S}}^{(r)}\triangleq\frac{1}{p}\sum_{j=1}^p\mathbf{\delta}_j^{(r)}$ to devices.
\tikzmark{d}  
\vspace{0.1cm}
\STATE  \textbf{end parallel for}
\STATE \textbf{end}
\STATE \textbf{Output:} ${\boldsymbol{x}}^{(R-1)}$
\vspace{- 0.1cm}
\end{algorithmic}
\end{algorithm}
\vspace{-0.1in}

\vspace{-0.05in}
\subsection{Heterogeneous Setting}
\vspace{-0.05in}
In this section, we focus on the optimization problem of (\ref{eq:main}) where $q_1=\ldots=q_p=\frac{1}{p}$ with full device participation ($k=p$). 
These results can be extended to the scenario with devices sampling.
%In the previous section, we discussed Algorithm~\texttt{FedSKETCH}, which is originally designed for homogeneous setting.
For non i.i.d. data, the \texttt{FedSKETCH} algorithm, designed for homogeneous setting, may fail to perform well in practice. 
The main reason is that in FL, devices are using local stochastic descent direction which could be different than global descent direction when the data distribution are non-identical. 
For that reason, to mitigate the negative impact of data heterogeneity, we introduce a new algorithm called \texttt{FedSKETCHGATE} described in Algorithm~\ref{Alg:PFLHet}. 
% This algorithm leverages the idea of
The main idea is that using an approximation of the global gradient, $\mathbf{c}_j^{(r)}$ allows to correct the local gradient direction, see  gradient tracking technique applied in~\citep{liang2019variance,haddadpour2020federated}. 
Using \texttt{PRIVIX} variant, the correction vector $\mathbf{c}_j^{(r)}$ at device $j$ and communication round $r$ is computed in Line~\ref{line:cj_privix}. 
Using \texttt{HEAPRIX} variant, \texttt{FedSKETCHGATE} updates $\tilde{\mathbf{S}}^{(r)}$ via Line~\ref{line:tildeS}.


\begin{remark}\label{rmk:bidirect}
Most of the existing communication-efficient algorithms with compression only consider gradient-compression from devices to server. 
However, Algorithms~\ref{Alg:PFLHom} and~\ref{Alg:PFLHet} improve the communication efficiency from server to devices as it exploits low-dimensional sketches in a bidirectional manner. 
\end{remark}
For both \texttt{FedSKETCH} and \texttt{FedSKETCHGATE}, \texttt{HEAPRIX} variant requires a second round of communication, unlike \texttt{PRIVIX}. 
Therefore, in Cross-Device FL setting, where there could be millions of devices, \texttt{HEAPRIX} variant may not be practical, and we note that it could be more suitable for Cross-Silo FL setting. 
%\begin{algorithm}[t]
%%\DrawBox[draw=white,fill=blue!6]{a}{b}
%%\DrawBoxx[draw=white,fill=blue!6]{c}{d}
%\caption{\texttt{FedSKETCHGATE}($R$, $\tau, \eta, \gamma$)}\label{Alg:PFLHet}
%\begin{algorithmic}[1]
%\STATE \textbf{Inputs:} $\boldsymbol{x}^{(0)}=\boldsymbol{x}^{(0)}_j$ shared by all local devices, global and local learning rates $\gamma$ and $\eta$.
%\STATE \textbf{for $r=0, \ldots, R-1$ do}
%\STATE   \textbf{parallel for device $j=1,\ldots,p$ do}:
%\STATE  \hspace{0.02in} \textbf{if PRIVIX variant:} \label{line:cj_privix}
%{\small$$\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left[{\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}\right)-{\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}_{j}\right)\right]$$}
%\STATE {\small where ${\mathbf{\Phi}}^{(r)}\triangleq \texttt{PRIVIX}(\mathbf{S}^{(r-1)})$}
%\vspace{0.1cm}
%%\tikzmark{a}  
%\STATE  \hspace{0.02in} \textbf{if HEAPRIX variant:} 
%{\small $$\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left(\mathbf{\Phi}^{(r)}-\mathbf{\Phi}^{(r)}_j\right)$$}
%%\tikzmark{b}
%\STATE  Set $\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma\mathbf{\Phi}^{(r)}$ and $\boldsymbol{x}_j^{(0,r)}=\boldsymbol{x}^{(r)}$ 
%\STATE \hspace{0.02in} \textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}
%\STATE  $\quad  $Sample mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}$
%\STATE $\quad  $$\boldsymbol{x}^{(\ell+1,r)}_{j}=\boldsymbol{x}^{(\ell,r)}_j-\eta \left( \tilde{\mathbf{g}}_{j}^{(\ell,r)}-\mathbf{c}_j^{(r)}\right)$ \label{eq:update-rule-alg-heter1}
%\STATE \hspace{0.05in}\textbf{end for}
%\STATE Device $j$ broadcasts $\mathbf{S}^{(r)}_{j}\triangleq\mathbf{S}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right)$.
%\STATE Server \textbf{computes} $ {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}\mathbf{S}^{(r)}_{j}$ and  \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.
%\vspace{0.1cm}
%%\tikzmark{c}  
%\STATE  \hspace{0.04in}\textbf{if HEAPRIX variant:} 
%\STATE  Device $j$ computes $\mathbf{\Phi}^{(r)}_j\triangleq \texttt{HEAPRIX}[\mathbf{S}_j^{(r)}]$
%\STATE  Second round of communication to obtain $\mathbf{\delta}_j^{(r)} :=  \mathbf{S}_j\left(\texttt{HEAVYMIX}[\mathbf{S}^{(r)}]\right)$  \label{line:tildeS}
%\STATE  Broadcasts $\tilde{\mathbf{S}}^{(r)}\triangleq\frac{1}{p}\sum_{j=1}^p\mathbf{\delta}_j^{(r)}$ to devices
%%\tikzmark{d}  
%\vspace{0.1cm}
%\STATE \textbf{end parallel for}
%\STATE \textbf{end}
%\STATE \textbf{Output:} ${\boldsymbol{x}}^{(R-1)}$
%\vspace{- 0.1cm}
%\end{algorithmic}
%\end{algorithm}




%Note that these algorithms are more applicable to cross-silo setting where the number of devices are not extremely large and most of them are available.
%For the \texttt{FedSKETCHGATE} with \texttt{PRIVIX} variant, the correction vector $\mathbf{c}_j^{(r)}$ at device $j$ and communication round $r$ is computed using the update rule $\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left({\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}\right)-{\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}_{j}\right)\right)$ where $\mathbf{S}^{(r-1)}_{j}\triangleq\mathbf{S}\left(\boldsymbol{x}_j^{(0,r-1)}-~{\boldsymbol{x}}_{j}^{(\tau,r-1)}\right)$ is computed and stored at device $j$ from previous communication round $r-1$. 
%The term $\mathbf{S}^{(r-1)}$ is computed similar to \texttt{FedSKETCH} in \eqref{eq:average-skestching}. 
%For \texttt{FedSKETCHGATE}, the server needs to compute $\tilde{\mathbf{S}}^{(r)}$ using \eqref{eq:glbl-updts}. 
%Then, device $j$ computes $\mathbf{\Phi}_j\triangleq \texttt{HEAPRIX}[\mathbf{S}_j^{(r)}]$ and $  {\mathbf{\Phi}}\triangleq \texttt{HEAPRIX}(\mathbf{S}^{(r-1)})$ and updates the correction vector $\mathbf{c}_j^{(r)}$ using the recursion $\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left(\mathbf{\Phi}-\mathbf{\Phi}_j\right)$.
%\begin{algorithm*}[t]
%\caption{\texttt{FedSKETCHGATE}($R$, $\tau, \eta, \gamma$): Private Federated Learning with Sketching and gradient tracking. }\label{Alg:PFLHet}
%\begin{algorithmic}[1]
%\State{\textbf{Inputs:} $\boldsymbol{x}^{(0)}=\boldsymbol{x}^{(0)}_j$ shared by all local devices, communication rounds $R$, local updates $\tau$, global and local learning rates $\gamma$ and $\eta$.}
%\State{\textbf{for $r=0, \ldots, R-1$ do}}
%\State{$\qquad$\textbf{parallel for device $j=1,\ldots,p$ do}:}
%\State{$\qquad \quad$ \textbf{if PRIVIX variant:} }
%\State{$\qquad\qquad$ Set $\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left({\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}\right)-{\texttt{PRIVIX}}\left(\mathbf{S}^{(r-1)}_{j}\right)\right)$}\label{line:cj_privix}
%
%\State{$\qquad\qquad$ Computes ${\mathbf{\Phi}}^{(r)}\triangleq \texttt{PRIVIX}(\mathbf{S}^{(r-1)})$}
%
%\State{$\qquad \quad$ \textbf{if HEAPRIX variant:} }
%\State{$\qquad\qquad$ Set $\mathbf{c}_j^{(r)}=\mathbf{c}_j^{(r-1)}-\frac{1}{\tau}\left(\mathbf{\Phi}^{(r)}-\mathbf{\Phi}^{(r)}_j\right)$}
%\State{$\qquad\quad \quad$ Computes ${\mathbf{\Phi}}^{(r)}\triangleq \texttt{HEAVYMIX}\left[{\mathbf{S}}^{(r-1)}\right]+\texttt{PRIVIX}\left[{\mathbf{S}}^{(r-1)}- \tilde{\mathbf{S}}^{(r-1)}\right]$}
%
%\State{$\qquad\quad$ Set $\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma\mathbf{\Phi}^{(r)}$ and $\boldsymbol{x}_j^{(0,r)}=\boldsymbol{x}^{(r)}$ }
%\State{$\qquad\quad $\textbf{for} $\ell=0,\ldots,\tau-1$ \textbf{do}}
%\State{$\qquad\quad\quad$ Sample a mini-batch $\xi_j^{(\ell,r)}$ and compute $\tilde{\mathbf{g}}_{j}^{(\ell,r)}\triangleq\nabla{f}_j(\boldsymbol{x}^{(\ell,r)}_j,\xi_j^{(\ell,r)})$}
%\State{$\qquad\quad\quad$ $\boldsymbol{x}^{(\ell+1,r)}_{j}=\boldsymbol{x}^{(\ell,r)}_j-\eta \left( \tilde{\mathbf{g}}_{j}^{(\ell,r)}-\mathbf{c}_j^{(r)}\right)$ \label{eq:update-rule-alg-heter1}}
%\State{$\qquad\quad$\textbf{end for}}
%\State{$\qquad\quad\quad$Device $j$ sends $\mathbf{S}^{(r)}_{j}\triangleq\mathbf{S}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(\tau,r)}\right)$ back to the server.}
%\State{$\qquad$Server \textbf{computes} }
%\State{$\qquad\qquad {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}\mathbf{S}^{(r)}_{j}$ and  \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.}
%\vspace{0.1cm}
%\State{$\qquad$ \textbf{if HEAPRIX variant:} }
%\State{$\qquad\quad\quad$ Device $j$ computes $\mathbf{\Phi}^{(r)}_j\triangleq \texttt{HEAPRIX}[\mathbf{S}_j^{(r)}]$}
%\State{$\qquad \qquad$ Second round of communication to obtain $\delta_j^{(r)} :=  \mathbf{S}_j\left(\texttt{HEAVYMIX}[\mathbf{S}^{(r)}]\right)$ } \label{line:tildeS}
%\State{$\qquad\qquad$ Broadcasts $\tilde{\mathbf{S}}^{(r)}\triangleq\frac{1}{p}\sum_{j=1}^p\delta_j^{(r)}$ to devices}
%
%\State{$\qquad$\textbf{end parallel for}}
%\State{\textbf{end}}
%\State{\textbf{Output:} ${\boldsymbol{x}}^{(R-1)}$}
%\vspace{- 0.1cm}
%\end{algorithmic}
%\end{algorithm*}

\vspace{-0.05in}
\section{Convergence Analysis}\label{sec:cnvg-an}
\vspace{-0.05in}
We first state commonly used assumptions required in the following convergence analysis (reminder of our notations can be found Table~\ref{tab:notations} of the Appendix).
%\vspace{-0.05in}
%\subsection{Common Assumptions}
%\vspace{-0.05in}
\begin{assumption}[Smoothness and Lower Boundedness]\label{Assu:1}
The local objective function $f_j(\cdot)$ of device $j$ is differentiable for $j\in [p]$ and $L$-smooth, i.e., $\|\nabla f_j(\boldsymbol{x})-\nabla f_j(\mathbf{y})\|\leq L\|\boldsymbol{x}-\mathbf{y}\|,\: \forall \;\boldsymbol{x},\mathbf{y}\in\mathbb{R}^d$. Moreover, the optimal objective function $f(\cdot)$ is bounded below by ${f^*} := \min_{\boldsymbol{x}} f(\boldsymbol{x})>-\infty$. 
\end{assumption}

%Assumption~\ref{Assu:1} is common in stochastic optimization. 
We present our results for PL, convex and general non-convex objectives. \citep{karimi2016linear} show that PL condition implies strong convexity property with same module (PL objectives can also be non-convex, hence strong convexity does not imply the PL condition necessarily).

\vspace{-0.05in}
\subsection{Convergence of  \texttt{FEDSKETCH} } 
\vspace{-0.05in}
We start with the homogeneous case where data is i.i.d. among local devices, and therefore, the stochastic local gradient of each worker is an unbiased estimator of the global gradient.
We make the following assumption under that setting:
\begin{assumption}[Bounded Variance]\label{Assu:1.5}
For all $j\in [m]$, we can sample an independent mini-batch $\ell_j$ of size $|\xi_j^{(\ell,r)}| = b$ and compute an unbiased stochastic gradient  $\tilde{\mathbf{g}}_j = \nabla f_j(\boldsymbol{x}; \xi_j)$, $\mathbb{E}_{\xi_j}[\tilde{\mathbf{g}}_j] = \nabla f(\boldsymbol{x})=\mathbf{g}$ with the variance bounded by a constant $\sigma^2$, i.e., $\mathbb{E}_{\xi_j}\left[\|\tilde{\mathbf{g}}_j-\mathbf{g}\|^2\right]\leq \sigma^2$.
\end{assumption}
%{\color{blue}In this part due to space limitation we only provide the convergence analysis only for PL and we refer reader to the Section~\ref{sec:gen-proof-hom} in Appendix for the convergence analysis of general non-convex and convex objectives.}
%Due to space limitation, we refer the readers to Section~\ref{sec:gen-proof} in the Appendix for convex objectives.
\begin{theorem}\label{thm:homog_case}
Suppose Assumptions~\ref{Assu:1}-\ref{Assu:1.5} holds. 
Given $0<m\leq d$ and considering Algorithm~\ref{Alg:PFLHom} with sketch size $B=O\left(m\log\left(\frac{d R}{\delta}\right)\right)$ and $\gamma\geq k$, with probability $1-\delta$ we have: 

In the \textbf{non-convex} case, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies  $\frac{1}{R}\sum_{r=0}^{R-1}\mathbb{E}\left[\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2\right]\leq {\epsilon}$ if: 
 
\noindent $\bullet$ \texttt{FS-PRIVIX}, for $\eta=\frac{1}{L\gamma}\sqrt{\frac{1}{R\tau\left(\frac{c d}{m }+\frac{1}{k}\right)}}$: $R=O\left(1/\epsilon\right)$ and $\tau=O\left(( \frac{d}{m}+\frac{1}{k})/(\epsilon)\right)$.

\noindent $\bullet$ \texttt{FS-HEAPRIX}, for $\eta=\frac{1}{L\gamma}\sqrt{\frac{1}{R\tau\left(\frac{cd-m}{m}+\frac{1}{k}\right)}}$: $R=O\left(1/\epsilon\right)$ and $\tau=O\left(( \frac{c d-m}{m}+\frac{1}{k})/\epsilon\right)$.

In the \textbf{PL or strongly convex} case, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies $\mathbb{E}[f({\boldsymbol{x}}^{(R-1)})-f({\boldsymbol{x}}^{(*)})]\leq \epsilon$ if  we set:  

%\noindent {\color{blue}$\bullet$ \texttt{FS-PRIVIX}, for $\eta=1/(2L(cd/mk+1)\tau\gamma)$:    
\noindent $\bullet$ \texttt{FS-PRIVIX}, for $\eta=\frac{1}{2L(cd/mk+1)\tau\gamma}$: $R=O\left(\left(d/m+\frac{1}{k}\right)\kappa\log\left(1/\epsilon\right)\right)$ and $\tau=O\left((d/m+1)\Big/\left(d/m+1/k\right)\epsilon\right)$.

\noindent $\bullet$ \texttt{FS-HEAPRIX}, for $\eta=\frac{1}{2L\left((cd-m)/m+1/k\right)\tau\gamma}$: $R = O\left(\left((d-m)/m+1/k\right)\kappa\log\left(1/\epsilon\right)\right)$ and $ \tau =O\left(d/m\Big/\left(\left((d/m-1)+1/k\right)\epsilon\right)\right)$.

%In the \textbf{Convex} case, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies $ \mathbb{E}\Big[f({\boldsymbol{x}}^{(R-1)})-f({\boldsymbol{x}}^{(*)})\Big]\leq \epsilon$ if we set:  

%\noindent {\color{blue}$\bullet$ \texttt{FS-PRIVIX}, for $\eta=1/(2L(cd/mk+1)\tau\gamma)$:    
%\noindent $\bullet$ \texttt{FS-PRIVIX}, for $\eta=\frac{1}{2L(cd/m+1/k)\tau\gamma}$: $R =O\left(L\left(1/k+d/m\right)/\epsilon\log\left(1/\epsilon \right)\right)$ and $\tau=O\left(1/\epsilon^2\right)$.

%\noindent $\bullet$ \texttt{FS-HEAPRIX}, for $\eta=\frac{1}{2L\left((cd-m)/mk+1\right)\tau\gamma}$:   $R =O\left(L\left(1/k+(d-m)/m\right)/\epsilon\log\left(1/\epsilon \right)\right)$ and $\tau=O\left(1/\epsilon^2\right)$.


%\todo{Move convex section to the appendix!}
\end{theorem}


The bounds in Theorem~\ref{thm:homog_case} suggest that, under the homogeneous setting, if we set $d=m$, i.e. no compression, the number of communication rounds to achieve the $\epsilon$-error matches with the number of iterations required to achieve the same error under a centralized setting. 
Furthermore, we can see that \texttt{FS-HEAPRIX} either improves communication or computational complexity over \texttt{FS-PRIVIX}. 
Additionally, computational complexity scales down with the number of sampled devices. 
To further stress on the impact of using sketching methods, we also compare our results with prior works in terms of total number of communicated bits per device. 
Please refer to Section~\ref{thm:homog_case_append} of the Appendix for the convergence bounds using convex objectives.

\paragraph{Comparison with~\citep{ivkin2019communication}}
From a privacy aspect, we note that~\citep{ivkin2019communication} requires for the central server to have access to exact values of top$_m$ gradients, hence does not preserve privacy, whereas our schemes do not need those exact values. 
From a communication cost point of view, for strongly convex objective and compared to~\citep{ivkin2019communication}, we improve the total communication per worker from 
$RB=O\left(\frac{ d}{\epsilon}\log\left(\frac{d}{\delta\sqrt{\epsilon}}\max\left(\frac{ d}{m},\frac{1}{\sqrt{\epsilon}}\right)\right)\right)$ to 
\[ \Resize{8cm}{RB=O\left(\kappa(d-m+\frac{m}{k})\log\frac{1}{\epsilon}\log\left(\frac{\kappa d}{\delta}(\frac{d-m}{m}+1/k)\log\frac{1}{\epsilon}\right)\right)}\ .\]
%Similar comparison for PL objectives is summarized in Table~\ref{table:1}. 
We note that while reducing communication cost, our scheme requires $\tau=O(d/m((\frac{ d-m}{m}+1/k)\epsilon))>1$, which scales down with the number of sampled devices $k$. 
Moreover, unlike~\citep{ivkin2019communication}, we do not use the classical bounded gradient assumption and thus obtain stronger results with weaker assumptions. 
Regarding general non-convex objectives, our result improves the total communication cost per worker displayed in~\citep{ivkin2019communication} from $RB=O\left(\max(\frac{1}{\epsilon^2},\frac{d^2}{k^2\epsilon}\right)\log(\frac{d}{\delta}\max(\frac{1}{\epsilon^2},\frac{d^2}{k^2\epsilon})))$ for \emph{only one device} to $RB=O(\frac{m}{\epsilon}\log(\frac{d}{\epsilon\delta}))$. 
We also highlight that we can obtain similar rates for Algorithm~\ref{Alg:PFLHom} in heterogeneous environment if we make the additional uniform boundedness gradient assumption.

\textbf{Note:}
Such improved communication cost over prior works can be explained by -- the joint exploitation of \emph{sketching} -- the reduction of the dimension of communicated messages -- the use of \emph{local updates} -- the reduction of the number of communication rounds reaching a specific convergence error.
%\begin{table*}[ht]
%    \centering
%    \caption{Comparison of results with compression and periodic averaging in the homogeneous setting. Here, $p$ is the number of devices, $\mu$ is the PL constant, $m$ is the number of bins of hash tables, $d$ is the dimension of the model, $\kappa$ is the condition number, $\epsilon$ is the target accuracy, $R$ is the number of communication rounds, and $\tau$ is the number of local updates. UG and PP stand for Unbounded Gradient and Privacy Property respectively.}
%\label{table:1}
%    \resizebox{1.0\linewidth}{!}{
%    \begin{tabular}{llll}
%        \toprule
%        Reference        & PL/Strongly Convex   & UG & PP
%        \\
%        %\midrule
%        %\makecell{\citep{li2019privacy}}  & \makecell[l]{$-$}   & \makecell[l]{$-$}               & \makecell{$R\!=\!O\left(\frac{\mu^2 d}{\epsilon^{2}}\right)$ \\ $\tau\!=\!1\\
%        %B=O\left(k\log\left(\frac{dR}{\delta}\right)\right)$\\
%        %$pRB=O\left(\frac{p\mu^2 d}{\epsilon^{2}}k\log\left(\frac{\mu^2d^2}{\epsilon^2\delta}\right)\right)$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{52}}
%        %\\
%
%        \midrule
%        \makecell{\textbf{Ivkin et al.~\citep{ivkin2019communication}}}  &  \makecell[l]{$R=O\left(\max\left(\frac{\mu^2 d}{\sqrt{\epsilon}},\frac{1}{\epsilon}\right)\right)$,\: $\tau=1$,\: $B=O\left(m\log\left(\frac{dR}{\delta}\right)\right)$\\
%        $pRB=O\left(\frac{p\mu^2 d}{\epsilon}m\log\left(\frac{d}{\delta\sqrt{\epsilon}}\max\left({\mu^2 d},\frac{1}{\sqrt{\epsilon}}\right)\right)\right)$}                                                                                           & \makecell{\ding{55}} & \makecell{\ding{55}}
%        \\
%        
%        %\midrule
%        %\makecell{\citep{karimireddy2019scaffold}}  & \makecell[l]{$R=O\left(\frac{1}{\epsilon}\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
%        %$B=O\left(d\right)$\\
%        %$pRB=O\left(\frac{pd}{\epsilon}\right)$}   & \makecell[l]{$R=O\left(\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
%        %$B=O\left(d\right)$\\
%        %$pRB=O\left(p\kappa d\log\left(\frac{1}{\epsilon}\right)\right)$}               & \makecell{$R=O\left(\frac{1}{\epsilon}\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
%        %$B=O\left(d\right)$\\
%        %$pRB=O\left(\frac{pd}{\epsilon}\right)$}                                                                            & \makecell{\ding{52}} & \makecell{\ding{55}}
%        %\\
%        \midrule
%    \makecell{\textbf{Theorem~\ref{thm:homog_case}}} %& \makecell[l]{$\boldsymbol{R=O\left(\frac{1}{\epsilon}\right)}$ \\[3pt] $\boldsymbol{\tau=O\left(\frac{\mu^2d}{k\epsilon}\right)}$\\[3pt]
%       %$\boldsymbol{B=O\left(m\log\left(\frac{dR}{\delta}\right)\right)}$\\[3pt]
%       %$\boldsymbol{kBR=O\left(\frac{mk}{\epsilon}\log\left(\frac{d}{\epsilon\delta}\right)\right)}$}  
%       & \makecell[l]{$\boldsymbol{R=O\left(\kappa\left(\frac{\mu^2 d-1}{k}+1\right)\log\left(\frac{1}{\epsilon}\right)\right)}$,\: $\boldsymbol{\tau=O\left(\frac{\left({\mu^2 d}\right)}{k\left(\frac{\mu^2 d}{k}+1\right)\epsilon}\right)}$,\:$\boldsymbol{B=O\left(m\log\left(\frac{dR}{\delta}\right)\right)}$\\
%       $\boldsymbol{kBR=O\left({m}\kappa(\mu^2d-1+k)\log\frac{1}{\epsilon}\log\left(\frac{\kappa(d\frac{\mu^2d-1}{k}+d)\log\frac{1}{\epsilon}}{\delta}\right)\right)}$}                                                                                   & \makecell{\ding{52}} & \makecell{\ding{52}}   \\
%        \bottomrule
%    \end{tabular}
%    }
%\end{table*}
%\end{comment}


\vspace{-0.05in}
\subsection{Convergence of  \texttt{FedSKETCHGATE}} 
\vspace{-0.05in}

We start with a bounded local variance assumption:
\begin{assumption}[Bounded Local Variance]\label{Assu:2}
For all $j\in [p]$, we can sample an independent mini-batch $\Xi_j$ of size $|{\xi}_j| = b$ and compute an unbiased stochastic gradient $\tilde{\mathbf{g}}_j = \nabla f_j(\boldsymbol{x}; \xi_j)$ with $\mathbb{E}_{\xi}[\tilde{\mathbf{g}}_j] = \nabla f_{j}(\boldsymbol{x})={\mathbf{g}}_j$. 
Moreover, the variance of local stochastic gradients is bounded such that $
\mathbb{E}_{\xi}\left[\|\tilde{\mathbf{g}}_j-{\mathbf{g}}_j\|^2\right]\leq \sigma^2$.
\end{assumption}
%Due to space limitation, we refer the readers to Section~\ref{sec:gen-proof} in the Appendix for the convergence analysis of general PL or strongly objective.
\begin{theorem}\label{thm:hetreg_case}
Suppose Assumptions~\ref{Assu:1} and~\ref{Assu:2} hold. Given $0<m\leq d$, and considering \texttt{FedSKETCHGATE} in Algorithm~\ref{Alg:PFLHet} with sketch size $B=O\left(m\log\left(\frac{d R}{\delta}\right)\right)$ and $\gamma\geq p$ with probability $1-\delta$ we have  


%%%%%%%%%%%%%%%

In the \textbf{non-convex} case, $\eta=\frac{1}{L\gamma}\sqrt{\frac{mp}{R\tau\left(cd\right)}}$, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies  $\frac{1}{R}\sum_{r=0}^{R-1}\mathbb{E}\left[\left\|\nabla f({\boldsymbol{x}}^{(r)})\right\|_2^2\right]\leq {\epsilon}$ if: 
 
\noindent $\bullet$ \texttt{FS-PRIVIX:}    
\begin{equation}\notag
R=O((d+m)/m\epsilon) \quad \textrm{and} \quad \tau=O(1/(p\epsilon))\ .
 \end{equation}
\noindent $\bullet$ \texttt{FS-HEAPRIX:} $R=O(d/m\epsilon)$ and $\tau=O(1/(p\epsilon))$.

In the \textbf{PL or Strongly convex} case, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies $\mathbb{E}\Big[f({\boldsymbol{x}}^{(R-1)})-f({\boldsymbol{x}}^{(*)})\Big]\leq \epsilon$ if: 

\noindent $\bullet$ \texttt{FS-PRIVIX}, for $\eta=1/(2L(\frac{cd}{m}+1)\tau\gamma)$: $R=O\left((\frac{d}{m}+1)\kappa \log(1/\epsilon)\right)$ and $\tau=O\left(1/(p\epsilon)\right)$

\noindent $\bullet$ \texttt{FS-HEAPRIX}, for $\eta=m/(2cLd\tau\gamma)$: $R=O\left((\frac{d}{m})\kappa \log(1/\epsilon)\right)$ and $\tau=O\left(1/(p\epsilon)\right)$.

% In the \textbf{convex} case, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies $ \mathbb{E}[f({\boldsymbol{x}}^{(R-1)})-f({\boldsymbol{x}}^{(*)})]\leq \epsilon$ if: 
 
%\noindent $\bullet$ \texttt{FS-PRIVIX}, for $\eta=1/(2L(c d/m+1)\tau\gamma)$:  $R=O\left(L(d/m+1)\epsilon \log(1/\epsilon)\right)$ and $\tau=O\left(1/(p\epsilon^2)\right)$.

%\noindent $\bullet$ \texttt{FS-HEAPRIX}, for $\eta=m/(2L c d\tau\gamma)$: $R=O\left(L(d/m)\epsilon \log(1/\epsilon)\right)$ and $\tau=O\left(1/(p\epsilon^2)\right)$.
  %%%%%%%%%%%%
 \end{theorem}







%\begin{table*}[ht]
%    \centering
%    \caption{Comparison of results with compression and periodic averaging in the heterogeneous setting. Here, $p$ is the number of devices, $\mu$ is compression of hash table, $d$ is the dimension of the model, $\kappa$ is condition number, $\epsilon$ is target accuracy, $R$ is  the number of communication rounds, and $\tau$ is the number of local updates. UG and PP stand for Unbounded Gradient and Privacy Property respectively.}
%\label{table:2}
%    \resizebox{1.0\linewidth}{!}{
%    \begin{tabular}{lllll}
%        \toprule
%%                    &  \multicolumn{3}{c}{Objective function} &
%%        \\ \cmidrule(r){2-4}
%        Reference        & non-convex                                        & General Convex   & UG & PP
%        \\
%        \midrule
%        \makecell{\textbf{Li et al.~\citep{li2019privacy}}}  & \makecell[l]{$-$}                & \makecell[l]{$R\!=\!O\left(\frac{\mu^2 d}{\epsilon^{2}}\right)$ \\ $\tau\!=\!1$\\
%        $B=O\left(m\log\left(\frac{\mu^2d^2}{\epsilon^2\delta}\right)\right)$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{52}}
%        \\
%
%        %\midrule
%        %\makecell{\citep{ivkin2019communication}}  & \makecell[l]{$-$}   & \makecell[l]{$R=O\left(\frac{\mu^2 d}{\epsilon}\right)$\\  $\tau=1$\\ $B=O\left(k\log\left(\frac{dR}{\delta}\right)\right)$\\
%        %$pRB=O\left(\frac{p\mu^2 d}{\epsilon}k\log\left(\frac{\mu^2d^2}{\epsilon\delta}\right)\right)$}               & \makecell{$-$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{55}}
%        %\\
%        
%        \midrule
%        \makecell{\textbf{Rothchild et al.~\citep{rothchild2020fetchsgd}}}  & \makecell[l]{$R=O\left(\max(\frac{1}{\epsilon^2},\frac{d^2-md}{m^2\epsilon})\right)$ \\ $\tau=1$\\
%        $B=O\left(m\log\left(\frac{d}{\epsilon^2\delta}\right)\right)$\\
%        $BR=O\left(\frac{m}{\epsilon^2}\max(\frac{1}{\epsilon^2},\frac{d^2-md}{m^2\epsilon})\log\left(\frac{d}{\delta}\max(\frac{1}{\epsilon^2},\frac{d^2-md}{m^2\epsilon})\right)\right)$}       & \makecell[l]{$-$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{55}}
%        \\
%        \midrule
%        \makecell{\textbf{Rothchild et al.~\citep{rothchild2020fetchsgd}}}  & \makecell[l]{$R=O\left(\frac{\max(I^{2/3},2-\alpha)}{\epsilon^3}\right)$ \\ $\tau=1$\\
%        $B=O\left(\frac{m}{\alpha}\log\left(\frac{d\max(I^{2/3},2-\alpha)}{\epsilon^3\delta}\right)\right)$\\
%        $BR=O\left(\frac{m\max(I^{2/3},2-\alpha)}{\epsilon^3\alpha}\log\left(\frac{d\max(I^{2/3},2-\alpha)}{\epsilon^3\delta}\right)\right)$
%        }       & \makecell[l]{$-$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{55}}
%%        \\
%%        \midrule
%%       \makecell{\textbf{Theorem~\ref{thm:hetreg_case}}} & \makecell[l]{$\boldsymbol{R=O\left(\frac{\mu^2d+1}{\epsilon}\right)}$ \\[3pt] $\boldsymbol{\tau=O\left(\frac{1}{p\epsilon}\right)}$\\[3pt]
%%       $\boldsymbol{B=O\left(m\log\left(\frac{\mu^2d^2+d}{\epsilon\delta}\right)\right)}$\\[3pt]
%%       $\boldsymbol{BR=O\left(\frac{m\left(\mu^2d+1\right)}{\epsilon}\log\left(\frac{\mu^2d^2+d}{\epsilon\delta}\log\left(\frac{1}{\epsilon}\right)\right)\right)}$
%%       }   & 
%%       \makecell[l]{$\boldsymbol{R\!=\!O\left(\frac{1+\mu^2d}{\epsilon}{\color{black}\log\left(\frac{1}{\epsilon}\right)}\right)}$\\[3pt]
%%       $\boldsymbol{\tau\!=\!O\left(\frac{1}{p\epsilon^2}\right)}$\\[3pt]
%%       $\boldsymbol{B=O\left(m\log\left(\frac{\mu^2d^2+d}{\epsilon\delta}\log\left(\frac{1}{\epsilon}\right)\right)\right)}$
%%}                                                                            & \makecell{\ding{52}} & \makecell{\ding{52}}
%   \\
%        \midrule
%              \makecell{\textbf{Theorem~\ref{thm:hetreg_case}}} & \makecell[l]{$\boldsymbol{R=O\left(\frac{\mu^2d}{\epsilon}\right)}$ \\[3pt] $\boldsymbol{\tau=O\left(\frac{1}{p\epsilon}\right)}$\\[3pt]
%       $\boldsymbol{B=O\left(m\log\left(\frac{\mu^2d^2}{\epsilon\delta}\right)\right)}$\\[3pt]
%       $\boldsymbol{BR=O\left(\frac{m\left(\mu^2d\right)}{\epsilon}\log\left(\frac{\mu^2d^2}{\epsilon\delta}\log\left(\frac{1}{\epsilon}\right)\right)\right)}$}   & \makecell[l]{$\boldsymbol{R\!=\!O\left(\frac{\mu^2d}{\epsilon}{\color{black}\log\left(\frac{1}{\epsilon}\right)}\right)}$\\[3pt]
%       $\boldsymbol{\tau\!=\!O\left(\frac{1}{p\epsilon^2}\right)}$\\[3pt]
%       $\boldsymbol{B=O\left(m\log\left(\frac{\mu^2d^2}{\epsilon\delta}\right)\right)}$}                                                                            & \makecell{\ding{52}} & \makecell{\ding{52}}
%   \\
%        \bottomrule
%    \end{tabular}
%    }
%\end{table*}

Theorem~\ref{thm:hetreg_case} implies that the number of communication rounds and local updates are similar to the corresponding quantities in homogeneous setting except for the non-convex case where the number of  rounds also depends on the compression rate (summarized Table~\ref{table:1}-\ref{table:2} of the Appendix). For the convergence result of convex objectives please see Section~\ref{thm:homog_case_append} in appendix.


{
We note that the convergence analysis of FS-\texttt{PRIVIX} provided in \citep{li2019privacy} for convex objectives is further tightened in our contribution. 
Moreover, FS-\texttt{HEAPRIX} improves the communication complexity of FS-\texttt{PRIVIX} for both PL and non-convex objectives which is empirically validated in Figures~\ref{fig:MNIST-iid1} and \ref{fig:MNIST-iid0}.
}

\vspace{-0.05in}
\subsection{Comparison with Prior Methods} 

% We stress that privacy is another purpose of using unbiased sketching in addition to communication efficiency. 
Main competing baselines of our methods are distributed algorithms based on sketching. 
Nonetheless, we also compare with prior non-sketching based distributed algorithms (\citep{karimireddy2019scaffold,basu2019qsparse,reisizadeh2020fedpaq,haddadpour2020federated}) in Section~\ref{app:comparison} of the Appendix.


\noindent\textbf{\citep{li2019privacy}.} Note that our convergence analysis does not rely on the bounded gradient assumption. 
We also improve both the number of communication rounds $R$ and the size of transmitted bits $B$ per communication round (please see Table~\ref{table:2} of Section~\ref{app:comparison} in appendix).  
Additionally, we highlight that, while~\citep{li2019privacy} provides a convergence analysis for convex objectives, our analysis holds for PL (thus strongly convex case), general convex and general non-convex objectives.

\noindent\textbf{\citep{rothchild2020fetchsgd}.}
%Consider the two variants of \texttt{FetchSGD} in~\citep{rothchild2020fetchsgd}. 
%While, in our schemes, we do not need to have access to the exact entries of gradients, since the approaches in~\citep{rothchild2020fetchsgd} are based on $top_k$ queries, both of their proposed algorithms require having access to the exact value of $top_k$ gradients, hence they do not preserve privacy. 
Due to gradient tracking, our algorithm tackles data heterogeneity, while algorithms in~\citep{rothchild2020fetchsgd} do not. 
Thereby, in \texttt{FedSKETCHGATE} each device has to store an additional state vector compared to~\citep{rothchild2020fetchsgd}. 
Yet, as our method is built upon an unbiased compressor, server does not need to store any additional error correction vector.
The convergence results for both \texttt{FetchSGD} variants in~\citep{rothchild2020fetchsgd} rely on the uniform bounded gradient assumption which may not be applicable with $L$-smoothness assumption when data distribution is highly heterogeneous, as it is commonly the case in FL, see~\citep{bayoumi2020tighter}.
% while our bounds do not assume such boundedness.
Besides, Theorem 1~\citep{rothchild2020fetchsgd} assumes that \emph{Contraction Holds} for the sequence of gradients which may not hold in practice, yet based on this strong assumption, their total communication cost ($RB$) in order to achieve $\epsilon$ error is $RB=O\left(m\max(\frac{1}{\epsilon^2},\frac{d^2-dm}{m^2\epsilon})\log\left(\frac{d}{\delta}\max(\frac{1}{\epsilon^2},\frac{d^2-dm}{m^2\epsilon})\right)\right)$.
For the sake of comparison, we let the compression ratio in~\citep{rothchild2020fetchsgd} to be $\frac{m}{d}$. 
In contrast, without any extra assumptions, our results in Theorem~\ref{thm:hetreg_case} for \texttt{PRIVIX} and \texttt{HEAPRIX} are respectively $RB=O(\frac{(d+m)}{\epsilon}\log(\frac{{(\frac{d^2}{m})}+d}{\epsilon\delta}))$ and $RB=O(\frac{d}{\epsilon}\log(\frac{d^2}{\epsilon m\delta}))$ which improves the total communication cost of Theorem 1 in~\citep{rothchild2020fetchsgd} under regimes such that $\frac{1}{\epsilon}\geq d$ or $d\gg m$. 
Theorem 2 in~\citep{rothchild2020fetchsgd} is based the \emph{Sliding Window Heavy Hitters} assumption, which is similar to the gradient diversity assumption in~\citep{li2018federated,haddadpour2019convergence}. 
Under that assumption, the total communication cost is shown to be $RB=O\left(\frac{m\max(I^{2/3},2-\alpha)}{\epsilon^3\alpha}\log\left(\frac{d\max(I^{2/3},2-\alpha)}{\epsilon^3\delta}\right)\right)$ where $I$ is a constant related to the window of gradients.
We improve this bound under weaker assumptions in a regime where $\frac{I^{2/3}}{\epsilon^2}\geq d$. 
We also provide bounds for PL, convex and non-convex objectives contrary to~\citep{rothchild2020fetchsgd}. 
% Finally, we note that algorithms in~\citep{rothchild2020fetchsgd} are using momentum at server. 
% While we do not use it explicitly, we can modify our algorithms to include momentum easily.



% \vspace{0.05in}\noindent\textbf{Comparison with~SCAFFOLD}{\color{blue}\:  To compare with~\citep{karimireddy2019scaffold} which does not use gradient compression, we let $m=d$ (no compression). In this case, similar to~\citep{haddadpour2020federated}, our communication complexities and number of local updates match with corresponding bounds obtained by SCAFFOLD with difference that in downlink (from devices to server) we only send one vector while SCAFFOLD needs to send two vectors (additional control variate).

%Extensive comparison with related methods, involving sketches or not, such as \citep{rothchild2020fetchsgd} or \citep{karimireddy2019scaffold}, can be found Section~\ref{app:comparison} of the Appendix.
 
%Additionally, unlike~\citep{rothchild2020fetchsgd} only focusing on non-convex objectives, we provide the convergence analysis for PL (thus strongly convex case), general convex and general non-convex objectives. 

%Finally, the algorithms in~\citep{rothchild2020fetchsgd} require additional server memory to store the compression error correction while our does not.

%Finally, although the algorithm in~\citep{rothchild2020fetchsgd} requires additional memory for the server to store the compression error correction vector, our algorithm does not need such additional storage. 
%Yet, unlike~\citep{rothchild2020fetchsgd}, our algorithm requires devices to store a local state vector. %and additionally need a second round of communication for \texttt{HEAPRIX}.

\begin{figure}[t]
	\begin{center}
		\mbox{\hspace{-0.25in}			  
		 \includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch20_iid1_train_loss.eps} \hspace{-0.12in}
		 \includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch20_iid1_test_acc.eps} 
		 }
		 		\mbox{\hspace{-0.25in}		
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch20_iid1_train_loss.eps}\hspace{-0.12in}
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch20_iid1_test_acc.eps}
		}
		\mbox{\hspace{-0.25in}	
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch50_iid1_train_loss.eps} \hspace{-0.12in}
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch50_iid1_test_acc.eps} 
		}
				\mbox{\hspace{-0.25in}		
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch50_iid1_train_loss.eps}\hspace{-0.12in}
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch50_iid1_test_acc.eps}
		}
	\end{center}
	\vspace{-0.1in}
	\caption{Homogeneous case: Comparison of compressed optimization methods on LeNet CNN.}
    \label{fig:MNIST-iid1}
    \vspace{-0.1in}
\end{figure}

%These results are summarized in Table~\ref{table:2}.
\vspace{-0.15in}
\section{Numerical Study}\label{sec:experiment}
\vspace{-0.05in}

In this section, we provide empirical results on MNIST benchmark dataset to demonstrate the effectiveness of our proposed algorithms. 
We train LeNet-5 Convolutional Neural Network (CNN) architecture introduced in~\citep{lecun1998gradient}, with $60\,000$ parameters.
We compare Federated SGD (\texttt{FedSGD}) as the full-precision baseline, along with four sketching methods \texttt{SketchSGD}~\citep{ivkin2019communication}, \texttt{FetchSGD}~\citep{rothchild2020fetchsgd}, and two FedSketch variants \texttt{FS-PRIVIX} and \texttt{FS-HEAPRIX}.
Note that in Algorithm~\ref{Alg:PFLHom}, \texttt{FS-PRIVIX} with global learning rate $\gamma=1$ is equivalent to the \texttt{DiffSketch} algorithm proposed in~\citep{li2018federated}. Also, \texttt{SketchSGD} is slightly modified to compress the change in local weights (instead of local gradient in every iteration), and \texttt{FetchSGD} is implemented with second round of communication for fairness. (The original proposal does not include second round of communication, which performs worse with small sketch size.) As suggested in~\citep{rothchild2020fetchsgd}, the momentum factor of \texttt{FetchSGD} is set to $0.9$, and we also follow some recommended implementation tricks to improve its performance, which are detailed in the Appendix.
The number of workers is set to $50$ and we report the results for 1 and 5 local epochs. A local epoch is finished when all workers go through their local data samples once. The local batch size is 30. In each round, we randomly choose half of the devices to be active. 
We tune the learning rates ($\eta$ and $\gamma$, if applicable) over log-scale and report the best results, for both \emph{homogeneous} and \emph{heterogeneous} setting. 
In the former case, each device receives uniformly drawn data samples, and in the latter, it only receives samples from one or two classes among ten. 



\textbf{Homogeneous case.} In Figure~\ref{fig:MNIST-iid1}, we provide the training loss and test accuracy with different number of local epochs and sketch size, $(t,k)=(20,40)$ and $(50,100)$. 
Note that, these two choices of sketch size correspond to a $75\times$ and $12\times$ compression ratio, respectively. We conclude that
\vspace{-0.1in}
\begin{itemize}
    \item In general, increasing the compression ratio sacrifices the learning performance. In all cases, \texttt{FS-HEAPRIX} performs the best in terms of both training objective and test accuracy, among all compressed methods.
    
    \item \texttt{FS-HEAPRIX} is better than \texttt{FS-PRIVIX}, especially with small sketches (high compression ratio). \texttt{FS-HEAPRIX} yields acceptable extra test error compared to full-precision \texttt{FedSGD}, particularly when considering the high compression ratio (e.g., $75\times$). 
    
    \item The training performance of \texttt{FS-HEAPRIX} improves when the number of local updates increases. \emph{That is, the proposed method is able to further reduce the communication cost by reducing the number of rounds required for communication.} This is also consistent with our theoretical findings. 
\end{itemize}
\vspace{-0.1in}
In general, \texttt{FS-HEAPRIX} outperforms all competing methods, and a sketch size of $(50,100)$ is sufficient to approach the accuracy of full-precision \texttt{FedSGD}.

\begin{figure}[t]
	\begin{center}
		\mbox{\hspace{-0.15in}			   
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch20_iid0_train_loss.eps} \hspace{-0.12in}
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch20_iid0_test_acc.eps} 
		}
				\mbox{\hspace{-0.15in}		
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch20_iid0_train_loss.eps} \hspace{-0.12in}
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch20_iid0_test_acc.eps}
		}
		\mbox{\hspace{-0.15in}	
		 \includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch50_iid0_train_loss.eps} \hspace{-0.12in}
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local1_sketch50_iid0_test_acc.eps} 
		}
				\mbox{\hspace{-0.15in}		
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch50_iid0_train_loss.eps}\hspace{-0.12in}
		\includegraphics[width=0.25\textwidth]{MNIST_figures/local5_sketch50_iid0_test_acc.eps}
		}
	\end{center}
	\vspace{-0.1in}
	\caption{Heterogeneous case: Comparison of compressed optimization algorithms on LeNet CNN.}
    \label{fig:MNIST-iid0}
    \vspace{-0.1in}
\end{figure}

\textbf{Heterogeneous case.} We plot similar set of results in Figure~\ref{fig:MNIST-iid0} for non-i.i.d. data distribution, which leads to more twists and turns in the training curves. 
We see that \texttt{SketchSGD} performs very poorly in the heterogeneous case, which is improved by error tracking and momentum in \texttt{FetchSGD}, as expected. 
However, both of these methods are worse than our proposed \texttt{FedSketchGATE} methods, which can achieve similar generalization accuracy as full-precision \texttt{FedSGD}, even with small sketch size (i.e., $75\times$ compression). 
Note that, slower convergence and worse generalization of \texttt{FedSGD} in non-i.i.d. data distribution case is also reported in e.g.~\citep{mcmahan2016communication,chen2020toward}. 

We also notice in Figure~\ref{fig:MNIST-iid0} the edge of \texttt{FS-HEAPRIX} over \texttt{FS-PRIVIX} in terms of training loss and test accuracy. 
However, we see that in the heterogeneous setting, more local updates tend to undermine the learning performance, especially with small sketch size.  
Nevertheless, when the sketch size is not too small, i.e., $(50,100)$, \texttt{FS-HEAPRIX} can still provide comparable test accuracy as \texttt{FedSGD} in both cases.
Our empirical study demonstrates that \texttt{FedSketch} (and \texttt{FedSketchGATE}) frameworks are able to perform well in homogeneous (resp. heterogeneous) settings, with high compression rate. 
In particular, \texttt{FedSketch} methods are beneficial over \texttt{SketchedSGD}~\citep{ivkin2019communication} and \texttt{FetchSGD}~\citep{rothchild2020fetchsgd} in all cases. 
\texttt{FS-HEAPRIX} performs the best among all the tested compressed algorithms, which in many cases achieves similar generalization accuracy as full-precision FedSGD with small sketch size. 


\section{Conclusion}
\vspace{-0.05in}

In this paper, we introduced \texttt{FedSKETCH} and \texttt{FedSKETCHGATE} algorithms for homogeneous and heterogeneous data distribution setting respectively for Federated Learning wherein communication between server and devices is only performed using count sketch. 
Our algorithms, thus, provide communication-efficiency and privacy, through random hashes based sketches. 
We analyze the convergence error for \emph{non-convex}, \emph{PL} and \emph{general convex} objective functions in the scope of Federated Optimization.  
We provide insightful numerical experiments showcasing the advantages of our \texttt{FedSKETCH} and \texttt{FedSKETCHGATE} methods over current federated optimization algorithm. The proposed algorithms outperform competing compression method and can achieve comparable test accuracy as Federated SGD, with high compression ratio.



\newpage
\clearpage\balance
\bibliography{reference}
\bibliographystyle{ACM-Reference-Format}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
%\input{Appendix}

\onecolumn

{\centering
  {\Large\bfseries Appendix for \texttt{FedSKETCH}: Communication-Efficient Federated Learning
via Sketching \par}}



%\section{Generalized FL Algorithm with compression}



The appendix is organized as follows:
Section~\ref{app:notations} recalls important notations used throughout the paper and provides the formulation of related algorithms used in the main paper and omitted for the sake of the page limit.
We present in Section~\ref{app:comparison} of this supplementary file, a through comparison with notable related works.
Section~\ref{app:proofs} contains the proofs of our results and Section~\ref{app:numericals} presents additional numerical runs.

\section{Notations and Definitions}\label{app:notations}

\paragraph{Notation.} Here we denote the count sketch of the vector $\boldsymbol{x}$ by $\mathbf{S}(\boldsymbol{x})$ and with an abuse of notation, we indicate the expectation over the randomness of count sketch with $\mathbb{E}_{\mathbf{S}}[.]$. 
We illustrate the random subset of the devices selected by the central server with $\mathcal{K}$ with size $|\mathcal{K}|=k\leq p$, and we represent the expectation over the device sampling with $\mathbb{E}_{\mathcal{K}}[.]$. 


\begin{table}[htbp]\caption{Table of Notations}
\begin{center}% used the environment to augment the vertical space
% between the caption and the table
\begin{tabular}{r c p{10cm} }
\toprule
$p$ & $\triangleq$ & Number of devices\\
$k$ & $\triangleq$ & Number of sampled devices for homogeneous setting\\
$\mathcal{K}^{(r)}$ & $\triangleq$ & Set of sampled devices in communication round $r$\\
$d$ & $\triangleq$ &  Dimension of the model \\
$\tau$ & $\triangleq$ & Number of local updates\\
$R$ & $\triangleq$ & Number of communication rounds\\
$B$ & $\triangleq$ &  Size of transmitted bits \\
$R\times B$ & $\triangleq$ &  Total communication cost per device \\
$\kappa$ & $\triangleq$ & Condition number\\
$\epsilon$ & $\triangleq$ & Target accuracy\\
$\mu$ & $\triangleq$ & PL constant \\
$m$ & $\triangleq$ &  Number of bins of hash tables \\
 $\mathbf{S}(\boldsymbol{x})$  & $\triangleq$ &  Count sketch of the vector $\boldsymbol{x}$\\
 $\mathbb{U}(.)$  & $\triangleq$ &  Class of unbiased compressor, see Definition~\ref{def:unbiased}\\
\bottomrule
\end{tabular}
\end{center}
\label{tab:notations}
\end{table}

\begin{definition}[\pl]\label{assum:pl}
A function $f(\boldsymbol{x})$ satisfies the \pl (PL)~ condition with constant $\mu$ if $\frac{1}{2}\|\nabla f(\boldsymbol{x})\|_2^2\geq \mu\big(f(\boldsymbol{x})-f(\boldsymbol{x}^*)\big),\: \forall \boldsymbol{x}\in\mathbb{R}^d $ with $\boldsymbol{x}^*$ is an optimal solution.
\end{definition}


% \begin{definition}
 %A randomized mechanism $\mathcal{O}$ satisfies $\epsilon-$differential privacy, if for input data ${S}_1$ and ${S}_2$ differing by up to one point, and for output $D$ of $\mathcal{O}$,
 %\begin{align}\notag
    % \Pr\left[\mathcal{O}(S_1)\in D\right]\leq \exp{\left(\epsilon\right)}\Pr\left[\mathcal{O}(S_2)\in D\right] \, .
 %\end{align}
 %\end{definition}
  %For smaller $\epsilon$, it becomes difficult to specify the input data, hence, implying stronger privacy.
\subsection{Count sketch}
 In this paper, we exploit the commonly used \texttt{Count Sketch} ~\citep{DBLP:journals/tcs/CharikarCF04} 
 which uses two sets of functions that encode any input vector $\boldsymbol{x}$ \textbf{into a hash table} $\boldsymbol{S}_{m\times t}(\boldsymbol{x})$. 
Pairwise independent hash functions $\{h_{j,1\leq j\leq t }:[d]\rightarrow m\}$ are used along with another set of pairwise independent sign hash functions $\{\text{sign}_{j,1\leq j\leq t}: [d]\rightarrow \{+1,-1\}\}$ to map entries of $\boldsymbol{x}$ (${x}_i, \:1\leq i\leq d$) into $t$ different columns of $\mathbf{S}_{m\times t}$, wherein, to lower the dimension of the input vector, we usually have $d\gg mt$.  
The final update reads $\mathbf{S}[j][h_j(i)]=\mathbf{S}[j][h_{j}(i)]+\text{sign}_j(i){x}_i$ for any $1 \leq j \leq t$. Generating compressed output is described in Algorithm~\ref{alg:csketch}.

 \begin{algorithm}[H]
\caption{Count Sketch ({\texttt{CS})~\citep{DBLP:journals/tcs/CharikarCF04}} }\label{alg:csketch}
 \begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\boldsymbol{x}\in\mathbb{R}^{d}, t, k, \mathbf{S}_{m\times t}, h_j (1\leq i\leq t), \text{sign}_j (1\leq i\leq t)$
 \STATE \textbf{Compress vector $\boldsymbol{x}\in\mathbb{R}^{d}$ into $\mathbf{S}\left(\boldsymbol{x}\right)$:}
 \STATE \textbf{for} $\boldsymbol{x}_i\in\boldsymbol{x}$ \textbf{do}
 \STATE \quad\textbf{for $j=1,\cdots,t$ do}
 \STATE \quad\quad $\mathbf{S}[j][h_j(i)]=\mathbf{S}[j-1][h_{j-1}(i)]+\text{sign}_j(i).\boldsymbol{x}_i$ 
 \STATE \quad\textbf{end for}
 \STATE \textbf{end for}
 \STATE \textbf{return} $\mathbf{S}_{m\times t}(\boldsymbol{x})$
 \end{algorithmic}
 \end{algorithm}
% Count Sketch uses two sets of functions that encode any input vector $\boldsymbol{x}$ \textbf{into a hash table} $\boldsymbol{S}_{m\times t}(\boldsymbol{x})$. 
% Pairwise independent hash functions $\{h_{j,1\leq j\leq t }:[d]\rightarrow m\}$ are used along with another set of pairwise independent sign hash functions $\{\text{sign}_{j,1\leq j\leq t}: [d]\rightarrow \{+1,-1\}\}$ to map entries of $\boldsymbol{x}$ ($\boldsymbol{x}_i, \:1\leq i\leq d$) into $t$ different columns of $\mathbf{S}_{m\times t}$, wherein to lower the dimension of the input vector we usually have $d\gg mt$.  

\subsection{\texttt{PRIVIX} method and compression error of \texttt{HEAPRIX}}
For the sake of completeness we review \texttt{PRIVIX} algorithm that is also mentioned in~\citep{li2019privacy} as follows:

\begin{algorithm}[H]
\caption{\texttt{PRIVIX}/\texttt{DiffSketch}~\citep{li2019privacy}: Unbiased compressor based on sketching. }\label{Alg:privix}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\boldsymbol{x}\in\mathbb{R}^{d}, t, m, \mathbf{S}_{m\times t}, h_j (1\leq i\leq t), sign_j (1\leq i\leq t)$
\STATE \textbf{Query} $\tilde{\boldsymbol{x}}\in\mathbb{R}^d$ \textbf{from $\mathbf{S(\boldsymbol{x})}$:}
\STATE \textbf{for} $i=1,\ldots,d$ \textbf{do}
\STATE \quad\quad ${\tilde{\boldsymbol{x}}}[i]=\text{Median}\{\text{sign}_j(i).\mathbf{S}[j][h_j(i)]:1\leq j\leq t\}$ 
\STATE \textbf{end for}
\STATE \textbf{Output:} ${\tilde{\boldsymbol{x}}}$
\end{algorithmic}
\end{algorithm}



Regarding the compression error of sketching we restate the following Corollary from the main body of this paper:
\begin{corollary}
Based on~Theorem 3 of~\citep{horvath2020better} and using Algorithm~\ref{alg:heaprix}, we have $\mathcal{C}(x)\in \mathbb{U}(c \frac{d}{m})$. This shows that unlike \texttt{PRIVIX} (Algorithm~\ref{Alg:privix}) the compression noise can be made as small as possible using large size of hash table.
\end{corollary}

\begin{proof}
The proof simply follows from Theorem 3 in~\citep{horvath2020better} and Algorithm~\ref{alg:heaprix} by setting $\Delta_1=c\frac{d}{m}$ and $\Delta_2=1+c\frac{d}{m}$ we obtain $\Delta=\Delta_2+\frac{1-\Delta_2}{\Delta_1}=c\frac{d}{m}=O\left(\frac{d}{m}\right)$ for the compression error of \texttt{HEAPRIX}. 
\end{proof}

\clearpage
\section{Convergence of \texttt{FedSketchGate} for Convex Objectives}\label{sec:cvx_cvrg_app}
\begin{theorem}\label{thm:homog_case_append}
Suppose Assumptions~\ref{Assu:1}-\ref{Assu:1.5} hold. 
Given $0<m\leq d$ and considering Algorithm~\ref{Alg:PFLHom} with sketch size $B=O\left(m\log\left(\frac{d R}{\delta}\right)\right)$ and $\gamma\geq k$, with probability $1-\delta$ for \textbf{Convex} case, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies $ \mathbb{E}\Big[f({\boldsymbol{x}}^{(R-1)})-f({\boldsymbol{x}}^{(*)})\Big]\leq \epsilon$ if we set:  

%\noindent {\color{blue}$\bullet$ \texttt{FS-PRIVIX}, for $\eta=1/(2L(cd/mk+1)\tau\gamma)$:    
\noindent $\bullet$ \texttt{FS-PRIVIX}, for $\eta=\frac{1}{2L(cd/m+1/k)\tau\gamma}$: $R =O\left(L\left(1/k+d/m\right)/\epsilon\log\left(1/\epsilon \right)\right)$ and
$\tau=O\left(1/\epsilon^2\right)$.

\noindent $\bullet$ \texttt{FS-HEAPRIX}, for $\eta=\frac{1}{2L\left((cd-m)/mk+1\right)\tau\gamma}$:   $R =O\left(L\left(1/k+(d-m)/m\right)/\epsilon\log\left(1/\epsilon \right)\right)$ and $\tau=O\left(1/\epsilon^2\right)$.


%\todo{Move convex section to the appendix!}
\end{theorem}

\begin{theorem}\label{thm:hetreg_case_appendix_cvx}
Suppose Assumptions~\ref{Assu:1} and~\ref{Assu:2} hold. Given $0<m\leq d$, and considering \texttt{FedSKETCHGATE} in Algorithm~\ref{Alg:PFLHet} with sketch size $B=O\left(m\log\left(\frac{d R}{\delta}\right)\right)$ and $\gamma\geq p$ with probability $1-\delta$ for \textbf{convex} case, $\{ {\boldsymbol{x}}^{(r)}\}_{r=>0}$ satisfies $ \mathbb{E}[f({\boldsymbol{x}}^{(R-1)})-f({\boldsymbol{x}}^{(*)})]\leq \epsilon$ if: 
 
\noindent $\bullet$ \texttt{FS-PRIVIX}, for $\eta=1/(2L(c d/m+1)\tau\gamma)$:  $R=O\left(L(d/m+1)\epsilon \log(1/\epsilon)\right)$ and $\tau=O\left(1/(p\epsilon^2)\right)$.

\noindent $\bullet$ \texttt{FS-HEAPRIX}, for $\eta=m/(2L c d\tau\gamma)$: $R=O\left(L(d/m)\epsilon \log(1/\epsilon)\right)$ and $\tau=O\left(1/(p\epsilon^2)\right)$.
  %%%%%%%%%%%%
 \end{theorem}



\section{Summary of comparison of our results with prior works}\label{app:comparison}
For the purpose of further clarification, we summarize the comparison of our results with related works. 
We recall that $p$ is the number of devices, $d$ is the dimension of the model, $\kappa$ is the condition number, $\epsilon$ is the target accuracy, $R$ is  the number of communication rounds, and $\tau$ is the number of local updates. 
We start with the homogeneous setting comparison.
Comparison of our results and existing ones for homogeneous and heterogeneous setting are given respectively Table~\ref{table:1} and Table~\ref{table:2}.


% \begin{table}[H]
%     \centering
%     \caption{Comparison of results with compression and periodic averaging in the homogeneous setting. 
%     Here, $p$ is the number of devices, $\mu$ is the PL constant, $m$ is the number of bins of hash tables, $d$ is the dimension of the model, $\kappa$ is the condition number, $\epsilon$ is the target accuracy, $R$ is the number of communication rounds, and $\tau$ is the number of local updates.
%      UG and PP stand for Unbounded Gradient and Privacy Property respectively.}
% \label{table:1}
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{llll}
%         \toprule
%         Reference        & PL/Strongly Convex   & UG & PP
%         \\
%         \midrule
%         \makecell{\citep{li2019privacy}}  & \makecell[l]{$-$}   & \makecell[l]{$-$}               & \makecell{$R\!=\!O\left(\frac{\mu^2 d}{\epsilon^{2}}\right)$ \\ $\tau\!=\!1\\
%         B=O\left(k\log\left(\frac{dR}{\delta}\right)\right)$\\
%         $pRB=O\left(\frac{p\mu^2 d}{\epsilon^{2}}k\log\left(\frac{\mu^2d^2}{\epsilon^2\delta}\right)\right)$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{52}}
%         \\
%         \midrule
%         \makecell{\textbf{Ivkin et al.~\citep{ivkin2019communication}}}  &  \makecell[l]{$R=O\left(\max\left(\frac{ d}{m\sqrt{\epsilon}},\frac{1}{\epsilon}\right)\right)$,\: $\tau=1$,\: $B=O\left(m\log\left(\frac{dR}{\delta}\right)\right)$\\
%       $pRB=O\left(\frac{p d}{m\epsilon}\log\left(\frac{d}{\delta\sqrt{\epsilon}}\max\left(\frac{ d}{m},\frac{1}{\sqrt{\epsilon}}\right)\right)\right)$}                                                                                           & \makecell{\ding{55}} & \makecell{\ding{55}}
%         \\
        
%         \midrule
%         \makecell{\citep{karimireddy2019scaffold}}  & \makecell[l]{$R=O\left(\frac{1}{\epsilon}\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
%         $B=O\left(d\right)$\\
%         $pRB=O\left(\frac{pd}{\epsilon}\right)$}   & \makecell[l]{$R=O\left(\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
%         $B=O\left(d\right)$\\
%       $pRB=O\left(p\kappa d\log\left(\frac{1}{\epsilon}\right)\right)$}               & \makecell{$R=O\left(\frac{1}{\epsilon}\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
%         $B=O\left(d\right)$\\
%       $pRB=O\left(\frac{pd}{\epsilon}\right)$}                                                                            & \makecell{\ding{52}} & \makecell{\ding{55}}
%         \\
%         \midrule
%     \makecell{\textbf{Theorem~\ref{thm:homog_case}}} & \makecell[l]{$\boldsymbol{R=O\left(\frac{1}{\epsilon}\right)}$ \\[3pt] $\boldsymbol{\tau=O\left(\frac{\mu^2d}{k\epsilon}\right)}$\\[3pt]
%       $\boldsymbol{B=O\left(m\log\left(\frac{dR}{\delta}\right)\right)}$\\[3pt]
%       $\boldsymbol{kBR=O\left(\frac{mk}{\epsilon}\log\left(\frac{d}{\epsilon\delta}\right)\right)}$}  
      
%     %   & \makecell[l]{$\boldsymbol{R=O\left(\kappa\left(\frac{ d-m}{m k}+1\right)\log\left(\frac{1}{\epsilon}\right)\right)}$,\: $\boldsymbol{\tau=O\left(\frac{d}{k\left(\frac{ d}{k}+m\right)\epsilon}\right)}$,\:$\boldsymbol{B=O\left(m\log\left(\frac{dR}{\delta}\right)\right)}$\\
%     %   $\boldsymbol{kRB=O\left({m}\kappa(d-m+mk)\log\frac{1}{\epsilon}\log\left(\frac{\kappa(d\frac{d-m}{mk}+d)\log\frac{1}{\epsilon}}{\delta}\right)\right)}$}                                                                                   & \makecell{\ding{52}} & \makecell{\ding{52}}   
%       \\
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}


\begin{table}[H]
    \centering
    \caption{Comparison of results with compression and periodic averaging in the homogeneous setting. 
    Here, $p$ is the number of devices, $\mu$ is the PL constant, $m$ is the number of bins of hash tables, $d$ is the dimension of the model, $\kappa$ is the condition number, $\epsilon$ is the target accuracy, $R$ is the number of communication rounds, and $\tau$ is the number of local updates.
     UG and PP stand for Unbounded Gradient and Privacy Property respectively. }
\label{table:1}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{llll}
        \toprule
        Reference        & Non-Convex   & UG & PP
        \\
        \midrule
    \makecell{\citep{li2019privacy}}  &   \makecell[l]{$-$}                                                                                      &  \makecell[l]{$-$}       &  \makecell[l]{$R\!=\!O\left(\frac{\mu^2 d}{\epsilon^{2}}\right)$ ,\: $\tau=1$\\
    $B=O\left(k\log\left(\frac{dR}{\delta}\right)\right)$\\
       $pRB=O\left(\frac{p\mu^2 d}{\epsilon^{2}}k\log\left(\frac{\mu^2d^2}{\epsilon^2\delta}\right)\right)$}       \\
        \midrule
        \makecell{Ivkin et al.~\citep{ivkin2019communication}}  &  \makecell[l]{$R=O\left(\max\left(\frac{ d}{m\sqrt{\epsilon}},\frac{1}{\epsilon}\right)\right)$,\: $\tau=1$,\: $B=O\left(m\log\left(\frac{dR}{\delta}\right)\right)$\\
      $pRB=O\left(\frac{p d}{m\epsilon}\log\left(\frac{d}{\delta\sqrt{\epsilon}}\max\left(\frac{ d}{m},\frac{1}{\sqrt{\epsilon}}\right)\right)\right)$}                                                                                           & \makecell{\ding{55}} & \makecell{\ding{55}}
       % \\
        
        %\midrule
        %\makecell{\citep{karimireddy2019scaffold}}  & \makecell[l]{$R=O\left(\frac{1}{\epsilon}\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
        %$B=O\left(d\right)$\\
        %$pRB=O\left(\frac{pd}{\epsilon}\right)$}   & \makecell[l]{$R=O\left(\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
        %$B=O\left(d\right)$\\
      %$pRB=O\left(p\kappa d\log\left(\frac{1}{\epsilon}\right)\right)$}               & \makecell{$R=O\left(\frac{1}{\epsilon}\right)$ \\ $\tau=O\left(\frac{1}{p\epsilon}\right)$\\
       % $B=O\left(d\right)$\\
      %$pRB=O\left(\frac{pd}{\epsilon}\right)$}
        \\
        \midrule
    \makecell{\textbf{Theorem~\ref{thm:homog_case}}} & \makecell[l]{$\boldsymbol{R=O\left(\frac{1}{\epsilon}\right)}$ \\
    $\boldsymbol{\tau=O\left(( \mu^2(c d-m)+\frac{\mu^2}{k})\frac{1}{\epsilon}\right)}$\\$\boldsymbol{B=O(m \log ( \frac{dR}{\delta}))}$
    \\$\boldsymbol{kBR=O(mk/\epsilon \log ( \frac{d}{\epsilon\delta} ))}$}    & \makecell{\ding{52}} & \makecell{\ding{55}}\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\begin{table}[H]
    \centering
    \caption{Comparison of results with compression and periodic averaging in the heterogeneous setting. 
%    Here, $p$ is the number of devices, $\mu$ is compression of hash table, $d$ is the dimension of the model, $\kappa$ is condition number, $\epsilon$ is target accuracy, $R$ is  the number of communication rounds, and $\tau$ is the number of local updates. 
    UG and PP stand for Unbounded Gradient and Privacy Property respectively. }
\label{table:2}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lllll}
        \toprule
%                    &  \multicolumn{3}{c}{Objective function} &
%        \\ \cmidrule(r){2-4}
        Reference        & non-convex                                        & General Convex   & UG & PP
        \\
        \midrule
        \makecell{\textbf{Basu et al.~\citep{basu2019qsparse} (with $\gamma=m/d$)}}  & \makecell[l]{$R=O\left(\frac{d}{m\epsilon^{1.5}}\right)$\\  $\tau=O\left(\frac{m}{pd\sqrt{\epsilon}}\right)$\\ $B=O\left(d\right)$\\
        $RB=O\left(\frac{d^2}{m\epsilon^{1.5}}\right)$}   & \makecell[l]{$-$}               & \makecell{\ding{55}}                                                                            & \makecell{\ding{55}} 
        \\
        \midrule
        \makecell{\textbf{Li et al.~\citep{li2019privacy}}}  & \makecell[l]{$-$}                & \makecell[l]{$R\!=\!O\left(\frac{ d}{m\epsilon^{2}}\right)$ \\ $\tau\!=\!1$\\
        $B=O\left(m\log\left(\frac{d^2}{m\epsilon^2\delta}\right)\right)$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{52}}
        \\
    
        \midrule
        \makecell{\textbf{Rothchild et al.~\citep{rothchild2020fetchsgd}}}  & \makecell[l]{$R=O\left(\max(\frac{1}{\epsilon^2},\frac{d^2-md}{m^2\epsilon})\right)$ \\ $\tau=1$\\
        $B=O\left(m\log\left(\frac{d}{\delta}\max(\frac{1}{\epsilon^2},\frac{d^2-md}{m^2\epsilon})\right)\right)$\\
        $RB=O\left({m}\max(\frac{1}{\epsilon^2},\frac{d^2-md}{m^2\epsilon})\log\left(\frac{d}{\delta}\max(\frac{1}{\epsilon^2},\frac{d^2-md}{m^2\epsilon})\right)\right)$}       & \makecell[l]{$-$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{55}}
        \\
        \midrule
        \makecell{\textbf{Rothchild et al.~\citep{rothchild2020fetchsgd}}}  & \makecell[l]{$R=O\left(\frac{\max(I^{2/3},2-\alpha)}{\epsilon^3}\right)$ \\ $\tau=1$\\
        $B=O\left(\frac{m}{\alpha}\log\left(\frac{d\max(I^{2/3},2-\alpha)}{\epsilon^3\delta}\right)\right)$\\
        $RB=O\left(\frac{m\max(I^{2/3},2-\alpha)}{\epsilon^3\alpha}\log\left(\frac{d\max(I^{2/3},2-\alpha)}{\epsilon^3\delta}\right)\right)$
        }       & \makecell[l]{$-$}                                                                            & \makecell{\ding{55}} & \makecell{\ding{55}}
  \\
       \midrule
             \makecell{\textbf{Theorem~\ref{thm:hetreg_case}}} & \makecell[l]{$\boldsymbol{R=O\left(\frac{d}{m\epsilon}\right)}$ \\ 
             $\boldsymbol{\tau=O(\frac{1}{p\epsilon})}$\\
     $\boldsymbol{B=O(m\log(\frac{d^2}{m\epsilon\delta}))}$\\
     $\boldsymbol{RB=O(\frac{d}{\epsilon}\log(\frac{d^2}{m\epsilon\delta}\log(\frac{1}{\epsilon})))}$}  &
     \makecell[l]{$\boldsymbol{R\!=\!O(\frac{d}{m\epsilon}{\color{black}\log(\frac{1}{\epsilon})})}$\\[3pt]
     $\boldsymbol{\tau\!=\!O(\frac{1}{p\epsilon^2})}$\\[3pt]
     $\boldsymbol{B=O(m\log(\frac{d^2}{m\epsilon\delta}))}$}   
                                                                              & \makecell{\ding{52}} & \makecell{\ding{52}}
  \\
        \bottomrule
    \end{tabular}
    }
\end{table}


\noindent\textbf{Comparison  with~\citep{haddadpour2020federated} and~\citep{reisizadeh2020fedpaq}}
Convergence analysis of algorithms in~\citep{haddadpour2020federated} relies on unbiased compression, while in this paper our FL algorithm based on \texttt{HEAPRIX} enjoys from unbiased compression with equivalent biased compression variance. Moreover, we highlight that the convergence analysis of \texttt{FedCOMGATE} is based on the extra assumption of boundedness of the difference between the average of compressed vectors and compressed averages of vectors. 
However, we do not need this extra assumption as it is satisfied naturally due to linearity of sketching. 
Finally, as pointed out in Remark~\ref{rmk:bidirect}, our algorithms enjoy from a bidirectional compression property, unlike \texttt{FedCOMGATE} in general. 
Furthermore, since results in~\citep{haddadpour2020federated} improve the communication complexity of FedPAQ algorithm, developed in \citep{reisizadeh2020fedpaq}, hence \texttt{FedSKETCH} and \texttt{FedSKETCHGATE} improves the communication complexity obtained in \citep{reisizadeh2020fedpaq}. 
%Therefore, for instance in case of general non-convex our algorithm improves the total number of transmitted bits per device from $RB=O\left(\frac{d}{m\log(\frac{d}{\delta})}\frac{d}{\epsilon}\right)$ (by letting $q=\frac{d}{mt}=\frac{d}{m\log(\frac{d}{\delta})}$ and $B=O\left(d\right)$) to $RB=O\left(\log(\frac{d^2}{m\epsilon\delta})\frac{d}{\epsilon}\right)$. }

\noindent\textbf{\citep{basu2019qsparse}.}  We note that the algorithm in~\citep{basu2019qsparse} uses a composed compression and quantization while our algorithm is solely based on compression. 
So, in order to compare with algorithms in~\citep{basu2019qsparse} we only consider Qsparse-local-SGD with compression and we let compression factor $\gamma=\frac{m}{d}$ (to compare with the same compression ratio induced with sketch size of $mt$). For strongly convex objective in   Qsparse-local-SGD to achieve convergence error of $\epsilon$ they require $R=O\left(\kappa\frac{d}{m\sqrt{\epsilon}}\right)$ and $\tau=O\left(\frac{m}{pd\sqrt{\epsilon}}\right)$, which is improved to $R=O\left(\frac{\kappa d}{m}\log (1/\epsilon)\right)$ and $\tau=O\left(\frac{1}{p\epsilon}\right)$ for PL \:   objectives. Similarly, for non-convex objective~\citep{basu2019qsparse} requires $R=O\left(\frac{d}{m\epsilon^{1.5}}\right)$ and $\tau=O\left(\frac{m}{pd\sqrt{\epsilon}}\right)$, which is improved to $R=O\left(\frac{d}{m\epsilon}\right)$ and $\tau=O\left(\frac{1}{p\epsilon}\right)$. We note that we reduce communication rounds at the cost of increasing number of local updates (which scales down with number of devices, $p$). Additionally, we highlight that our \texttt{FedSKETCHGATE} exploits the gradient tracking idea to deal with data heterogeneity, while algorithms in~\citep{basu2019qsparse} does not develop such mechanism and may suffer from poor convergence in heterogeneous setting. We also note that setting $\tau=1$ and using $top_{m}$ compressor, the QSPARSE-local-SGD algorithm becomes similar to distributed SGD with sketching as they both use the error feedback framework to improve the compression variance.  
Finally, since the average of sparse vectors may not be sparse in general the number of transmitted bits from server to devices in QSPARSE-Local-SGD in~\citep{basu2019qsparse} may not be sparse in general ($B=O(d)$), however our algorithms enjoy from bidirectional compression properly due to lower dimension and linearity properties of sketching ($B=O(m\log(\frac{Rd}{\delta}))$). Therefore, the total number of bits per device for strongly convex and non-convex objective is improved respectively from $RB=O\left(\kappa\frac{d^2}{m\sqrt{\epsilon}}\right)$ and $RB=O\left(\frac{d^2}{m\epsilon^{1.5}}\right)$ in~\citep{basu2019qsparse} to $RB=O\left({\kappa d\log(\frac{\kappa d^2}{m\delta}\log (\frac{1}{\epsilon})) }\log (1/\epsilon)\right)=O\left({\kappa d\max\Big(\log(\frac{\kappa d^2}{m\delta}}),\log^2 (1/\epsilon)\Big)\right)$ and $RB=O\left(\log(\frac{d^2}{m\epsilon\delta})\frac{d}{\epsilon}\right)$.


Additionally, as we noted using sketching for transmission implies two way communication from master to devices and vice e versa. Therefore, in order to show efficacy of our algorithm we compare our convergence analysis with the obtained rates in the following related work:

\textbf{\citep{philippenko2020artemis}.} 
The reference~\citep{philippenko2020artemis} considers two-way compression from parameter server to devices and vice versa. They provide the convergence rate of $R=O\left(\frac{\omega^{\text{Up}}\omega^{\text{Down}}}{\epsilon^2}\right)$ for strongly-objective functions where $\omega^{\text{Up}}$ and $\omega^{\text{Down}}$ are uplink and downlink's compression noise (specializing to our case for the sake of comparison $\omega^{\text{Up}}=\omega^{\text{Down}}=\theta\left(d\right)$) for general heterogeneous data distribution. In contrast, while our algorithms are using bidirectional compression due to use of sketching for communication, our convergence rate for strongly-convex objective is $R=O(\kappa\mu^2d\log\left(\frac{1}{\epsilon}\right))$ with probability $1-\delta$.  

We would like to also mention that there prior studies such as~ \citep{tang2019doublesqueeze} and \citep{zheng2019communication} that analyze the two-way compression, but since \citep{philippenko2020artemis} is the state-of-the-art on this topic we only compared our results with these papers.


\clearpage

\section{Theoretical Proofs}\label{app:proofs}



We will use the following fact (which is also used in~\citep{li2019convergence,haddadpour2019convergence}) in proving results.
\begin{fact}[\citep{li2019convergence,haddadpour2019convergence}]\label{fact:1}
Let
$\{x_i\}_{i=1}^p$ denote any fixed deterministic sequence. We sample a multiset $\mathcal{P}$ (with size $K$) uniformly at random where $x_j$ is sampled  with probability $q_j$ for $1\leq j\leq p$ with replacement.  Let $\mathcal{P} = \{i_1,\ldots, i_K\} \subset[p]$ (some $i_j$s may have the same value). Then
\begin{align}
    \mathbb{E}_{\mathcal{P}}\left[\sum_{i\in \mathcal{P}}x_i\right]=\mathbb{E}_{\mathcal{P}}\left[\sum_{k=1}^Kx_{i_k}\right]=K\mathbb{E}_{\mathcal{P}}\left[x_{i_k}\right]=K\left[\sum_{j=1}^pq_jx_j\right]
\end{align}
\end{fact}
For the sake of the simplicity, we review an assumption for the quantization/compression, that naturally holds for \texttt{PRIVIX} and \texttt{HEAPRIX}.

\begin{assumption}[\citep{haddadpour2020federated}]\label{Assu:quant}
The output of the compression operator $Q(\boldsymbol{x})$ is an unbiased estimator of its input $\boldsymbol{x}$, and its variance grows with the squared of the squared of $\ell_2$-norm of its argument, i.e., $\mathbb{E}\left[Q(\boldsymbol{x})\right]=\boldsymbol{x}$ and $\mathbb{E}\left[\left\|Q(\boldsymbol{x})-\boldsymbol{x}\right\|^2\right]\leq \omega\left\|\boldsymbol{x}\right\|^2$ .
\end{assumption}
We note that the sketching \texttt{PRIVIX} and \texttt{HEAPRIX}, satisfy Assumption~\ref{Assu:quant} with $\omega=c \frac{d}{m}$ and $\omega=c\frac{d}{m}-1$ respectively with probability $1-\frac{\delta}{R}$ per communication round. 
Therefore, all the results in Theorem~\ref{thm:homog_case}, by taking union over the all probabilities of each communication rounds, are concluded with probability $1-\delta$  by plugging $\omega=c\frac{d}{m}$ and $\omega=c\frac{d}{m}-1$ respectively into the corresponding convergence bounds.

\subsection{Proof of Theorem~\ref{thm:homog_case}}

In this section, we study the convergence properties of our \texttt{FedSKETCH} method presented in Algorithm~\ref{Alg:PFLHom}. 
Before developing the proofs for \texttt{FedSKETCH} in the homogeneous setting, we first mention the following intermediate lemmas. 


\begin{lemma}\label{lemma:tasbih1-iid}
Using unbiased compression and under Assumption~\ref{Assu:1.5}, we have the following bound: 
\begin{align}
\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{{\mathbf{S},\xi^{(r)}}}\Big[\|\tilde{\mathbf{g}}_{\mathbf{S}}^{(r)}\|^2\Big]\right]&=\mathbb{E}_{{\xi}^{(r)}}\mathbb{E}_{\mathbf{S}}\Big[\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\Big]\leq (k\omega+1)\frac{\tau\sigma^2}{k}+(\omega+1)\left[\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\right] \label{eq:lemma1}
\end{align}
\end{lemma}

\begin{proof}
\begin{align}
&\mathbb{E}_{{\xi^{(r)}|\boldsymbol{w}^{(r)}}}\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{{\mathbf{S}}}\Big[\|\frac{1}{k}\sum_{j\in \mathcal{K}} \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]\right]\nonumber\\
=&\mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{\mathbf{S}}\Big[\|\frac{1}{k}\sum_{j\in\mathcal{K}}\underbrace{\mathbf{S}\left(\overbrace{\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j}^{\tilde{\mathbf{g}}_j^{(r)}}\right)}_{\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}}\|^2\Big]\right]\right]\nonumber\\
\stackrel{\text{\ding{192}}}{=}&\mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\left[\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}-\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbb{E}_{\mathbf{S}}\left[\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]\|^2\right]+\|\mathbb{E}_{\mathbf{S}}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S},j}^{(r)}\right]\|^2\right]\right]\nonumber\\
\stackrel{\text{\ding{193}}}{=}&\mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\mathbb{E}_{\mathbf{S}}\left[\|\frac{1}{k}\left[\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}-\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\right]\|^2\right]+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
%\stackrel{}{=}& \mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\left[\text{Var}_{\mathbf{S}}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]\right]+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
\stackrel{}{\leq}& \mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\text{Var}_{\mathbf{S}_j}\left[\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
\stackrel{}{\leq}& \mathbb{E}_{{\xi}^{(r)}}\left[\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\omega\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
\stackrel{}{=}& \left[\mathbb{E}_{\xi}\left[\frac{1}{k}\sum_{j\in\mathcal{K}}\omega\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\mathbb{E}_{{\xi}^{(r)}}\|\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
\stackrel{}{=} &\left[\mathbb{E}_{\xi}\left[{\omega}\sum_{j=1}^pq_j\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\left[\text{Var}\left(\frac{1}{k}\sum_{j\in\mathcal{K}}\tilde{\mathbf{g}}_{j}^{(r)}\right)+\|\frac{1}{k}\sum_{j\in\mathcal{K}}{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\right]\nonumber\\
\stackrel{}{=}& {\omega}\sum_{j=1}^pq_j\mathbb{E}_{\xi}\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k^2}\sum_{j\in\mathcal{K}}\text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)+\|\frac{1}{k}\sum_{j\in\mathcal{K}}{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
\stackrel{\ding{195}}{\leq}&{\omega}\sum_{j=1}^pq_j\mathbb{E}_{\xi}\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\mathbb{E}_{\mathcal{K}}\left[\frac{1}{k^2}\sum_{j\in\mathcal{K}}\tau\sigma^2+\frac{1}{k}\sum_{j\in\mathcal{K}}\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
=&{\omega}\sum_{j=1}^pq_j\left[\text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)+\left\|\mathbf{g}_{j}^{(r)}\right\|^2\right]+\left[\frac{\tau\sigma^2}{k}+\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
\stackrel{\ding{196}}{\leq}&{\omega}\sum_{j=1}^pq_j\left[\tau\sigma^2+\left\|\mathbf{g}_{j}^{(r)}\right\|^2\right]+\left[\frac{\tau\sigma^2}{k}+\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\nonumber\\
=&(k\omega+1)\frac{\tau\sigma^2}{k}+(\omega+1)\left[\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\right]\label{eq:lemma111}%\nonumber\\
%&....\nonumber\\
%&\stackrel{}{\leq} \mathbb{E}_{{\xi}^{(r)}}\left[\left[\frac{1}{k}\sum_{j=1}^pq_j\omega\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2+\sum_{j=1}q_j\|\tilde{\mathbf{g}}_{j}^{(r)}\|^2\right]\right]\nonumber\\
%&=\left(\frac{\omega}{k}+1\right)\sum_{j=1}^pq_j\mathbb{E}_{{\xi}^{(r)}}\left\|\tilde{\mathbf{g}}_{j}^{(r)}\right\|^2\nonumber\\
%&=\left(\frac{\omega}{k}+1\right)\sum_{j=1}^pq_j\left[\text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)+\left\|{\mathbf{g}}_{j}^{(r)}\right\|^2\right]
\end{align}
where \text{\ding{192}} holds due to $\mathbb{E}\left[\left\|\boldsymbol{x}\right\|^2\right]=\text{Var}[\boldsymbol{x}]+\left\|\mathbb{E}[\boldsymbol{x}]\right\|^2$, \text{\ding{193}} is due to $\mathbb{E}_{\mathbf{S}}\left[\frac{1}{p}\sum_{j=1}^p\tilde{\mathbf{g}}_{\mathbf{S}j}^{(r)}\right]=\frac{1}{p}\sum_{j=1}^m\tilde{\mathbf{g}}_{j}^{(r)}$.


Next we show that from Assumptions~\ref{Assu:2}, we have 
\begin{align}\label{eq:100000}
    \mathbb{E}_{\xi^{(r)}}\left[\Big[\|{\tilde{\mathbf{g}}_j^{(r)}}-{\mathbf{g}_j^{(r)}}\|^2\Big]\right]\leq \tau \sigma^2
\end{align}
To do so, note that 
\begin{align}
    \text{Var}\left(\tilde{\mathbf{g}}_{j}^{(r)}\right)=\mathbb{E}_{\xi^{(r)}}\left[\left\|{\tilde{\mathbf{g}}_j^{(r)}}-{\mathbf{g}_j^{(r)}}\right\|^2\right]  \stackrel{\text{\ding{192}}}{=}\mathbb{E}_{\xi^{(r)}}\left[\left\|\sum_{c=0}^{\tau-1}\left[\tilde{\mathbf{g}}_j^{(c,r)}-\mathbf{g}_j^{(c,r)}\right]\right\|^2\right]&{=}\text{Var}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right)\nonumber\\
    &\stackrel{\text{\ding{193}}}{=}\sum_{c=0}^{\tau-1}\text{Var}\left(\tilde{\mathbf{g}}_j^{(c,r)}\right)\nonumber\\
    &{=}\sum_{c=0}^{\tau-1}\mathbb{E}\left[\left\|\tilde{\mathbf{g}}_j^{(c,r)}-\mathbf{g}_j^{(c,r)}\right\|^2\right]\nonumber\\
    &\stackrel{\text{\ding{194}}}{\leq}\tau\sigma^2\label{eq:var_b_mid}
    \end{align}
where in \text{\ding{192}} we use the definition of ${\tilde{\mathbf{g}}}_j^{(r)}$ and ${{\mathbf{g}}}_j^{(r)}$, in \text{\ding{193}} we use the fact that mini-batches are chosen in i.i.d. manner at each local machine, and \text{\ding{194}} immediately follows from Assumptions~\ref{Assu:1.5}.

Replacing $\mathbb{E}_{\xi^{(r)}}\left[\|{\tilde{\mathbf{g}}_j^{(r)}}-{\mathbf{g}_j^{(r)}}\|^2\right]$ in \eqref{eq:lemma111} by its upper bound in \eqref{eq:100000} implies that 
\begin{align}
\mathbb{E}_{{\xi^{(r)}|\boldsymbol{w}^{(r)}}}\mathbb{E}_{\mathbf{S},\mathcal{K}}\Big[\|\frac{1}{k}\sum_{j\in\mathcal{K}} \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]
\leq (k\omega+1)\frac{\tau\sigma^2}{k}+(\omega+1)\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(r)}\|^2\label{eq:lemma112}
\end{align}

Further note that we have 
\begin{align}
\left\|{\mathbf{g}}_j^{(r)}\right\|^2&=\|\sum_{c=0}^{\tau-1}\mathbf{g}_j^{(c,r)}\|^2\stackrel{}{\leq} \tau\sum_{c=0}^{\tau-1}\|\mathbf{g}_j^{(c,r)}\|^2\label{eq:mid-bounding-absg}
\end{align} 
where the last inequality is due to $\left\|\sum_{j=1}^n\boldsymbol{a}_i\right\|^2\leq n\sum_{j=1}^n\left\|\boldsymbol{a}_i\right\|^2$, which together with \eqref{eq:lemma112} leads to the following bound:
\begin{align}
    \mathbb{E}_{{\xi^{(r)}|\boldsymbol{w}^{(r)}}}\mathbb{E}_{\mathbf{S}}\Big[\|\frac{1}{k}\sum_{j\in\mathcal{K}} \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}^{(c,r)}_j\right)\|^2\Big]\leq(k\omega+1)\frac{\tau\sigma^2}{k}+\tau({\omega}+1)\sum_{j=1}^pq_j\|{\mathbf{g}}_{j}^{(c,r)}\|^2,
\end{align}
and the proof is complete.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}\label{lemma:cross-inner-bound-unbiased}
  Under Assumption~\ref{Assu:1}, and according to the \texttt{FedCOM} algorithm the expected inner product between stochastic gradient and full batch gradient can be bounded with:
\begin{align}
    - \mathbb{E}_{\xi,\mathbf{S},\mathcal{K}}\left[\left\langle\nabla f({\boldsymbol{w}}^{(r)}),{{\tilde{\mathbf{g}}}^{(r)}}\right\rangle\right]&\leq \frac{1}{2}\eta\frac{1}{m}\sum_{j=1}^m\sum_{c=0}^{\tau-1}\left[-\|\nabla f({\boldsymbol{w}}^{(r)})\|_2^2-\|\nabla{f}(\boldsymbol{w}_j^{(c,r)})\|_2^2+L^2\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\|_2^2\right]\label{eq:lemma3-thm2}
\end{align}

\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
We have:
\begin{align}
    &-\mathbb{E}_{\{{\xi}^{(t)}_{1}, \ldots, {\xi}^{(t)}_{m}|{\boldsymbol{w}}^{(t)}_{1},\ldots,  {\boldsymbol{w}}^{(t)}_{m}\}} \mathbb{E}_{\mathbf{S},\mathcal{K}}\left[ \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}_{\mathbf{S},\mathcal{K}}^{(r)}\big\rangle\right]\nonumber\\
    =&-\mathbb{E}_{\{{\xi}^{(t)}_{1}, \ldots, {\xi}^{(t)}_{m}|{\boldsymbol{w}}^{(t)}_{1},\ldots,  {\boldsymbol{w}}^{(t)}_{m}\}}\left[\left\langle \nabla f({\boldsymbol{w}}^{(r)}),\eta\sum_{j\in\mathcal{K}}q_j\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right\rangle\right]\nonumber\\
    =&-\left\langle \nabla f({\boldsymbol{w}}^{(r)}),\eta\sum_{j=1}^mq_j\sum_{c=0}^{\tau-1}\mathbb{E}_{\xi,\mathbf{S}}\left[\tilde{\mathbf{g}}_{j,\mathbf{S}}^{(c,r)}\right]\right\rangle\nonumber\\
        &=-\eta\sum_{c=0}^{\tau-1}\sum_{j=1}^mq_j\left\langle \nabla f({\boldsymbol{w}}^{(r)}),{\mathbf{g}}_j^{(c,r)}\right\rangle\nonumber\\ 
     \stackrel{\text{\ding{192}}}{=}&\frac{1}{2}\eta\sum_{c=0}^{\tau-1}\sum_{j=1}^mq_j\left[-\|\nabla f({\boldsymbol{w}}^{(r)})\|_2^2-\|{{\nabla{f}}}(\boldsymbol{w}_j^{(c,r)})\|_2^2+\|\nabla f({\boldsymbol{w}}^{(r)})-\nabla{f}(\boldsymbol{w}_j^{(c,r)})\|_2^2\right]\nonumber\\
    \stackrel{\text{\ding{193}}}{\leq}&\frac{1}{2}\eta\sum_{c=0}^{\tau-1}\sum_{j=1}^mq_j\left[-\|\nabla f({\boldsymbol{w}}^{(r)})\|_2^2-\|\nabla{f}(\boldsymbol{w}_j^{(c,r)})\|_2^2+L^2\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\|_2^2\right]
   \label{eq:bounding-cross-no-redundancy}
\end{align}

where \ding{192} is due to $2\langle \mathbf{a},\mathbf{b}\rangle=\|\mathbf{a}\|^2+\|\mathbf{b}\|^2-\|\mathbf{a}-\mathbf{b}\|^2$, and \ding{193} follows from Assumption~\ref{Assu:1}.
\end{proof}



The following lemma bounds the distance of local solutions from global solution at $r$th communication round.
\begin{lemma}\label{lemma:dif-under-pl-sgd-iid}
Under Assumptions~\ref{Assu:1.5} we have:
\begin{align}\notag
      \mathbb{E}\left[\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\|_2^2\right]&\leq\eta^2\tau\sum_{c=0}^{\tau-1}\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\eta^2\tau\sigma^2
\end{align}

\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}
Note that
\begin{align}\notag
 \mathbb{E}\left[\left\|{\boldsymbol{w}}^{(r)}-\boldsymbol{w}_j^{(c,r)}\right\|_2^2\right]&=\mathbb{E}\left[\left\|{\boldsymbol{w}}^{(r)}-\left({\boldsymbol{w}}^{(r)}-\eta\sum_{k=0}^{c}\tilde{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]\nonumber\\
 &=\mathbb{E}\left[\left\|\eta\sum_{k=0}^{c}\tilde{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &\stackrel{\text{\ding{192}}}{=}\mathbb{E}\left[\left\|\eta\sum_{k=0}^{c}\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\left[\left\|\eta\sum_{k=0}^{c}{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &\stackrel{\text{\ding{193}}}{=}\eta^2\sum_{k=0}^{c}\mathbb{E}\left[\left\|\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\left(c+1\right)\eta^2\sum_{k=0}^{c}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
  &{\leq}\eta^2\sum_{k=0}^{\tau-1}\mathbb{E}\left[\left\|\left(\tilde{\mathbf{g}}_j^{(k,r)}-{\mathbf{g}}_j^{(k,r)}\right)\right\|_2^2\right]+\tau\eta^2\sum_{k=0}^{\tau-1}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
  &\stackrel{\text{\ding{194}}}{\leq}\eta^2\sum_{k=0}^{\tau-1}\sigma^2+\tau\eta^2\sum_{k=0}^{\tau-1}\left[\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2\right]\nonumber\\
 &{=}\eta^2\tau\sigma^2+\eta^2\sum_{k=0}^{\tau-1}\tau\left\|{\mathbf{g}}_j^{(k,r)}\right\|_2^2
\end{align}
where \ding{192} comes from $\mathbb{E}\left[\mathbf{x}^2\right]=\text{Var}\left[\mathbf{x}\right]+\left[\mathbb{E}\left[\mathbf{x}\right]\right]^2$ and \ding{193} holds because $\text{Var}\left(\sum_{j=1}^n\mathbf{x}_j\right)=\sum_{j=1}^n\text{Var}\left(\mathbf{x}_j\right)$ for i.i.d. vectors $\mathbf{x}_i$ (and i.i.d. assumption comes from i.i.d. sampling), and finally \ding{194} follows from Assumption~\ref{Assu:1.5}.
\end{proof}

\subsubsection{Main result for the non-convex setting}
Now we are ready to present our result for the homogeneous setting. We first state and prove the result for the general non-convex objectives.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[non-convex]\label{thm:lsgwd-lr} For \texttt{FedSKETCH}$(\tau, \eta, \gamma)$, for all $0\leq t\leq R\tau-1$,  under Assumptions~\ref{Assu:1} to~\ref{Assu:1.5}, if the learning rate satisfies \begin{align}
   1\geq {\tau^2 L^2\eta^2}+\left(\omega+\frac{1}{k}\right){\eta\gamma L}{\tau}
\label{eq:cnd-thm4.3}
\end{align}
and all local model parameters are initialized at the same point ${\boldsymbol{w}}^{(0)}$, then the average-squared gradient after $\tau$ iterations is bounded as follows:
\begin{align}
        \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq \frac{2\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)}{\eta\gamma\tau R}+{L\eta\gamma{\left(\omega+\frac{1}{k}\right)}}\sigma^2+{L^2\eta^2\tau }\sigma^2 \ , \label{eq:thm1-result} 
\end{align}
where $\boldsymbol{w}^{(*)}$ is the global optimal solution with  function value $f(\boldsymbol{w}^{(*)})$.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
Before proceeding with the proof of Theorem~\ref{thm:lsgwd-lr}, we would like to highlight that 
\begin{align}
    \boldsymbol{w}^{(r)}- ~{\boldsymbol{w}}_{j}^{(\tau,r)}=\eta\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)} \ .\label{eq:decent-smoothe}
\end{align}

From the updating rule of Algorithm~\ref{Alg:PFLHom} we have

{
\begin{align}\notag
     {\boldsymbol{w}}^{(r+1)}=\boldsymbol{w}^{(r)}-\gamma\eta\left(\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbf{S}\Big(\sum_{c=0,r}^{\tau-1}\tilde{\mathbf{g}}_{j}^{(c,r)}\Big)\right)=\boldsymbol{w}^{(r)}-\gamma\left[\frac{\eta}{k}\sum_{j\in\mathcal{K}}\mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_{j}^{(c,r)}\right)\right] \ .
\end{align}
}
In what follows, we use the following notation to denote the stochastic gradient used to update the global model at $r$th communication round $$\tilde{\mathbf{g}}_{\mathbf{S},\mathcal{K}}^{(r)}\triangleq\frac{\eta}{p}\sum_{j=1}^{p}\mathbf{S}\left(\frac{\boldsymbol{w}^{(r)}- ~{\boldsymbol{w}}_{j}^{(\tau,r)}}{\eta}\right)=\frac{1}{k}\sum_{j\in\mathcal{K}}\mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right).$$ 
and notice that $\boldsymbol{w}^{(r)} = \boldsymbol{w}^{(r-1)} - \gamma \tilde{\mathbf{g}}^{(r)}$.


Then using the unbiased estimation property of sketching we have:
\begin{align}\notag
  \mathbb{E}_\mathbf{S}\left[\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\right]=\frac{1}{k}\sum_{j\in\mathcal{K}}\left[-\eta\mathbb{E}_\mathbf{S}\left[ \mathbf{S}\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right)\right]\right]=\frac{1}{k}\sum_{j\in\mathcal{K}}\left[-\eta\left(\sum_{c=0}^{\tau-1}\tilde{\mathbf{g}}_j^{(c,r)}\right)\right]\triangleq \tilde{\mathbf{g}}_{\mathbf{S},\mathcal{K}}^{(r)} \ .
\end{align}



%%%%%%%%%%%%%


From the $L$-smoothness gradient assumption on global objective, by using  $\tilde{\mathbf{g}}^{(r)}$ in inequality (\ref{eq:decent-smoothe}) we have:
\begin{align}
    f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\leq -\gamma \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle+\frac{\gamma^2 L}{2}\|\tilde{\mathbf{g}}^{(r)}\|^2\label{eq:Lipschitz-c1}
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By taking expectation on both sides of above inequality over sampling, we get:
\begin{align}
    \mathbb{E}\left[\mathbb{E}_\mathbf{S}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\Big]\right]&\leq -\gamma\mathbb{E}\left[\mathbb{E}_\mathbf{S}\left[ \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\big\rangle\right]\right]+\frac{\gamma^2 L}{2}\mathbb{E}\left[\mathbb{E}_\mathbf{S}\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\right]\nonumber\\
    &\stackrel{(a)}{=}-\gamma\underbrace{\mathbb{E}\left[\left[ \big\langle\nabla f({\boldsymbol{w}}^{(r)}),\tilde{\mathbf{g}}^{(r)}\big\rangle\right]\right]}_{(\mathrm{I})}+\frac{\gamma^2 L}{2}\underbrace{\mathbb{E}\left[\mathbb{E}_\mathbf{S}\Big[\|\tilde{\mathbf{g}}_\mathbf{S}^{(r)}\|^2\Big]\right]}_{\mathrm{(II)}} \ . \label{eq:Lipschitz-c-gd}
\end{align}
We proceed to use Lemma~\ref{lemma:tasbih1-iid}, Lemma~\ref{lemma:cross-inner-bound-unbiased}, and Lemma~\ref{lemma:dif-under-pl-sgd-iid}, to bound  terms $(\mathrm{I})$ and $(\mathrm{II})$ in right hand side of (\ref{eq:Lipschitz-c-gd}), which gives
\begin{align}
     &\mathbb{E}\left[\mathbb{E}_{\mathbf{S}}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\Big]\right]\nonumber\\
     \leq& \gamma\frac{1}{2}\eta\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\left[-\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2-\left\|\mathbf{g}_j^{(c,r)}\right\|_2^2+L^2\eta^2\sum_{c=0}^{\tau-1}\left[\tau\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\sigma^2\right]\right]\nonumber\\
     &\quad+\frac{\gamma^2 L(\omega+1)}{2}\left[{\eta^2\tau}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\|\mathbf{g}^{(c,r)}_{j}\|^2\right]+\frac{\gamma^2\eta^2 L(\omega+\frac{1}{k})}{2}{\tau \sigma^2}\nonumber\\
     \stackrel{\text{\ding{192}}}{\leq}&\frac{\gamma\eta}{2}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\left[-\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2-\left\|\mathbf{g}_j^{(c,r)}\right\|_2^2+\tau L^2\eta^2\left[\tau\left\|{\mathbf{g}}_j^{(c,r)}\right\|_2^2+\sigma^2\right]\right]\nonumber\\
     &\quad+\frac{\gamma^2 L(\omega+1)}{2}\left[{\eta^2\tau}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\|\mathbf{g}^{(c,r)}_{j}\|^2\right]+\frac{\gamma^2\eta^2 L(\omega+\frac{1}{k})}{2}\left(\tau \sigma^2\right)\nonumber\\
     =&-\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\nonumber\\
     &\quad-\left(1-{\tau L^2\eta^2\tau}-{(\omega+1)\eta\gamma L}{\tau}\right)\frac{\eta\gamma}{2}\sum_{j=1}^pq_j\sum_{c=0}^{\tau-1}\|\mathbf{g}^{(c,r)}_{j}\|^2+\frac{L\tau\gamma\eta^2 }{2}\left(L\tau\eta+\gamma(\omega+\frac{1}{k})\right)\sigma^2\nonumber\\
     \stackrel{\text{\ding{193}}}{\leq}& -\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2+\frac{L\tau\gamma\eta^2 }{2}\left(kL\tau\eta+\gamma(\omega+\frac{1}{k})\right)\sigma^2 \ ,  \label{eq:finalll}
\end{align}
where in \ding{192} we incorporate outer summation $\sum_{c=0}^{\tau-1}$, and  \ding{193} follows from condition 
\begin{align}\notag
   1\geq {\tau L^2\eta^2\tau}+({\omega}+1)\eta\gamma L{\tau} \ . 
\end{align}
Summing up for all $R$ communication rounds and  rearranging the terms gives:
\begin{align}\notag
    \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq \frac{2\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)}{\eta\gamma\tau R}+{L\eta\gamma{(\omega+\frac{1}{k})}}\sigma^2+{L^2\eta^2\tau }\sigma^2 \ . 
\end{align}
From the above inequality, is it easy to see that in order to achieve a linear speed up, we need to have $\eta\gamma=O\left(\frac{1}{\sqrt{R \tau\left(\omega+\frac{1}{k}\right)}}\right)$.
\end{proof}


\begin{corollary}[Linear speed up] 
In (\ref{eq:thm1-result}) for the choice of  $\eta\gamma=O\left(\frac{1}{L}\sqrt{\frac{1}{R\tau\left(\omega+\frac{1}{k}\right)}}\right)$, and $\gamma\geq k$  the  convergence rate reduces to:
\begin{align}
    \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2&\leq O\left(\frac{L\sqrt{\left(\omega+\frac{1}{k}\right)}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{*})\right)}{\sqrt{R\tau}}+\frac{\left(\sqrt{\left(\omega+\frac{1}{k}\right)}\right)\sigma^2}{\sqrt{R\tau}}+\frac{\sigma^2}{R\left(\omega+\frac{1}{k}\right)\gamma^2}\right).\label{eq:convg-error}
\end{align}
Note that according to (\ref{eq:convg-error}), if we pick  a fixed constant value for  $\gamma$, in order to achieve an $\epsilon$-accurate solution, $R=O\left(\frac{1}{\epsilon}\right)$ communication rounds and $\tau=O\left(\frac{\omega+\frac{1}{k}}{\epsilon}\right)$ local updates are necessary. %We also highlight  that (\ref{eq:convg-error}) also allows us to choose $R=O\left(\frac{\omega+1}{\epsilon}\right)$ and $\tau=O\left(\frac{1}{k\epsilon}\right)$ to get the  same convergence rate.
\end{corollary}

\begin{remark}\label{rmk:cnd-lr}

Condition in (\ref{eq:cnd-thm4.3}) can be rewritten as 
\begin{align}
    \eta&\leq \frac{-\gamma L\tau\left(\omega+\frac{1}{k}\right)+\sqrt{\gamma^2 \left(L\tau\left(\omega+\frac{1}{k}\right)\right)^2+4L^2\tau^2}}{2L^2\tau^2}\nonumber\\
    &= \frac{-\gamma L\tau\left(\omega+\frac{1}{k}\right)+L\tau\sqrt{\left(\omega+\frac{1}{k}\right)^2\gamma^2 +4}}{2L^2\tau^2}\nonumber\\
    &=\frac{\sqrt{\left(\omega+\frac{1}{k}\right)^2\gamma^2 +4}-\left(\omega+\frac{1}{k}\right)\gamma}{2L\tau} \ . \label{eq:lrcnd}
\end{align}

So based on (\ref{eq:lrcnd}), if we set $\eta=O\left(\frac{1}{L\gamma}\sqrt{\frac{1}{R\tau\left(\omega+\frac{1}{k}\right)}}\right)$, it implies that:
\begin{align}
    R\geq \frac{\tau }{\left(\omega+\frac{1}{k}\right)\gamma^2\left(\sqrt{\left(\omega+\frac{1}{k}\right)^2\gamma^2+4}-\left(\omega+\frac{1}{k}\right)\gamma\right)^2} \ . \label{eq:iidexact}
\end{align}
We note that $\gamma^2\left(\sqrt{\left(\omega+\frac{1}{k}\right)^2\gamma^2+4}-\left(\omega+\frac{1}{k}\right)\gamma\right)^2=\Theta(1)\leq 5 $ therefore even for $\gamma\geq m$ we need to have 
\begin{align}
    R\geq \frac{\tau }{5\left(\omega+\frac{1}{k}\right)}=O\left(\frac{\tau }{\left(\omega+\frac{1}{k}\right)}\right)\label{eq:lrbnd-homog} \ .
\end{align}

Therefore, for the choice of $\tau=O\left(\frac{\omega+\frac{1}{k}}{\epsilon}\right)$, due to condition in (\ref{eq:lrbnd-homog}), we need to have $R=O\left(\frac{1}{\epsilon}\right)$. %Similarly, we can have $R=O\left(\frac{\omega+1}{\epsilon}\right)$ and $\tau=O\left(\frac{1}{k\epsilon}\right)$.}


\end{remark}

\begin{corollary}[Special case, $\gamma=1$]
By letting $\gamma=1$, $\omega=0$ and $k=p$ the convergence rate in (\ref{eq:thm1-result}) reduces to 
\begin{align}\notag
     \frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2&\leq \frac{2\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)}{\eta R\tau}+\frac{L\eta }{p}\sigma^2+{L^2\eta^2\tau }\sigma^2 \ ,
\end{align}
which matches the rate  obtained in~\citep{wang2018cooperative}. In this case the communication complexity and the number of local updates become 
\begin{align}\notag
    {R}=O\left(\frac{p}{\epsilon}\right), \:\:\: \tau=O\left(\frac{1}{\epsilon}\right) \, ,
\end{align}
which simply implies  that in this special case the convergence rate of our algorithm reduces to the  rate obtained in~\citep{wang2018cooperative}, which indicates the tightness of  our analysis.
\end{corollary}



\subsubsection{Main result for the PL/Strongly convex setting}
 
We now turn to stating the convergence rate for the homogeneous setting under PL condition which naturally leads to the same rate for strongly convex functions.
\begin{theorem}[PL or strongly convex]\label{thm:pl-iid}
For \texttt{FedSKETCH}$(\tau, \eta, \gamma)$, for all $0\leq t\leq R\tau-1$,  under Assumptions~\ref{Assu:1} to~\ref{Assu:1.5} and~\ref{assum:pl}, if the learning rate satisfies 
\begin{align}\notag
   1\geq {\tau^2 L^2\eta^2}+\left(\omega+1\right){\eta\gamma L}{\tau}  \
%\label{eq:cnd-thm4.3}
\end{align}

and if the all the models are initialized with $\boldsymbol{w}^{(0)}$ we obtain:
\begin{align}\notag
        \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \left(1-\eta\gamma{\mu\tau}\right)^R\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\mu}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(\omega+\frac{1}{k}\right)\frac{\gamma\eta L\sigma^2}{2}\right]
\end{align}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
From (\ref{eq:finalll}) under condition:
\begin{align}\notag
       1\geq {\tau L^2\eta^2\tau}+{{(\omega+1)}\eta\gamma L}{\tau} 
\end{align}
we obtain:
\begin{align}\notag
         \mathbb{E}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(r)})\Big]&\leq -\eta\gamma\frac{\tau}{2}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2+\frac{L\tau\gamma\eta^2 }{2}\left(L\tau\eta+\gamma(\omega+\frac{1}{k})\right)\sigma^2\nonumber\\
         &\leq -\eta\mu\gamma{\tau} \left(f({\boldsymbol{w}}^{(r)})-f({\boldsymbol{w}}^{(r)})\right)+\frac{L\tau\gamma\eta^2 }{2}\left(L\tau\eta+\gamma(\omega+\frac{1}{k})\right)\sigma^2 
\end{align}
which leads to the following bound:
\begin{align}\notag
            \mathbb{E}\Big[f({\boldsymbol{w}}^{(r+1)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq \left(1-\eta\mu\gamma{\tau}\right) \Big[f({\boldsymbol{w}}^{(r)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{L\tau\gamma\eta^2 }{2}\left(L\tau\eta+{(\omega+\frac{1}{k})}\gamma\right)\sigma^2
\end{align}
By setting $\Delta=1-\eta\mu\gamma{\tau}$ we obtain  the following bound:
\begin{align}\notag
            &\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\nonumber\\
            \leq& \Delta^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1-\Delta^R}{1-\Delta}\frac{L\tau\gamma\eta^2 }{2}\left(L\tau\eta+{(\omega+\frac{1}{k})}\gamma\right)\sigma^2\nonumber\\
            \leq& \Delta^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1}{1-\Delta}\frac{L\tau\gamma\eta^2 }{2}\left(L\tau\eta+{(\omega+\frac{1}{k})}\gamma\right)\sigma^2\nonumber\\
            =&{\left(1-\eta\mu\gamma{\tau}\right)}^R \Big[f({\boldsymbol{w}}^{(0)})-f({\boldsymbol{w}}^{(*)})\Big]+\frac{1}{\eta\mu\gamma{\tau}}\frac{L\tau\gamma\eta^2 }{2}\left(L\tau\eta+{(\omega+\frac{1}{k})}\gamma\right)\sigma^2
\end{align}
\end{proof}


\begin{corollary}
If we  let $\eta\gamma\mu\tau\leq\frac{1}{2}$, $\eta=\frac{1}{2L\left(\omega+\frac{1}{k}\right)\tau\gamma }$ and $\kappa=\frac{L}{\mu}$ the convergence error in Theorem~\ref{thm:pl-iid}, with $\gamma\geq k$ results in:

\begin{align}\notag
&\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\nonumber\\
\leq& e^{-\eta\gamma{\mu\tau}R}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\mu}}\left[\frac{1}{2} \tau L^2\eta^2\sigma^2+\left(\omega+\frac{1}{k}\right)\frac{\gamma\eta L\sigma^2}{2}\right]\nonumber\\
\leq& e^{-\frac{R}{2\left(\omega+\frac{1}{k}\right)\kappa}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\mu}}\left[\frac{1}{2} L^2\frac{\tau\sigma^2}{L^2\left(\omega+\frac{1}{k}\right)^2\gamma^2\tau^2}+\frac{\left(1+\omega\right) L\sigma^2}{2\left(\omega+\frac{1}{k}\right)L\tau }\right]\nonumber\\
=&O\left(e^{-\frac{R}{2\left(\omega+\frac{1}{k}\right)\kappa}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{\sigma^2}{\left(\omega+\frac{1}{k}\right)^2\gamma^2\mu\tau}+\frac{\left(\omega+1\right)\sigma^2}{\mu\left(\omega+\frac{1}{k}\right) \tau }\right)
%\nonumber\\
%=&O\left(e^{-\frac{R}{2\left(\frac{\omega}{k}+1\right)\kappa}}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{\sigma^2}{\gamma^2\mu\tau}+\frac{\left(\omega+1\right)\sigma^2}{\mu\left(\frac{\omega}{k}+1\right) \tau k}\right)
\end{align}
which indicates  that to achieve an error of $\epsilon$, we need to have $R=O\left(\left(\omega+\frac{1}{k}\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $\tau=\frac{\left(\omega+1\right)}{\left(\omega+\frac{1}{k}\right)\epsilon}$. %{Additionally, we note that if $\gamma\rightarrow\infty$, yet $R=O\left(\left(\frac{\omega}{k}+1\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $\tau=\frac{\left(\omega+1\right)}{k\left(\frac{\omega}{k}+1\right)\epsilon}$ will be necessary.}
\end{corollary}

%\newpage
\subsubsection{Main result for the general convex setting}

\begin{theorem}[Convex]\label{thm:cvx-iid}
 For a general convex function $f(\boldsymbol{w})$ with optimal solution $\boldsymbol{w}^{(*)}$, using  \texttt{FedSKETCH}$(\tau, \eta, \gamma)$ to optimize $\tilde{f}(\boldsymbol{w},\phi)=f(\mathbf{\boldsymbol{w}})+\frac{\phi}{2}\left\|\boldsymbol{w}\right\|^2$,  for all $0\leq t\leq R\tau-1$,  under Assumptions~\ref{Assu:1} to~\ref{Assu:1.5}, if the learning rate satisfies 
 \begin{align}\notag
   1\geq {\tau^2 L^2\eta^2}+\left({\omega}+1\right){\eta\gamma L}{\tau} 
%\label{eq:cnd-thm4.3}
\end{align}
%\todo{Continue from here!}

and if the all the models initiate with $\boldsymbol{w}^{(0)}$, with $\phi=\frac{1}{\sqrt{\tau}}$ and $\eta=\frac{1}{2L\gamma\tau\left(1+\frac{\omega}{k}\right)}$ we obtain:
\begin{align}
        \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]&\leq e^{-\frac{\sqrt{\tau} R}{2L\left(\omega+\frac{1}{k}\right) }}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)\nonumber\\
        &\qquad +\left[\frac{\sigma^2}{8\sqrt{\tau}\gamma^2\left(\omega+\frac{1}{k}\right)^2} +\frac{\sigma^2}{4\sqrt{\tau}} \right] +\frac{1}{2\sqrt{\tau}}\left\|\boldsymbol{w}^{(*)}\right\|^2\label{eq:cvx-iid}
\end{align}{{}}
\end{theorem}
We note that above theorem implies that to achieve a convergence error of $\epsilon$ we need to have $R=O\left(L\left(\omega+\frac{1}{k}\right)\frac{1}{\epsilon}\log\left(\frac{1}{\epsilon}\right)\right)$ and $\tau=O\left(\frac{1}{\epsilon^2}\right)$.


\begin{proof}
Since $\tilde{f}(\boldsymbol{w}^{(r)},\phi)=f(\boldsymbol{w}^{(r)})+\frac{\phi}{2}\left\|\boldsymbol{w}^{(r)}\right\|^2$ is $\phi$-PL, according to Theorem~\ref{thm:pl-iid}, we have:
\begin{align}
   & \tilde{f}(\boldsymbol{w}^{(R)},\phi)-\tilde{f}(\boldsymbol{w}^{(*)},\phi)\nonumber\\
   =&{f}(\boldsymbol{w}^{(r)})+\frac{\phi}{2}\left\|\boldsymbol{w}^{(r)}\right\|^2-\left({f}(\boldsymbol{w}^{(*)})+\frac{\phi}{2}\left\|\boldsymbol{w}^{(*)}\right\|^2\right)\nonumber\\
    \leq& \left(1-\eta\gamma{\phi\tau}\right)^R\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\phi}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(\frac{1}{k}+\omega\right)\frac{\gamma\eta L\sigma^2}{2}\right]\label{eq:mid-cvx}
\end{align}
Next rearranging (\ref{eq:mid-cvx}) and replacing $\mu$ with $\phi$ leads to the following error bound:
\begin{align}\notag
  &  {f}(\boldsymbol{w}^{(R)})-f^*\\\notag
  \leq& \left(1-\eta\gamma{\phi\tau}\right)^R\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\phi}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(\frac{1}{k}+\omega\right)\frac{\gamma\eta L\sigma^2}{2}\right]\\\notag
  &\qquad +\frac{\phi}{2}\left(\left\|\boldsymbol{w}^*\right\|^2-\left\|\boldsymbol{w}^{(r)}\right\|^2\right)\\\notag
    \leq& e^{-\left(\eta\gamma{\phi\tau}\right)R}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\frac{1}{{\phi}}\left[\frac{1}{2} L^2\tau\eta^2\sigma^2+\left(\frac{1}{k}+\omega\right)\frac{\gamma\eta L\sigma^2}{2}\right] +\frac{\phi}{2}\left\|\boldsymbol{w}^{(*)}\right\|^2 
\end{align}
Next, if we set $\phi=\frac{1}{\sqrt{\tau}}$ and $\eta=\frac{1}{2\left(\frac{1}{k}+{\omega}\right)L\gamma \tau}$, we obtain that
\begin{align}\notag
        &{f}(\boldsymbol{w}^{(R)})-f^*\\\notag
        \leq& e^{-\frac{\sqrt{\tau}R}{2\left(\frac{1}{k}+\omega\right)L }}\left(f(\boldsymbol{w}^{(0)})-f(\boldsymbol{w}^{(*)})\right)+\sqrt{\tau}\left[\frac{\sigma^2}{8\tau\gamma^2\left(\frac{1}{k}+\omega\right)^2} +\frac{\sigma^2}{4\tau }\right] +\frac{1}{2\sqrt{\tau}}\left\|\boldsymbol{w}^{(*)}\right\|^2 ,
\end{align}
thus the proof is complete. 
\end{proof}

\newpage




\newpage
%\section{Proof of Main Theorems}
%The proof of Theorem~\ref{thm:homog_case} follows directly from the results in~\citep{haddadpour2020federated}. 


%\subsection{Proof of Theorem~\ref{thm:homog_case}}
%Based on Assumption~\ref{Assu:quant} we have:
%\begin{theorem}[\citep{haddadpour2020federated}]\label{thm:fromhaddad}
 %Consider \texttt{FedCOM} in~\citep{haddadpour2020federated}. Suppose that the conditions in Assumptions~\ref{Assu:1},~\ref{Assu:1.5} and~\ref{Assu:quant} hold. If the local data distributions of all users are identical (homogeneous setting), then we have  
 %\begin{itemize}
    % \item \textbf{non-convex:}  By choosing stepsizes as $\eta=\frac{1}{L\gamma}\sqrt{\frac{p}{R\tau\left(\frac{\omega}{p}+1\right)}}$ and $\gamma\geq p$, the sequence of iterates satisfies  $\frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq {\epsilon}$ if we set
     %$R=O\left(\frac{1}{\epsilon}\right)$ and $ \tau=O\left(\frac{\frac{\omega}{p}+1}{{p}\epsilon}\right)$.
     %\item \textbf{Strongly convex or PL:}
      %By choosing stepsizes as $\eta=\frac{1}{2L\left(\frac{\omega}{p}+1\right)\tau\gamma}$ and $\gamma\geq m$, we obtain that the iterates satisfy $\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if  we set
     %$R=O\left(\left(\frac{\omega}{p}+1\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon}\right)$.
    % \item \textbf{Convex:} By choosing stepsizes as $\eta=\frac{1}{2L\left(\frac{\omega}{p}+1\right)\tau\gamma}$ and $\gamma\geq p$, we obtain that the iterates satisfy $ \mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if we set
     %$R=O\left(\frac{L\left(1+\frac{\omega}{p}\right)}{\epsilon}\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon^2}\right)$.
% \end{itemize}
%\end{theorem}

%\begin{proof}
%Since the sketching \texttt{PRIVIX} and \texttt{HEAPRIX}, satisfy Assumption~\ref{Assu:quant} with $\omega=\mu^2d$ and $\omega=\mu^2d-1$ respectively with probability $1-\delta$.  Therefore, all the results in Theorem~\ref{thm:homog_case}, conclude from Theorem~\ref{thm:fromhaddad} with probability $1-\delta$ and plugging $\omega=\mu^2d$ and $\omega=\mu^2d-1$ respectively into the corresponding convergence bounds.
%\end{proof}


\subsection{Proof of Theorem~\ref{thm:hetreg_case}}
The proof of Theorem~\ref{thm:hetreg_case} follows directly from the results in~\citep{haddadpour2020federated}. We first mention the general Theorem~\ref{thm:fromhaddad-het} from~\citep{haddadpour2020federated} for general compression noise $\omega$. Next, since the sketching \texttt{PRIVIX} and \texttt{HEAPRIX}, satisfy Assumption~\ref{Assu:quant} with $\omega=c\frac{d}{m}$ and $\omega=c \frac{d}{m}-1$ respectively with probability $1-\frac{\delta}{R}$ per communication round,  all the results in Theorem~\ref{thm:hetreg_case}, conclude from Theorem~\ref{thm:fromhaddad-het}  with probability $1-\delta$ (by  taking union over the all probabilities of each communication rounds with probability $1-\delta/R$) and plugging $\omega=c \frac{d}{m}$ and $\omega=c \frac{d}{m}-1$ respectively into the corresponding convergence bounds. 
For the heterogeneous setting, the results in~\citep{haddadpour2020federated} requires the following extra assumption that naturally holds for the sketching: 

\begin{assumption}[\citep{haddadpour2020federated}]\label{assum:009}
The compression scheme $Q$ for the heterogeneous data distribution setting satisfies the following condition $
    \mathbb{E}_Q[\|\frac{1}{m}\sum_{j=1}^m Q(\boldsymbol{x}_j)\|^2-\|Q(\frac{1}{m}\sum_{j=1}^m \boldsymbol{x}_j)\|^2]\leq G_q$.
\end{assumption}
We note that since sketching is a linear compressor, in the case of our algorithms for heterogeneous setting we have $G_q=0$. 


Next, we restate the Theorem in~\citep{haddadpour2020federated} here as follows:

\begin{theorem}\label{thm:fromhaddad-het}
 Consider \texttt{FedCOMGATE} in~\citep{haddadpour2020federated}. If Assumptions~\ref{Assu:1},~\ref{Assu:2},~\ref{Assu:quant}  and~\ref{assum:009} hold, then even for the case the local data distribution of users are different  (heterogeneous setting) we have
 \begin{itemize}
     \item \textbf{non-convex:} By choosing stepsizes as $\eta=\frac{1}{L\gamma}\sqrt{\frac{p}{R\tau\left(\omega+1\right)}}$ and $\gamma\geq p$, we obtain that the iterates satisfy  $\frac{1}{R}\sum_{r=0}^{R-1}\left\|\nabla f({\boldsymbol{w}}^{(r)})\right\|_2^2\leq \epsilon$ if we set
     $R=O\left(\frac{\omega+1}{\epsilon}\right)$ and $ \tau=O\left(\frac{1}{p\epsilon}\right)$.
     \item \textbf{Strongly convex or PL:}
      By choosing stepsizes as $\eta=\frac{1}{2L\left(\frac{\omega}{p}+1\right)\tau\gamma}$ and ${\gamma\geq \sqrt{p\tau}}$, we obtain that the iterates satisfy $\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if we set
      $R=O\left(\left(\omega+1\right)\kappa\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon}\right)$.
     \item \textbf{Convex:}  By choosing stepsizes as $\eta=\frac{1}{2L\left(\omega+1\right)\tau\gamma}$ and ${\gamma\geq \sqrt{p\tau}}$, we obtain that the iterates satisfy $\mathbb{E}\Big[f({\boldsymbol{w}}^{(R)})-f({\boldsymbol{w}}^{(*)})\Big]\leq \epsilon$ if we set
     $R=O\left(\frac{L\left(1+\omega\right)}{\epsilon}\log\left(\frac{1}{\epsilon}\right)\right)$ and $ \tau=O\left(\frac{1}{p\epsilon^2}\right)$.
 \end{itemize}
 
\end{theorem}
\begin{proof}
Since the sketching methods \texttt{PRIVIX} and \texttt{HEAPRIX}, satisfy the Assumption~\ref{Assu:quant} with $\omega=c\frac{d}{m}$ and $\omega=c\frac{d}{m}-1$ respectively with probability $1-\frac{\delta}{R}$ per communication round, we conclude the proofs of Theorem~\ref{thm:hetreg_case} using Theorem~\ref{thm:fromhaddad-het} with probability $1-\delta$ (by taking union over all communication rounds) and plugging $\omega=c\frac{d}{m}$ and $\omega=c\frac{d}{m}-1$ respectively into the convergence bounds.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse

\section{Comparison to Related Methods}


Before we compare with prior studies we highlight that privacy is another purpose of using unbiased sketching in addition to communication efficiency. Therefore, our main competing schemes are distributed algorithms based on sketching. Yet, for the sake of showing the effectiveness of our algorithms we also compare with prior non-sketching based distributed algorithms. 
\vspace{0.05in}

\subsection{Comparison with~\citep{li2019privacy}.} Note that our convergence analysis does not rely on the bounded gradient assumption. We also improve both the number of communication rounds $R$ and the size of transmitted bits $B$ per communication round. 
Additionally, we highlight that, while~\citep{li2019privacy} provides a convergence analysis for convex objectives, our analysis holds for PL (thus strongly convex case), general convex and general non-convex objectives.

\subsection{Comparison with~\citep{rothchild2020fetchsgd}.}
%Consider the two variants of \texttt{FetchSGD} in~\citep{rothchild2020fetchsgd}. 
%While, in our schemes, we do not need to have access to the exact entries of gradients, since the approaches in~\citep{rothchild2020fetchsgd} are based on $top_k$ queries, both of their proposed algorithms require having access to the exact value of $top_k$ gradients, hence they do not preserve privacy. 
Due to gradient tracking, our algorithm tackles data heterogeneity issue, while algorithms in~\citep{rothchild2020fetchsgd} does not particularly. 
As a consequence, in \texttt{FedSKETCHGATE} each device has to store an additional state vector compared to~\citep{rothchild2020fetchsgd}. 
Yet, unlike ~\citep{rothchild2020fetchsgd}, as our method is built upon an unbiased compressor, server does not need to store any additional error correction vector.
The convergence results for both of two variants of \texttt{FetchSGD} in~\citep{rothchild2020fetchsgd} rely on the uniform bounded gradient assumption which may not be applicable with $L$-smoothness assumption when data distribution is highly heterogeneous, as in FL, see~\citep{bayoumi2020tighter}, while our bounds do not assume such boundedness.
{\color{blue}Besides, Theorem 1~\citep{rothchild2020fetchsgd} assumes that \emph{Contraction Holds} for the sequence of gradients which may not hold in practice, yet based on this strong assumption their total communication cost ($RB$) in order to achieve $\epsilon-$ error is $RB=O\left(m\max(\frac{1}{\epsilon^2},\frac{d^2-dm}{m^2\epsilon})\log\left(\frac{d}{\delta}\max(\frac{1}{\epsilon^2},\frac{d^2-dm}{m^2\epsilon})\right)\right)$.
Note that for the sake of comparison we let the compression ratio in~\citep{rothchild2020fetchsgd} to be $\frac{m}{d}$. 
In contrast, without any extra assumptions, our results in Theorem~\ref{thm:hetreg_case} for \texttt{PRIVIX} and \texttt{HEAPRIX} are respectively $RB=O(\frac{(d+m)}{\epsilon}\log(\frac{{(\frac{d^2}{m})}+d}{\epsilon\delta}))$ and $RB=O(\frac{d}{\epsilon}\log(\frac{d^2}{\epsilon m\delta}))$ which improves the total communication cost of Theorem 1 in~\citep{rothchild2020fetchsgd} under regimes such that $\frac{1}{\epsilon}\geq d$ or $d\gg m$. 
Theorem 2 in~\citep{rothchild2020fetchsgd} is based on the assumption of \emph{Sliding Window Heavy Hitters}, which is similar to the gradient diversity assumption in~\citep{li2018federated,haddadpour2019convergence}. 
They show that, under such an assumption, the total communication cost is $RB=O\left(\frac{m\max(I^{2/3},2-\alpha)}{\epsilon^3\alpha}\log\left(\frac{d\max(I^{2/3},2-\alpha)}{\epsilon^3\delta}\right)\right)$ where $I$ is a constant related to the window of gradients assumption.
Our result improves the latter bound with weaker assumptions in a regime where $\frac{I^{2/3}}{\epsilon^2}\geq d$. 
We also provide bounds for PL, convex and non-convex objectives unlike~\citep{rothchild2020fetchsgd}. 
Finally, we note that algorithms in~\citep{rothchild2020fetchsgd} are using momentum at sever. 
While we do not use it explicitly, we note that we can modify our algorithms to easily add momentum.}

\noindent\textbf{Comparison  with~\citep{haddadpour2020federated}}
{\color{blue}Convergence analysis of algorithms in~\citep{haddadpour2020federated} relies on unbiased compression, while in this paper our FL algorithm based on \texttt{HEAPRIX} enjoys from unbiased compression with equivalent biased compression variance. Moreover, we highlight that convergence analysis of \texttt{FedCOMGATE} is based on the extra assumption that bounds the difference between average of compressed vectors and compressed averages of vectors. However, we do not need this extra assumption as it is satisfied naturally due to linearity of sketching. Finally, as pointed out in Remark~\ref{rmk:bidirect}, our algorithms enjoy from bidirectional compression property, unlike \texttt{FedCOMGATE} in general.} %Therefore, for instance in case of general non-convex our algorithm improves the total number of transmitted bits per device from $RB=O\left(\frac{d}{m\log(\frac{d}{\delta})}\frac{d}{\epsilon}\right)$ (by letting $q=\frac{d}{mt}=\frac{d}{m\log(\frac{d}{\delta})}$ and $B=O\left(d\right)$) to $RB=O\left(\log(\frac{d^2}{m\epsilon\delta})\frac{d}{\epsilon}\right)$. }

\subsection{Comparison with~\citep{basu2019qsparse}}{\color{blue}\:  We note that the algorithm in~\citep{basu2019qsparse} uses a composed compression and quantization, however our algorithm is based on only compression. So, in order to compare with algorithms in~\citep{basu2019qsparse} we only consider Qsparse-local-SGD with compression and we let compression factor $\gamma=\frac{m}{d}$ (based on $top_{mt}$ where $mt$ is the size of sketches). For strongly convex objective in   Qsparse-local-SGD to achieve convergence error of $\epsilon$ they require $R=O\left(\kappa\frac{d}{m\sqrt{\epsilon}}\right)$ and $\tau=O\left(\frac{m}{pd\sqrt{\epsilon}}\right)$, which is improved to $R=O\left(\frac{\kappa d}{m}\log (1/\epsilon)\right)$ and $\tau=O\left(\frac{1}{p\epsilon}\right)$ for PL   objectives. Similarly, for non-convex objective~\citep{basu2019qsparse} requires $R=O\left(\frac{d}{m\epsilon^{1.5}}\right)$ and $\tau=O\left(\frac{m}{pd\sqrt{\epsilon}}\right)$, which is improved to $R=O\left(\frac{d}{m\epsilon}\right)$ and $\tau=O\left(\frac{1}{p\epsilon}\right)$. We note that we reduce communication rounds at the cost of increasing number of local updates (which scales down with number of devices, $p$). Additionally, we highlight that our \texttt{FedSKETCHGATE} exploits the gradient tracking idea to deal with data heterogeneity, while algorithms in~\citep{basu2019qsparse} does not develop such mechanism and may suffer from poor convergence in heterogeneous setting. We also note that setting $\tau=1$ and using $top_{m}$ compressor, the QSPARSE-local-SGD algorithm becomes similar to distributed SGD with sketching as they both use the error feedback framework to improve the compression variance.  
Finally, since the average of sparse vectors may not be sparse in general the number of transmitted bits from server to devices in QSPARSE-Local-SGD in~\citep{basu2019qsparse} may not be sparse in general ($B=O(d)$), however our algorithms enjoy from bidirectional compression properly due to lower dimension and linearity properties of sketching ($B=O(m\log(\frac{Rd}{\delta}))$). Therefore, the total number of bits per device for strongly convex and non-convex objective is improved respectively from $RB=O\left(\kappa\frac{d^2}{m\sqrt{\epsilon}}\right)$ and $RB=O\left(\frac{d^2}{m\epsilon^{1.5}}\right)$ in~\citep{basu2019qsparse} to $RB=O\left({\kappa d\log(\frac{\kappa d^2}{m\delta}\log (\frac{1}{\epsilon})) }\log (1/\epsilon)\right)=O\left({\kappa d\max\Big(\log(\frac{\kappa d^2}{m\delta}}),\log^2 (1/\epsilon)\Big)\right)$ and $RB=O\left(\log(\frac{d^2}{m\epsilon\delta})\frac{d}{\epsilon}\right)$.}

\subsection{Comparison with~SCAFFOLD}{\color{blue}\:  To compare with~\citep{karimireddy2019scaffold} which does not use gradient compression, we let $m=d$ (no compression). In this case, similar to~\citep{haddadpour2020federated}, our communication complexities and number of local updates match with corresponding bounds obtained by SCAFFOLD with difference that in downlink (from devices to server) we only send one vector while SCAFFOLD needs to send two vectors (additional control variate).}
\fi


\clearpage
\section{Numerical Experiments and Additional Results}\label{app:numericals}

\subsection{Implementation of FetchSGD}
Our implementation of \texttt{FetchSGD} basically follows the original paper (Algorithm 1 in~\citep{rothchild2020fetchsgd}). The only difference is that, in the original algorithm, the local workers compress the gradient (in every local step) and transmit it to the central server. In our setting, we extend to the case with multiple local updates, where the difference in local weights are transmitted (same as the standard FL framework). Also, TopK compression is used to decode the sketches at the central server. We apply the same implementation trick that when accumulating the errors, we only count the non-zero coordinates and leave other coordinates zero for the accumulator. This greatly improves the empirical performance.

\subsection{Additional Plots for the MNIST Experiments}
% We are adding in this section, numerical runs for an intermediary number of local updates $\tau =2$ and confirm for both cases the trend we observe in the plots of the main text regarding increasing this number. Our results illustrate the advantage of our proposed FS-HEAPRIX strategy in communication-efficient federated learning.

\subsubsection{Homogeneous setting}
In the homogeneous case, each node has same data distribution. 
To achieve this setting, we randomly choose samples uniformly from 10 classes of hand-written digits. 
The train loss and test accuracy are provided in Figure~\ref{fig:MNIST-iid1-app}, where we report local epochs $\tau=2$ in addition to the main context (single local update). 
The number of users is set to 50, and in each round of training we randomly pick half of the nodes to be active (i.e., receiving data and performing local updates). 
We can draw similar conclusion: FS-HEAPRIX consistently performs better than other competing methods. The test accuracy increases with larger $\tau$ in homogeneous setting.

\begin{figure}[h]
	\begin{center}
		\mbox{%				  
		 \includegraphics[width=1.9in]{MNIST_figures/local1_sketch20_iid1_train_loss.eps} 
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch20_iid1_train_loss.eps} 
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch20_iid1_train_loss.eps}}
		
		\mbox{%
		\includegraphics[width=1.9in]{MNIST_figures/local1_sketch20_iid1_test_acc.eps} %
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch20_iid1_test_acc.eps} %
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch20_iid1_test_acc.eps}%
		}
		\mbox{%	   
		\includegraphics[width=1.9in]{MNIST_figures/local1_sketch50_iid1_train_loss.eps}% 
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch50_iid1_train_loss.eps} %
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch50_iid1_train_loss.eps}}
		\mbox{%	
		\includegraphics[width=1.9in]{MNIST_figures/local1_sketch50_iid1_test_acc.eps}% 
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch50_iid1_test_acc.eps} %
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch50_iid1_test_acc.eps}
		}
	\end{center}
	\caption{MNIST Homogeneous case: Comparison of compressed optimization methods on LeNet CNN architecture.}
    \label{fig:MNIST-iid1-app}
\end{figure}

\subsubsection{Heterogeneous setting}

Analogously, we present experiments on MNIST dataset under heterogeneous data distribution, including $\tau=2$. We simulate the setting by only sending samples from one digit to each local worker (very few nodes get two classes). We see from Figure~\ref{fig:MNIST-iid0-app} that FS-HEAPRIX shows consistent advantage over competing methods. SketchedSGD performs poorly in this case.

\begin{figure}[h]
	\begin{center}
		\mbox{%			   
		\includegraphics[width=1.9in]{MNIST_figures/local1_sketch20_iid0_train_loss.eps}% 
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch20_iid0_train_loss.eps}% 
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch20_iid0_train_loss.eps} }
		
		\mbox{%	
		\includegraphics[width=1.9in]{MNIST_figures/local1_sketch20_iid0_test_acc.eps} %
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch20_iid0_test_acc.eps} %
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch20_iid0_test_acc.eps}
		}
		
		\mbox{%	  
		\includegraphics[width=1.9in]{MNIST_figures/local1_sketch50_iid0_train_loss.eps}% 
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch50_iid0_train_loss.eps}% 
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch50_iid0_train_loss.eps}}
		
		\mbox{%	
		\includegraphics[width=1.9in]{MNIST_figures/local1_sketch50_iid0_test_acc.eps}% 
		\includegraphics[width=1.9in]{MNIST_figures/local2_sketch50_iid0_test_acc.eps} %
		\includegraphics[width=1.9in]{MNIST_figures/local5_sketch50_iid0_test_acc.eps}
		}
	\end{center}
	\caption{MNIST Heterogeneous case: Comparison of compressed optimization algorithms on LeNet CNN architecture.}
    \label{fig:MNIST-iid0-app}
\end{figure}



\subsection{Additional Experiments: CIFAR-10}


We conduct similar sets of experiments on CIFAR10 dataset. We also use the simple LeNet CNN structure, as in practice small models are more favorable in federated learning, due to the limitation of mobile devices. The test accuracy is presented in Figure~\ref{fig:CIFAR-homog} and Figure~\ref{fig:CIFAR-heter}, for respectively homogeneous and heterogeneous data distribution. In general, we retrieve similar information as from MNIST experiments: our proposed FS-HEAPRIX improves FS-PRIVIX and SketchedSGD in all cases. We note that although the test accuracy provided by LeNet cannot reach the state-of-the-art accuracy given by some huge models, it is also informative in terms of comparing the relative performance of different sketching methods.



\begin{figure}[h]
	\begin{center}
		\mbox{%			   
		\includegraphics[width=2.3in]{CIFAR_figures/cifar_local1_sketch20_iid1_test_acc.eps}  
		 \includegraphics[width=2.3in]{CIFAR_figures/cifar_local5_sketch20_iid1_test_acc.eps}
		}
		\mbox{%	
	\includegraphics[width=2.3in]{CIFAR_figures/cifar_local1_sketch50_iid1_test_acc.eps} 
		\includegraphics[width=2.3in]{CIFAR_figures/cifar_local5_sketch50_iid1_test_acc.eps}
		}
	\end{center}
	\caption{Homogeneous case: CIFAR10: Comparison of compressed optimization methods on LeNet CNN.}
    \label{fig:CIFAR-homog}
\end{figure}



\begin{figure}[h]
	\begin{center}
		\mbox{%			   
\includegraphics[width=2.3in]{CIFAR_figures/cifar_local1_sketch20_iid0_test_acc.eps} 
		\includegraphics[width=2.3in]{CIFAR_figures/cifar_local5_sketch20_iid0_test_acc.eps}
		}
		\mbox{%	
		\includegraphics[width=2.3in]{CIFAR_figures/cifar_local1_sketch50_iid0_test_acc.eps}
		\includegraphics[width=2.3in]{CIFAR_figures/cifar_local5_sketch50_iid0_test_acc.eps}
		}
	\end{center}
	\caption{Heterogeneous case: CIFAR10: Comparison of compressed optimization methods on LeNet CNN.}
    \label{fig:CIFAR-heter}
\end{figure}



%We conduct similar sets of experiments on CIFAR10 dataset. We also use the simple LeNet CNN structure, as in practice small models are more favorable in federated learning, due to the limitation of mobile devices. The test accuracy is presented in Figure~\ref{fig:CIFAR}, for both homogeneous and heterogeneous data distribution. In general, we retrieve similar information as from MNIST experiments: our proposed FS-HEAPRIX improves FS-PRIVIX and SketchedSGD in all cases. We note that although the test accuracy provided by LeNet cannot reach the state-of-the-art accuracy given by some huge models, it is also informative in terms of comparing the relative performance of different sketching methods.

%\begin{figure}[h]
%	\begin{center}
%		\subfigure[CIFAR10 Homogeneous case.]{
%		\mbox{			  
%		 \includegraphics[width=1.7in]{CIFAR_figures/cifar_local1_sketch20_iid1_test_acc.eps}  
%		 \includegraphics[width=1.7in]{CIFAR_figures/cifar_local5_sketch20_iid1_test_acc.eps}
%		\includegraphics[width=1.7in]{CIFAR_figures/cifar_local1_sketch50_iid1_test_acc.eps} 
%		\includegraphics[width=1.7in]{CIFAR_figures/cifar_local5_sketch50_iid1_test_acc.eps}
%
%		}
%		}
%		\subfigure[CIFAR10 Heterogeneous case.]{
%
%	
%		\mbox{	
%		\includegraphics[width=1.7in]{CIFAR_figures/cifar_local1_sketch20_iid0_test_acc.eps} 
%		\includegraphics[width=1.7in]{CIFAR_figures/cifar_local5_sketch20_iid0_test_acc.eps}
%		\includegraphics[width=1.7in]{CIFAR_figures/cifar_local1_sketch50_iid0_test_acc.eps}
%		\includegraphics[width=1.7in]{CIFAR_figures/cifar_local5_sketch50_iid0_test_acc.eps}
%		}
%		}
%	\end{center}
%	\caption{CIFAR10: Comparison of compressed optimization methods on LeNet CNN.}
%    \label{fig:CIFAR}
%\end{figure}




\end{document}



