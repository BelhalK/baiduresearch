%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%\documentclass[sigconf]{acmart}
\documentclass[sigconf, anonymous, review]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{amsfonts}     
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{wrapfig}
%\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{balance}
\usepackage{url}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{multirow}
% \usepackage{caption}
%\usepackage{subcaption}
%\usepackage{amsmath}
%\usepackage[switch]{lineno}
\newcommand{\zz}[1]{\textcolor{blue}{#1}}
\newcommand{\Xc}{{\mathcal X}}
\newcommand{\Zc}{{\mathcal Z}}
\newcommand{\Pn}{\mathbb P^{(n)}}
\newcommand{\Qn}{\mathbb Q^{(n)}}
\newcommand{\pr}{{\mathbb P}}
\newcommand{\ex}{\mathbb E}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Variational Flow Graphical Model}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \streetaddress{Rono-Hills}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \streetaddress{30 Shuangqing Rd}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \streetaddress{8600 Datapoint Drive}
  \city{San Antonio}
  \state{Texas}
  \country{USA}
  \postcode{78229}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
This paper introduces a novel approach to embed flow-based models with hierarchical structures. The proposed model learns the representation of high dimensional data via a message-passing scheme by carefully integrating flow-based functions through variational inference. 
Meanwhile, our model produces a representation of the data using a lower dimension, thus overcoming the drawbacks of many flow-based models, usually requiring a high dimensional latent space involving many trivial variables. With the proposed aggregation nodes, the model provides a new approach for distribution modeling and graphical inference on datasets.  Multiple experiments on synthetic and real datasets show the benefits of our~proposed~method.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
 There are two general approaches for graphical inference: \textit{exact inference} and \textit{approximate inference}. Exact inference~\cite{sanner2012symbolic,kahle2008junction} resorts to an exact numerical calculation procedure of the quantity of interest. 
However, in most cases, exact inference is either \emph{computationally involved} or simply \emph{intractable}. 
Variational Inference (VI) is computationally efficient and has been applied to tackle the large scale inference problem~\cite{jordan1999introduction,hoffman2013stochastic}.
In Variational Inference, mean-field approximation~\cite{xing2012generalized} and variational message passing~\cite{bishop2003vibes,winn2005variational} are two common approaches for graphical models.
These approximation methods are limited by the choice of distributions that are inherently unable to recover the true posterior, often leading to a loose approximation. 

To tackle the probabilistic inference problem, alternative models have been developed under the name of \emph{tractable probabilistic models~(TPMs)}. These models include probabilistic decision graphs~\cite{jaeger2006learning}, arithmetic circuits~\cite{darwiche2003differential}, and-or search spaces~\cite{marinescu2005and},
multi-valued decision diagrams~\cite{dechter2007and},
cutset networks~\cite{rahman2014cutset}, sum-product nets~\cite{sanchez2021sum}, probabilistic sentential decision diagrams~\cite{kisa2014probabilistic}, and probabilistic circuits~\cite{choi2020probabilistic}. Probabilistic circuits~(PCs) leverage the recursive mixture models and distributional factorization to establish tractable probabilistic inference. PCs also aim to attain a TPM with improved expressive power. 

Apart from probabilistic inference, generative models have been developed to model high dimensional datasets and to learn the  meaningful  hidden data representations by leveraging the approximation power of neural networks. These models also provide a possible approach to generate new samples from  underlining distributions. Variational Auto-Encoders (VAEs)~\cite{kingma2013auto} and Generative Adversarial Networks (GAN)~\cite{Goodfellow14} are widely applied to different categories of datasets. Flow-based models~\cite{Dinh2016DensityEU, dinh2014nice,rezende2015variational,berg2018sylvester} leverage invertable neural networks and can estimate the density values of data samples as well. Unlike TPMs, it is usually difficult to directly use generative models to perform probabilistic inference on datasets. 

In this paper, we  introduce \textsc{Variational Flow Graphical (VFG)} models. By leveraging the expressive power of neural networks, VFGs can learn  latent representations from data. 
VFGs also lie in the stream of tractable neural networks that allow to perform inference on graphical structures.  
Sum-product networks~\cite{sanchez2021sum} and probabilistic circuits~\cite{choi2020probabilistic} are falling into this type of models as well. 
Sum-product networks and probabilistic circuits depend on mixture models and probabilistic factorization in graphical structure for inference. VFGs rely on approximation and consistency of  nodes in graphical structures for tractable inference.
Our contributions are summarized as follows. 


\textbf{Contributions.}\ Dealing with high dimensional data using graphical models exacerbates this systemic inability to model the latent structure of the data efficiently, i.e., to sample from the posterior distribution of the latent variables given the observed data.
To overcome these significant limitations, we propose a new framework, a variational hierarchical graphical flow model:
\begin{itemize}
    \item \textbf{Hierarchical and Flow-Based:} Introducing the Variational Flow Graphical (VFG) model, we propose a novel graph architecture uniting the hierarchical latent structures and flow-based models.  Our model outputs a tractable posterior distribution used as an approximation of the true posterior of the hidden node states in the considered graph structure. 
    
    \item \textbf{Numerical Inference:}  Aggregation nodes are introduced into our model in order to integrate hierarchical information through a variational forward-backward message passing scheme.  We highlight the benefits of our VFG model on  numerical applications: the missing values imputation problem and the numerical inference on graphical datasets. VFGs can  achieve improved evidence lower bound (ELBO) and likelihood values due to the implicitly invertible flow-based model structure. 
    
   \item \textbf{Representation Learning:} We specifically show in our experiments that our model achieves to disentangle the factors of variation underlying the high dimensional data given as input.
\end{itemize}


\noindent\textbf{Roadmap:} Section~\ref{sec:prelim} presents important concepts used in the paper.
Section~\ref{sec:main} introduces the Variational Flow Graphical  (VFG) model to tackle the posterior distribution learning of high dimensional data.
Section~\ref{sec:algrithm} provides the algorithms used to train VFG models. 
Section~\ref{sec:infer} discusses how to perform inference with a trained VFG model. Section~\ref{sec:numerical} showcases the advantages of VFG on various tasks. Section~\ref{sec:discuss} and Section~\ref{sec:conclusion} provide a discussion and conclusion of the paper.
%The Appendix is devoted to proofs. 
% Section~\ref{sec:conclusion} concludes our work.

\vspace{0.05in}
\noindent\textbf{Notation:} We denote by $[L]$ the set $ \{1, \cdots, L\}$, for all $L >1$, and by $\textbf{\text{KL}}(p || q ) := \int_{\mathcal{Z}} p(z) \log(p(z)/q(z)) \mathrm{d}z$ the Kullback-Leibler divergence from $q$ to $p$, two probability density functions defined on the set $\mathcal{Z} \subset \mathbb{R}^d$ for any dimension $d >0$.

\vspace{-0.05in}
\section{Preliminaries}\label{sec:prelim}
\vspace{-0.05in}
We introduce the general principles and notations of  variational inference  and  flow-based models then explain how they can naturally be embedded with graphical models.

%\vspace{0.0in}
\textbf{Variational Inference:}
Following the setting discussed above, the functional mapping $\mathbf{f}: \mathcal{Z} \xrightarrow{} \mathcal{X} $ can be viewed as a decoding process and the mapping $\mathbf{f}^{-1}$: $ \mathcal{X} \xrightarrow{}  \mathcal{Z}$ as an encoding one between  random variables $\mathbf{z} \in \mathcal{Z}$  and $\mathbf{x} \in \mathcal{X}$ with densities $\mathbf{z} \sim p(\mathbf{z}), \mathbf{x} \sim p_{\theta}(\mathbf{x}|\mathbf{z}).$
To learn the parameters $\theta$, we maximize the following marginal log-likelihood $ \log p(\mathbf{x}) = \log \int p(\mathbf{z})  p_{\theta}(\mathbf{x}|\mathbf{z})d\mathbf{z}$.
Direct optimization of the log-likelihood is usually not an option due to the intractable latent structure. Instead VI employs a parameterized family of so-called variational distributions $q_{\phi}(\mathbf{z}|\mathbf{x})$ to approximate the true posterior $p(\mathbf{z}|\mathbf{x}) \varpropto  p(\mathbf{z})  p_{\theta}(\mathbf{x}|\mathbf{z})$.
The goal of VI is to minimize the distance, in terms of Kullback-Leibler ($\mathbf{KL}$) divergence, between the variational candidate and the true posterior $\textbf{\text{KL}}(q_{\phi}(\mathbf{z}|\mathbf{x})||p(\mathbf{z}|\mathbf{x}) )$.
This optimization problem can be shown to be equivalent to maximizing the following evidence lower bound (ELBO) objective, noted $\mathcal{L}(\mathbf{x}; \theta, \phi)$: 

\vspace{-0.15in}
\begin{align}\label{eq:vi_elbo}
   & \log p(\mathbf{x})
    \geqslant \mathcal{L}(\mathbf{x}; \theta, \phi) \\ \notag
    &=\mathbb{E}_{q_{\phi}(\mathbf{z}|\mathbf{x})} \big[\log p_{\theta}(\mathbf{x}|\mathbf{z}) \big] - \textbf{\text{KL}}(q_{\phi}(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))\\ \notag
    &:=-\mathcal{F}(\theta, \phi) \, .
\end{align}
\vspace{-0.15in}

\noindent In Variational Auto-Encoders~(VAEs,~\cite{kingma2013auto,rezende2014stochastic}), the calculation of the reconstruction term requires sampling from the posterior distribution along with using the reparameterization trick, i.e.,
\begin{align} \label{eq:vae_recon}
\mathbb{E}_{q_{\phi}(\mathbf{z}|\mathbf{x})} \big[\log p_{\theta}(\mathbf{x}|\mathbf{z}) \big] \simeq \frac{1}{U}\sum_{u=1}^U \log p(\mathbf{x}| \mathbf{z}_{u}). \end{align} 
Here $U$ is the number of latent variable samples drawn from the posterior $q_{\phi}(\mathbf{z}|\mathbf{x})$ regarding data $\mathbf{x}$. 

%\vspace{0.08in}
\textbf{Flow-based Models:}
%Normalizing flows~\cite{kingma2018glow,rezende2015variational} 
Flow-based models~\cite{Dinh2016DensityEU, dinh2014nice,rezende2015variational,berg2018sylvester} 
correspond to a probability distribution transformation using  a sequence of invertible and differentiable mappings, noted $\mathbf{f}: \mathcal{Z} \xrightarrow[]{} \mathcal{X}$ between two random variables $\mathbf{z} \in \mathcal{Z}$ of density $p(\mathbf{z})$ and $\mathbf{x} \in \mathcal{X}$. The observed variable $\mathbf{x} \sim p_{\theta}(\mathbf{x})$ is assumed to be distributed according to an unknown distribution $p_{\theta}(\mathbf{x})$ parameterized by some parameter $\theta$. 
By defining the aforementioned invertible maps $\{\mathbf{f}_{\ell} \}_{\ell =1}^L$, and by the chain rule and inverse function theorem, the variable $\mathbf{x}=\mathbf{f}(\mathbf{z})$ has a tractable probability density function~(pdf) given as:

\vspace{-0.15in}
\begin{align}\label{eq:flow}
\log p_{\theta}(\mathbf{x})& = \log p(\mathbf{z})  + \log \bigg| \text{det} ( \frac{\partial \mathbf{z} }{\partial \mathbf{x}} ) \bigg| 
\\\notag
& =  \log p(\mathbf{z}) + \sum_{i=1}^L\log \bigg| \text{det} ( \frac{\partial \mathbf{h}^i } {\partial \mathbf{h}^{i-1}}) \bigg| \, ,
\end{align}


\vspace{-0.15in}
\noindent where we have $\mathbf{h}^0 = \mathbf{x}$ and $\mathbf{h}^L = \mathbf{z}$ for conciseness. 
The scalar value $\log |\text{det}( \partial \mathbf{h}^i/\partial \mathbf{h}^{i-1})|$ is the logarithm of the absolute value of the determinant of the Jacobian matrix $\partial \mathbf{h}^i/\partial \mathbf{h}^{i-1}$, also called the log-determinant. 
Eq.~(\ref{eq:flow}) yields a simple mechanism to build families of distributions that, from an initial density and a succession of invertible transformations, returns tractable density functions that one can sample from (by sampling from the initial density and applying the transformations). \cite{rezende2015variational} propose an approach to construct flexible posteriors by transforming  a simple base posterior with a sequence of flows. Firstly a stochastic latent variable is draw from base posterior $\mathcal{N}(\mathbf{z}_0|\mathbf{\mu}(\mathbf{x}), \mathbf{\sigma}(\mathbf{x}) )$. With $K$ flows, latent variable $\mathbf{z}_0$ is transformed to $\mathbf{z}_k$.The reformed negative EBLO is given by 

\vspace{-0.15in}
%{\small
\begin{align}\label{eq:flow2}
\mathcal{F}(\theta, \phi) = & \mathbb{E}_{q_{\phi}} \big[\log q_{\phi}(\mathbf{z}|\mathbf{x}) -  \log p_{\theta}(\mathbf{x},\mathbf{z}) \big] \\ \notag
= & \mathbb{E}_{q_{0}} \big[\log q_{0}(\mathbf{z}_0|\mathbf{x}) -  \log p_{\theta}(\mathbf{x},\mathbf{z}) \big] \\ \notag
&-\mathbb{E}_{q_{0}} \big[\sum_{k=1}^K \log\big|\det(\frac{\partial \mathbf{f}_k( \mathbf{z}_k; \psi_k)}{\partial \mathbf{z}_k}) \big| \big].
\end{align}%}

\vspace{-0.15in}
\noindent Here $\mathbf{f}_k$ is the $k$th flow with parameter $\psi_k$, i.e. $\mathbf{z}_K = \mathbf{f}_K \circ \cdots  \mathbf{f}_2 \circ  \mathbf{f}_1(\mathbf{z}_0)$. The parameters of the flows are considered as functions of data sample $\mathbf{x}$, and they determine the final distribution in amortized inference.


Several recent models have been proposed by leveraging the invertible  flow-based models. Graphical normalizing flow~\cite{wehenkel2021graphical}  learns a DAG structure from the input data under  sparse penalty and maximum likelihood estimation. 
The bivariate causal discovery method proposed in~\cite{khemakhem2021causal} relies on  autoregressive structure of flow-based models and the  log-likelihood ratio asymmetry for causal-effect inference. Here, we propose a framework that generalizes flow-based models~\cite{Dinh2016DensityEU, dinh2014nice,rezende2015variational,berg2018sylvester} to graphical variable inference.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.1in}
\section{Variational Flow Graphical Model}\label{sec:main}

Assume that there exist a relation between each data sample components and their corresponding variable in the latent space.
Then, it is possible to define a graphical model using normalizing flows, as introduced Section~\ref{sec:prelim}, leading to exact latent variable inference and log-likelihood evaluation of the model. 
We call this model a \textit{Variational Flow Graphical Model} (VFG) and introduce it in the sequel.
%\vspace{-0.1in}
\subsection{Evidence Lower Bound of Variational Flow Graphical Models}
\begin{figure}
\vspace{-0.05in}
\begin{center}
 \includegraphics[width=0.8\linewidth]{fig/tree_node.png}
\end{center}
\vspace{-0.15in}
\caption{\small  (Left)  Node $\mathbf{h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent node, $\mathbf{h}^{2,1}$; $\oplus$ is an aggregation node, and circles stand for non-aggregation~nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$.  Thin lines are identity functions, and thick lines are flow functions.  }
\label{fig:tree}
\vspace{-0.1in}
\end{figure}
A VFG model  $\mathbb{G}=\{\mathcal{V}, \mathbf{f}\} $ consists of a set of nodes ($\mathcal{V}$) and a set of edges ($\mathbf{f}$).
Figure~\ref{fig:tree}-Right gives an illustration of a tree structure induced by a VFG model. An edge can be either a flow function or an identity function. There are two types of nodes in a VFG: \emph{aggregation} nodes and \emph{non-aggregation} nodes. 
A non-aggregation node connects other nodes with a single flow function or an identity function. An aggregation node has multiple children, and it  connects with each of  them with an identity function. 	Unlike classical graphical models, a node in a VFG model may represent multiple variables. A node only belongs to one layer, and one layer can have multiple nodes. Moreover, each latent variable belongs to only one node in a VFG. 


In the following sections of this paper, identity function is considered as a special case of flow functions.
We apply variational inference to assemble the layers of a VFG and learn parameters from data samples. Different from VAEs~\cite{kingma2013auto,rezende2014stochastic}, the recognition model~(encoder) and the generative model~(decoder) in a VFG share the same  neural net structure and parameters. Moreover, the latent variables in a VFG lie in a hierarchy structure and  are generated with deterministic flow functions.

The hierarchical generative network comprises $L$ layers, $\mathbf{h}^l$ denotes the latent variable in layer $l$, and $\theta$ is the vector of model parameters. We use $\mathbf{h}^{l, i}$ to denote the $i$th node's latent variable  in layer $l$, and $\mathbf{h}^{(j)}$ to represent node $j$'s latent variable without specification of the layer number, and $j$ is the node index on a tree or graph.  
The joint distribution of the hierarchical model is given by:

\vspace{-0.15in}
\begin{align}\notag
p_{\theta}(\mathbf{x}, \mathbf{h}) = p( \mathbf{h}^{L}) p(\mathbf{h}^{L-1} | \mathbf{h}^{L}) \cdot \cdot  \cdot p(\mathbf{h}^{1} | \mathbf{h}^{2})  p(\mathbf{x} | \mathbf{h}^{1}) \, .
\end{align}

\vspace{-0.15in}
\noindent where $\mathbf{h}=\{\mathbf{h}^1, \cdots, \mathbf{h}^L \}$ denotes the set of latent variables of the model. The hierarchical generative model is given by factorization $p(\mathbf{x}|\mathbf{h}^L) =  \mathbf{\Pi}_{l=1}^{L-1}p(\mathbf{h}^{l} | \mathbf{h}^{l+1}) p(\mathbf{x} | \mathbf{h}^{1}) $, and the prior distribution is $p(\mathbf{h}^L)$. Note that only the \emph{root nodes} have \emph{prior distributions}. 

The  probability  density function $p(\mathbf{h}^{l-1} | \mathbf{h}^{l})$ in the generative model is parameterized with one or multiple invertible  flow-based functions.  We follow the variational inference  to approximate the posterior distribution of latent variables. 
The hierarchical posterior~(recognition model) is factorized as
\begin{align}\label{eq:posterior}
q_{\theta}(\mathbf{h}| \mathbf{x}) =  q(\mathbf{h}^1 | \mathbf{x})  q(\mathbf{h}^2 | \mathbf{h}^1) \cdot \cdot  \cdot  q(\mathbf{h}^{L} | \mathbf{h}^{L-1}).
\end{align}

\vspace{-0.15in}
\noindent Evaluation of the posterior(recognition model)~\eqref{eq:posterior} involves forward information flows from the bottom of the tree to the top, and similarly, sampling  the generative model takes the reverse direction. By leveraging the hierarchical conditional independence in both  generative model and  posterior,  the ELBO regarding the model is given by 

\vspace{-0.15in}
\begin{align} \label{eq:elbo}
&\log p_{\theta}(\mathbf{x})
    \geqslant \mathcal{L}(\mathbf{x}; \theta) \\ \notag
&= \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}^{1:L})  \big] - \sum_{l=1}^{L} \mathbf{KL}^l.
\end{align}
\vspace{-0.15in}

\noindent Here $\mathbf{KL}^l$ is the Kullback-Leibler divergence between the posterior and generative model in layer $l$. The first term in~(\ref{eq:elbo}) evaluates data reconstruction.  
When $1\leqslant l \leqslant L$, 

\vspace{-0.15in}
\begin{align}\label{eq:kl}
\mathbf{KL}^l 
=\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\mathbf{h}^{l+1}) \big].
\end{align}
\vspace{-0.15in}

\noindent When $l=L$, 
$\mathbf{KL}^L =  \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{L}|\mathbf{h}^{L-1})- \log p(\mathbf{h}^{L})  \big].$ It is easy to extend the computation of the ELBO~(\ref{eq:elbo}) to DAGs with topology ordering of the nodes (and thus of the layers). 
Let $ch(i)$ and $pa(i)$ denote node $i$'s child set and parent set, respectively.
Then, the ELBO for a DAG structure reads:

\vspace{-0.15in}
\begin{align}\label{eq:elbo_dag}
\mathcal{L}(\mathbf{x}; \theta) =& \mathbb{E}_{q(\mathbf{h}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h})  \big] -  \sum_{i \in \mathcal{V}  \setminus  \mathcal{R}_{ \mathbb{G} }} \textbf{\text{KL}}^{(i)}  \\ \notag
&-    \sum_{i \in  \mathcal{R}_{ \mathbb{G} }  }  \textbf{\text{KL}}\big(q(\mathbf{h}^{(i)} | \mathbf{h}^{ch(i)} )   || p(\mathbf{h}^{(i)})  \big) .
\end{align}
\vspace{-0.15in}

\noindent Here $\mathbf{KL}^{(i)}=\mathbb{E}_{q(\mathbf{h}|\mathbf{x})}\big[  \log q(\mathbf{h}^{(i)}|\mathbf{h}^{ch(i)})   - \log p(\mathbf{h}^{(i)}|\mathbf{h}^{pa(i)}) \big]$.  $\mathcal{R}_{ \mathbb{G}}$ is the set of root or source nodes of DAG $\mathbb{G} = \{\mathcal{V}, \mathbf{f}\}$. Assuming there are $k$ leaf nodes on a tree or a DAG model, corresponding to $k$ sections of the input sample $\mathbf{x} = [\mathbf{x}^{(1)}, ..., \mathbf{x}^{(k)}]$, then the hidden variables in both~(\ref{eq:elbo}) and~(\ref{eq:elbo_dag}) are computed with forward and backward message passing. 

% \begin{figure}
% \vspace{-0.05in}
% \begin{center}
%  \includegraphics[width=1.6in]{fig/tree_node_message.png}
% \end{center}
%   \caption{Forward and backward message passing to generate each node's hidden variable. Forward message passing  approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions. }
% \label{fig:tree_message}
% \vspace{-0.1in}
% \end{figure}
% \vspace{-0.1in}

\begin{figure}[H]
\begin{center}
 \includegraphics[width=0.5\linewidth]{fig/tree_message2.png}
\end{center}
\vspace{-0.2in}
\caption{Recognition model consists of froward message from  data to approximate the posterior distributions; the generative model is realized by backward message from the root node and generates the samples or reconstructions in each layer.}
\label{fig:tree_message}
\vspace{-0.1in}
\end{figure}
% || Forward and backward message passing to generate each node's hidden variable. Forward message passing  approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions.




\vspace{-0.05in}
\subsection{ Aggregation Nodes}\label{sec:node_aggr}
 There are two approaches to aggregate signals from different nodes: average-based and concatenation-based. We rather focus on average-based aggregation in this paper,  and Figure~\ref{fig:node_aggre}-Left gives  an example denoted by the operator $\oplus$. 
Let $\mathbf{f}_{(i, j)}$ be the direct edge~(function) from node $i$ to node $j$, and $\mathbf{f}^{-1}_{ (i, j)}$ or  $\mathbf{f}_{ (j, i)}$ defined as its inverse function. Then, we observe that at node $i$
 \begin{align*}
&  \mathbf{h}^{(i)} = \frac{1}{|ch(i)|} \sum_{j \in ch(i) } \mathbf{f}_{(j,i)}(\mathbf{h}^{(j)})  , \\ &\widehat{\mathbf{h}}^{(i)} = \frac{1}{|pa(i)|} \sum_{j \in pa(i) } \mathbf{f}^{-1}_{ (i,j)}(\widehat{\mathbf{h}}^{(j)}) \, .
\end{align*}
Notice that the above two equations hold even when node $i$ has only one child or parent.
In the sequel, we consider that all latent variables, noted $\mathbf{h}^{l, i}$, for all $l \in [L]$ and $i \in \mathbb{N}$, are distributed according to Laplace distributions. 
With the identity function between the  parent and its children, there are \emph{node consistency rules} regarding an \emph{average aggregation node}: {\it(a)} the parent value is the mean of its children, i.e., $\mathbf{h}^{(i)} = \frac{1}{|ch(i)|} \sum_{j \in ch(i)} \mathbf{h}^{(j)}$; {\it(b)} the  child node have the same reconstruction value with its parent, i.e., $\widehat{\mathbf{h}}^{(j)} = \widehat{\mathbf{h}}^{(i)}, \forall j \in ch(i)$. 
\begin{figure}[h]
\vspace{-0.1in}
\begin{center}
 \includegraphics[width=1.0in]{fig/aggre_node.png}
  \includegraphics[width=1.0in]{fig/aggre_model.png}
\end{center}
\vspace{-0.15in}
   \caption{(Left) Aggregation node $\mathbf{h}^{l+1,1}$ has three children, $\mathbf{h}^{l,1}$, $\mathbf{h}^{l,2}$, and $\mathbf{h}^{l,3}$. (Right) A VFG model with one aggregation node, $\mathbf{h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.}
\label{fig:node_aggre}
\end{figure}
\vspace{-0.15in}

We use Figure~\ref{fig:node_aggre}-Right as an example to illustrate a simple model that  has only one average aggregation node. Then (\ref{eq:elbo}) yields the ELBO,
\begin{align}  \label{eq:one_agg_node}
&\log p(\mathbf{x})   \geqslant  \mathcal{L}(\mathbf{x}; \theta) \\ \notag
&= \mathbb{E}_{q(\mathbf{h} | \mathbf{x})} \big[\log p(\mathbf{x}|\widehat{\mathbf{h}}^1)\big] -\mathbb{E}_{q(\mathbf{h} | \mathbf{x})} \big[\log q(\mathbf{h}^1 | \mathbf{x})\\ \notag
& -\log p(\mathbf{h}^1|\widehat{\mathbf{h}}^2)\big] - \textbf{\text{KL}}\big(q(\mathbf{h}^2 | \mathbf{h}^1) | p(\mathbf{h}^2)\big).
\end{align}
From  Figure~\ref{fig:node_aggre}-Right,  $\mathbf{h}^{(r)}$ is the root, and has $k$ children, $\mathbf{h}^{(t)}, t = 1,...,k$, and $k=3$. 
With $\mathbf{f}_t$ as the flow function connecting $\mathbf{h}^{(t)}$ and $\mathbf{x}^{(t)}$, according to the aggregation rules, we get:
\begin{equation}
\begin{split}
& \mathbf{h}^{(t)} = \mathbf{f}_t(\mathbf{x}^{(t)})\, ,\quad  \widehat{\mathbf{h}}^{(r)} = \mathbf{h}^{(r)} = \frac{1}{k}\sum_{t=1}^k \mathbf{h}^{(t)} , \\
&\widehat{\mathbf{h}}^{(t)}= \widehat{\mathbf{h}}^{(r)}, \ t = 1,...,k \, .
 \end{split}
\end{equation}
 Assume the data to be normally distributed, then 
\begin{align}\notag
 &\log p(\mathbf{x}|\widehat{\mathbf{h}}^1) 
 =-\sum_{t=1}^k\bigg\{ \underbrace{\frac{1}{2\sigma^2_{\mathbf{x}}}\big|\big| \mathbf{x}^{(t)} - \mathbf{f}_t^{-1}(\widehat{\mathbf{h}}^{(r)})\big|\big|_2^2 }_{\textrm{By} \  \widehat{\mathbf{x}}^{(t)}=\mathbf{f}_t^{-1}(\widehat{\mathbf{h}}^{(t)})=\mathbf{f}_t^{-1}(\widehat{\mathbf{h}}^{(r)})} \bigg\}+C \\
 &  \log p(\mathbf{h}^1|\widehat{\mathbf{h}}^2) = -\sum_{t=1}^k\big\{ \underbrace{\big|\big|  \mathbf{f}_t(\mathbf{x}^{(t)}) - \widehat{\mathbf{h}}^{(r)}\big|\big|_1}_{\textrm{By} \  \widehat{\mathbf{h}}^{2}= \widehat{\mathbf{h}}^{(r)}, \  \mathbf{h}^{(t)} = \mathbf{f}_t(\mathbf{x}^{(t)})} \big\} +C .
 \end{align} 
 Here $\sigma_{\mathbf{x}}$ is a constant standard deviation. 
We notice that maximizing the ELBO, or minimizing the $\mathbf{KL}$ term of an aggregation node implies the aggregation rule (b). 

\subsection{Universal Approximation Property}
Coupling layer based flows have universal  approximation power~\cite{Takeshi2020}. Following the analysis for flows~\cite{Takeshi2020}, we can prove that coupling layer based VFGs have universal approximation as well. 

\section{Algorithm and Implementation}\label{sec:algrithm}
In this section, we develop the training algorithm, see Algorithm~\ref{alg:main}, that outputs the fitted vector of parameters resulting from the maximization of the ELBO objective function~(\ref{eq:elbo}) or~(\ref{eq:elbo_dag}) depending on what graph structure is used.
In Algorithm~\ref{alg:main}, the inference of the latent variables is performed via forwarding message passing, cf. Line~6, and their reconstructions are computed in backward message passing, cf. Line~11.
Unlike Variational Autoencoders (VAE), the variance of latent variables in a VFG is approximated rather than parameterized with neural networks. 
A VFG is a deterministic network passing latent variable values between nodes. 
The reconstruction~(likelihood) terms in each layer are computed with forward and backward node states. 
Ignoring explicit neural network parameterized variances for all latent nodes enables us to use flow-based models as both the encoders and decoders. 
We thus obtain a deterministic ELBO objective~(\ref{eq:elbo})-~(\ref{eq:elbo_dag}) that can efficiently be optimized with standard stochastic optimizers. 
\begin{algorithm}[h]
   \caption{Inference model parameters with  forward and backward message propagation}
   \label{alg:main}
\begin{algorithmic}[1]
%\SetCustomAlgoRuledWidth{0.45\textwidth}
   \STATE {\bfseries Input:} Data distribution $\mathcal{D}$,  $\mathbb{G} = \{\mathcal{V}, \mathbf{f}\}$
   \FOR {$s=0,1,...$} 
   \STATE  Sample minibatch $b$ samples $\{\mathbf{x}_1, ..., \mathbf{x}_b \}$ from $\mathcal{D}$;
   \FOR{$i \in \mathcal{V}$}\label{line:for2}
    \STATE  \textcolor{blue}{// forward message passing}
   \STATE $\mathbf{h}^{(i)} = \frac{1}{|ch(i)|} \sum_{j \in ch(i) } \mathbf{f}_{(j,i)}(\mathbf{h}^{(j)})$; \label{line:forward} 
    \ENDFOR
    \STATE $\widehat{\mathbf{h}}^{(i)} = \mathbf{h}^{(i)} \ \  \text{if} \ i \in \mathcal{R}_{\mathbb{G}} $ or $i \in$ layer L;
   \FOR{$i \in \mathcal{V}$}
   \STATE \textcolor{blue}{// backward message passing}
   \STATE $\widehat{\mathbf{h}}^{(i)} = \frac{1}{|pa(i)|} \sum_{j \in pa(i) } \mathbf{f}^{-1}_{ (i,j)}(\widehat{\mathbf{h}}^{(j)}) $;\label{line:backward}  
   \ENDFOR
    \STATE  $\mathbf{h} =  \{\mathbf{h}^{(t)} \big |  t \in \mathcal{V} \}$, $\widehat{\mathbf{h}} =  \{\widehat{\mathbf{h}}^{(t)} \big | t \in \mathcal{V} \}$;
    \STATE Approximate the $\mathbf{KL}$ terms in ELBO for each layer with b samples;
    \STATE Updating VFG model $\mathbb{G}$ with gradient ascending: $\theta^{(s+1)}_{\mathbf{f}} = \theta^{(s)}_{\mathbf{f}} + \nabla_{\theta_{\mathbf{f}}}\frac{1}{b} \sum_{i=1}^b  \mathcal{L}(\mathbf{x}_b; \theta^{(s)}_{\mathbf{f}})   \, .$\label{line:update} 
   \ENDFOR
\end{algorithmic}
\end{algorithm}

%\subsection{Layer-wise Training}
From a practical perspective, layer-wise training strategy can improve the accuracy of a model especially when it is constructed of more than two layers. 
In such a case, the parameters of only one layer are updated with backpropagation of the gradient of the loss function while keeping the other layers fixed at each optimization step. 
By maximizing the ELBO~(\ref{eq:elbo}) or~(\ref{eq:elbo_dag}) with the above algorithm, the \emph{node consistency rules} in Section~\ref{sec:node_aggr} are expected to be satisfied. 
Inference on sub-graphs  can be further improved with random masking strategy, and more details are given in the supplemental file. 
%We can improve the inference on sub-graphs  by using the random masking method introduced  in the .

In training algorithm 1, the backward variable state $\widehat{\mathbf{h}}^l$ in  layer $l$  is generated according to $p(\widehat{\mathbf{h}}^l | \widehat{\mathbf{h}}^{l+1})$, and at the root  layer,  node state $\widehat{\mathbf{h}}^{\mathcal{R}}$ is set  equal to  $\mathbf{h}^{\mathcal{R}}$ that is
from  the posterior $q(\mathbf{h}|\mathbf{x})$, not from the prior $p(\mathbf{h}^{\mathcal{R}})$. So we can see all the forward and backward latent variables are sampled from the posterior $q(\mathbf{h}|\mathbf{x})$. 
%We state this point clearly in line 172 of the main file.


\vspace{-0.1in}
\section{Inference on VFG Models }\label{sec:infer}
Given a  VFG model, we aim to infer the state of any node given observed ones. 
Relations between variables at different nodes can also be inferred via our flow-based graphical model. \begin{figure}[h]
\begin{center}
 \includegraphics[width=0.3\linewidth]{fig/two_layer_infer.png}
 %\hspace{0.15in}
 \includegraphics[width=0.57\linewidth]{fig/tree_infer.png}
\end{center}
\vspace{-0.1in}
 \caption{{\small (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and  pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}}
\label{fig:two_layer_infer}
\vspace{-0.1in}
\end{figure}

The hidden state of the parent node $j$ in an  aggregation model can be computed by the observed children as follows:
 \begin{align}\label{eq:aggr_obs_ch}
\mathbf{h}^{(j)}  = \frac{1}{|ch(j) \cap O|}\sum_{i \in ch(j) \cap O} \mathbf{h}^{(i)} \, ,
\end{align}
where $O$ is the set of observed leaf nodes, see Figure~\ref{fig:two_layer_infer}-left for an illustration. 
Observe that for either a tree or a DAG, the state of any given node is updated via messages received from its children. Figure~\ref{fig:two_layer_infer} illustrates this inference mechanism for trees in which the structure enables us to perform message passing among the nodes. 
We derive the following Lemma establishing the relation between two leaf nodes:
%\color{red}{wrong}
\begin{lemma}\label{lm:apprx}
Let $\mathbb{G}$ be a  variational flow graphical model (with a tree structure) with $L$ layers, and $i$ and $j$ are two leaf nodes with $a$ as the closest common ancestor. Given observed value at node $i$, the value of node $j$ can be approximated by   $\widehat{\mathbf{x}}^{j} =  \mathbf{f}_{(a,j)}(\mathbf{f}_{(i, a)}(\mathbf{x}^{(i)}))$. Here $\mathbf{f}_{(i, a)}$ is the flow function path from node $i$ to node $a$. 
\end{lemma}
\begin{proof}
Without loss generality, we assume that there are relationships among different data sections, and the value of one section can be imputed by other sections. 
According to the \emph{node consistency rule} {\it(b)}  discussed in section~\ref{sec:node_aggr}, at an aggregation node  $a$, the latent value of a child node $j$ has the same reconstruction value as the parent node.  
The reconstruction of the child node $j$ can be calculated with the reconstruction of the parent node, i.e., $\widehat{\mathbf{h}}^{(j)} = \mathbf{f}_{(a,j)}(\widehat{\mathbf{h}}^{a)})$. 
Recalling the reconstruction term in the ELBO~(\ref{eq:elbo}), at each node we have $\mathbf{h}^{(a)} = \widehat{\mathbf{h}}^{(a)}$. Hence for node $a$'s descendent $j$, we have $\widehat{\mathbf{h}}^{(j)} = \mathbf{f}_{(a,j)}(\mathbf{h}^{(a)})$, and $\mathbf{f}_{(a,j)}$ is the flow function path from $a$ to $j$. 
The value of node $a$ can be computed by the value of its descendent $i$, i.e., $\mathbf{h}^{(a)} = \mathbf{f}_{(i,a)}(\mathbf{h}^{(i)})$. Hence, we have $\widehat{\mathbf{x}}^{(j)} =  \mathbf{f}_{(a,j)}(\mathbf{f}_{(i, a)}(\mathbf{x}^{(i)}))$.
\end{proof}

Considering the flow-based model~(\ref{eq:flow}), we have the following identity for each node of the graph structure:
\begin{align*}
& p(\mathbf{h}^{(i)} | \mathbf{h}^{pa(i)})  = p(\mathbf{h}^{pa(i)}) \big|\det(\frac{\partial \mathbf{h}^{pa(i)} }{\partial \mathbf{h}^{(i)}})\big| \\
& =
p(\mathbf{h}^{pa(i)}) \big|\det(\mathbf{J}_{\mathbf{h}^{pa(i)}}(\mathbf{h}^{(i)}))\big| \, .
\end{align*}
Let $O$ be the set of observed leaf nodes, $j$ be an unobserved node, and $a$ the closest ancestor of $O \cup \{a\}$. 
Then the state of $j$ can be imputed with $\widehat{\mathbf{x}}^{j}= \mathbf{f}_{(a,j)}(\mathbf{f}_{(O, a)}(\mathbf{x}^{(i)}))$, where $\mathbf{f}_{(O, a)}$ is the flow function path from all nodes in $O$ to $a$. Lemma~\ref{lm:apprx} provides an approach to conduct inference on a tree and impute missing values in the dataset. It is easy to extend the inference method to DAG-VFG models. 



%\vspace{-0.1in}
\section{Numerical Experiments}\label{sec:numerical}
The first  application we present is the imputation of missing values. We compare our method with several baseline models  on a synthetic dataset.
The second set of experiments is to evaluate VFG models on three different datasets, i.e.,  MNIST, Caltech101, and Omniglot, with ELBO and likelihoods as the score.  
The third application we present is the task of learning posterior distribution of the latent variables corresponding to the hidden explanatory factors of variations in the data~\cite{bengio2013representation}.
For that latter application, the model is trained and evaluated on the MNIST handwritten digits dataset.
 
 

In this paper,  we would rather assume the VFG graph structures are given and fixed.  In the experiments, the VFG structures are designed heuristically (as other neural networks) for the sake of numerical illustrations.  Learning the structure of VFG is an interesting research problem and is left for future works. A simple approach for VFG structure learning is to regularize the graph with the DAG structure penalty~\cite{Zheng2018,wehenkel2021graphical}.
%\vspace{-0.05in}
\subsection{Evaluation on Inference with Missing Entries Imputation}%\label{sec:exp:mnist}
%\vspace{-0.05in}
We now focus on the task of imputing missing entries in a graph structure.
For all the following experiments, the models are trained on the training set and are used to infer the missing entries of samples in the testing set. The baselines for this set of experiments include mean value method~(Means), iterative imputation~(Iterative)~\cite{buck1960method}, and multivariate imputation by chained equation~(MICE)~\cite{van2011mice}.  Mean Squared Error as the metric of reference in order to compare the different methods for the imputation task. 

\begin{figure}[b!]
  \centering
       \includegraphics[width=0.25\textwidth]{fig/sim_box.eps}
%   \includegraphics[width=1.6in]{fig/sim_elbo.eps}
    \captionof{figure}{Synthetic datasets: MSE boxplots of VFG and baseline methods.}
    \label{fig:sim}
\end{figure}


\begin{table*}[t]
\captionof{table}{Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.}%\vspace{-0.05in}}
\centering
{\small
\label{tab:elbo}
\begin{tabular}{l | c  c   c  c  c  c }
\hline
 \multirow{2}{0nc}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{MNIST}} & \multicolumn{2}{c}{\textbf{Caltech101}} & \multicolumn{2}{c}{\textbf{Omniglot}} \\
 %\hline 
 & -ELBO & NLL  &  -ELBO & NLL  & -ELBO & NLL  \\
\hline
 VAE~\cite{kingma2013auto} & 86.55 $\pm$ 0.06  & 82.14 $\pm$ 0.07& 110.80 $\pm$ 0.46 & 99.62 $\pm$ 0.74 & 104.28 $\pm$ 0.39 & 97.25 $\pm$ 0.23 \\
Planer~\cite{rezende2015variational} & 86.06 $\pm$ 0.31 & 81.91 $\pm$ 0.22 & 109.66 $\pm$ 0.42 & 98.53 $\pm$ 0.68 & 102.65 $\pm$ 0.42 & 96.04 $\pm$ 0.28 \\
IAF~\cite{kingma2016improving} & 84.20 $\pm$ 0.17& 80.79 $\pm$ 0.12 & 111.58 $\pm$ 0.38 & 99.92 $\pm$ 0.30 & 102.41 $\pm$ 0.04 & 96.08 $\pm$ 0.16 \\
SNF~\cite{berg2018sylvester} & 83.32 $\pm$ 0.06 & 80.22 $\pm$ 0.03 & 104.62 $\pm$ 0.29 & 93.82 $\pm$ 0.62 & 99.00 $\pm$ 0.04 & 93.77 $\pm$ 0.03 \\
\hline
VFG &\textbf{80.80 $\pm$ 0.76} & \textbf{63.66 $\pm$ 0.14} & \textbf{67.26 $\pm$ 0.53} & \textbf{65.74 $\pm$ 0.84}  &\textbf{80.16 $\pm$ 0.73 } & \textbf{78.65 $\pm$ 0.66}\\  
%VFG~(b=6) & \textbf{73.21} &  67.74\\ 
\hline
\end{tabular}
\vspace{-0.1in}
}
\end{table*}

\textbf{Synthetic dataset: } In this set of experiments, we study the proposed model with synthetic datasets.
We generate $10$ synthetic datasets (using different seeds) of $1\,300$ data points, $1\,000$ for the training phase of the model, $300$ for imputation testing. 
Each data sample  has $8$ dimensions with $2$ latent variables. 
Let $z_1 \sim \mathcal{N}(0,1.0^2)$ and $z_2 \sim  \mathcal{N}(1.0,2.0^2)$ be the latent variables. For a sample $\mathbf{x}$, we have  $x_1=x_2 = z_1, x_3=x_4= 2\textrm{sin}(z_1), x_5=x_6 =z_2$, and $x_7= x_8 = z_2^2$.  In the testing dataset, $x_3$, $x_4$, $x_7$, and $x_8$ are missing. We use a VFG model with a single average aggregation node that has four children, and each child connects the parent with a flow function consisting of 3 coupling layers~\cite{Dinh2016DensityEU}. 
Each child takes 2 variables as input data section, and the latent dimension of the VFG is $2$.
We compare, Figure~\ref{fig:sim}, our VFG method with the baselines described above using boxplots on obtained MSE values for those $10$ simulated datasets.
We can see that the proposed VFG model performs much better than mean value, iterative, and MICE methods. 

\subsection{ELBO and Likelihood}\label{sec:exp:elbo}
We further qualitatively compared VFG with existing methods on variational inference using three standard datasets.
The evaluation datasets and setup are exactly following two standard flow-based variational models, sylvester normalizing flows~\cite{berg2018sylvester} and~\cite{rezende2015variational}.  We use the  tree VFG structure as shown in Figure~\ref{fig:z_tsne}-Left for three datasets. We train the tree-VFG model using the following ELBO objective. Empirically, a small $\beta$ yields better ELBO and NLL values, and we set $\beta$ around 0.1 in the experiments. 
\begin{align} \notag %\label{eq:elbo_b}
&\text{ELBO}= \mathcal{L}(\mathbf{x}; \theta) 
    = \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}^{1:L})  \big] - \beta\sum_{l=1}^{L} \mathbf{KL}^l.
\end{align}

In Table~\ref{tab:elbo}, we provide the negative evidence lower bound~(-ELBO) and the estimated negative likelihood~(NLL) for baseline methods on three datasets, MNIST, Caltech101, and Omniglot. The results of the baseline methods are from~\cite{berg2018sylvester}. These methods are VAE based methods  enhanced with  normalizing flows.  They use 16 flows to improve the posterior estimation. SNF is orthogonal sylvester flow method with a bottleneck of M = 32. We set the VFG coupling block\cite{Dinh2016DensityEU} number with  $\mathcal{B}=4$, and following~\cite{berg2018sylvester} we run multiple times to get the mean and standard derivation as well. VFG can achieve superior EBLO and NLL values as given in Table~\ref{tab:elbo}.  The main reason why VFGs can achieve superior variational inference results (ELBOs and  NLLs) in Table~\ref{tab:elbo} is due to VFGs's approximately invertiable flow-based model structure. 
The intrinsic invertibility ensures the decoder or generative model in the VFG to achieve smaller reconstruction errors for data samples and hence smaller NLL values.

%\end{center}


\subsection{Latent Representation Learning on MNIST}\label{sec:exp:mnist}

In this set of experiments, we evaluate  Variational Flow Graphical Models on latent representation learning of the MNIST dataset~\cite{Lecunmnist2010}. We construct a tree  VFG model depicted in Figure~\ref{fig:z_tsne}-Left.  
In the first layer, there are 4 flow functions, and each of them takes $14\times 14$ image blocks as the input. 
Thus a $28\times 28$ input image is divided into four $14\times 14$ blocks as the input of VFG model. 
The latent dimension for this model is $196$. 
Following~\cite{Sorrenson2020}, the VFG model is trained with image labels to learn the latent representation of the input data. 
We set the parameters of $\mathbf{h}^L$'s prior distribution as a function of image label, i.e., $\lambda^L(u)$, where $u$ denotes the image label. In practice, we use $10$ trainable $\lambda^L$s regarding the $10$ digits. 
The images in the second row of Figure~\ref{fig:reconst} are reconstructions of MNIST samples extracted from the testing set, displayed in the first row of the same Figure, using our proposed VFG model.  
\begin{figure}%[h]
    \centering
       \includegraphics[width=0.2\textwidth]{fig/tree_mnist.png}
       \includegraphics[width=0.2\textwidth]{fig/z_Y.eps}
%   \includegraphics[width=1.6in]{fig/sim_elbo.eps}
        \captionof{figure}{(Left) Tree structure for MNIST; (Right) MNIST: t-SNE plot of latent variables from VFG learned with labels.}
    \label{fig:z_tsne}
\vspace{-0.15in}
\end{figure}
\begin{figure}[H]
    \centering
       \includegraphics[width=0.4\textwidth]{fig/reconst_Y.png}
        \captionof{figure}{(Top) original MNIST digits. (Bottom) reconstructed images using VFG.}
    \label{fig:reconst}
    \vspace{-0.15in}
\end{figure}


Figure~\ref{fig:z_tsne}-Right shows  t-distributed stochastic neighbor embedding (t-SNE)~\cite{maaten2008visualizing} plot of $2,000$ testing images's latent variables learned with our model, and $200$ for each~digit. 
Figure~\ref{fig:z_tsne}-Right illustrates that VFG can learn separated latent representations to distinguish different hand-written numbers.

%\subsubsection{Latent structure learning on MNIST}
\begin{figure}[h!]
\begin{center}
 \includegraphics[width=0.75\linewidth]{fig/6_19.png}
  \includegraphics[width=0.75\linewidth]{fig/60_92.png} 
    \includegraphics[width=0.75\linewidth]{fig/119_157.png} 
\end{center}
\vspace{-0.1in}
\captionof{figure}{MNIST: Increasing each latent variable from a small value to a larger one.}\label{fig:mnist_dis}\vspace{-0.1in}
\end{figure}

To provide a description of the learned latent representation, we first obtain the root latent variables of a set of images through forward message passing. Each latent variable's values are changed increasingly within a range centered at the value of the latent variable obtained from last step. 
This perturbation is performed for each image in the set.
Figure~\ref{fig:mnist_dis} shows the change of images by increasing one latent variable from a small value to a larger one. The figure presents some of the latent variables that have obvious effects on images, and most of the $d=196$ variables do not impact the generation significantly. Latent variables $i=6$ and $i=60$ control the digit width. Variable $i=19$ affects the brightness.  $i=92, i=157$ and some of the variables not displayed here control the style of the generated digits. 

%\subsection{Manifold Learning}
%\subsection{Image Generation}


\section{Discussion}~\label{sec:discuss}
One of the motivations for VFG is to develop a tractable model that can be used for inference and sampling on datasets.
As long as the variable states in the aggregation nodes are consistent, we can always apply VFGs to missing value inference. We provide more discussion on the structures of VFGs below.
\vspace{-0.05in}
\subsection{Benefits of Encoder-decoder Parameter Sharing}
There are several advantages for the encoder and decoder to share parameters. 
 Firstly, it makes the network's structure simple. 
 Secondly, the training and inference can be simplified with concise and simple graph structures. 
 Thirdly, by leveraging invertible flow-based functions, VFGs obtain achieve tighter ELBOs in comparison with VAE based models.% The main reason why VFGs can achieve superior ELBOs and negative likelihood values (NLLs) in Table~\ref{tab:elbo} is due to VFGs' approximate invertibility property. 
 The intrinsic invertibility introduced by flow functions ensures the decoder or generative model in a VFG  achieves smaller reconstruction errors for data samples and hence smaller NLL values and tighter ELBO. Whereas without the intrinsic constraint of invertibility or any help or regularization from the encoder, VAE-based models have to learn an unassisted mapping function~(decoder) to reconstruct all data samples with the latent variables, and there are always some discrepancy errors in the reconstruction that lead to relatively larger NLL values and hence inferior ELBOs.

\subsection{Structures of VFGs}
 In the experiments, the model structures have been chosen heuristically and for the sake of numerical illustrations. A tree VFG model can be taken as a dimension reduction model that can be used for missing value imputation as well. Variants of those structures will lead to different numerical results and at this point, we can not claim any generalization regarding the impact of the VFG structure on the outputs. Learning the structure of VFG is an interesting research problem and is left for future works.  VFG structures could be learned through the regularization of DAG structures~\cite{Zheng2018,wehenkel2021graphical}.
 
 VFGs rely on minimizing the KL term to achieve  \emph{consistency} among the nodes in an aggregation node. As long as the aggregation nodes retain consistency, the model always has a tight ELBO and can be applied for tractable posterior inference. 
 According to the recent theoretical study~\cite{teshima2020coupling}, coupling-based flows are endowed with the universal approximation power. 
 Hence, we believe that the consistency of aggregation nodes on a  VFG can be attained with a training algorithm and thus a tight ELBO. 


%\subsection{Connection to  GFlowNet }

%\vspace{-0.1in}
\section{Conclusion}\label{sec:conclusion}
%\vspace{-0.05in}
In this paper, we propose VFG, a variational flow graphical model that aims at bridging the gap between  flow-based models and the paradigm of graphical models.
Our VFG model learns the latent representation of  the input data through message passing between nodes in the model structure.
The posterior inference, of the latent nodes given input observations, is facilitated by the careful embedding of  flow functions in the general graph structure.
Experiments on different datasets illustrate the effectiveness of the model. 
Future work includes applying our VFG model to fine grained data relational structure learning and reasoning. 






%\section{Appendices}
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

%%
%% If your work has an appendix, this is the place to put it.
\appendix
\newpage

The proposed variational flow graphical models assemble flow functions with tree or DAG structures via variational inference on aggregation nodes. In this supplemental file, we first present additional results, then we give more details on  the methodology of VFG models. Section~\ref{sec:exp_supp} provides additional experiments; universal approximation property of VFGs is given in  Section~\ref{sec:univ_approx}; Section~\ref{sec:elbo_cal_supp} adds details on ELBO computation; Section~\ref{sec:app_train} provides more details on the training algorithm; Section~\ref{sec:aggr_supp} presents other details on the aggregation node; ELBO derivation  can be found in Section~\ref{sec:ebl_deri}. 



\section{Additional Results}\label{sec:exp_supp}

 All the experiments are conducted on NVIDIA-TITAN X (Pascal) GPUs. 
In  the experiments, we use the same  coupling block~\cite{Dinh2016DensityEU} to construct different flow functions. 
The coupling block consists of three fully connected layers~(of dimension $64$) separated by two RELU layers along with the coupling trick. 
Each flow function has block number $\mathcal{B} \geqslant 2$. 
All latent variables, $\mathbf{h}^{i}, i \in \mathcal{V}$ are forced to be non-negative via Sigmoid or RELU functions. 
Non-negativeness can help the model to identify sparse structures of the latent space. 


% \textbf{Baselines:} We use the following  for data imputation:%\vspace{-0.05in}
% \begin{enumerate}
% \item \textit{Mean Value:} Using training set mean values to replace the missing entries in the testing set.  

% \item \textit{Iterative Imputation:} A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a Round-Robin fashion.   

% \item \textit{Multivariate Imputation by Chained Equation (MICE):} This method imputes the missing entries with multiple rounds of inference. This method can handle different types of data.
% \end{enumerate}
%\end{itemize}  


\subsection{Imputation on California Housing Dataset}

We further investigate the method on a real dataset.  
%~\cite{chsklearn} 
The California Housing dataset  has 8 feature entries and $20\,640$ data samples. 
We use the first $20\,000$ samples for training  and $100$ of the rest for testing.  
We get  4 data sections, and each section contains 2 variables. 
In the testing set, the second section is assumed missing for illustration purposes, as the goal is to impute this missing section. In addition to the three baselines in introduced the main file, we also compared with KNN~(k-nearest neighbor) method. We use the baseline implementations in~\cite{scikit-learn} or~\cite{Impyute} in the experiments. 


Here, we construct a tree structure VFG with 2 layers. 
The first layer has two aggregation nodes, and each of them has two children. 
The second layer consists of one aggregation node that has two children connecting with the first layer.  
Each flow function has 4 coupling blocks. 
We can see Table~\ref{tab:imp_arrhytmia} that our model yields significantly better results than any other method in terms of prediction error. 
{\small
\begin{table}[ht]
\centering
%	\resizebox{\columnwidth}{!}{%
 \begin{tabular}{l | c  }\hline
\textit{Methods} & \textit{Imputation MSE}  \\
\hline
Mean Value &1.993 \\
%\hline
MICE & 1.951\\
%\hline
Iterative Imputation & 1.966\\
%\hline
%KNN (k=3) &1.974 \\
% %\hline
KNN (k=5) &1.969 \\
\hline
VFG & \textbf{1.356} \\  
\hline
\end{tabular}
%}
\captionof{table}{California Housing dataset: Imputation Mean Squared Error (MSE) results.} \label{tab:imp_arrhytmia}\vspace{-0.1in}
\end{table}}





% \begin{figure}[H]
%     \centering
%       \includegraphics[width=0.3\textwidth]{fig/tree_mnist.png}
%     \captionof{figure}{The tree structure for MNIST.}
%     \label{fig:struct}
% \end{figure}
% \subsection{Inference on DAGs}
% In this set of experiments, we compare VFGs against Bayesian networks and sum-product networks on inference capabilities. 

%\subsection{Representation Learning with MNIST}

%\subsection{ELBO and Likelihood}

%\section{Connection to  GFlowNet }

%===============================================

\




\section{Universal Approximation}~\label{sec:univ_approx}

In~\cite{Takeshi2020}, the authors analysis the approximation power of coupling-based flows. Under mild conditions,  coupling-based flows are universal distributional approximators. Following ~\cite{Takeshi2020}, we show that VFGs are also universal approximators. We first give several definitions regarding universal approximation, then we prove that VFGs have universal approximation. 




For a measurable mapping $\mathbf{f}: \mathbb{R}^m \rightarrow  \mathbb{R}^n$ and a subset $K \subset   \mathbb{R}^m $,
we have the following definition,
\begin{align*} 
|| \mathbf{f}||_{p,K}= \bigg(\int_{K} ||f(x)||^p dx \bigg)^{1/p}.
\end{align*}
Here $||\cdot||$ is the Euclidean norm of $\mathbb{R}^n$. We also define $||\mathbf{f}||_{\text{sup},K} := \text{sup}_{x\in K} || \mathbf{f}(x)||$. 

\begin{definition}
($L^p$-/sup-universality) Let $\mathcal{M}$ be a model which is a set of measurable mappings from $\mathbb{R}^m$ to $\mathbb{R}^n$. Let $p\in [1, \infty)$, and let $\mathcal{G}$ be  a set of measurable mappings $\mathbf{g}: U_{\mathbf{g}} \rightarrow \mathcal{R}^n$, where $U_{\mathbf{g}}$ is a measurable subset of $\mathbb{R}^m$ which may depend on $\mathbf{g}$. We say that $\mathcal{M}$ has the $L^p$-universal approximation property for $\mathcal{G}$ if for any $\mathbf{g}\in \mathcal{G}$, any $\epsilon > 0$, and any compact subset $K \in U_{\mathbf{g}}$, there exists  $\mathbf{f} \in \mathcal{M}$ such that $|| \mathbf{f}-\mathbf{g}||_{p, K} < \epsilon$. We define the sup-universality analogously by replacing  $|| \cdot ||_{p, K}||$ with $|| \cdot ||_{p, K}||_{sup, K}$ .
\end{definition}



\begin{definition}
(Distributional universality) Let $\mathcal{M}$ be a model which is a set of measurable mappings from $\mathbb{R}^m$ to $\mathbb{R}^n$. We say that a model $\mathcal{M}$  is a distributional universal approximator or has the distributional universal approximation property if, for any absolutely continuous probability measure $\mu$ on $\mathbb{R}^m$ and any probability measure $\nu$ on $\mathbb{R}^n$, there exists a sequence $\{\mathbf{f}_i\}^{\infty}_{i=1} \subset \mathcal{M}$ such that $(\mathbf{f}_i)_*\mu$ converges to $\nu$ in distribution as $i \rightarrow \infty$, where $(\mathbf{f}_i)_*\mu :=\mu \circ \mathbf{f}_i^{-1}$. 
\end{definition}
%{\color{red} Question here about the equation!}



\begin{definition}
(Immersion and submanifold) $\mathbf{g}:\mathfrak{M}  \rightarrow \mathfrak{N} $ is said to be an immersion if rank($\mathbf{g}$)$=m=$dim($\mathfrak{M}$) everywhere. If $\mathbf{g}$ is injective~(one-to-one) immersion, then $\mathbf{g}$ establish an one-to-one correspondence of  $\mathfrak{M}$ and the subset $\Tilde{\mathfrak{M}} = \mathbf{g}(\mathfrak{M})$ of $\mathfrak{N}$. If we use this correspondence to endow $\Tilde{\mathfrak{M}}$ with a topology and $\mathcal{C}^{\infty}$ structure, then $\Tilde{\mathfrak{M}}$ will be called a submanifold (or immersed submanifold) and $\mathbf{g}:\mathfrak{M} \rightarrow \Tilde{\mathfrak{M}}$ is a diffeomorphism. 
\end{definition}

% \begin{definition}
% ($\mathcal{C}^2$-diffeomorphisms: $\mathcal{D}^2$). We define $\mathcal{D}^2$  as the set of all $\mathcal{C}^2$-diffeomorphisms $\mathbf{g}: U_\mathbf{g} \rightarrow \textrm{Im}(\mathbf{g}) $, where $U_\mathbf{g} \subset \mathbb{R}^m$ is an open set $\mathcal{C}^2$-diffeomorphic to $\mathbb{R}^m$, which may depend on $\mathbf{g}$.
% \end{definition}
%{\color{red} Need revision to fit submanifold! }



\begin{definition}
($\mathcal{C}^r$-diffeomorphisms for Submanifold: $\mathcal{Q}^r$). We define $\mathcal{Q}^r$  as the set of all $\mathcal{C}^r$-diffeomorphisms $\mathbf{g}: U_\mathbf{g} \rightarrow \mathfrak{U} $, where $U_\mathbf{g} \subset \mathbb{R}^m$ is an open set $\mathcal{C}^r$-diffeomorphic to $\mathfrak{U}$, which may depend on $\mathbf{g}$, and $\mathfrak{U}$ is a submanifold of $\mathbb{R}^n$.
\end{definition}

We use $m$ to represent the root dimension of a VFG, and $n$ to denote the dimension of data samples. VFGs learn the data manifold embedding the $\mathbb{R}^n$. We define $\mathcal{C}_c^{\infty}(\mathbb{R}^{m-1})$ as the set of all compactly-supported $\mathcal{C}^{\infty}$ mappings from $\mathbb{R}^{m-1}$ to $\mathbb{R}$. For a function set $\mathcal{T}$, we define $\mathcal{T}$-ACF as the set of affine coupling flows that  are assembled  with functions in $\mathcal{T}$, and  we use VFG$_{\mathcal{T}-ACF}$ to represent the set of VFGs constructed using flows in $\mathcal{T}$-ACF. 



\begin{theorem} \label{thm:lp_univ}
($L^p$-universality) Let $p \in [0, \infty)$ . Assume $\mathcal{H}$ is a sup-universal approximator for $\mathcal{C}_c^{\infty}(\mathbb{R}^{m-1})$, and that it concists of $\mathcal{C}^1$-functions. Then  VFG$_{\mathcal{H}-ACF}$ is an $L^p$-universal approximator for $\mathcal{Q}_c^0$ . 
\end{theorem}
%for $m\geq 2$, and $n \geq 4$
\begin{proof}
%Overlapping VFGs
%For $n\geq 2m$,  

We  construct a VFG structure that forms a mapping from $\mathbb{R}^m$ to $\mathbb{R}^n$.  Let $r=n\mod m$. 

If  $r =0$, it is easy to construct an one-layer  VFG tree model $\mathbf{f}$ with  the root as an aggregation node. The children divide the $n$ input entries into $\tau = n/m$ sections, and each section  connects the aggregation node with a flow function. 



Given an injective immersion $\mathbf{g}:\mathfrak{M}  \rightarrow \mathfrak{N}$, function $\mathbf{g}$ can be represented with the the concatenation of a set of functions, i.e.  $\mathbf{g}=[\mathbf{g}_1, ..., \mathbf{g}_{\tau}]^{\top}$. According to the function decomposition theory~\cite{kuo2010decompositions}, the inverse  can be represent as the summation of  functions $\mathbf{g}^{-1}_{i}, 1\leq i \leq \tau$, i.e. $\mathbf{g}^{-1} = \frac{1}{\tau} \sum_{i=1}^{\tau} \mathbf{g}^{-1}_{i}$. Each $\mathbf{g}_{i} \in \mathcal{D}^2$, and $\tilde{\mathfrak{M}}_{i} = \mathbf{g}_{i}(\mathfrak{M})$ is diffeomorphic to $\mathfrak{M}$. According to  Theorem 2 in~\cite{Takeshi2020}, $\mathcal{H}-ACF$ is  an  universal approximater  for each $\mathbf{g}_{i} $. Therefore,  VFG $\mathbf{f}$ has  universal approximation for immersion $\mathbf{g}:\mathfrak{M}  \rightarrow \mathfrak{N}$. 



If $r \neq 0$, let $\tau = \lfloor n/m \rfloor$. We  divide the  $\tau$th section and the remaining $r$ entries into two equal small sections that are denoted with $\tau$ and $\tau +1$. Sections $\tau$ and $\tau +1$ have  $r$  overlapped entries. Similarly, we can construct an one-layer VFG $\mathbf{f}$ with $\tau +1$ children,  and  each child takes a section as the input.

The input coordinate index of $\mathbf{g}_{\tau}$ in $\mathbb{R}^m$ is $I_{\tau} = \big[1,2,..., \lceil (m+r)/2 \rceil \big]$, and the out put index of $\mathbf{g}_{\tau}$  in $\mathbb{R}^n$  is $I_{\tau} + \gamma = \big[\gamma + 1, \gamma + 2,...,  \gamma + \lceil (m+r)/2 \rceil \big]$, and $\gamma = (\tau-1)m$. The input coordinate index of $\mathbf{g}_{\tau+1}$ in $\mathbb{R}^m$ is $I_{\tau + 1} = \big[m- \lceil (m+r)/2 \rceil + 1, ..., m-1, m \big]$, and the out put index of $\mathbf{g}_{\tau +1}$  in $\mathbb{R}^n$  is $I_{\tau+1} + \gamma $.   We can see that the m dimension is divided into two sets, the overlapped set $O = \big[ m- \lceil (m+r)/2 \rceil + 1, \lceil (m+r)/2 \rceil  \big]$, and the remaining set $R$ containing the rest dimensions. 

The mapping $\mathbf{g}:\mathfrak{M}  \rightarrow \mathfrak{N}$ is decomposed into $\tau + 1$ functions, i.e.  $\mathbf{g}=[\mathbf{g}_1, ..., \mathbf{g}_{\tau}, \mathbf{g}_{\tau+1}]^{\top}$, and the inverse $\mathbf{g}^{-1}$ is adjusted here:  $\mathbf{g}_j^{-1} = \frac{1}{\omega} \sum_{i=1}^{\omega} \mathbf{g}^{-1}_{i(j)}$. When $j\in O$, $\omega = \tau +1$, and all $\mathbf{g}^{-1}_i$s will be involved; when $j\in R$, $\omega = \tau$, and either $\mathbf{g}^{-1}_{\tau}$  or $\mathbf{g}^{-1}_{\tau +1}$ is omitted due to the missing of  entry $j$ in the function output. 

The mapping  $\mathbf{g}_{\tau}$  is a diffeomorphism from manifold  $\mathfrak{M}_{\tau}$ ( $\mathfrak{M}_{\tau} \subset \mathfrak{M}$) to  sub-manifold $\tilde{\mathfrak{M}}_{\tau}$  of $\mathfrak{N}$. Similarly 
 $\mathbf{g}_{\tau+1}$ is a  diffeomorphism from  $\mathfrak{M}_{\tau+1}$  to  manifold $\tilde{\mathfrak{M}}_{\tau+1}$.  For each $\mathbf{g}_{\i}$, $i:1\leq i \leq \tau +1$, it can unversally approximated with a function in $\mathcal{H}-ACF$. Here we can construct a VFG with universal approximation for $\mathbf{g}$. 

\end{proof}

% \begin{remark}
% VFG$_{\mathcal{H}-ACF}$ is an distributional universal approximator.
% \end{remark}


\section{Additional Details on ELBO}\label{sec:elbo_cal_supp}
%We use generative model to denote the mapping 
This section presents more details on ELBO calculation. 
The recognition model in a VFG is the neural network~(encoder) used to approximate the posterior of latent variables. 
With invertible neural networks~(flows), the recognition model and the generative model in a VFG share the same structure and parameters.  As shown in Figure~\ref{fig:tree_message},    the recognition and generative models are realized with  forward and backward message passing, respectively.

Maximize the ELBOs~(\ref{eq:elbo},\ref{eq:elbo_dag}) requires evaluation of both the reconstruction and the $\mathbf{KL}$ terms. It involves node state samples from the posterior in both forward and backward messages.% In this section, we first gives the conditional distributions in both  models, then give more details on the computation of ELBO. We start from tree models, and it is easy to extend to DAG models.
% \begin{figure}[h]
% \begin{center}
%  \includegraphics[width=0.8\linewidth]{fig/tree_message2.png}
% \end{center}
% \vspace{-0.2in}
% \caption{ The recognition model consists of froward message from  data to approximate the posterior distributions; the generative model is realized by backward message from the root node and generates the reconstruction in each layer. }
% \label{fig:tree_message2}
% \vspace{-0.15in}
% \end{figure}

%\subsection{Message Passing}


\subsection{ELBO Calculation}
Maximizing the ELBO~(\ref{eq:elbo}) equals to  optimizing the parameters of the flows, $\theta$.  Similar to VAEs~\cite{kingma2013auto,rezende2014stochastic}, we apply forward message passing~(encoding) to approximate the posterior  distribution of each layer's latent variables, and backward message passing~(decoding) to  generate the reconstructions as shown in Figure~\ref{fig:tree_message}. VFGs are hierarchical aggregation model that requires  information from all the leaf nodes  aggregates at the root nodes. 

For the following sections, we use $\mathbf{h}^l$ to represent the node state of layer $l$ in the forward message, and $\widehat{\mathbf{h}}^l$ for the node state in the backward message. For all layer, both $\mathbf{h}^l$ and $\widehat{\mathbf{h}}^l$  are sampled from the posterior. At the rood nodes, we have  $\widehat{\mathbf{h}}^{\mathcal{R}}=\mathbf{h}^{\mathcal{R}}$ . 
The calculation of the data reconstruction term in~\eqref{eq:elbo} requires  samples of  $\mathbf{h}^l$ and $\widehat{\mathbf{h}}^l$ from the posterior. It corresponds to the encoding and decoding procedures in VAE model~\cite{kingma2013auto,rezende2014stochastic} as given by Eq.~(\ref{eq:vae_recon}). Specifically, we have

\vspace{-0.15in}
\begin{align} \label{eq:post_smp}
 &\mathbf{h}^l \sim q(\cdot | \mathbf{h}^{l-1})  \quad \quad \textrm{where} \quad 1\leqslant l \leqslant L \ ., \\  \label{eq:prior_smp}
 &\widehat{\mathbf{h}}^l \sim p(\cdot | \widehat{\mathbf{h}}^{l+1}) \quad \quad \textrm{where} \quad 0\leqslant l \leqslant L-1 \ .
 \end{align}
 
\vspace{-0.15in}
\noindent At the root node,  we have $\widehat{\mathbf{h}}^L=\mathbf{h}^L \sim q(\cdot | \mathbf{h}^{L-1}) $.  
The computation of $ \mathbf{h}^l$s and $\widehat{\mathbf{h}}^l$s with~(\ref{eq:post_smp}) and~(\ref{eq:prior_smp}) requires message passing via the flow functions shown in Figure~\ref{fig:tree_message}. Latent node states in both~(\ref{eq:post_smp}) and~(\ref{eq:prior_smp})  are directly obtained with flow functions.

The reconstruction term in ELBO~(\ref{eq:elbo}) can be computed with the backward message from the generative model $p(\mathbf{x}| \widehat{\mathbf{h}}^{1})$, i.e.,

\vspace{-0.15in}
\begin{align*} 
&\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}^{1:L})\big]
=\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\widehat{\mathbf{h}}^{1:L})  \big] \\
&\simeq \frac{1}{U}\sum_{u=1}^U \log p(\mathbf{x}| \widehat{\mathbf{h}}^{1:L}_u) = \frac{1}{U}\sum_{u=1}^U \log p(\mathbf{x}| \widehat{\mathbf{h}}^{1}_u)\\ & \simeq   \log p(\mathbf{x}| \widehat{\mathbf{h}}^{1}) .
 \end{align*}

\vspace{-0.15in}
\noindent For a VFG model, we set $U=1$. In the last term,  $p(\mathbf{x}| \widehat{\mathbf{h}}^{1})$ is either Gaussian or Binary distribution parameterized with $\widehat{\mathbf{x}}$ generated via the flow function with $\widehat{\mathbf{h}}^{1}$ as the input.  For any $l \in [L]$, the calculation of the $\mathbf{KL}^l$ term is done in a similar manner, and  \eqref{eq:kl} requires  both  forward message  from  the posterior and backward message from the generative model, i.e.,

\vspace{-0.15in}
\begin{align}\label{eq:KL_l}
\mathbf{KL}^l=&\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\widehat{\mathbf{h}}^{l+1}) \big] \\ \notag
\simeq & \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\widehat{\mathbf{h}}^{l+1}).
\end{align} 

In a tree structured VFG, each layer may consist of multiple nodes. In layer $l$, each node's  $\mathbf{KL}$ value can be  computed individually, thus $\mathbf{KL}^l = \sum_{i\in l} \mathbf{KL}^{(i)}$, and here each $i$ represents a node. For a DAG structure, we can individually evaluate each node's $\mathbf{KL}$ term. 


\subsection{Distributions of Latent Variables}
We first discuss additional details on the distributions of latent variables.

\subsubsection{Generative Model}~\label{sec:generative}
In a tree VFG, the sample reconstruction in the generative model consists of layer-wise backward message passing, i.e., latent variable generation in each layer.  For any $l, 0 \leq l \leq L-1$, latent variable backward state~(reconstruction) $\widehat{\mathbf{h}}^{l}$ is propagated   from layer $l+1$ via the flow function $\mathbf{f}_l$ between the two layers with $\widehat{\mathbf{h}}^{l}= \mathbf{f}_l^{-1}(\widehat{\mathbf{h}}^{l+1})$. 
The prior $p(\mathbf{h}^L)$ for the root latent variable $\mathbf{h}^L$ is  Laplace(0,1). With a sample $\widehat{\mathbf{h}}^L$ from the posterior, i.e., $\widehat{\mathbf{h}}^L=\mathbf{h}^L \sim q(\cdot | \mathbf{h}^{L-1}) $,  the conditional distribution for latent variable in layer $l$ is   $p(\cdot | \widehat{\mathbf{h}}^{l+1}):=\text{Laplace}(\widehat{\mathbf{h}}^l, 1)$. Here the location parameter is generated from layer $l+1$, i.e., $\widehat{\mathbf{h}}^{l}= \mathbf{f}_l^{-1}(\widehat{\mathbf{h}}^{l+1})$. 
For a latent variable  $\mathbf{h}^l$ sampling from the posterior, its log-likelihood regarding $p(\cdot | \widehat{\mathbf{h}}^{l+1})$ in~(\ref{eq:KL_l}) is given by 
\begin{align} \notag
& \log p(\mathbf{h}^l | \widehat{\mathbf{h}}^{l+1}) = -\|\mathbf{h}^l- \widehat{\mathbf{h}}^l\|_1 -d\cdot\log2  .
\end{align}
Here $d = dim(\mathbf{h}^l)$. Hence, minimizing  $\mathbf{KL}$s is to minimize the $\ell_1$ distance between latent variables and their reconstructions. 


\subsubsection{Recognition Model}
The forward message passing in the recognition model consists of  layer-wise sample generation. In layer $l, 1 \leq l \leq L$, latent variable forward state $\mathbf{h}^{l}$ is propagated   from layer $l-1$ via the flow function $\mathbf{f}_{l-1}$ between the two layers with $\mathbf{h}^{l}= \mathbf{f}_{l-1}(\mathbf{h}^{l-1})$. 

 We assume each entry of  hidden variable $\mathbf{h}^{l}$ follows a Laplace distribution, i.e., $\mathbf{h}_j^{l} \sim \text{Laplace}(\mu_j^{l}, s_j^{l})$ for layer $l$'s $j$th entry. Here $\mu_j^{l}$ is the location and $s_j^{l}$ is the scale. Compared with other distributions, Laplace can introduce sparsity to the model and it works well in practice. At level $l \in [L]$, we set $q(\cdot|\mathbf{h}^{l-1}) := \text{Laplace}(\mathbf{\mu}^l, \mathbf{s}^l)$ with
\begin{align}\label{eq:posteriorapp}
&\mathbf{\mu}^l = \text{median}\big(H\big), \ \  \mathbf{s}^l =\frac{1}{B}\sum_{b=1}^B|\mathbf{h}^l(\mathbf{x}_b) - \mathbf{\mu}^l| \ .
\end{align}
Here $H=\{\mathbf{h}^l(\mathbf{x}_b)| 1\leqslant b \leqslant B\}$ is a batch of latent values generated from a batch of data samples with size $B$, i.e., $X_{B} = \{\mathbf{x}_b | 1\leqslant b \leqslant B\}$. The median operation is performed element-wisely. For each $\mathbf{x}_b$, $\mathbf{h}^l(\mathbf{x}_b)=\mathbf{f}^{l-1}\big(\mathbf{h}^{l-1}(\mathbf{x}_b)\big)$.

In summary, we use Laplace distribution to model latent distributions. $q(\cdot|\mathbf{h}^{l-1})$ is a Laplace with location and scale equal to the median and scale of a batch of $\mathbf{h}^{l}$, respectively; $p(\cdot|\widehat{\mathbf{h}}^{l+1})$ is a  Laplace parameterized  with  $(\widehat{\mathbf{h}}^{l}, 1.0)$ as the location and scale parameters. Hence with $\mathbf{h}^{l}$ we can compute the log-likelihoods on RHS of~(\ref{eq:KL_l}) and thus the $\mathbf{KL}^l$ value. We use Laplace distribution for latent variables because it may introduce sparsity in latent variables. We also tried Gaussian and Gamma distributions, and they also work well in most tasks. VFGs leverage the universal approximation power~\cite{teshima2020coupling} of coupling flows, and tractable inference could be achieved with different latent distributions. 


\subsection{$\mathbf{KL}$ Term}
For any $l,  1 \leq l \leq L-1$, the calculation of the $\mathbf{KL}^l$ term~(\ref{eq:kl}) requires  message passing and samples  from both recognition and  generative models, i.e.,
\begin{align}\label{eq:KL_lapp}
\mathbf{KL}^l= & \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\widehat{\mathbf{h}}^{l+1}) \big] \\ \notag
\simeq  & \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\widehat{\mathbf{h}}^{l+1}).
\end{align} 
%We use Laplace distribution to model latent distributions.
Here $q(\cdot|\mathbf{h}^{l-1})$ is a Laplace with location and scale equal to the median and scale defined in~\eqref{eq:posteriorapp}; $p(\cdot|\widehat{\mathbf{h}}^{l+1})$ is a  Laplace parameterized  with  $(\widehat{\mathbf{h}}^{l}, 1.0)$ as discussed in~\ref{sec:generative}. Hence with $\mathbf{h}^{l}$ we can compute the log-likelihoods on RHS of~(\ref{eq:KL_lapp}) and thus the $\mathbf{KL}^l$ value. When $l=L$, 
$\mathbf{KL}^L =  \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{L}|\mathbf{h}^{L-1})- \log p(\mathbf{h}^{L})  \big] \simeq \log q(\mathbf{h}^{L}|\mathbf{h}^{L-1})- \log p(\mathbf{h}^{L}) .$


 
Assume  $k$ leaf nodes on a tree or a DAG model, corresponding to $k$ sections of  input sample $\mathbf{x} = [\mathbf{x}^{(1)}, ..., \mathbf{x}^{(k)}]$. Then the hidden variables in both~(\ref{eq:elbo}) and~(\ref{eq:elbo_dag}) are computed with forward and backward message passing. 
%Next, we provide more details about the nodes.

In practice, we set $U=1$ for efficiency. With a batch of training samples, the structure of flow functions make the  forward  and  backward message passing very efficient, and  thus the estimation of the ELBO.  


\subsection{Reconstruction Term}

The reconstruction term in ELBO~(\ref{eq:elbo}) can be computed with the backward message from the generative model $p(\mathbf{x}| \widehat{\mathbf{h}}^{1})$, i.e.,
\begin{align*} 
&\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}^{1:L})\big]
=\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\widehat{\mathbf{h}}^{1:L})  \big] \\
&\simeq \frac{1}{U}\sum_{u=1}^U \log p(\mathbf{x}| \widehat{\mathbf{h}}^{1:L}_u) = \frac{1}{U}\sum_{u=1}^U \log p(\mathbf{x}| \widehat{\mathbf{h}}^{1}_m) \\
&\simeq  \log p(\mathbf{x}| \widehat{\mathbf{h}}^{1}) .
 \end{align*}
For a VFG model, we set $U=1$. In the last term,  $p(\mathbf{x}| \widehat{\mathbf{h}}^{1})$ is either Gaussian or binary distribution parameterized with $\widehat{\mathbf{x}}$ generated via the flow function with $\widehat{\mathbf{h}}^{1}$ as the input. 




\section{Additional Details on Training}\label{sec:app_train}


%\subsection{Enhanced Training}
One import motivation of VFG is that we aim to develop a model that is tractable of inference on datasets. 
A very simple application case with inference is to impute the missing values of data samples.  
It motivates us to use the imputation loss as the objective along with the KL terms from the ELBO as regularization terms. 

Inference on a VFG model requires the aggregation node's state to be imputed from observed children's state, as shown in~(\ref{eq:aggr_obs_ch}).
Then, unobserved children's state can be inferred from their parent.  
The inference ability of VFG via imputation can be reinforced by \emph{masking out} some sections of the training samples. 
The training objective can be changed to force the model to impute the value of the masked sections. 
For example in a tree model, the alternative objective function reads 
\begin{align}  \label{eq:elbo_tree_mask}
& \mathcal{L}(\mathbf{x}, O_{\mathbf{x}}; \theta) \\ \notag
= & \sum_{t: 1\leqslant t \leqslant k, t\notin O}
 \mathbb{E}_{q(\mathbf{h}^{1}|\mathbf{x}^{O_{\mathbf{x}}} )} \bigg[ \log p( \mathbf{x}^{(t)}|  \widehat{\mathbf{h}}^{1})   \bigg] \\ \notag
 &- \sum_{l=1}^{L-1}  \mathbb{E}_{q(\mathbf{h}|\mathbf{x})} \bigg[ \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1}) - \log p( \mathbf{h}^{l}|  \widehat{\mathbf{h}}^{l+1})   \bigg]    
\\ \notag
& -  \textbf{\text{KL}}\big(q(\mathbf{h}^L | \mathbf{h}^{L-1} )   | p(\mathbf{h}^L)  \big).
\end{align} %}
where $O_{\mathbf{x}}$ is the index set of leaf nodes  with observation, and $\mathbf{x}^{O_{\mathbf{x}}}$ is the union of observed data sections. 
Considering a minibatch of training samples, the objectives function in~(\ref{eq:elbo})
and~(\ref{eq:elbo_tree_mask}) can thus be optimized sequentially. 
The training with \emph{random masking} is described in Algorithm~\ref{alg:rand_mask}. 

\begin{algorithm}[H]
%{\small
%\algsetup{indent=0.25em}
   \caption{Inference model parameters with random masking}
   \label{alg:rand_mask}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Data distribution $\mathcal{D}$,  $\mathbb{G} = \{\mathcal{V}, \mathbf{f}\}$
   \FOR {$s=0,1,...$}
   \STATE  Sample minibatch $b$ samples $\{\mathbf{x}_1, ..., \mathbf{x}_b \}$ from $\mathcal{D}$;
   \STATE  
    Optimize~(\ref{eq:elbo}) with Line~4 to Line~15 in Algorithm~\ref{alg:main};
    \STATE  Sample a subset of the $k$ data sections as data observation set $O_{\mathbf{x}}$; $O \leftarrow O_{\mathbf{x}}$;
   \FOR{$i \in \mathcal{V}$}
    \STATE  \textcolor{blue}{// forward message passing}
   \STATE $\mathbf{h}^{(i)} = \frac{1}{|ch(i) \cap O |} \sum_{j \in ch(i) \cap O} \mathbf{f}_{(j,i)}(\mathbf{h}^{(j)})$; 
     \STATE  $O \leftarrow O \cup \{i\}$ if $ch(i) \cap O \neq \emptyset $; 
    \ENDFOR
    \STATE $\widehat{\mathbf{h}}^{(i)} = \mathbf{h}^{(i)} \ \  \text{if} \ i \in \mathcal{R}_{\mathbb{G}} $ or $i \in$ layer L;
   \FOR{$i \in \mathcal{V}$}
   \STATE \textcolor{blue}{// backward message passing}
   \STATE $\widehat{\mathbf{h}}^{(i)} = \frac{1}{|pa(i)|} \sum_{j \in pa(i) } \mathbf{f}^{-1}_{ (i,j)}(\widehat{\mathbf{h}}^{(j)}) $;%\label{line:backward}  
   \ENDFOR
    \STATE  $\mathbf{h} =  \{\mathbf{h}^{(t)} \big |  t \in \mathcal{V} \cap O \}$, $\widehat{\mathbf{h}} =  \{\widehat{\mathbf{h}}^{(t)} \big | t \in \mathcal{V} \}$;
    \STATE Approximate the $\mathbf{KL}$ terms in ELBO for each layer with b samples;
    \STATE Updating VFG with gradient of~(\ref{eq:elbo_tree_mask}): $\theta^{(s+1)}_{\mathbf{f}} = \theta^{(s)}_{\mathbf{f}} + \nabla_{\theta_{\mathbf{f}}}\frac{1}{b} \sum_{i=1}^b  \mathcal{L}(\mathbf{x}_b, O_{\mathbf{x}}; \theta^{(s)}_{\mathbf{f}})   \, ,$
   \ENDFOR
\end{algorithmic}%}
\end{algorithm}

%{\color{red}


 In practice, we use Algorithm 2 along with Algorithm 1 to train a VFG model.   Training with Algorithm 1 is to improve the data fitting and the \emph{consistency} of aggregation nodes. Geometrically speaking, training with Algorithm 2 is to enhance the learned relational knowledge among leaf nodes.
 Though Algorithm 2 improves the model's inference capability in a heuristic way, it works in practice.
 %}
 
 
% \subsection{Algorithm Convergence}
%
%% \subsection{Algorithm Convergence without Masking}
%We have the following assumptions about the loss function  $\mathcal{L}()$,
%
%\begin{itemize}
%	\item [(1)]  $\mathcal{L}()$ is twice differentiable and has $\ell$-Lipschitz gradient, i.e. $\forall \theta_1, \theta_2$,
%	\begin{align*}
%	||  \bigtriangledown  \mathcal{L}(\theta_1) -  \bigtriangledown  \mathcal{L}(\theta_2) || \leq \ell || \theta_1 - \theta_2||.
%	\end{align*}
%	Also the upper bound of $\mathcal{L}(\theta^*)$ is a finite value, i.e. $\mathcal{L}(\theta^*) <  \infty$.
%	\item [(2)] At time $t$ the algorithm can access a bounded noisy gradient and Hessian matrix, and also the gradient and the  Frobenius norm of the  Hessian matrix is bounded by a constant values. 
%	\begin{align} \notag
%	 &|| \bigtriangledown \mathcal{L}(\theta_t) || \leq R_1 ,\\ \notag
%	 & ||g_\mathcal{L}||  \leq R_1  ,\\
%	 & ||\bigtriangledown^2 \mathcal{L}(\theta_t)||_F \leq R_2  \label{eq:hess}.
%	\end{align}	
%\end{itemize}
%
%
%\begin{theorem} \label{thm:alg1}
% Let $\mathcal{L}( \mathbf{x}; \theta)$ be the loss function  for the for the  Algorithm~\ref{alg:main} with  parameter set $\theta$ and $x$ as the input. With  assumptions (1)-(2) on $\mathcal{L}()$  and  generalized Adam method as the  updating steps, Algorithm~\ref{alg:main} can reach a stationary solution.  
%\end{theorem}



%\subsection{Algorithm Convergence with Masking}

% \begin{theorem}
% With the assumptions given in Theorem~\ref{thm:alg1}, the convergence rate for Algorithm~\ref{alg:rand_mask} is given by 
% \end{theorem}

%{\color{red} Nodes are partially involved with masking training, and it will involves more iteration numbers. Needs a figure here! }
%The random masking training process is an encoding-decoding procedure. 
%{\color{red}   Needs a figure here! }

\section{Aggregation Node}\label{sec:aggr_supp}
We present additional details on aggregation nodes here. 
Let $\mathbf{f}_{(i, j)}$ be the direct edge~(function) from node $i$ to node $j$, and $\mathbf{f}^{-1}_{ (i, j)}$ or  $\mathbf{f}_{ (j, i)}$ defined as its inverse function. Then, at an aggregation node $i$ that has multiple~($|ch(i)|$) children, its latent variable in forward message passing is the mean of all children's output, i.e.,
 %{\small
 \begin{align}\label{eq:child_avg}
&  \mathbf{h}^{(i)} = \frac{1}{|ch(i)|} \sum_{j \in ch(i) } \mathbf{f}_{(j,i)}(\mathbf{h}^{(j)})  .
\end{align}%}

On the other hand, if node $i$ in a DAG  has multiple parents, the reconstruction of its latent variable is the mean of all parents' output , i.e.,
% {\small
 \begin{align}\label{eq:parent_avg}
 &\widehat{\mathbf{h}}^{(i)} = \frac{1}{|pa(i)|} \sum_{j \in pa(i) } \mathbf{f}^{-1}_{ (i,j)}(\widehat{\mathbf{h}}^{(j)}) \, .
\end{align} %} 
Notice that the above two equations hold even when node $i$ has only one child or parent.

\begin{figure}[ht]
\begin{center}
 \includegraphics[width=0.15\linewidth]{fig/dag_aggr.png}
\end{center}
\vspace{-0.1in}
\caption{ {\small  Aggregation node on a DAG.}}
\label{fig:dag_aggr}
%\vspace{-0.08in}
\end{figure}

Besides averaging, the aggregation nodes also ensure the latent variable on the two ends of an identity function are \emph{consistent}. We use node $i$ in the DAG presented in Figure~\ref{fig:dag_aggr} as an example. Node $i$ has two parents, $u$ and $v$; and two children, $d$ and $e$. Node $i$  connects its parents and children with identity functions. According to~(\ref{eq:child_avg}) and~(\ref{eq:parent_avg}), we have $\mathbf{h}^{(i)} = (\mathbf{h}^{(d)}+\mathbf{h}^{(e)})/2$  and $\widehat{\mathbf{h}}^{(i)} = (\widehat{\mathbf{h}}^{(u)}+\widehat{\mathbf{h}}^{(v)})/2$.
Here aggregation \emph{consistency} means, for $i$'s children, their forward state should be consistent with $i$'s backward state, i.e., 
{
\begin{align}\label{eq:i_child}
\mathbf{h}^{(d)} = \mathbf{h}^{(e)} = \widehat{\mathbf{h}}^{(i)} .
\end{align}} 
For $i$'s parents, their backward state should be consistent with $i$'s forward state, i.e.,
%{\small 
\begin{align}\label{eq:i_parent}
\widehat{\mathbf{h}}^{(u)}  = \widehat{\mathbf{h}}^{(v)}  = \mathbf{h}^{(i)} .
\end{align}%} 
We utilize the $\mathbf{KL}$ term in the ELBOs to ensure~(\ref{eq:i_child}) and~(\ref{eq:i_parent}) can be satisfied during parameter updating. The $\mathbf{KL}$ term regarding node $i$ is
%{\small 
\begin{align*}%\label{eq:kl_i}
\textbf{\text{KL}}^{(i)} = & \mathbb{E}_{q(\mathbf{h}|\mathbf{x})}\big[  \log q(\mathbf{h}^{(i)}|\mathbf{h}^{ch(i)})  - \log p(\mathbf{h}^{(i)}|\widehat{\mathbf{h}}^{pa(i)}) \big]  \\ \notag
\simeq  & \log q(\mathbf{h}^{(i)}|\mathbf{h}^{ch(i)})  - \log p(\mathbf{h}^{(i)}|\widehat{\mathbf{h}}^{pa(i)}).
\end{align*} %}
Here 
{%\small 
\begin{align} \notag
&\log p(\mathbf{h}^{(i)}|\widehat{\mathbf{h}}^{pa(i)}) \\ \notag
= &\frac{1}{2}\big(\log p(\mathbf{h}^{(i)}|\widehat{\mathbf{h}}^{(u)}) + p(\mathbf{h}^{(i)}|\widehat{\mathbf{h}}^{(v)})\big)\\ \notag
=& \frac{1}{2}\big(-\|\mathbf{h}^{(i)}- \widehat{\mathbf{h}}^{(u)}\|_1 -\|\mathbf{h}^{(i)}- \widehat{\mathbf{h}}^{(v)}\|_1-2d\cdot\log2 \big).
\end{align}}
Hence minimizing $\textbf{\text{KL}}^{(i)}$ is equal to minimizing $\{\|\mathbf{h}^{(i)}- \widehat{\mathbf{h}}^{(u)}\|_1 + \|\mathbf{h}^{(i)}- \widehat{\mathbf{h}}^{(v)}\|_1 \}$ which achieves the consistent objective in~\eqref{eq:i_parent}. 

Similarly,  $\textbf{\text{KL}}$s  of $i$'s children intend to realize consistency given in~\eqref{eq:i_child}. We use node $d$ as an example.  The $\textbf{\text{KL}}$ term regarding $d$ is 
%{\small 
\begin{align*}%\label{eq:kl_i}
\textbf{\text{KL}}^{(d)} = & \mathbb{E}_{q(\mathbf{h}|\mathbf{x})}\big[  \log q(\mathbf{h}^{(d)}|\mathbf{h}^{ch(d)})  - \log p(\mathbf{h}^{(d)}|\widehat{\mathbf{h}}^{pa(d)}) \big] \\
\simeq & \log q(\mathbf{h}^{(d)}|\mathbf{h}^{ch(d)})  - \log p(\mathbf{h}^{(d)}|\widehat{\mathbf{h}}^{pa(d)}).
\end{align*} 
With 
\begin{align*}%\label{eq:kl_i}
 \log p(\mathbf{h}^{(d)}|\widehat{\mathbf{h}}^{pa(d)})= & \log p(\mathbf{h}^{(d)}|\widehat{\mathbf{h}}^{(i)})
 =  -\|\mathbf{h}^{(d)}- \widehat{\mathbf{h}}^{(i)}\|_1 - d\cdot\log2,
\end{align*} %}
minimizing $\textbf{\text{KL}}^{(d)}$ is to minimize $\|\mathbf{h}^{(d)}- \widehat{\mathbf{h}}^{(i)}\|_1$ that targets at \eqref{eq:i_child}. In summary, by maximizing the ELBO of a VFG, the aggregation consistency can be  attained along with fitting the model to the data. 


%\section{Appendix}
\section{Derivation of the ELBOs for  Trees and DAGs }\label{sec:ebl_deri}
\subsection{ELBO of Tree Models}\label{appd:tree_elbo}

Let each data sample has $k$ sections, i.e., $\mathbf{x} = [\mathbf{x}^{(1)}, ..., \mathbf{x}^{(k)}]$. VFGs are graphical models that can integrate different sections or components of the dataset.  We assume that for each pair of connected nodes, the edge is an invertible flow function. 
The vector of parameters for all the edges is denoted by $\theta$. 
The forward message passing starts from $\mathbf{x}$ and ends at $\mathbf{h}^L$, and backward message passing in the reverse direction. We start with the hierarchical generative tree network structure illustrated by an example in Figure~\ref{fig:tree_vfg}. 
Then the marginal likelihood term of the data reads
\begin{align*}
p(\mathbf{x}| \mathbf{\theta}) = \sum_{\mathbf{h}^1, ..., \mathbf{h}^L} p(\mathbf{h}^L | \theta)p(\mathbf{h}^{L-1} | \mathbf{h}^{L},\theta) \cdot \cdot  \cdot  p(\mathbf{x} | \mathbf{h}^{1}, \theta) \, .
\end{align*}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.8in]{fig/tree_vfg.png}
    \caption{A  tree VFG with $L=5$ and three aggregation nodes.}
    \label{fig:tree_vfg}
\end{figure}
\noindent The hierarchical generative model is given by factorization
\begin{align}\label{eq:prior}
p(\mathbf{h}) =  p( \mathbf{h}^{L})\mathbf{\Pi}_{l=1}^{L-1}p(\mathbf{h}^{l} | \mathbf{h}^{l+1}) .
\end{align}
The probability density function $p(\mathbf{h}^{l-1} | \mathbf{h}^{l})$ in the generative model is modeled with one or multiple invertible normalizing flow functions. The hierarchical posterior~(recognition network) is factorized as
\begin{align}\label{eq:posterior2}
q_{\theta}(\mathbf{h}| \mathbf{x}) =  q(\mathbf{h}^1 | \mathbf{x})  q(\mathbf{h}^2 | \mathbf{h}^1) \cdot \cdot  \cdot  q(\mathbf{h}^{L} | \mathbf{h}^{L-1}).
\end{align}
Draw samples from the generative model~(\ref{eq:prior})
involves sequential conditional sampling from the top of the tree to the bottom, and computation of the recognition model~(\ref{eq:posterior2}) takes the reverse direction. Notice that
\begin{align*} %\label{eq:chain}
q(\mathbf{h}| \mathbf{x}) = q(\mathbf{h}^1 | \mathbf{x})  q(\mathbf{h}^{2:L} | \mathbf{h}^1) \, .
\end{align*}
With the hierarchical structure of a tree, we further have
\begin{align} \label{eq:chain_post}
&q(\mathbf{h}^{l:L}|\mathbf{h}^{l-1}) = q(\mathbf{h}^{l}|\mathbf{h}^{l-1}) q(\mathbf{h}^{l+1:L}|\mathbf{h}^{l}\mathbf{h}^{l-1}) =q(\mathbf{h}^{l}|\mathbf{h}^{l-1}) q(\mathbf{h}^{l+1:L}|\mathbf{h}^{l})  \\ \label{eq:chain_prior}
& p(\mathbf{h}^{l:L})=  p(\mathbf{h}^{l}|\mathbf{h}^{l+1:L})p(\mathbf{h}^{l+1:L})=p(\mathbf{h}^{l}|\mathbf{h}^{l+1})p(\mathbf{h}^{l+1:L})
\end{align}
By leveraging  the conditional independence  in the chain structures of both recognition and generative models, the derivation of trees' ELBO becomes easier.
%{\small
\begin{align*}
\log p(\mathbf{x}) &= \log \int p(\mathbf{x}|\mathbf{h})p(\mathbf{h}) d \mathbf{h} \\
&= \log \int \frac{q(\mathbf{h}|\mathbf{x})}{q(\mathbf{h}|\mathbf{x})} p(\mathbf{x}|\mathbf{h})p(\mathbf{h}) d \mathbf{h} \\
& \geqslant \mathbb{E}_{q(\mathbf{h}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}) -  \log q(\mathbf{h}|\mathbf{x}) +  \log p(\mathbf{h}) \big]\\ \notag
&= \mathcal{L}(x; \theta).
\end{align*}%}
The last step is due to the Jensen inequality. With $\mathbf{h} =\mathbf{h}^{1:L} $, 
 %{\small
\begin{align} \notag  %\label{eq:tree_elbo}
&\log p(\mathbf{x})  \geqslant  \mathcal{L}(x; \theta) \\ \notag
=& \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}^{1:L}) -  \log q(\mathbf{h}^{1:L}|\mathbf{x}) +  \log p(\mathbf{h}^{1:L}) \big] \\ \label{eq:elbo12L}
=&  \underbrace{\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}^{1:L})\big]}_{\parbox{10.5em}{(a) Reconstruction of the data}}  \\ \notag
&-  \underbrace{\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log q(\mathbf{h}^{1:L}|\mathbf{x}) - \log p(\mathbf{h}^{1:L}) \big] }_{\textbf{\text{KL}}^{1:L}}
\end{align}%}
With conditional independence in   the hierarchical structure, we have 
$$q(\mathbf{h}^{1:L}|\mathbf{x})=q(\mathbf{h}^{2:L}|\mathbf{h}^1\mathbf{x})q(\mathbf{h}^{1}|\mathbf{x})=q(\mathbf{h}^{2:L}|\mathbf{h}^1)q(\mathbf{h}^{1}|\mathbf{x}).$$
The second term of~(\ref{eq:elbo12L}) can be further expanded as 
% {\small
 \begin{align} \notag 
\textbf{\text{KL}}^{1:L} =&  \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{1}|\mathbf{x})  +  \log q(\mathbf{h}^{2:L}|\mathbf{h}^{1})  - \log p(\mathbf{h}^{1}|\mathbf{h}^{2:L}) - \log p(\mathbf{h}^{2:L})  \big].
\end{align}%}
Similarly, with conditional independence of the hierarchical latent variables, $ p(\mathbf{h}^{1}|\mathbf{h}^{2:L})= p(\mathbf{h}^{1}|\mathbf{h}^{2})$. Thus
%{\small
 \begin{align} \notag 
\textbf{\text{KL}}^{1:L} =&  \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{1}|\mathbf{x})   - \log p(\mathbf{h}^{1}|\mathbf{h}^{2})  \\
&+  \log q(\mathbf{h}^{2:L}|\mathbf{h}^{1})- \log p(\mathbf{h}^{2:L})  \big]\\ \notag
=&  \underbrace{\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{1}|\mathbf{x})   - \log p(\mathbf{h}^{1}|\mathbf{h}^{2}) \big]}_{\mathbf{KL}^1}   + \underbrace{\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{2:L}|\mathbf{h}^{1})- \log p(\mathbf{h}^{2:L})  \big]}_{\mathbf{KL}^{2:L}}.
\end{align}%}
We can further expand the $\mathbf{KL}^{2:L}$ term following similar conditional independent rules regarding the tree structure.
At level $l$, we get
$$\textbf{\text{KL}}^{l:L} 
= \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l:L}|\mathbf{h}^{l-1})- \log p(\mathbf{h}^{l:L})  \big].$$
With~(\ref{eq:chain_post}) and~(\ref{eq:chain_prior}), it is easy to show that
 %{\small
 \begin{align} \label{eq:kl_lL}
\textbf{\text{KL}}^{l:L} 
=&  \underbrace{\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\mathbf{h}^{l+1}) \big]}_{\mathbf{KL}^l}  + \underbrace{\mathbb{E}_{q(\mathbf{h}^{l:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l+1:L}|\mathbf{h}^{l})- \log p(\mathbf{h}^{l+1:L})  \big]}_{\mathbf{KL}^{l+1:L}}.
\end{align}%}
The ELBO~(\ref{eq:elbo12L}) can be written as 
 %{\small
 \begin{align} \label{eq:elbo0}
\mathcal{L}(\mathbf{x}; \theta) = \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}^{1:L})  \big] - \sum_{l=1}^{L-1} \mathbf{KL}^l -\mathbf{KL}^L.
\end{align}
When $1\leqslant l \leqslant L-1$
 \begin{align} \label{eq:kl_l}
 \mathbf{KL}^l=\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\mathbf{h}^{l+1}) \big].
 \end{align}%}
%As discussed in section~\ref{sec:vfg_inference}, evaluation of the terms in~(\ref{eq:elbo0}) requires samples of both the posterior and the prior in each layer of the tree structure.
According to conditional independence, the expectation regarding variational distribution layer $l$ just depends on layer $l-1$. We can simplify the expectation each term of~(\ref{eq:elbo0}) with the default assumption that all latent variables are generated regarding data sample $\mathbf{x}$.  Therefore the ELBO~(\ref{eq:elbo0}) can be simplified as 
 {\small
 \begin{align} \label{eq:elbo1}
\mathcal{L}(\mathbf{x}; \theta) = \mathbb{E}_{q(\mathbf{h}^{1}|\mathbf{x})}\big[ \log p(\mathbf{x}|\widehat{\mathbf{h}}^{1})  \big] - \sum_{l=1}^{L} \mathbf{KL}^l.
\end{align}}
The $\mathbf{KL}$
term~(\ref{eq:kl_l}) becomes
 {\small
\begin{align*}
 \mathbf{KL}^l=\mathbb{E}_{q(\mathbf{h}^{l}|\mathbf{h}^{l-1})}\big[  \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1})   - \log p(\mathbf{h}^{l}|\widehat{\mathbf{h}}^{l+1}) \big].
 \end{align*}}
When $l=L$, 
 {\small$$\mathbf{KL}^L =  \mathbb{E}_{q(\mathbf{h}^{L}|\mathbf{h}^{L-1})}\big[  \log q(\mathbf{h}^{L}|\mathbf{h}^{L-1})- \log p(\mathbf{h}^{L})  \big].$$}

\subsection{Improve ELBO Estimation with Flows}
%To compute the EBLO, one way is to approximate  $\mathbf{KL}$ terms with the latent values generated from a batch of training data samples.

In this paper we follow the approach in~\cite{rezende2015variational,kingma2016improving,berg2018sylvester}  using normalizing flows to further improve posterior estimation on a tree VFG model. At each layer,  minimizing  $\mathbf{KL}$ term is to is to optimize the parameters of the network so that the posterior is closer to the prior. As shown in Figure~\ref{fig:tree_message}, for layer $l$,  we can take the  encoding-decoding procedures~(discussed in section~\ref{sec:elbo_cal_supp}) as transformation of the posterior distribution from layer $l$ to $l+1$, and then transform it back. By  counting in the transformation difference~\cite{rezende2015variational,kingma2016improving,berg2018sylvester}, the $\mathbf{KL}$ at layer $l$ becomes 
{\small
\begin{align*}
\mathbf{KL}^l=&\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\bigg[  \log q(\mathbf{h}^{l}|\mathbf{h}^{l-1}) +\log \bigg|\det \frac{\partial \mathbf{h}^{l}}{\partial \mathbf{h}^{l+1}}\bigg|  + \log \bigg|\det \frac{\partial \widehat{\mathbf{h}}^{l+1}}{\partial \widehat{\mathbf{h}}^{l}}\bigg|   - \log p(\mathbf{h}^{l}|\widehat{\mathbf{h}}^{l+1}) \bigg]\\
\simeq & \frac{1}{U}\sum_{u=1}^U\bigg[ \log q(\mathbf{h}^{l}_u|\mathbf{h}^{l-1}) +\log \bigg|\det \frac{\partial \mathbf{h}^{l}_u}{\partial \mathbf{h}^{l+1}_u}\bigg|  + \log \bigg|\det \frac{\partial \widehat{\mathbf{h}}^{l+1}_u}{\partial \widehat{\mathbf{h}}^{l}_u}\bigg|   - \log p(\mathbf{h}^{l}_u|\widehat{\mathbf{h}}_u^{l+1}) \bigg].
\end{align*}}





\subsection{ELBO of DAG Models}\label{appd:dag_elbo}
Note that if we reverse the edge directions in a DAG, the resulting graph is still a DAG graph.  
The nodes can be listed in a topological order regarding the DAG structure as shown in Figure~\ref{fig:dag}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1.8in]{fig/dag.png}
    \caption{A DAG  with inverse topology order \big\{ \{1,2,3\}, \{4,5\}, \{6\},  \{7\} \big\}, and they  correspond to layers 0 to 3.  }
    \label{fig:dag}
\end{figure}
%{\color{red} aggregation node}

By taking the topology order as the layers in tree structures, we can derive the ELBO for DAG structures.  
Assume the DAG structure has $L$ layers, and the root nodes are in layer $L$. 
We denote by $\mathbf{h}$ the vector of latent variables, then following~(\ref{eq:elbo12L}) we develop the ELBO as
\begin{align}  \label{eq:dag_elbo}
&\log p(\mathbf{x})  \geqslant  \mathcal{L}(x;\theta) 
=   \mathbb{E}_{q(\mathbf{h} | \mathbf{x})} \bigg[ \log  \frac{p(\mathbf{x}, \mathbf{h})}{q(\mathbf{h}|\mathbf{x})}  \bigg]  \\ \notag
=&  \underbrace{ \mathbb{E}_{q(\mathbf{h} | \mathbf{x})} \bigg[ \log  p(\mathbf{x} |\mathbf{h})  \bigg] }_{  \parbox{10.5em}{Reconstruction of the data}}  -  \underbrace{  \mathbb{E}_{q(\mathbf{h}| \mathbf{x})} \bigg[  \log q(\mathbf{h}|\mathbf{x}) - \log p( \mathbf{h}) \bigg] }_{\textbf{\text{KL}}} \, .   \notag
\end{align} %\log  \frac{p( \mathbf{h})}{q(\mathbf{h}|\mathbf{x})} 
Similarly the KL term can be expanded as in the tree structures. 
For nodes in layer $l$
%{\small 
\begin{align*} 
\textbf{\text{KL}}^{l:L} 
= &\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l:L}|\mathbf{h}^{1:l-1})- \log p(\mathbf{h}^{l:L})  \big].
\end{align*} %}
%\\%=&  \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l:L}|\mathbf{h}^{ch(l)})- \log p(\mathbf{h}^{l:L})  \big].
Note that $ch(l)$ may include nodes from layers lower than $l-1$, and $pa(l)$ may include nodes from layers higher than $l$.
Some nodes in $l$ may not have parent. Based on conditional independence with the topology order of a DAG, we have 
%{\small 
\begin{align} \label{eq:dag_chain_q}
&q(\mathbf{h}^{l:L}|\mathbf{h}^{1:l-1})=q(\mathbf{h}^{l}|\mathbf{h}^{1:l-1})q(\mathbf{h}^{l+1:L}|\mathbf{h}^{l})\\ \label{eq:dag_chain_p}
=&q(\mathbf{h}^{l}|\mathbf{h}^{1:l-1})q(\mathbf{h}^{l+1:L}|\mathbf{h}^{1:l}) p(\mathbf{h}^{l:L}) =p(\mathbf{h}^{l}|\mathbf{h}^{l+1:L})p(\mathbf{h}^{l+1:L})% = p(\mathbf{h}^{l}|\mathbf{h}^{pa(l)})p(\mathbf{h}^{l+1:L}) 
\end{align} %}

Following~(\ref{eq:kl_lL}) and with~(\ref{eq:dag_chain_q}-\ref{eq:dag_chain_p}), we have 
%{\small
 \begin{align} \notag
\textbf{\text{KL}}^{l:L} 
=&  \mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l}|\mathbf{h}^{1:l-1})   - \log p(\mathbf{h}^{l}|\mathbf{h}^{l+1:L}) \big] \\ \notag
&+ \underbrace{\mathbb{E}_{q(\mathbf{h}^{l:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{l+1:L}|\mathbf{h}^{1:l})- \log p(\mathbf{h}^{l+1:L})  \big]}_{\mathbf{KL}^{l+1:L}}.
\end{align} %}
Furthermore,
%{\small
\begin{align*} 
q(\mathbf{h}^{l}|\mathbf{h}^{1:l-1})=q(\mathbf{h}^{l}|\mathbf{h}^{ch(l)}), \quad  \quad   p(\mathbf{h}^{l}|\mathbf{h}^{l+1:L}) = p(\mathbf{h}^{l}|\mathbf{h}^{pa(l)}).
\end{align*} %}
Hence,
%{\small
 \begin{align} \label{eq:dag_kl_lL}
\textbf{\text{KL}}^{l:L} 
=&  \underbrace{\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[ \log q(\mathbf{h}^{l}|\mathbf{h}^{ch(l)})  - \log p(\mathbf{h}^{l}|\mathbf{h}^{pa(l)}) \big]}_{\textbf{\text{KL}}^{l}} +\textbf{\text{KL}}^{l+1:L} 
\end{align} %}
For nodes in layer $l$,
\begin{align} \notag
\textbf{\text{KL}}^{l} =& \sum_{i \in l} \underbrace{\mathbb{E}_{q(\mathbf{h}^{1:L}|\mathbf{x})}\big[  \log q(\mathbf{h}^{(i)}|\mathbf{h}^{ch(i)})  - \log p(\mathbf{h}^{(i)}|\mathbf{h}^{pa(i)}) \big]}_{\textbf{\text{KL}}^{(i)}} .
\end{align}
Recurrently applying~(\ref{eq:dag_kl_lL}) to (\ref{eq:dag_elbo}) yields
%{\small
\begin{align}\notag %\label{eq:elbo_dag_1}
\mathcal{L}(\mathbf{x}; \theta) =& \mathbb{E}_{q(\mathbf{h}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h})  \big] -  \sum_{i \in \mathcal{V}  \setminus  \mathcal{R}_{ \mathbb{G} }} \textbf{\text{KL}}^{(i)} \\ \notag
&-    \sum_{i \in  \mathcal{R}_{ \mathbb{G} }  }  \textbf{\text{KL}}\big(q(\mathbf{h}^{(i)} | \mathbf{h}^{ch(i)} )   || p(\mathbf{h}^{(i)})  \big) .
\end{align} %}
For node $i$, 
$$\textbf{\text{KL}}^{(i)} = \mathbb{E}_{q(\mathbf{h}|\mathbf{x})}\big[  \log q(\mathbf{h}^{(i)}|\mathbf{h}^{ch(i)})  - \log p(\mathbf{h}^{(i)}|\mathbf{h}^{pa(i)}) \big].$$




\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
