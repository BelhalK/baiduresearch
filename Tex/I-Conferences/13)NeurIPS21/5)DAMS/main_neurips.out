\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Decentralized Adaptive Training and Divergence of DADAM}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Related Work}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Decentralized Optimization }{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Divergence of DADAM}{section.2}% 5
\BOOKMARK [1][-]{section.3}{On the Convergence of Decentralized Adaptive Gradient Methods}{}% 6
\BOOKMARK [2][-]{subsection.3.1}{Importance and Difficulties of Consensus on Adaptive Learning Rates}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.2}{Unifying Decentralized Adaptive Gradient Framework}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.3}{Application to AMSGrad algorithm}{section.3}% 9
\BOOKMARK [2][-]{subsection.3.4}{Application to AdaGrad algorithm}{section.3}% 10
\BOOKMARK [1][-]{section.4}{Numerical Experiments}{}% 11
\BOOKMARK [2][-]{subsection.4.1}{Sensitivity to the Learning Rate}{section.4}% 12
\BOOKMARK [1][-]{section.5}{Conclusion}{}% 13
\BOOKMARK [1][-]{appendix.A}{Proof of Auxiliary Lemmas}{}% 14
\BOOKMARK [1][-]{appendix.B}{Proof of Theorem 2}{}% 15
\BOOKMARK [1][-]{appendix.C}{Proof of Theorem 3}{}% 16
\BOOKMARK [1][-]{appendix.D}{Proof of Theorem 4}{}% 17
\BOOKMARK [1][-]{appendix.E}{Convergence Analysis: Proof Sketch}{}% 18
\BOOKMARK [1][-]{appendix.F}{Additional Experiments and Details}{}% 19
