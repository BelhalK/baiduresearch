\begin{thebibliography}{10}

\bibitem{Proc:Agrawal_NIPS19}
Naman Agarwal, Ananda~Theertha Suresh, Felix~X. Yu, Sanjiv Kumar, and Brendan
  McMahan.
\newblock cpsgd: Communication-efficient and differentially-private distributed
  {SGD}.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, pages 7575--7586, 2018.

\bibitem{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
  Konstantinov, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock {\em arXiv preprint arXiv:1809.10505}, 2018.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning},
  3(1):1--122, 2011.

\bibitem{Proc:Chang18}
Ken Chang, Niranjan Balachandar, Carson~K. Lam, Darvin Yi, James~M. Brown,
  Andrew Beers, Bruce~R. Rosen, Daniel~L. Rubin, and Jayashree
  Kalpathy{-}Cramer.
\newblock Distributed deep learning networks among institutions for medical
  imaging.
\newblock {\em J. Am. Medical Informatics Assoc.}, 25(8):945--954, 2018.

\bibitem{chen2020quantized}
Congliang Chen, Li~Shen, Haozhi Huang, Qi~Wu, and Wei Liu.
\newblock Quantized adam with error feedback.
\newblock {\em arXiv preprint arXiv:2004.14180}, 2020.

\bibitem{chilimbi2014project}
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In {\em Symposium on Operating Systems Design and Implementation},
  pages 571--582, 2014.

\bibitem{Proc:Covington_2016}
Paul Covington, Jay Adams, and Emre Sargin.
\newblock Deep neural networks for youtube recommendations.
\newblock In {\em Proceedings of the 10th {ACM} Conference on Recommender
  Systems, Boston, MA, USA, September 15-19, 2016}, pages 191--198. {ACM},
  2016.

\bibitem{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In {\em Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pages 561--574, 2017.

\bibitem{Proc:Dean_NIPS12}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc~V. Le,
  Mark~Z. Mao, Marc'Aurelio Ranzato, Andrew~W. Senior, Paul~A. Tucker, Ke~Yang,
  and Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in Neural Information Processing Systems 25: 26th
  Annual Conference on Neural Information Processing Systems 2012. Proceedings
  of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States},
  pages 1232--1240, 2012.

\bibitem{duchi2011dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock {\em IEEE Transactions on Automatic control}, 57(3):592--606, 2011.

\bibitem{Duchi10-adagrad}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In {\em {COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010}, pages 257--269, 2010.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{Proc:GAN_NIPS14}
Ian~J. Goodfellow, Jean Pouget{-}Abadie, Mehdi Mirza, Bing Xu, David
  Warde{-}Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems 27: Annual
  Conference on Neural Information Processing Systems 2014, December 8-13 2014,
  Montreal, Quebec, Canada}, pages 2672--2680, 2014.

\bibitem{Arxiv:Goyal17}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock {\em CoRR}, abs/1706.02677, 2017.

\bibitem{Proc:Graves_ICASSP13}
Alex Graves, Abdel{-}rahman Mohamed, and Geoffrey~E. Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em {IEEE} International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP} 2013, Vancouver, BC, Canada, May 26-31, 2013},
  pages 6645--6649. {IEEE}, 2013.

\bibitem{Proc:Resnet_CVPR16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em 2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pages
  770--778. {IEEE} Computer Society, 2016.

\bibitem{hong2017prox}
Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1529--1538, 2017.

\bibitem{Proc:Ivkin_NIPS19}
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica,
  and Raman Arora.
\newblock Communication-efficient distributed {SGD} with sketching.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 13144--13154, 2019.

\bibitem{jiang2018linear}
Peng Jiang and Gagan Agrawal.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 2530--2541, 2018.

\bibitem{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock {\em arXiv preprint arXiv:1901.09847}, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In {\em International Conference on Machine Learning}, pages
  3478--3487, 2019.

\bibitem{lu2019gnsd}
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong.
\newblock Gnsd: A gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In {\em 2019 IEEE Data Science Workshop (DSW)}, pages 315--321, 2019.

\bibitem{mikami2018massively}
Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, Yuichi Kageyama, et~al.
\newblock Massively distributed sgd: Imagenet/resnet-50 training in a flash.
\newblock {\em arXiv preprint arXiv:1811.05233}, 2018.

\bibitem{Arxiv:MnihKSGAWR13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em CoRR}, abs/1312.5602, 2013.

\bibitem{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock {\em arXiv preprint arXiv:1901.09109}, 2019.

\bibitem{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48, 2009.

\bibitem{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Proc:Seide14}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em {INTERSPEECH} 2014, 15th Annual Conference of the
  International Speech Communication Association, Singapore, September 14-18,
  2014}, pages 1058--1062. {ISCA}, 2014.

\bibitem{shi2019convergence}
Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu.
\newblock A convergence analysis of distributed sgd with
  communication-efficient gradient sparsification.
\newblock In {\em IJCAI}, pages 3411--3417, 2019.

\bibitem{AlphaGo_17}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  Yutian Chen, Timothy~P. Lillicrap, Fan Hui, Laurent Sifre, George van~den
  Driessche, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nat.}, 550(7676):354--359, 2017.

\bibitem{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem{CV_review18}
Athanasios Voulodimos, Nikolaos Doulamis, Anastasios~D. Doulamis, and Eftychios
  Protopapadakis.
\newblock Deep learning for computer vision: {A} brief review.
\newblock {\em Comput. Intell. Neurosci.}, 2018:7068349:1--7068349:13, 2018.

\bibitem{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1299--1309, 2018.

\bibitem{Article:Wei_2017}
Jian Wei, Jianhua He, Kai Chen, Yi~Zhou, and Zuoyin Tang.
\newblock Collaborative filtering and deep learning based recommendation system
  for cold start items.
\newblock {\em Expert Systems with Applications}, 69:29--39, 2017.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock {\em arXiv preprint arXiv:1705.07878}, 2017.

\bibitem{Proc:Wu_ICML18}
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.
\newblock Error compensated quantized {SGD} and its applications to large-scale
  distributed optimization.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  5321--5329. {PMLR}, 2018.

\bibitem{yang2019swalp}
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew~Gordon
  Wilson, and Chris De~Sa.
\newblock Swalp: Stochastic weight averaging in low precision training.
\newblock In {\em International Conference on Machine Learning}, pages
  7015--7024. PMLR, 2019.

\bibitem{NLP_review18}
Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria.
\newblock Recent trends in deep learning based natural language processing
  [review article].
\newblock {\em {IEEE} Comput. Intell. Mag.}, 13(3):55--75, 2018.

\bibitem{sentiment_review18}
Lei Zhang, Shuai Wang, and Bing Liu.
\newblock Deep learning for sentiment analysis: {A} survey.
\newblock {\em Wiley Interdiscip. Rev. Data Min. Knowl. Discov.}, 8(4), 2018.

\end{thebibliography}
