@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}


@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}

@inproceedings{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1709--1720},
  year={2017}
}

@inproceedings{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1299--1309},
  year={2018}
}

@inproceedings{chen2020toward,
  title={Toward Communication Efficient Adaptive Gradient Method},
  author={Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
  booktitle={ACM-IMS Foundations of Data Science Conference (FODS)},
  address  = {Seattle, WA},
  year={2020}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:1901.09847},
  year={2019}
}

@inproceedings{shi2019convergence,
  title={A Convergence Analysis of Distributed SGD with Communication-Efficient Gradient Sparsification.},
  author={Shi, Shaohuai and Zhao, Kaiyong and Wang, Qiang and Tang, Zhenheng and Chu, Xiaowen},
  booktitle={IJCAI},
  pages={3411--3417},
  year={2019}
}

@inproceedings{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4447--4458},
  year={2018}
}

@article{chen2020quantized,
  title={Quantized Adam with Error Feedback},
  author={Chen, Congliang and Shen, Li and Huang, Haozhi and Wu, Qi and Liu, Wei},
  journal={arXiv preprint arXiv:2004.14180},
  year={2020}
}

@article{wen2017terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:1705.07878},
  year={2017}
}

@inproceedings{jiang2018linear,
  title={A linear speedup analysis of distributed deep learning with sparse and quantized communication},
  author={Jiang, Peng and Agrawal, Gagan},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={2530--2541},
  year={2018}
}

@article{wangni2017gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  journal={arXiv preprint arXiv:1710.09854},
  year={2017}
}

@inproceedings{haddadpour2019trading,
  title={Trading redundancy for communication: Speeding up distributed sgd for non-convex optimization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  booktitle={International Conference on Machine Learning},
  pages={2545--2554},
  year={2019},
  organization={PMLR}
}

@article{aji2017sparse,
  title={Sparse communication for distributed gradient descent},
  author={Aji, Alham Fikri and Heafield, Kenneth},
  journal={arXiv preprint arXiv:1704.05021},
  year={2017}
}

@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}


@inproceedings{de2017understanding,
  title={Understanding and optimizing asynchronous low-precision stochastic gradient descent},
  author={De Sa, Christopher and Feldman, Matthew and R{\'e}, Christopher and Olukotun, Kunle},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={561--574},
  year={2017}
}

@inproceedings{yang2019swalp,
  title={SWALP: Stochastic weight averaging in low precision training},
  author={Yang, Guandao and Zhang, Tianyi and Kirichenko, Polina and Bai, Junwen and Wilson, Andrew Gordon and De Sa, Chris},
  booktitle={International Conference on Machine Learning},
  pages={7015--7024},
  year={2019},
  organization={PMLR}
}

@incollection{bottou2008,
	Author = {Bottou, L\'{e}on and Bousquet, Olivier},
	Booktitle = {Advances in Neural Information Processing Systems 20},
	Editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
	Pages = {161--168},
	Publisher = {Curran Associates, Inc.},
	Title = {The Tradeoffs of Large Scale Learning},
	Year = {2008}}
	
	
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{karimi2019non,
  title={Non-asymptotic analysis of biased stochastic approximation scheme},
  author={Karimi, Belhal and Miasojedow, Blazej and Moulines, Eric and Wai, Hoi-To},
  booktitle={Conference on Learning Theory},
  pages={1944--1974},
  year={2019},
  organization={PMLR}
}


@article{ghadimi2013stochastic,
	Author = {Ghadimi, Saeed and Lan, Guanghui},
	Date-Added = {2018-10-27 15:50:56 -0400},
	Date-Modified = {2018-10-27 15:50:56 -0400},
	Journal = {SIAM Journal on Optimization},
	Number = {4},
	Pages = {2341--2368},
	Publisher = {SIAM},
	Title = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
	Volume = {23},
	Year = {2013}}
	
@article{alistarh2018convergence,
  title={The convergence of sparsified gradient methods},
  author={Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Khirirat, Sarit and Konstantinov, Nikola and Renggli, C{\'e}dric},
  journal={arXiv preprint arXiv:1809.10505},
  year={2018}
}

@article{jegou2010product,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}

@article{chen2010approximate,
  title={Approximate nearest neighbor search by residual vector quantization},
  author={Chen, Yongjian and Guan, Tao and Wang, Cheng},
  journal={Sensors},
  volume={10},
  number={12},
  pages={11259--11273},
  year={2010},
  publisher={Molecular Diversity Preservation International}
}

@inproceedings{reddi2019convergence,
	title={On the convergence of adam and beyond},
	author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@article{boyd2011distributed,
	title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
	author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan and others},
	journal={Foundations and Trends{\textregistered} in Machine learning},
	volume={3},
	number={1},
	pages={1--122},
	year={2011},
	publisher={Now Publishers, Inc.}
}

@article{duchi2011dual,
	title={Dual averaging for distributed optimization: Convergence analysis and network scaling},
	author={Duchi, John C and Agarwal, Alekh and Wainwright, Martin J},
	journal={IEEE Transactions on Automatic control},
	volume={57},
	number={3},
	pages={592--606},
	year={2011},
	publisher={IEEE}
}

@article{nedic2009distributed,
	title={Distributed subgradient methods for multi-agent optimization},
	author={Nedic, Angelia and Ozdaglar, Asuman},
	journal={IEEE Transactions on Automatic Control},
	volume={54},
	number={1},
	pages={48},
	year={2009}
}

@inproceedings{hong2017prox,
	title={Prox-PDA: The proximal primal-dual algorithm for fast distributed nonconvex optimization and learning over networks},
	author={Hong, Mingyi and Hajinezhad, Davood and Zhao, Ming-Min},
	booktitle={International Conference on Machine Learning},
	pages={1529--1538},
	year={2017},
}



@inproceedings{lu2019gnsd,
  title={GNSD: A gradient-tracking based nonconvex stochastic algorithm for decentralized optimization},
  author={Lu, Songtao and Zhang, Xinwei and Sun, Haoran and Hong, Mingyi},
  booktitle={2019 IEEE Data Science Workshop (DSW)},
  pages={315--321},
  year={2019}
}

@inproceedings{koloskova2019decentralized,
  title={Decentralized stochastic optimization and gossip algorithms with compressed communication},
  author={Koloskova, Anastasia and Stich, Sebastian U and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3478--3487},
  year={2019}
}



@inproceedings{chilimbi2014project,
	title={Project adam: Building an efficient and scalable deep learning training system},
	author={Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
	booktitle={Symposium on Operating Systems Design and Implementation},
	pages={571--582},
	year={2014}
}

@article{nazari2019dadam,
	title={DADAM: A consensus-based distributed adaptive gradient method for online optimization},
	author={Nazari, Parvin and Tarzanagh, Davoud Ataee and Michailidis, George},
	journal={arXiv preprint arXiv:1901.09109},
	year={2019}
}