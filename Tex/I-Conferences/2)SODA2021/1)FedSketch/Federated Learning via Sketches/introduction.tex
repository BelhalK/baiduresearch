\section{Introduction}
Increasing applications in machine learning include the learning of a complex model across a large amount of devices in a distributed manner.
In the particular case of federated learning, the training data is stored across these multiple devices and can not be centralized.
Two natural problems arise from this setting. 
First, communications bottlenecks appear when a central server and the multiple devices must exchange gradient-informed quantities.
Then, privacy-related issues due to the protection of the sensitive individual data must be taken into account.

The former has extensively been tackled via quantization \cite{alistarh2017qsgd}, sparsification \cite{wangni2018gradient} and compression \cite{bernstein2018signsgd} methods yielding to a drastic reduction of the number of bits required to communicate those gradient-related information.
Solving the privacy issue has been widely executed injecting an additional layer of random noise in order to respect differential-privacy property of the method.


With the focus of communication-efficiency, \cite{ivkin2019communication} proposes a distributed SGD algorithm using sketching and they provide the convergence analysis in homogeneous data distribution setting. 

Also with focus on privacy,  in~\cite{li2019privacy}, the authors derive a single framework in order to tackle these issues jointly and introduce \texttt{DiffSketch} based on the Count Sketch operator. Compression and privacy is performed using random hash functions such that no third parties are able to access the original data. Yet, \cite{li2019privacy} does not provide the convergence analysis for the \texttt{DiffSketch} in Federated setting. In this work, we provide a thorough convergence analysis for the Federated Learning using sketching.

The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item Based on the current compression methods, we provide a new algorithm -- \texttt{HEAPRIX} -- that displays an unbiased estimator of the full gradient we ought to communicate to the central parameter server. We theoretically show that \texttt{HEAPRIX} jointly reduces the cost of communication between devices and server, preserves privacy and is unbiased.
    
    \item We develop a general algorithm for communication-efficient and privacy preserving federated learning based on this novel compression algorithm. 
Those methods, namely \texttt{FedSKETCH} and \texttt{FedSKETCHGATE}, are derived under \textit{homogeneous} and \textit{heterogeneous} data distribution settings.
   
    \item Non asymptotic analysis of our method is established for convex, \pl\: (generalization of strongly-convex) and nonconvex functions in Theorem \ref{thm:homog_case} and Theorem \ref{thm:hetreg_case} for respectively the i.i.d. and non i.i.d. case,  and highlight an improvement in the number of iteration required to achieve a stationarity point.
\end{itemize}
\section{Related Work}
In this section, we provide a summary of the prior related research efforts as follows:

\paragraph{ Local SGD with Periodic Averaging:}
Compared to baseline SGD where model averaging happens in every iteration, the main idea behind \emph{Local SGD with periodic averaging} comes from the intuition of variance reduction by periodic model averaging \cite{zhang2016parallel} with purpose of saving communication rounds. While Local SGD has been proposed in \cite{mcmahan2016communication,konevcny2016federated} under the title of Federated Learning Setting, the convergence analysis of Local SGD is studied in~\cite{zhou2017convergence,yu2018parallel,stich2018local,wang2018cooperative}. The convergence analysis of Local SGD is improved in the follow up works~\cite{haddadpour2019trading,basu2019qsparse,haddadpour2019convergence,bayoumi2020tighter,stich2019error} in majority for homogeneous data distribution setting. The convergence analysis is further extended to heterogeneous setting, wherein studied under the title of \emph{Federated Learning}, with improved rates in~\cite{yu2019linear,li2019convergence,sahu2018convergence,liang2019variance,haddadpour2019convergence,karimireddy2019scaffold}. 

Additionally, a few recent Federated Learning/Local SGD with adaptive gradient methods can be found in \cite{reddi2020adaptive,}.

\todo{Revise this section!}


\paragraph{Gradient Compression Based Algorithms for Distributed Setting:} \cite{ivkin2019communication} develop a solution for leveraging sketches of full gradients in a distributed setting while training a global model using SGD \cite{robbins1951stochastic, bottou2008tradeoffs}. They introduce \texttt{Sketched-SGD} and establish a communication complexity of order $\mathcal{O}(\log(d))$ (per round) where $d$ is the dimension of the parameters, i.e. the dimension of the gradient.
Other recent solutions to reduce the communication cost include quantized gradient as developed in \cite{alistarh2017qsgd,lin2017deep,stich2018sparsified,horvath2019stochastic}. 
Yet, their dependence on the number of devices $p$ makes them harder to be used in some settings. Additionally, there are other research efforts such as \cite{haddadpour2020federated,reisizadeh2019fedpaq,basu2019qsparse,horvath2019stochastic} that exploit compression in Federated Learning or distributed communication-efficient optimization. 
Finally, the recent work in \cite{horvath2020better} exploits variance reduction technique with compression jointly in distributed optimization.



\paragraph{Privacy-preserving Setting:} Differentially private methods for federated learning have been extensively developed and studied in \cite{li2019privacy,liu2019enhancing} recently. 

The remaining of the paper is organized as follows.
Section \ref{sec:problem} gives a formal presentation of the general problem. 
Section \ref{sec:compression} describes the various compression algorithms used for communication efficiency and privacy preservation, and introduces our new compression method.
The training algorithms are provided in Section \ref{sec:algos} and their respective analysis in the strongly-convex or nonconvex cases are provided Section \ref{sec:cnvg-an}.

\textbf{Notation:} For the rest of the paper we indicate the number of communication rounds and number of bits per round per device with $R$ and $B$ respectively. For the rest of the paper we indicate the count sketch of any vector $\boldsymbol{x}$ with $\mathbf{S}(\boldsymbol{x})$.








 

%\section{Related Work}



%\section{Gradient Descent based Algorithm and Main Results}

%\subsection{Distributed SGD (Baseline)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%