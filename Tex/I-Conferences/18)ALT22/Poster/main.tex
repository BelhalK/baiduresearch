%\title{LaTeX Portrait Poster Template}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% a0poster Portrait Poster
% LaTeX Template
% Version 1.0 (22/06/13)
%
% The a0poster class was created by:
% Gerlinde Kettl and Matthias Weiser (tex@kettl.de)
% 
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a0,landscape]{a0poster}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multicol} % This is so we can have multiple columns of text side-by-side
\usepackage{subfigure}
\columnsep=100pt % This is the amount of white space between the columns in the poster
\columnseprule=3pt % This is the thickness of the black line between the columns in the poster

\usepackage[svgnames]{xcolor} % Specify colors by their 'svgnames', for a full list of all colors available see here: http://www.latextemplates.com/svgnames-colors

%\usepackage{empheq}
\usepackage[most]{tcolorbox}

\tcbset{colback=yellow!10!white, colframe=red!50!black, 
        highlight math style= {enhanced, %<-- needed for the ’remember’ options
            colframe=red,colback=red!10!white,boxsep=0pt}
        }

\definecolor{opticlimb}{rgb}{0,0.65,0.65}
\definecolor{blue}{RGB}{0,0,255}
\usepackage{tcolorbox}

% \usepackage[bitstream-charter]{mathdesign}
\usepackage[authoryear,round]{natbib}
\usepackage{bbm}
\usepackage{enumitem} 
\usepackage{xargs} 
\usepackage{amssymb,amsthm,bm}
%\usepackage{mathtools}
%\usepackage{xargs}
%\usepackage{stmaryrd} 

\bibliographystyle{plainnat}
\usepackage{graphicx} % Required for including images
\graphicspath{{figures/}} % Location of the graphics files
\usepackage{booktabs} % Top and bottom rules for table
%\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures
%\usepackage{amsfonts, amsmath, amsthm, amssymb,bm} % For math fonts, symbols and environments
\usepackage{wrapfig} % Allows wrapping text around tables and figures

\usepackage{cmbright}
\usepackage{avant}
%\usepackage{tikz}
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
%\usepackage[font=small,labelfont=bf,tableposition=top]{caption}

\renewcommand{\familydefault}{\sfdefault}
\theoremstyle{definition}
\newtheorem{defn}{Definition} % definition numbers are dependent on theorem numbers
\newtheorem*{exmp}{Example} % same for example numbers


\usepackage{mdframed}
%\theoremstyle{definition}
%\newtheorem{defn}{Definition} % definition numbers are dependent on theorem numbers
%\newtheorem*{exmp}{Example} % same for example numbers
%\usepackage{lipsum}%
%  {
%      \theoremstyle{plain}
%      \newtheorem{assumption}{M}
%      \newtheorem{lemma}{Lemma}
%      \newtheorem{remark}{Remark}
%      \newtheorem{prop}{Proposition}
%      \newtheorem{assumption_saem}{ISAEM}
%      \newtheorem{assumption_rm}{SA}
%      \newtheorem{assumption_iem}{IEM}
%      \newtheorem{assumption_imcem}{IMCEM}
%      \newtheorem{assumption_expo}{E}
%  }
\newmdtheoremenv{theo}{Theorem}
\newmdtheoremenv{coro}{Corollary}
\newmdtheoremenv{lem}{Lemma}

\usepackage{shortcuts_OPT}

\begin{document}

{~\hspace{6cm}\begin{tikzpicture}[remember picture, overlay]
     \node [anchor=north east, inner sep=3cm, yshift=-5.5cm]  at (current page.north east)
     {\includegraphics[height=6cm]{images/logo_baidu.jpeg}
       \includegraphics[height=5cm]{images/logo_cuhk.png}~~
     \includegraphics[height=5cm]{images/logo_x.jpg}};
  \end{tikzpicture}}
  
  \begin{tikzpicture}[remember picture, overlay]
     \node [inner sep=3cm, yshift=2cm]  at (current page.south)
     {\large The 33rd International Conference on Algorithmic Learning Theory};
  \end{tikzpicture}
%----------------------------------------------------------------------------------------
% POSTER HEADER 
%----------------------------------------------------------------------------------------

% The header is divided into two boxes:
% The first is 75% wide and houses the title, subtitle, names, university/organization and contact information
% The second is 25% wide and houses a logo for your university/organization or a photo of you
% The widths of these boxes can be easily edited to accommodate your content as you see fit

\begin{minipage}[b]{0.9\linewidth}
\veryHuge \color{Navy} \textbf{Minimization by Incremental Stochastic Surrogate Optimization for Large Scale Nonconvex Problems
}\\[1cm] \color{Black} %\color{Black}\textbf{COLT 2019} \color{Black}\\[1cm] % Title
%\Huge\textit{Identification de système dynamique}\\[2cm] % Subtitle
\huge \textbf{Belhal Karimi$^{1}$, Hoi-To Wai$^{2}$, Eric Moulines$^{3}$ and Ping Li$^{1}$}\\[0.5cm] % Author(s)
\huge Baidu Research$^1$, Chinese University~of Hong Kong$^2$, Ecole Polytechnique$^3$ \\[0.4cm] % University/organization
\large \texttt{belhalkarimi@baidu.com, htwai@se.cuhk.edu.hk, eric.moulines@polytechnique.edu, liping11@baidu.com@gmail.com}
\end{minipage}
%


\vspace{1cm} % A bit of extra whitespace between the header and poster content

%----------------------------------------------------------------------------------------

\begin{multicols}{3} % This is how many columns your poster will be broken into, a portrait poster is generally split into 2 columns

%----------------------------------------------------------------------------------------
% ABSTRACT
%----------------------------------------------------------------------------------------




%----------------------------------------------------------------------------------------
% INTRODUCTION
%----------------------------------------------------------------------------------------

%\color{SaddleBrown} % SaddleBrown color for the introduction
%\color{Navy} % Navy color
%\color{opticlimb}
%\color{Navy} % Navy color for the abstract
\color{DarkSlateGray}


\begin{tcolorbox}[colback=white!5!white,colframe=green!75!black,fonttitle=\sffamily\bfseries\large,title=Large Scale Optimization]
\begin{itemize}
\item \textbf{Objective:} \emph{Constrained} minimization problem of a finite sum of  functions:
\beq \label{eq:opt}
\min_{ \param \in \Param }~ {\cal L} ( \param ) \eqdef \frac{1}{n} \sum_{i=1}^n {\cal L}_i( \param) \eqsp,
\eeq
where ${\cal L} _i: \rset^p \to \rset$ is bounded from below and is (possibly) nonconvex and include a nonsmooth penalty.
\item The gap $\widehat{e}(\param ; \{ \op_i \}_{i=1}^n )$ plays a key role in the convergence analysis and we require this error to be L-smooth for some constant $L > 0$ 
Denote by $\pscal{\cdot}{\cdot}$ the scalar product, we also introduce the following stationary point condition:
\begin{defn} (Asymptotic Stationary Point Condition)\\
A sequence $(\theta^k)_{k\geq0}$ satisfies the asymptotic stationary point condition if
\begin{equation}\label{aspc}
f'( \param, {\bm d} ) \eqdef \lim_{ t \rightarrow 0^+ } \frac{ f ( \param + t {\bm d} ) - f( \param ) }{ t }  \geq 0.
\end{equation}
\end{defn}
\end{itemize}
\vspace{.1cm}
\end{tcolorbox}

\begin{tcolorbox}[colback=white!5!white,colframe=blue!75!black,fonttitle=\sffamily\bfseries\large,title=Majorization-Minimization Scheme]
\begin{itemize}


\begin{figure}[H]
\centering
        \includegraphics[width=0.5\textwidth]{fig/mmscheme}
\end{figure} 

\begin{figure}[H]
\centering
        \includegraphics[width=0.9\textwidth]{fig/misoalgofull}
\end{figure} 


\item \textbf{MISO Method}: fix any $n \geq 1$, we stop the SA at a random iteration $N$ with
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=white!5!white,colframe=blue!75!black,fonttitle=\sffamily\bfseries\large,title=An Inctractability for Latent Data Models]
\begin{itemize}[label=\textbullet, font=\LARGE \color{blue}]
\item Case when the surrogate functions computed in Algorithm \ref{alg:miso} \textbf{are not tractable}.
\item Assume that the surrogate can be expressed as an integral over a set of latent variables $z = (z_i \in \Zset, i \in [n]) \in \Zset[]$.
\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]
\begin{equation}\label{eq:integralsurrogate}
\sur{i}{\param}{\op} \eqdef \int_{\Zset}{\rsur{i}{\param}{\op}{z_i}  p_i(z_i ; \op)\mu_i(dz_i)}\quad \forall~(\param,\op) \in \Param \times \Param \eqsp.
\end{equation}
\end{tcolorbox}  

\item Our scheme is based on the computation, at each iteration, of stochastic auxiliary functions for a mini-batch of components. For $i \in [n]$, the auxiliary function, noted $\ssur{i}{\param}{\op}{ \{ z_m \}_{m=1}^{M}}$ is a Monte Carlo approximation of the surrogate function $\sur{i}{\param}{\op}$ defined by \eqref{eq:integralsurrogate} such that:
\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black]
\beq \label{eq:ssur}  
\ssur{i}{\param}{\op}{ \{ z_m \}_{m=1}^{M}} \eqdef \frac{1}{M} \sum_{m=1}^{M} \rsur{i}{\param}{\op}{z_m}\eqsp,
\eeq
\end{tcolorbox}
where $\{z_i^{m}\}_{m=0}^{M-1}$ is a Monte Carlo batch.
\end{itemize}

\vspace{.1cm}
\end{tcolorbox}


\begin{tcolorbox}[colback=white!5!white,colframe=blue!75!black,fonttitle=\sffamily\bfseries\large,title=MISSO Method]
\begin{itemize}
\item dd
\end{itemize}
\begin{figure}[H]
\centering
        \includegraphics[width=1.\textwidth]{fig/missoalgo}
\end{figure}
\end{tcolorbox}


\begin{tcolorbox}[colback=white!5!white,colframe=red!75!black,fonttitle=\sffamily\bfseries\large,title=Global Convergence Analysis]
\textbf{Assumptions}: we need a few regularity conditions in this case,
\begin{enumerate}
\item There exists a Borel measurable function $\hat{H}: \Prm \times \Xset \to \Prm$,\vspace{-.1cm}
\beq \notag
\hHX{\prm}{\state} - \PX{}{} \hHX{\prm}{\state} = \HX{\prm}{\state} - h( \prm ),~\forall~\prm \in \Prm, \state \in \Xset. 
\eeq
$\Longrightarrow$ existence of solution to the \emph{\color{red}Poisson equation}. 
\item For all $\prm \in \Prm$ and $\state \in \Xset$, $\| \hHX{\prm}{\state} \|  \leq L^{(0)}_{PH}, \| \PX{} \hHX{\prm}{\state} \| \leq L_{PH}^{(0)}$, and
\begin{equation}\notag
\textstyle \sup_{\state \in \Xset} \| \PX{} \hHX{\prm}{\state} - {P}_{\prm'} \hHX{\prm'}{\state}  \|  \leq  L_{PH}^{(1)} \| \prm - \prm' \|,~\forall~(\prm, \prm') \in \Prm^2.
\end{equation}
$\Longrightarrow$ \emph{\color{red}smoothness} of $\hHX{\prm}{\state}$, satisfied if $\PX{}$, $\HX{\prm}{\State}$ are smooth \wrt $\prm$.
\item It holds that $\sup_{\prm \in \Prm, \state \in \Xset}  \| \HX{\prm}{\state} - h(\prm) \| \leq \sigma$.\vspace{.3cm}

$\Longrightarrow$ requires the noise is \emph{\color{red}uniformly bounded} for all $x \in \Xset$.
\end{enumerate}
\textbf{Example}: assumptions 1 \& 2 are satisfied if the Markov kernel $\PX{\prm}$ is geometrically ergodic + smooth, and the drift term is smooth \wrt $\prm$. 
\begin{theo}\vspace{-.5cm}Suppose that
the step sizes are decreasing and $\gamma_1 \leq 0.5 \big( c_1(L+C_h) \big)^{-1}$ (+other conditions). 
%\beq\notag
%\gamma_{n+1} \leq \gamma_n, ~\gamma_n \leq a \gamma_{n+1},~ \gamma_n - \gamma_{n+1} \leq a' \gamma_n^2,~\gamma_1 \leq 0.5 \big( c_1(L+C_h) \big)^{-1} \eqsp, 
%\eeq
%for $a, a' > 0$ and all $n \geq 0$. 
Let $V_{0,n} \eqdef \EE[ V(\prm_0) - V(\prm_{n+1})]$,
{\large\[
 \EE [\| h( \prm_N ) \|^2 ] \leq
 \frac{ 2c_1 \big( V_{0,n} + C_{0,n} + \big( \sigma^2 L + C_\gamma \big) \sum_{k=0}^{n}
 \gamma_{k+1}^2 \big) }{ \sum_{k=0}^n \gamma_{k+1} } + 2 c_0 \eqsp.\vspace{-.1cm}
\]}
\end{theo}\vspace{-.2cm}
\begin{itemize}
\item Set $\gamma_k = (2 c_1 L (1+C_h) \sqrt{k})^{-1}$ $\Longrightarrow$ 
{\color{red}$\EE [ \|h( \prm_N ) \|^2 ] = {\cal O}( c_0 + \log n / \sqrt{n} )$} (same as Case 1).
\item \textbf{Proof idea:} challenge is that $\be_{n+1}$ is not zero-mean $\Longrightarrow$ bound the sum of $\EE[ \pscal{ \grd V(\prm_n) }{ \be_{n+1} } ]$ w/ Poisson equation + a novel decomposition [cf.~\emph{\color{blue}Lemma 2}].
%{\large\[
%\hHX{\prm}{\state} - \PX{}{} \hHX{\prm}{\state} = \HX{\prm}{\state} - h( \prm ),~\forall~\prm \in \Prm, \state \in \Xset,
%\]}
%where $\hat{H}: \Prm \times \Xset \to \Prm$ is a Borel measurable function.
\end{itemize}
\vspace{.1cm}
\end{tcolorbox}


\begin{tcolorbox}[colback=white!5!white,colframe=red!75!black,fonttitle=\sffamily\bfseries\large,title=Numerical Experiments]
\begin{itemize}
\item Logistic Regression on Traumabase dataset (severe hemorrhage):
\begin{figure}[H]
\centering
    \mbox{
        \includegraphics[width=0.5\textwidth]{fig/logisticdelta.eps}
        \includegraphics[width=0.5\textwidth]{fig/logisticbeta.eps}
    }
\end{figure} 

\item Bayesian variants of LeNet-5 and ResNet-18 on MNIST and CIFAR10:
\begin{figure}[H]
\centering
    \mbox{
        \includegraphics[width=0.5\textwidth]{fig/mnist.eps}
        \includegraphics[width=0.5\textwidth]{fig/cifar.eps}
    }
\label{fig:gmmplots}
\end{figure} 
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=white!5!white,colframe=blue!75!black,fonttitle=\sffamily\bfseries\large,title=Conclusion]
\begin{itemize}
\item {\color{blue} Theorem 1 \& 2} show the non-asymptotic convergence rate of biased SA scheme with smooth (possibly non-convex) Lyapunov function. 
\item With appropriate step size, in \emph{n} iterations the SA scheme finds 
$\EE[ \| h( \prm_N ) \|^2 ] = {\cal O}( c_0 + \log n / \sqrt{n} )$, where $c_0$ is the bias and $h(\cdot)$ is the mean field.
\item Applications to online EM and online policy gradient.
\end{itemize}
\end{tcolorbox}
\vspace{-1.3cm}
\small
%\nocite{*} % Print all references regardless of whether they were cited in the poster or not
\bibliography{references.bib}

%----------------------------------------------------------------------------------------
% ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

%\section*{Acknowledgements}

%\includegraphics[width=15cm]{opticlimb.png}\\


%----------------------------------------------------------------------------------------

\end{multicols}
\begin{center}
%\includegraphics[width=10cm]{logo_x.jpg}
%\includegraphics[width=10cm]{logo_inria.png}
%\includegraphics[width=15cm]{opticlimb.png}\\
\end{center}
\end{document}