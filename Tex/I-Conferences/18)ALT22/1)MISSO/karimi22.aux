\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{mcLachlan2008em}
\citation{jordan1999var}
\citation{razaviyayn2013unified}
\citation{lange2016mm}
\citation{mairal2015miso}
\citation{schmidt2017minimizing}
\citation{qian2019miso}
\providecommand \oddpage@label [2]{}
\jmlr@workshop{33rd International Conference on Algorithmic Learning Theory}
\jmlr@title{ Minimization by Incremental Stochastic Surrogate}{Minimization by Incremental Stochastic Surrogate Optimization for Large Scale Nonconvex Problems}
\jmlr@author{\Name {Belhal Karimi} \Email {belhalkarimi@baidu.com}\\ \addr Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, WA 98004, USA \AND \Name {Hoi-To Wai} \Email {htwai@se.cuhk.edu.hk}\\ \addr Department of SEEM, The Chinese University of Hong Kong, HK \AND \Name {Eric Moulines} \Email {eric.moulines@polytechnique.fr}\\ \addr Center of Applied Mathematics, Ecole Polytechnique, Palaiseau, FR \AND \Name {Ping Li} \Email {liping11@baidu.com}\\ \addr Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, WA 98004, USA }{\Name {Belhal Karimi} \Email {belhalkarimi@baidu.com}\\ \addr Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, WA 98004, USA \AND \Name {Hoi-To Wai} \Email {htwai@se.cuhk.edu.hk}\\ \addr Department of SEEM, The Chinese University of Hong Kong, HK \AND \Name {Eric Moulines} \Email {eric.moulines@polytechnique.fr}\\ \addr Center of Applied Mathematics, Ecole Polytechnique, Palaiseau, FR \AND \Name {Ping Li} \Email {liping11@baidu.com}\\ \addr Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, WA 98004, USA }
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\jmlr@date{March 7, 2022}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\newlabel{eq:opt}{{1}{1}{Introduction}{equation.0.1.1}{}}
\citation{mairal2015miso}
\citation{mensch2017stochastic}
\citation{mairal2015miso}
\citation{mcLachlan2008em,karimi2019convergence,karimi2019global}
\citation{ghahramani2015probabilistic}
\citation{neal2012bayesian,blundell2015weight,polson2017deep,rezende2014stochastic,li2017dropout,khan2018fast,osawa2019practical,karimi2021hwa}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{mairal2015miso}
\citation{razaviyayn2013unified}
\citation{mairal2015miso}
\citation{mairal2015miso}
\@writefile{toc}{\contentsline {section}{\numberline {2}Incremental Minimization of Finite Sum Nonconvex Functions}{3}{section.0.2}}
\newlabel{sec:framework}{{2}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{section.0.2}{}}
\newlabel{ass:sur}{{1}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{assumption.1}{}}
\newlabel{eq:lowerbd}{{3}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.3}{}}
\newlabel{ass:diff}{{2}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{assumption.2}{}}
\newlabel{eq:eq30}{{4}{3}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.4}{}}
\citation{mairal2015miso}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{line:upd}{{5}{4}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.5}{}}
\newlabel{miso:iter}{{6}{4}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The MISO method\nobreakspace  {}\citep  {mairal2015miso}.\relax }}{4}{figure.caption.1}}
\newlabel{alg:miso}{{1}{4}{The MISO method~\citep {mairal2015miso}.\relax }{figure.caption.1}{}}
\newlabel{eq:integralsurrogate}{{5}{4}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.5}{}}
\newlabel{eq:ssur}{{6}{4}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.6}{}}
\citation{bishop2006pattern}
\newlabel{line:unif}{{5}{5}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.12}{}}
\newlabel{line:mcsample}{{6}{5}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.13}{}}
\newlabel{line:ssur}{{7}{5}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.14}{}}
\newlabel{line:iter}{{8}{5}{Incremental Minimization of Finite Sum Nonconvex Functions}{ALC@unique.15}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The MISSO method.\relax }}{5}{algorithm.2}}
\newlabel{alg:misso}{{2}{5}{The MISSO method.\relax }{algorithm.2}{}}
\newlabel{pairmcem}{{7}{5}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.7}{}}
\citation{blei2017vi}
\citation{paisley2013,kingma,blundell2015weight}
\citation{blundell2015weight}
\citation{domke2020provable}
\citation{domke2020provable}
\newlabel{eq:vi}{{8}{6}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.8}{}}
\newlabel{eq:VI}{{9}{6}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.9}{}}
\newlabel{eq:variationalobjective}{{10}{6}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.10}{}}
\newlabel{eq:quad_sur}{{11}{6}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.11}{}}
\newlabel{eq:vi_grad}{{12}{6}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.12}{}}
\newlabel{pairvi}{{13}{6}{Incremental Minimization of Finite Sum Nonconvex Functions}{equation.0.2.13}{}}
\citation{van2000asymptotic,vershynin2018high,wainwright2019high}
\citation{vershynin2018high}
\citation{vershynin2018high}
\citation{wainwright2019high}
\citation{van2000asymptotic}
\@writefile{toc}{\contentsline {section}{\numberline {3}Convergence Analysis}{7}{section.0.3}}
\newlabel{sec:analysis}{{3}{7}{Convergence Analysis}{section.0.3}{}}
\newlabel{ass:lips}{{3}{7}{Convergence Analysis}{assumption.3}{}}
\newlabel{controlapprox}{{4}{7}{Convergence Analysis}{assumption.4}{}}
\citation{mairal2015miso}
\newlabel{eq:stationary_meas}{{14}{8}{Convergence Analysis}{equation.0.3.14}{}}
\newlabel{eq:sumsurrodet}{{15}{8}{Convergence Analysis}{equation.0.3.15}{}}
\newlabel{thm:main}{{1}{8}{Convergence Analysis}{theo.1}{}}
\newlabel{eq:misso_rate}{{16}{8}{Convergence Analysis}{equation.0.3.16}{}}
\citation{mairal2015miso}
\citation{robbins1971convergence}
\newlabel{thm:mainasymp}{{2}{9}{Convergence Analysis}{theo.2}{}}
\newlabel{lemmars}{{1}{9}{Convergence Analysis}{lem.1}{}}
\newlabel{chik}{{17}{10}{Convergence Analysis}{equation.0.3.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{10}{section.0.4}}
\newlabel{sec:numerical}{{4}{10}{Numerical Experiments}{section.0.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Binary logistic regression with missing values}{10}{subsection.0.4.1}}
\newlabel{logisticreg}{{4.1}{10}{Binary logistic regression with missing values}{subsection.0.4.1}{}}
\newlabel{eq:logistic}{{18}{10}{Binary logistic regression with missing values}{equation.0.4.18}{}}
\citation{jiang2018logistic}
\citation{delyon1999}
\citation{wei1990mcem}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Convergence of parameters $\delta _1$ and $\beta _1$, the first component of vectors ${\bm  {\delta }}$ and ${\bm  {\beta }}$, for the SAEM, the MCEM and the MISSO methods, plotted against the wallclock time (in seconds).\relax }}{11}{figure.caption.2}}
\newlabel{fig:misso_trauma}{{1}{11}{Convergence of parameters $\delta _1$ and $\beta _1$, the first component of vectors ${\bm \delta }$ and ${\bm \beta }$, for the SAEM, the MCEM and the MISSO methods, plotted against the wallclock time (in seconds).\relax }{figure.caption.2}{}}
\citation{lecun1998gradient}
\citation{lecun1998gradient}
\citation{lecun1998mnist}
\citation{he2016deep}
\citation{krizhevsky2012imagenet}
\citation{he2016deep}
\citation{wen2018flipout}
\citation{kingma:adam}
\citation{sutskever2013importance}
\citation{schmidt2017minimizing}
\citation{blundell2015weight}
\citation{zhou2020towards}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training Bayesian CNN using MISSO}{12}{subsection.0.4.2}}
\bibdata{ref}
\bibcite{bishop2006pattern}{{1}{2006}{{Bishop}}{{}}}
\bibcite{blei2017vi}{{2}{2017}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{blundell2015weight}{{3}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{delyon1999}{{4}{1999}{{Delyon et~al.}}{{Delyon, Lavielle, and Moulines}}}
\bibcite{domke2020provable}{{5}{2020}{{Domke}}{{}}}
\bibcite{ghahramani2015probabilistic}{{6}{2015}{{Ghahramani}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Negated ELBO versus time elapsed for fitting (Left) Bayesian LeNet-5 on MNIST and (Right) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods. The convergence is plotted against the wallclock time.\relax }}{13}{figure.caption.3}}
\newlabel{fig:lenetopt}{{2}{13}{Negated ELBO versus time elapsed for fitting (Left) Bayesian LeNet-5 on MNIST and (Right) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods. The convergence is plotted against the wallclock time.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{13}{section.0.5}}
\bibcite{he2016deep}{{7}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{jiang2018logistic}{{8}{2020}{{Jiang et~al.}}{{Jiang, Josse, Lavielle, and Group}}}
\bibcite{jordan1999var}{{9}{1999}{{Jordan et~al.}}{{Jordan, Ghahramani, Jaakkola, and Saul}}}
\bibcite{karimi2021hwa}{{10}{2021}{{Karimi and Li}}{{}}}
\bibcite{karimi2019convergence}{{11}{2019{a}}{{Karimi et~al.}}{{Karimi, Lavielle, and Moulines}}}
\bibcite{karimi2019global}{{12}{2019{b}}{{Karimi et~al.}}{{Karimi, Wai, Moulines, and Lavielle}}}
\bibcite{khan2018fast}{{13}{2018}{{Khan et~al.}}{{Khan, Nielsen, Tangkaratt, Lin, Gal, and Srivastava}}}
\bibcite{kingma:adam}{{14}{2015}{{Kingma and Ba}}{{}}}
\bibcite{kingma}{{15}{2014}{{Kingma and Welling}}{{}}}
\bibcite{krizhevsky2012imagenet}{{16}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lange2016mm}{{17}{2016}{{Lange}}{{}}}
\bibcite{lecun1998mnist}{{18}{1998}{{LeCun}}{{}}}
\bibcite{lecun1998gradient}{{19}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{li2017dropout}{{20}{2017}{{Li and Gal}}{{}}}
\bibcite{mairal2015miso}{{21}{2015}{{Mairal}}{{}}}
\bibcite{mcLachlan2008em}{{22}{2008}{{McLachlan and Krishnan}}{{}}}
\bibcite{mensch2017stochastic}{{23}{2017}{{Mensch et~al.}}{{Mensch, Mairal, Thirion, and Varoquaux}}}
\bibcite{meyn2012markov}{{24}{2012}{{Meyn and Tweedie}}{{}}}
\bibcite{neal2012bayesian}{{25}{2012}{{Neal}}{{}}}
\bibcite{osawa2019practical}{{26}{2019}{{Osawa et~al.}}{{Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and Yokota}}}
\bibcite{paisley2013}{{27}{2012}{{Paisley et~al.}}{{Paisley, Blei, and Jordan}}}
\bibcite{polson2017deep}{{28}{2017}{{Polson and Sokolov}}{{}}}
\bibcite{qian2019miso}{{29}{2019}{{Qian et~al.}}{{Qian, Sailanbayev, Mishchenko, and Richt{\'a}rik}}}
\bibcite{razaviyayn2013unified}{{30}{2013}{{Razaviyayn et~al.}}{{Razaviyayn, Hong, and Luo}}}
\bibcite{rezende2014stochastic}{{31}{2014}{{Rezende et~al.}}{{Rezende, Mohamed, and Wierstra}}}
\bibcite{robbins1971convergence}{{32}{1971}{{Robbins and Siegmund}}{{}}}
\bibcite{schmidt2017minimizing}{{33}{2017}{{Schmidt et~al.}}{{Schmidt, Le~Roux, and Bach}}}
\bibcite{sutskever2013importance}{{34}{2013}{{Sutskever et~al.}}{{Sutskever, Martens, Dahl, and Hinton}}}
\bibcite{van2000asymptotic}{{35}{2000}{{Van~der Vaart}}{{}}}
\bibcite{vershynin2018high}{{36}{2018}{{Vershynin}}{{}}}
\bibcite{wainwright2019high}{{37}{2019}{{Wainwright}}{{}}}
\bibcite{wei1990mcem}{{38}{1990}{{Wei and Tanner}}{{}}}
\bibcite{wen2018flipout}{{39}{2018}{{Wen et~al.}}{{Wen, Vicol, Ba, Tran, and Grosse}}}
\bibcite{zhou2020towards}{{40}{2020}{{Zhou et~al.}}{{Zhou, Karimi, Yu, Xu, and Li}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs of the Theoretical Results}{17}{section.0.A}}
\newlabel{app:proofs}{{A}{17}{Proofs of the Theoretical Results}{section.0.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of Theorem\nobreakspace  {}\ref  {thm:main}}{17}{subsection.0.A.1}}
\newlabel{eq:surbd}{{19}{18}{Proof of Theorem~\ref {thm:main}}{equation.0.A.19}{}}
\newlabel{eq:firsteq}{{20}{18}{Proof of Theorem~\ref {thm:main}}{equation.0.A.20}{}}
\newlabel{eq:afterarrange}{{21}{18}{Proof of Theorem~\ref {thm:main}}{equation.0.A.21}{}}
\newlabel{eq:prebdd}{{23}{19}{Proof of Theorem~\ref {thm:main}}{equation.0.A.23}{}}
\newlabel{eq:mkcal}{{25}{19}{Proof of Theorem~\ref {thm:main}}{equation.0.A.25}{}}
\newlabel{eq:gksur}{{26}{20}{Proof of Theorem~\ref {thm:main}}{equation.0.A.26}{}}
\newlabel{eq:gmbd}{{27}{20}{Proof of Theorem~\ref {thm:main}}{equation.0.A.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem\nobreakspace  {}\ref  {thm:mainasymp}}{21}{subsection.0.A.2}}
\newlabel{lemmarsapp}{{1}{21}{Proof of Theorem~\ref {thm:mainasymp}}{Lemma.1}{}}
\newlabel{eq:dvk}{{29}{22}{Proof of Theorem~\ref {thm:mainasymp}}{equation.0.A.29}{}}
\newlabel{eq:dxk}{{30}{22}{Proof of Theorem~\ref {thm:mainasymp}}{equation.0.A.30}{}}
\newlabel{eq:dek}{{31}{22}{Proof of Theorem~\ref {thm:mainasymp}}{equation.0.A.31}{}}
\newlabel{eq:esur}{{32}{23}{Proof of Theorem~\ref {thm:mainasymp}}{equation.0.A.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Proof of Lemma\nobreakspace  {}\ref  {lemmarsapp}}{23}{subsection.0.A.3}}
\newlabel{appendix:lemma}{{A.3}{23}{Proof of Lemma~\ref {lemmarsapp}}{subsection.0.A.3}{}}
\newlabel{chikbis}{{36}{24}{Proof of Lemma~\ref {lemmarsapp}}{equation.0.A.36}{}}
\citation{meyn2012markov}
\@writefile{toc}{\contentsline {section}{\numberline {B}Practical Details for the Binary Logistic Regression on the Traumabase}{25}{section.0.B}}
\newlabel{app:logistic}{{B}{25}{Practical Details for the Binary Logistic Regression on the Traumabase}{section.0.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Traumabase dataset quantitative variables}{25}{subsection.0.B.1}}
\newlabel{appendix:variables}{{B.1}{25}{Traumabase dataset quantitative variables}{subsection.0.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Metropolis-Hastings algorithm}{25}{subsection.0.B.2}}
\newlabel{app:trauma_mh}{{B.2}{25}{Metropolis-Hastings algorithm}{subsection.0.B.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces MH aglorithm\relax }}{25}{algorithm.3}}
\newlabel{alg:mh}{{3}{25}{MH aglorithm\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}MISSO Update}{26}{subsection.0.B.3}}
\newlabel{app:update_logistic}{{B.3}{26}{MISSO Update}{subsection.0.B.3}{}}
\newlabel{eq:surrogatedet}{{41}{26}{MISSO Update}{equation.0.B.41}{}}
\newlabel{ass:log1}{{1}{26}{MISSO Update}{assumptionL.1}{}}
\newlabel{eq:surrogatelogit}{{42}{27}{MISSO Update}{equation.0.B.42}{}}
\newlabel{eq:mixedsurrogate}{{43}{27}{MISSO Update}{equation.0.B.43}{}}
\newlabel{eq:msteplog}{{44}{28}{MISSO Update}{equation.0.B.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Wall clock time}{28}{subsection.0.B.4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Logistic Regression with missing values: running time in seconds for $10$ epochs.\relax }}{28}{table.caption.5}}
\newlabel{tab:tablelogisitc}{{1}{28}{Logistic Regression with missing values: running time in seconds for $10$ epochs.\relax }{table.caption.5}{}}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Plots against the epochs elapsed}{29}{subsection.0.B.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Convergence of parameters $\delta _1$ and $\beta _1$, the first component of vectors ${\bm  {\delta }}$ and ${\bm  {\beta }}$, for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against epochs elapsed.\relax }}{29}{figure.caption.6}}
\newlabel{fig:misso_trauma_epochs}{{3}{29}{Convergence of parameters $\delta _1$ and $\beta _1$, the first component of vectors ${\bm \delta }$ and ${\bm \beta }$, for the SAEM, the MCEM and the MISSO methods. The convergence is plotted against epochs elapsed.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Practical Details for the Incremental Variational Inference}{29}{section.0.C}}
\newlabel{app:vi}{{C}{29}{Practical Details for the Incremental Variational Inference}{section.0.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Neural Networks Architecture}{29}{subsection.0.C.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \vspace  {-0.05in}LeNet-5 architecture\relax }}{29}{table.caption.7}}
\newlabel{table:lenet}{{2}{29}{\vspace {-0.05in}LeNet-5 architecture\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Algorithms updates}{29}{subsection.0.C.2}}
\newlabel{bnn:updates}{{C.2}{29}{Algorithms updates}{subsection.0.C.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \vspace  {-0.1in}ResNet-18 architecture\relax }}{30}{table.caption.8}}
\newlabel{table:resnet}{{3}{30}{\vspace {-0.1in}ResNet-18 architecture\relax }{table.caption.8}{}}
\newlabel{eq:missoupdate}{{45}{30}{Algorithms updates}{equation.0.C.45}{}}
\newlabel{eq:drifts}{{46}{30}{Algorithms updates}{equation.0.C.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Plots against the epochs elapsed}{32}{subsection.0.C.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Negated ELBO versus epochs elapsed for fitting (Left) Bayesian LeNet-5 on MNIST and (Right) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods.\relax }}{32}{figure.caption.9}}
\newlabel{fig:lenetoptepochs}{{4}{32}{Negated ELBO versus epochs elapsed for fitting (Left) Bayesian LeNet-5 on MNIST and (Right) Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent runs of the methods.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Wallclock time}{32}{subsection.0.C.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Bayesian Deep Neural Network: running time in seconds for $100$ epochs.\relax }}{32}{table.caption.10}}
\newlabel{tab:tablebnn}{{4}{32}{Bayesian Deep Neural Network: running time in seconds for $100$ epochs.\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Resnet on CIFAR: running time in seconds for $200$ epochs.\relax }}{32}{table.caption.11}}
\newlabel{tab:tableresnet}{{5}{32}{Resnet on CIFAR: running time in seconds for $200$ epochs.\relax }{table.caption.11}{}}
\newlabel{jmlrend}{{C.4}{32}{end of  Minimization by Incremental Stochastic Surrogate}{section*.12}{}}
