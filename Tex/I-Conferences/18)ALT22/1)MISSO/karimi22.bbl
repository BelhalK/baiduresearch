\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bishop(2006)]{bishop2006pattern}
Christopher~M Bishop.
\newblock \emph{Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017vi}
David~M. Blei, Alp Kucukelbir, and Jon~D. McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML)}, pages 1613--1622, Lille, France, 2015.

\bibitem[Delyon et~al.(1999)Delyon, Lavielle, and Moulines]{delyon1999}
Bernard Delyon, Marc Lavielle, and Eric Moulines.
\newblock Convergence of a stochastic approximation version of the em
  algorithm.
\newblock \emph{The Annals of Statistics}, 27\penalty0 (1):\penalty0 94--128,
  03 1999.

\bibitem[Domke(2020)]{domke2020provable}
Justin Domke.
\newblock Provable smoothness guarantees for black-box variational inference.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, pages 2587--2596, Virtual Event, 2020.

\bibitem[Ghahramani(2015)]{ghahramani2015probabilistic}
Zoubin Ghahramani.
\newblock Probabilistic machine learning and artificial intelligence.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 452--459, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the 2016 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 770--778, Las Vegas, NV, 2016.

\bibitem[Jiang et~al.(2020)Jiang, Josse, Lavielle, and
  Group]{jiang2018logistic}
Wei Jiang, Julie Josse, Marc Lavielle, and TraumaBase Group.
\newblock Logistic regression with missing covariates parameter estimation,
  model selection and prediction within a joint-modeling framework.
\newblock \emph{Computational Statistics \& Data Analysis}, 145:\penalty0
  106907, 2020.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan1999var}
Michael~I. Jordan, Zoubin Ghahramani, Tommi~S. Jaakkola, and Lawrence~K. Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Mach. Learn.}, 37\penalty0 (2):\penalty0 183--233, 1999.

\bibitem[Karimi and Li(2021)]{karimi2021hwa}
Belhal Karimi and Ping Li.
\newblock {HWA}: Hyperparameters weight averaging in bayesian neural networks.
\newblock In \emph{Proceedings of the 3rd Symposium on Advances in Approximate
  Bayesian Inference (AABI)}, 2021.

\bibitem[Karimi et~al.(2019{\natexlab{a}})Karimi, Lavielle, and
  Moulines]{karimi2019convergence}
Belhal Karimi, Marc Lavielle, and {\'E}ric Moulines.
\newblock On the convergence properties of the mini-batch em and mcem
  algorithms.
\newblock 2019{\natexlab{a}}.

\bibitem[Karimi et~al.(2019{\natexlab{b}})Karimi, Wai, Moulines, and
  Lavielle]{karimi2019global}
Belhal Karimi, Hoi-To Wai, Eric Moulines, and Marc Lavielle.
\newblock On the global convergence of (fast) incremental expectation
  maximization methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan2018fast}
Mohammad~Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu~Lin, Yarin Gal, and
  Akash Srivastava.
\newblock Fast and scalable bayesian deep learning by weight-perturbation in
  adam.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 2616--2625, Stockholmsm{\"{a}}ssan, Stockholm,
  Sweden, 2018.

\bibitem[Kingma and Ba(2015)]{kingma:adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem[Kingma and Welling(2014)]{kingma}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{Proceedings of the 2nd International Conference on Learning
  Representations (ICLR)}, Banff, Canada, 2014.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1106--1114, Lake Tahoe, NV, 2012.

\bibitem[Lange(2016)]{lange2016mm}
Kenneth Lange.
\newblock \emph{{MM} optimization algorithms}.
\newblock {SIAM}, 2016.
\newblock ISBN 978-1-611-97439-3.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'{e}}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proc. {IEEE}}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Li and Gal(2017)]{li2017dropout}
Yingzhen Li and Yarin Gal.
\newblock Dropout inference in bayesian neural networks with alpha-divergences.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pages 2052--2061, Sydney, Australia, 2017.

\bibitem[Mairal(2015)]{mairal2015miso}
Julien Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  829--855, 2015.

\bibitem[McLachlan and Krishnan(2008)]{mcLachlan2008em}
Geoffrey~J. McLachlan and Thriyambakam Krishnan.
\newblock \emph{The {EM} algorithm and extensions}.
\newblock John Wiley \& Sons, second edition, 2008.
\newblock ISBN 978-0-471-20170-0.

\bibitem[Mensch et~al.(2017)Mensch, Mairal, Thirion, and
  Varoquaux]{mensch2017stochastic}
Arthur Mensch, Julien Mairal, Bertrand Thirion, and Ga{\"e}l Varoquaux.
\newblock Stochastic subsampling for factorizing huge matrices.
\newblock \emph{IEEE Transactions on Signal Processing}, 66\penalty0
  (1):\penalty0 113--128, 2017.

\bibitem[Meyn and Tweedie(2012)]{meyn2012markov}
Sean~P Meyn and Richard~L Tweedie.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Neal(2012)]{neal2012bayesian}
Radford~M Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and
  Yokota]{osawa2019practical}
Kazuki Osawa, Siddharth Swaroop, Mohammad~Emtiyaz Khan, Anirudh Jain, Runa
  Eschenhagen, Richard~E. Turner, and Rio Yokota.
\newblock Practical deep learning with bayesian principles.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 4289--4301, Vancouver, Canada, 2019.

\bibitem[Paisley et~al.(2012)Paisley, Blei, and Jordan]{paisley2013}
John~W. Paisley, David~M. Blei, and Michael~I. Jordan.
\newblock Variational bayesian inference with stochastic search.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning (ICML)}, Edinburgh, UK, 2012.

\bibitem[Polson and Sokolov(2017)]{polson2017deep}
Nicholas Polson and Vadim Sokolov.
\newblock Deep learning: a bayesian perspective.
\newblock \emph{Bayesian Analysis}, 12\penalty0 (4):\penalty0 1275--1304, 2017.

\bibitem[Qian et~al.(2019)Qian, Sailanbayev, Mishchenko, and
  Richt{\'a}rik]{qian2019miso}
Xun Qian, Alibek Sailanbayev, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Miso is making a comeback with better proofs and rates.
\newblock \emph{arXiv preprint arXiv:1906.01474}, 2019.

\bibitem[Razaviyayn et~al.(2013)Razaviyayn, Hong, and
  Luo]{razaviyayn2013unified}
Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo.
\newblock A unified convergence analysis of block successive minimization
  methods for nonsmooth optimization.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (2):\penalty0
  1126--1153, 2013.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the 31th International Conference on Machine
  Learning (ICML)}, pages 1278--1286, Beijing, China, 2014.

\bibitem[Robbins and Siegmund(1971)]{robbins1971convergence}
Herbert Robbins and David Siegmund.
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In \emph{Optimizing methods in statistics}, pages 233--257. Elsevier,
  1971.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George~E. Dahl, and Geoffrey~E. Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning (ICML)}, pages 1139--1147, Atlanta, GA, 2013.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wei and Tanner(1990)]{wei1990mcem}
Greg~CG Wei and Martin~A Tanner.
\newblock A monte carlo implementation of the em algorithm and the poor man's
  data augmentation algorithms.
\newblock \emph{Journal of the American statistical Association}, 85\penalty0
  (411):\penalty0 699--704, 1990.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{wen2018flipout}
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger~B. Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Karimi, Yu, Xu, and Li]{zhou2020towards}
Yingxue Zhou, Belhal Karimi, Jinxing Yu, Zhiqiang Xu, and Ping Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 810--821, 2020.

\end{thebibliography}
