\documentclass{article}

\usepackage{amssymb}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{amssymb}
\usepackage{amsfonts}     
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{amsthm}
\usepackage{balance}
\usepackage{mathtools} 
\usepackage{extarrows} 
\usepackage{microtype}
\usepackage{url}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage[font=small,labelfont=bf]{caption}

\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{lavander}{cmyk}{0,0.48,0,0}
\definecolor{violet}{cmyk}{0.79,0.88,0,0}
\definecolor{burntorange}{cmyk}{0,0.52,1,0}
\usepackage[colorlinks=true,linkcolor=ao(english),urlcolor=blue,citecolor=burntorange]{hyperref}
\usepackage{multirow}

\newcommand{\zz}[1]{\textcolor{blue}{#1}}
\newcommand{\Xc}{{\mathcal X}}
\newcommand{\Zc}{{\mathcal Z}}
\newcommand{\Pn}{\mathbb P^{(n)}}
\newcommand{\Qn}{\mathbb Q^{(n)}}
\newcommand{\pr}{{\mathbb P}}
\newcommand{\ex}{\mathbb E}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newcommand{\RN}[1]{%
	\textup{\lowercase\expandafter{\it \romannumeral#1}}%
}

\let\oldemptyset\emptyset
\let\emptyset\varnothing

\begin{document}

\section{Reviewer RkBA}
We sincerely thank the reviewer for the constructive comments. 

\begin{itemize}
\item *[Response to concerns on L130-L133]	Unlike classical graphical models, a node in a VFG model may represent multiple variables. A node only belongs to one layer, and one layer can have multiple nodes. Moreover, each latent variable belongs to only one node in a VFG. 

\item *[L127-128] There are several advantages for the encoder and decoder to share parameters. Firstly, it could make the network structure simple and retrain the expressive power as well. Secondly, the training and inference can be simplified with concise and simple graph structures. 

Thirdly, by leveraging invertible flow functions, VFGs obtain achieve tighter ELBOs in comparison with VAE based models. The main reason why VFGs can achieve superior ELBOs and negative likelihood values (NLLs) in table 1 is due to VFGs' approximate invertibility property. The intrinsic invertibility introduced by flow functions ensures the decoder or generative model in a VFG  achieves smaller reconstruction errors for data samples and hence smaller NLL values and tighter ELBO. Whereas without the intrinsic constraint of invertibility or any help or regularization from the encoder, VAE-based models have to learn a  mapping function~(decoder) to reconstruct all data samples with the latent variables, and there are always some discrepancy errors in the reconstruction that lead to relatively larger NLL values and hence inferior ELBOs.

\item *[L166 - 169] We need to clarify some vague notations that the reviewer might have concerns with.    We agree with the reviewer's opinion that evaluation of the data reconstruction term, the KL terms, and the ELBO  requires samples from the posterior distribution $q(\mathbf{h}|\mathbf{x})$. 
Actually, in training algorithm 1, both forward variable states $\mathbf{h}$ and backward variable states  $\widehat{\mathbf{h}}$ are indeed sampled from $q(\mathbf{h}|\mathbf{x})$. \textbf{The backward variable state $\widehat{\mathbf{h}}^l$ in  layer $l$  is generated according to $p(\widehat{\mathbf{h}}^l | \widehat{\mathbf{h}}^{l+1})$, and at the root  layer,  variable $\widehat{\mathbf{h}}^{\mathcal{R}}$ is set  equal to  $\mathbf{h}^{\mathcal{R}}$ that is
from  the posterior $q(\mathbf{h}|\mathbf{x})$, not from the prior $p(\mathbf{h}^{\mathcal{R}})$.} So we can see all the forward and backward latent variables are sampled from the posterior $q(\mathbf{h}|\mathbf{x})$. 
We state this point clearly in line 172 of the main file.

\item *[L 175] We do agree with the reviewer about L176, and the derivation can be simplified.  Your suggestion is well taken, and the derivation will be improved. 


\item The main reason to use Laplace distribution for latent variables is that it may introduce sparsity in latent variables. We also tried Gaussian and Gamma distributions, and they also work well in most tasks. VFGs leverage the universal approximation power [1] of coupling flows, and tractable inference could be achieved with different latent distributions. 

In current experiments, the dimension of the latent variables equals to the dimension of each data section $\mathbf{x}_i$. VFGs can have relatively more flexible dimensions for latent variables with concatenation aggregation nodes.  The distribution of $p(\mathbf{x}|\mathbf{h})$ is either Gaussian or Binary in current experiments. 

\end{itemize}

We thank the reviewer for the suggestions on the writing. The background section, description of reparameterization, and citations of VAE will be revised accordingly. 


[1] Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators, Teshima et.al., NeurIPS 2020.

\section{Reviewer 1svF}

We thank the reviewer for the constructive comments. The reviewer might have some misunderstanding about the paper, and we provide our response as follows. 

1. We need to clarify some vague notations that the reviewer might have concerns with.    We agree with the reviewer's opinion that evaluation of the data reconstruction term, the KL terms, and the ELBO  requires samples from the posterior distribution $q(\mathbf{h}|\mathbf{x})$. 
Actually, in training algorithm 1, both forward variable states $\mathbf{h}$ and backward variable states  $\widehat{\mathbf{h}}$ are indeed sampled from $q(\mathbf{h}|\mathbf{x})$. \textbf{The backward variable state $\widehat{\mathbf{h}}^l$ in  layer $l$  is generated according to $p(\widehat{\mathbf{h}}^l | \widehat{\mathbf{h}}^{l+1})$, and at the root  layer,  variable $\widehat{\mathbf{h}}^{\mathcal{R}}$ is set  equal to  $\mathbf{h}^{\mathcal{R}}$ that is
from  the posterior $q(\mathbf{h}|\mathbf{x})$, not from the prior $p(\mathbf{h}^{\mathcal{R}})$.} So we can see all the forward and backward latent variables are sampled from the posterior $q(\mathbf{h}|\mathbf{x})$. 
We state this point clearly in line 172 of the main file. We will give a clear explanation about this point in future versions. 



2.   Training with Algorithm 1 is to improve the data fitting and the consistency of aggregation nodes. Geometrically speaking, training with Algorithm 2 is to enhance the learned relational knowledge among leaf nodes. In practice, we use  Algorithm 2  along with Algirhtm 1 to train a VFG model. 
One import motivation of VFG is that we aim to develop a model that is tractable of inference on datasets. 
A very simple application case with inference is to impute the missing values of data samples.  
It motivates us to use the imputation loss as the objective and the KL terms from the ELBO as regularization terms. 
Hence Algorithm 2 aims at improving the model's heuristic inference capability while working in practice.

3. The notations are defined in lines 130-133. We repeat the definitions here to make it more clear.   In the paper, $\mathbf{h}^l$ denotes the latent variable in layer $l$.   We use $\mathbf{h}^{l, i}$ to denote the $i$th node's latent variable  in layer $l$, and $\mathbf{h}^{(j)}$ to represent node $j$'s latent variable without specification of the layer number, and $j$ is the node index on a tree or graph.  

4. The reviewer's suggestions on hierarchical VAE-style models are well taken, and we will give a more inclusive discussion about existing works. 


=========

We thank the reviewer for the response.  We have checked paper [1], and we found that  our algorithm and Algorithm 1 in paper [1] follow the same way to compute the latent variables and the gradients. For each  data sample, all of its hidden variables,  $\mathbf{h}$s and $\widehat{\mathbf{h}}$s,  are computed in one forward path and one backward path, and no sampling is involved in the whole process. At the root nodes, we set  $\widehat{\mathbf{h}} = \mathbf{h}$. Moreover, only the root nodes of a VFG have prior distributions.  In the computing of $\mathbf{h}$s and $\widehat{\mathbf{h}}$, no sampling of the prior is involved in the process. As we state clearly in line 177, we use one sample to approximate the latent variable, hence we do not sample multiple times for any latent variables.   


[1]Stochastic Backpropagation and Approximate Inference in Deep Generative Models, Rezende et. al., 2014.


=======

We appreciate the reviewer’s active response.  We do believe the notations, unclear statements, and a few typos in the paper may cause some trouble to the reviewer's understanding of the paper. A VFG provides an effective hierarchical aggregation mechanism for data modeling and only the root nodes have prior distributions.  

The backward path (decoder) p is the inverse function of the forward path (encoder) q.  Latent variables should include both forward latent variables $\mathbf{h}$ and backward ones $\widehat{\mathbf{h}}$. The ``q"  denotes the posterior for both forward states $\mathbf{h}$ and post-aggregation latent states $\widehat{\mathbf{h}}$.  We guess the notation of ``p” may lead the readers to link it with prior distributions, but it is not.  In future versions, we will use p to denote only the data reconstruction distribution and the roots' priors. Hence we agree with the reviewer that the $p$ in equation (9) should be revised as $q$ in future versions. 

There are some typos in the paper, i.e., the expectation terms in equations (5), (6), (7), line 151, and line 176 should include both forward latent variables $\mathbf{h}$ and backward ones $\widehat{\mathbf{h}}$. The reconstruction term in equation (5) will be 
$\mathbb{E}_{q(\mathbf{h}, \widehat{\mathbf{h}}|\mathbf{x})}\big[ \log p(\mathbf{x}|\mathbf{h}, \widehat{\mathbf{h}})  \big]$. We thank the reviewer again for the constructive comments. We will clarify the the definition of notations and address the typos and errors in the next version. 





\section{Reviewer aCtb}
We thank the reviewer for the constructive comments. The reviewer's main concerns are on the encoder-decoder parameter sharing and VFG structures. 

1.
There are several advantages for the encoder and decoder to share parameters. Firstly, it could make the network structure simple and retrain the expressive power as well. Secondly, the training and inference can be simplified with concise and simple graph structures. 

Thirdly, by leveraging invertible flow functions, VFGs obtain achieve tighter ELBOs in comparison with VAE based models. The main reason why VFGs can achieve superior ELBOs and negative likelihood values (NLLs) in table 1 is due to VFGs' approximate invertibility property. The intrinsic invertibility introduced by flow functions ensures the decoder or generative model in a VFG  achieves smaller reconstruction errors for data samples and hence smaller NLL values and tighter ELBO. Whereas without the intrinsic constraint of invertibility or any help or regularization from the encoder, VAE-based models have to learn a  mapping function~(decoder) to reconstruct all data samples with the latent variables, and there are always some discrepancy errors in the reconstruction that lead to relatively larger NLL values and hence inferior ELBOs.

2.
 In this paper,  we would rather assume the VFG graph structures are given and fixed.  In the experiments, the VFG structures are designed heuristically (as other neural networks) for the sake of numerical illustrations.  Learning the structure of VFG is an interesting research problem and is left for future works. A simple approach for VFG structure learning is to regularize the graph with the DAG structure penalty[1,2]. 

3. We sincerely thank the reviewer for the suggestions on the article structure, writing, and experiments. They are well taken and the next version will be revised accordingly. 

 

[1]DAGs with NO TEARS: Continuous Optimization for Structure Learning, Zheng et. al., 2018

[2] Graphical normalizing flows. In International Conference on Artificial Intelligence and Statistics, pp. 37-45. PMLR, 2021.


\section{Reviewer XH7q}
We sincerely thank the reviewer for the constructive comments and concerns on the article organization and several unclear points. 

1.
 We thank the reviewer for pointing out several issues with the article structure, the abstract, and the writing. The abstract and contribution sections will be revised based on the suggestions to give a more accurate description of the paper.  The background and bibliography will be edited to include introductions on tractable models, e.g. sum-product nets[1], probabilistic circuits[2], etc. 


2. 
One of the motivations for VFG is that we try to develop a tractable model that can be used for inference on datasets. As long as the variables in aggregation nodes are consistent, we always can apply VFGs to missing value inference or even probabilistic inference on datasets. In terms of tractability, VFGs are more close to sum-product networks[1] or probabilistic circuits [2]. Sum-product networks and probabilistic circuits depend on mixture models, probabilistic factorization in graphical structure for inference. While VFGs rely on aggregation variable consistency and approximation to perform tractable inference.  


3. 
 We have qualitatively compared the VFGs' inference capability with baseline methods on missing value imputation as shown in sections 6.1 and A.1.  We further qualitatively compared VFG with existing methods on variational inference using three different standard datasets.  The evaluation setups are exactly following two standard flow-based variational models, Sylvester normalizing flows[3] and [4].  The main reason why VFGs achieve superior variational inference results (ELBOs and  NLLs) in table 1 is due to VFGs' approximate invertibility property. The intrinsic invertibility ensures the decoder or generative model in a VFG  can obtain smaller reconstruction errors for data samples and hence smaller NLL values. Whereas without the intrinsic constraint of invertibility, VAEs have to learn a mapping function~(decoder) to reconstruct all data samples with the latent variables, and there are always some discrepancy errors that lead to relatively larger NLL values and hence inferior ELBOs.
 
 These results in sections  6.1, 6.2, and A.1. show that VFG indeed improves variational inference and also could be applied to missing value imputation. 


[1] Sum-product networks: A new deep architecture, Poon, Hoifung and Domingos, Pedro, 2011.
 
 [2] Probabilistic circuits: A unifying framework for tractable probabilistic models, Choi et. al., Technical report, 2020.
 
 [3] R. v. d. Berg, L. Hasenclever, J. M. Tomczak, and M. Welling. Sylvester normalizing flows for variational inference. arXiv preprint arXiv:1803.05649, 2018.
 
 [4] D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.


%$\oldemptyset$ $\emptyset$
\end{document}
