\documentclass{article}

\usepackage{amssymb}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{amssymb}
\usepackage{amsfonts}     
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{amsthm}
\usepackage{balance}
\usepackage{mathtools} 
\usepackage{extarrows} 
\usepackage{microtype}
\usepackage{url}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage[font=small,labelfont=bf]{caption}

\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{lavander}{cmyk}{0,0.48,0,0}
\definecolor{violet}{cmyk}{0.79,0.88,0,0}
\definecolor{burntorange}{cmyk}{0,0.52,1,0}
\usepackage[colorlinks=true,linkcolor=ao(english),urlcolor=blue,citecolor=burntorange]{hyperref}
\usepackage{multirow}

\newcommand{\zz}[1]{\textcolor{blue}{#1}}
\newcommand{\Xc}{{\mathcal X}}
\newcommand{\Zc}{{\mathcal Z}}
\newcommand{\Pn}{\mathbb P^{(n)}}
\newcommand{\Qn}{\mathbb Q^{(n)}}
\newcommand{\pr}{{\mathbb P}}
\newcommand{\ex}{\mathbb E}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newcommand{\RN}[1]{%
	\textup{\lowercase\expandafter{\it \romannumeral#1}}%
}

\let\oldemptyset\emptyset
\let\emptyset\varnothing

\begin{document}
\section{Reviewer 2LEc}
We sincerely thank the reviewer for the constructive comments for this paper. The concerns are addressed individually as follows. 

1. We agree with the reviewer that we should have qualitative results to compare VFG and existing methods on latent representation. 
However, in our experiments,  we have qualitatively compared the model's inference capability with baseline methods on missing value imputation as given in sections 6.1 and A.1.  We further qualitatively compared VFG with existing methods on variational inference using three standard datasets. VFG can achieve superior EBLO and NLL values as given in Table 1. The evaluation datasets and setup are exactly following two standard flow-based variational models, sylvester normalizing flows[1] and [2].  The main reason why VFGs can achieve superior variational inference results (ELBOs and  NLLs) in table 1 is due to VFGs's approximate invertibility property. The intrinsic invertibility ensures the decoder or generative model in a VFG  achieves smaller reconstruction errors for data samples and hence smaller NLL values.

These results in sections  6.1, 6.2, and A.1. show that VFG improves variational inference and also could be applied to missing value imputation. 

2. At an aggregation node, VFG relies on minimizing the ELBO to ensure the consistency of latent variables, e.g., equations (19) and (20).  There are always some errors in the minimization of the objective. 
The inference on an aggregation node depends on the consistency of latent variables in it. There are always some errors in the model that breaks the consistency, and it is the reason why approximation symbols are preferable in Lemma 1. We will include the discussion in the next version.

3.  In practice, we use Algorithm 2 along with Algorithm 1 to train a VFG model.  Algorithm 1 is to maximize the overall ELBO, and Algorithm 2 is to improve the model's inference capability. One important motivation of VFG is that we try to develop a model that can perform inference on datasets. A very simple application case of inference is to impute the missing values of data samples.  It motivates us to use the imputation loss as the objective and the KL terms in the ELBO as regularization. 
 Though Algorithm 2 improves the model's inference capability in a heuristic way, it works in practice.

4. VFG lies in the stream of tractable neural networks that allow us to perform inference on the graphical structures.  Sum-product networks and probabilistic circuits are falling into this type of model. Sum-product networks and probabilistic circuits depend on mixture models, probabilistic factorization in graphical structure for inference. While VFGs rely on variable consistency and approximation to perform tractable inference. We will add a more clear introduction about tractable models to the background section. 

5. The notations are defined in lines 130-133. We repeat the definitions here to make it more clear.   In the paper, $\mathbf{h}^l$ denotes the latent variable in layer $l$.   We use $\mathbf{h}^{l, i}$ to denote the $i$th node's latent variable  in layer $l$, and $\mathbf{h}^{(j)}$ to represent node $j$'s latent variable without specification of the layer number, and $j$ is the node index on a tree or graph. 


6. The sign in equation 2 should be negative. 


 [1] R. v. d. Berg, L. Hasenclever, J. M. Tomczak, and M. Welling. Sylvester normalizing flows for variational inference. arXiv preprint arXiv:1803.05649, 2018.
 
 [2] D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.
 
 
\section{Reviewer x7TA}
We truly thank the reviewer for the constructive comments and the long hours to review this paper. Our response to the comments is given as follows. 

1.
The main reason why VFGs can achieve superior ELBOs and negative likelihood values (NLLs) in table 1 is due to VFGs' approximate invertibility property. The intrinsic invertibility introduced by flow functions ensures the decoder or generative model in a VFG  achieves smaller reconstruction errors for data samples and hence smaller NLL values and tighter ELBO. Whereas without the intrinsic constraint of invertibility or any help or regularization from the encoder, VAE-based models have to learn a mapping function~(decoder) to reconstruct all data samples with the latent variables, and there are always some discrepancy errors in the reconstruction that lead to relatively larger NLL values and hence inferior ELBOs.


The main focus of graphical normalizing flow[1] is to learn the DAG graphical structure with flow models and sparse structure penalty.
Different from VFG's variational approach, graphical normalizing flow relies on maximum likelihood to learn model parameters. For VFGs,  the structure learning problem is interesting and left to future works, and we use heuristically designed structures in the experiments. 

One of the motivations for VFG is that we try to develop a tractable model that can be used for inference on datasets. As long as the variables in aggregation nodes are consistent, we always can apply VFGs to missing value inference or even probabilistic inference on datasets. In terms of tractability, VFGs are more close to sum-product networks or probabilistic circuits [4]. We will include the discussion and the references the reviewer mentioned in our future versions. 


2. Experiments in sections 6.1 and A.1 are meant to evaluate the inference capability of VFG with missing data imputation.   
Hence imputation baselines are utilized in this set of experiments for comparison.  

Whereas section 6.2 evaluates the variational inference of  VFG  for model learning on different datasets,  and  ELBO and negative likelihood values (NLLs)   are used to compare different models. The evaluation datasets and setup in this section are exactly following two standard flow-based variational inference models, Sylvester normalizing flows[1] and [2].

We agree with the reviewer about the experiments on latent variables. The t-SNE visualization cannot tell us too much about the latent representation. We will present quantitative results to compare VFGs and other models on latent representation learning in the next version.

%We have not 
% The experimental setup in this section is following Sylvester normalizing flows[1] and Planer flow[2]. 

In summary, we thank the reviewer again for the constructive comments. 


[1] "Graphical normalizing flows." In International Conference on Artificial Intelligence and Statistics, pp. 37-45. PMLR, 2021.

 [2] R. v. d. Berg, L. Hasenclever, J. M. Tomczak, and M. Welling. Sylvester normalizing flows for variational inference. arXiv preprint arXiv:1803.05649, 2018.
 
 [3] D. J. Rezende and S. Mohamed. Variational inference with normalizing flows. arXiv preprint arXiv:1505.05770, 2015.
 
 [4] Probabilistic circuits: A unifying framework for tractable probabilistic models, Choi et. al., Technical report, 2020.
 
 \section{Reviewer yczd}
We sincerely thank the reviewer for the constructive comments. Our response to the comments is listed as follows. 

1. 

We thank the reviewer for the suggestions on the writing and literature review.  The main focus of graphical normalizing flow[1] is to learn the DAG graphical structure from data with flow models and sparse structure penalty. Different from VFG's variational approach, graphical normalizing flow uses maximum likelihood to learn model parameters. Causal autoregressive flows[2]  uses the existing likelihood ratio theory for causal discovery. The bivariant causal discovery method proposed in [2] relies on the autoregressive structure of flow models in addition to the asymmetry of log-likelihood ratios for causal and effect variables.
 The reviewer's suggestions are well taken, and a detailed discussion about these models will be included in the next version. 


 2.
 
 In this paper,  we would rather assume the VFG graph structures are given and fixed.  In the experiments, the VFG structures are designed heuristically as other neural networks.  Learning the structure of VFG is an interesting research problem and is left for future works. A simple approach for VFG structure learning is to regularize the graph with the DAG structure penalty[4,1]. The generalization of the ELBO from trees to DAGs is discussed in section E.3. 
 
 An aggregation node with V-structure has multiple parents and multiple children as shown by the example in  Figure 12, the supplemental file. VFGs rely on minimizing the KL term to achieve consistency among the variables in an aggregation node (discussed in section C). As long as the aggregation nodes retain consistency, the model always has a tight ELBO and can be applied to tractable inference. According to the recent theoretical study[3], coupling-based flows have the universal approximation power. Hence we believe the consistency of aggregation nodes on a DAG VFG can be attained with a training algorithm and thus a tight ELBO. 

The main reason to use Laplace distribution for latent variables is that it may introduce sparsity in latent variables. We also tried Gaussian and Gamma distributions, and they also work well in most tasks. VFGs leverage the universal approximation of coupling flows, and tractable inference could be achieved with different latent distributions. 
 
 3. We have qualitatively compared the VFGs' inference capability with baseline methods on missing value imputation as shown in sections 6.1 and A.1.  We further qualitatively compared VFG with existing methods on variational inference using three different standard datasets.  The evaluation setups are exactly following two standard flow-based variational models, Sylvester normalizing flows[1] and [2].  The main reason why VFGs achieve superior variational inference results (ELBOs and  NLLs) in table 1 is due to VFGs' approximate invertibility property. The intrinsic invertibility ensures the decoder or generative model in a VFG  can obtain smaller reconstruction errors for data samples and hence smaller NLL values. Whereas without the intrinsic constraint of invertibility, VAEs have to learn a mapping function~(decoder) to reconstruct all data samples with the latent variables, and there are always some discrepancy errors that lead to relatively larger NLL values and hence inferior ELBOs.
 
 These results in sections  6.1, 6.2, and A.1. show that VFG indeed improves variational inference and also could be applied to missing value imputation. 
 
[1] Graphical normalizing flows. In International Conference on Artificial Intelligence and Statistics, pp. 37-45. PMLR, 2021.

[2] Causal autoregressive flows. International Conference on Artificial Intelligence and Statistics,pp.3520--3528. PMLR, 2021.

[3] Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators, Teshima et.al., NeurIPS 2020.

[4]DAGs with NO TEARS: Continuous Optimization for Structure Learning, Zheng et. al., 2018


\section{Reviewer nY28}
We truly thank the reviewer for the comments and would like to address the concerns as best as we can. 

1. In the experiments, the model structures have been chosen heuristically and for the sake of numerical illustrations. A tree VFG model can be taken as a dimension reduction model that could also be used for missing value imputation.  Variants of those structures will lead to different numerical plots and at this point, we can not claim any generalization regarding the impact of the VFG structure on the outputs.

2. Missing values imputation is one example of application. We also provide in the numerical section another totally different application on the MNIST dataset. Here, the goal is to learn a latent variable model in order to disentanglement purposes. The unconditional samples are thus the latent variables controlling the pixels (shape of the digits).

3.  In practice, we use Algorithm 2 along with Algorithm 1 to train a VFG model.  Algorithm 1 is to maximize the overall ELBO, and Algorithm 2 is to improve the model's inference capability. Algorithm 2 can improve the convergence of the model on most datasets.  



%$\oldemptyset$ $\emptyset$
\end{document}
