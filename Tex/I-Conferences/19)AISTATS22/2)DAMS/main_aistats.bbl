\begin{thebibliography}{10}

\bibitem{agarwal2019efficient}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 102--110, Long Beach, CA, 2019.

\bibitem{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 440--445, Copenhagen, Denmark,
  2017.

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD:} communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 1709--1720, Long Beach, CA, 2017.

\bibitem{assran2019stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael~G. Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 344--353, Long Beach, CA, 2019.

\bibitem{boyd2009fastest}
Stephen~P. Boyd, Persi Diaconis, Pablo~A. Parrilo, and Lin Xiao.
\newblock Fastest mixing markov chain on graphs with symmetries.
\newblock {\em {SIAM} J. Optim.}, 20(2):792--819, 2009.

\bibitem{boyd2004fastest}
Stephen~P. Boyd, Persi Diaconis, and Lin Xiao.
\newblock Fastest mixing markov chain on a graph.
\newblock {\em {SIAM} Rev.}, 46(4):667--689, 2004.

\bibitem{boyd2011distributed}
Stephen~P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Found. Trends Mach. Learn.}, 3(1):1--122, 2011.

\bibitem{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{chen2010approximate}
Yongjian Chen, Tao Guan, and Cheng Wang.
\newblock Approximate nearest neighbor search by residual vector quantization.
\newblock {\em Sensors}, 10(12):11259--11273, 2010.

\bibitem{chilimbi2014project}
Trishul~M. Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In {\em Proceedings of the 11th {USENIX} Symposium on Operating
  Systems Design and Implementation (OSDI)}, pages 571--582, Broomfield, CO,
  2014.

\bibitem{duchi2011dual}
John~C. Duchi, Alekh Agarwal, and Martin~J. Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock {\em {IEEE} Trans. Autom. Control.}, 57(3):592--606, 2012.

\bibitem{duchi2011adaptive}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em J. Mach. Learn. Res.}, 12:2121--2159, 2011.

\bibitem{ge2013optimized}
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun.
\newblock Optimized product quantization for approximate nearest neighbor
  search.
\newblock In {\em Proceedings of the 2013 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 2946--2953, Portland, OR, 2013.

\bibitem{hong2017prox}
Mingyi Hong, Davood Hajinezhad, and Ming{-}Min Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pages 1529--1538, Sydney, Australia, 2017.

\bibitem{jegou2010product}
Herv{\'{e}} J{\'{e}}gou, Matthijs Douze, and Cordelia Schmid.
\newblock Product quantization for nearest neighbor search.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 33(1):117--128,
  2011.

\bibitem{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U. Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 3478--3487, Long Beach, CA, 2019.

\bibitem{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{li2019convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In {\em Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 983--992, Naha,
  Japan, 2019.

\bibitem{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 5330--5340, 2017.

\bibitem{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and Bill Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem{di2016next}
Paolo~Di Lorenzo and Gesualdo Scutari.
\newblock {NEXT:} in-network nonconvex optimization.
\newblock {\em {IEEE} Trans. Signal Inf. Process. over Networks},
  2(2):120--136, 2016.

\bibitem{lu2019gnsd}
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong.
\newblock {GNSD:} a gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In {\em Proceedings of the {IEEE} Data Science Workshop (DSW)}, pages
  315--321, Minneapolis, MN, 2019.

\bibitem{luo2019adaptive}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
  Blaise~Ag{\"{u}}era y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 1273--1282, Fort
  Lauderdale, FL, 2017.

\bibitem{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock {DADAM:} {A} consensus-based distributed adaptive gradient method for
  online optimization.
\newblock {\em CoRR}, abs/1901.09109, 2019.

\bibitem{nedic2009distributed}
Angelia Nedic and Asuman~E. Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em {IEEE} Trans. Autom. Control.}, 54(1):48--61, 2009.

\bibitem{reddi2020adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In {\em Proceedings of the 9th International Conference on Learning
  Representations (ICLR)}, Virtual Event, Austria, 2021.

\bibitem{reddi2019convergence}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {Adam} and beyond.
\newblock In {\em Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock {EXTRA:} an exact first-order algorithm for decentralized consensus
  optimization.
\newblock {\em {SIAM} J. Optim.}, 25(2):944--966, 2015.

\bibitem{stich2018sparsified}
Sebastian~U. Stich, Jean{-}Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified {SGD} with memory.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 4452--4463, Montr{\'{e}}al, Canada, 2018.

\bibitem{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D\({}^{\mbox{2}}\): Decentralized training over decentralized data.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 4855--4863, Stockholmsm{\"{a}}ssan, Sweden, 2018.

\bibitem{tang2019doublesqueeze}
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji~Liu.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 6155--6165, Long Beach, CA, 2019.

\bibitem{wang2018atomo}
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary~B. Charles, Dimitris~S.
  Papailiopoulos, and Stephen~J. Wright.
\newblock {ATOMO:} communication-efficient learning via atomic sparsification.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 9872--9883, Montr{\'{e}}al, Canada, 2018.

\bibitem{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 1306--1316, Montr{\'{e}}al, Canada, 2018.

\bibitem{ward2019adagrad}
Rachel Ward, Xiaoxia Wu, and L{\'{e}}on Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 6677--6686, Long Beach, CA, 2019.

\bibitem{yan2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In {\em Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence (IJCAI)}, pages 2955--2961, Stockholm,
  Sweden, 2018.

\bibitem{yuan2016convergence}
Kun Yuan, Qing Ling, and Wotao Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock {\em {SIAM} J. Optim.}, 26(3):1835--1854, 2016.

\bibitem{zaheer2018adaptive}
Manzil Zaheer, Sashank~J. Reddi, Devendra~Singh Sachan, Satyen Kale, and Sanjiv
  Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 9815--9825, Montr{\'{e}}al, Canada, 2018.

\bibitem{zhou2018convergence}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em CoRR}, abs/1808.05671, 2018.

\bibitem{zou2018convergence}
Fangyu Zou and Li~Shen.
\newblock On the convergence of weighted adagrad with momentum for training
  deep neural networks.
\newblock {\em CoRR}, abs/1808.03408, 2018.

\end{thebibliography}
