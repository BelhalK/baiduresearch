\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1709--1720, Long Beach, 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov,
  Khirirat, and Renggli]{alistarh2018convergence}
D.~Alistarh, T.~Hoefler, M.~Johansson, N.~Konstantinov, S.~Khirirat, and
  C.~Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5973--5983, Montr{\'{e}}al, Canada, 2018.

\bibitem[Basu et~al.(2019)Basu, Data, Karakus, and Diggavi]{basu2019qsparse}
D.~Basu, D.~Data, C.~Karakus, and S.~N. Diggavi.
\newblock Qsparse-local-sgd: Distributed {SGD} with quantization,
  sparsification and local computations.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 14668--14679, Vancouver, Canada, 2019.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{bernstein2018signsgd}
J.~Bernstein, Y.~Wang, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock {SIGNSGD:} compressed optimisation for non-convex problems.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 559--568, Stockholmsm{\"{a}}ssan, Stockholm, Sweden,
  2018.

\bibitem[Bonawitz et~al.(2017)Bonawitz, Ivanov, Kreuter, Marcedone, McMahan,
  Patel, Ramage, Segal, and Seth]{bonawitz2017practical}
K.~Bonawitz, V.~Ivanov, B.~Kreuter, A.~Marcedone, H.~B. McMahan, S.~Patel,
  D.~Ramage, A.~Segal, and K.~Seth.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In \emph{Proceedings of the 2017 {ACM} {SIGSAC} Conference on
  Computer and Communications Security (CCS)}, pages 1175--1191, Dallas, TX,
  2017.

\bibitem[Bottou and Bousquet(2008)]{bottou-bousquet-2008}
L.~Bottou and O.~Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 161--168, Vancouver, Canada, 2008.

\bibitem[Charikar et~al.(2004)Charikar, Chen, and
  Farach{-}Colton]{DBLP:journals/tcs/CharikarCF04}
M.~Charikar, K.~C. Chen, and M.~Farach{-}Colton.
\newblock Finding frequent items in data streams.
\newblock \emph{Theoretical Computer Science}, 312\penalty0 (1):\penalty0
  3--15, 2004.
\newblock \doi{10.1016/S0304-3975(03)00400-6}.
\newblock URL \url{https://doi.org/10.1016/S0304-3975(03)00400-6}.

\bibitem[Chen et~al.(2020)Chen, Li, and Li]{chen2020toward}
X.~Chen, X.~Li, and P.~Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In \emph{ACM-IMS Foundations of Data Science Conference (FODS)},
  Seattle, WA, 2020.

\bibitem[Cormode and Muthukrishnan(2005)]{cormode2005improved}
G.~Cormode and S.~Muthukrishnan.
\newblock An improved data stream summary: the count-min sketch and its
  applications.
\newblock \emph{Journal of Algorithms}, 55\penalty0 (1):\penalty0 58--75, 2005.

\bibitem[Dwork(2006)]{DBLP:conf/icalp/Dwork06}
C.~Dwork.
\newblock Differential privacy.
\newblock In \emph{Automata, Languages and Programming, 33rd International
  Colloquium, {ICALP} 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part
  {II}}, volume 4052 of \emph{Lecture Notes in Computer Science}, pages 1--12.
  Springer, 2006.

\bibitem[Geyer et~al.(2017)Geyer, Klein, and Nabi]{geyer2017differentially}
R.~C. Geyer, T.~Klein, and M.~Nabi.
\newblock Differentially private federated learning: A client level
  perspective.
\newblock \emph{arXiv preprint arXiv:1712.07557}, 2017.

\bibitem[Haddadpour and Mahdavi(2019)]{haddadpour2019convergence}
F.~Haddadpour and M.~Mahdavi.
\newblock On the convergence of local descent methods in federated learning.
\newblock \emph{arXiv preprint arXiv:1910.14425}, 2019.

\bibitem[Haddadpour et~al.(2020)Haddadpour, Kamani, Mokhtari, and
  Mahdavi]{haddadpour2020federated}
F.~Haddadpour, M.~M. Kamani, A.~Mokhtari, and M.~Mahdavi.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock \emph{arXiv preprint arXiv:2007.01154}, 2020.

\bibitem[Hardy et~al.(2017)Hardy, Henecka, Ivey-Law, Nock, Patrini, Smith, and
  Thorne]{hardy2017private}
S.~Hardy, W.~Henecka, H.~Ivey-Law, R.~Nock, G.~Patrini, G.~Smith, and
  B.~Thorne.
\newblock Private federated learning on vertically partitioned data via entity
  resolution and additively homomorphic encryption.
\newblock \emph{arXiv preprint arXiv:1711.10677}, 2017.

\bibitem[Horv{\'a}th and Richt{\'a}rik(2020)]{horvath2020better}
S.~Horv{\'a}th and P.~Richt{\'a}rik.
\newblock A better alternative to error feedback for communication-efficient
  distributed learning.
\newblock \emph{arXiv preprint arXiv:2006.11077}, 2020.

\bibitem[Horv{\'a}th et~al.(2019)Horv{\'a}th, Kovalev, Mishchenko, Stich, and
  Richt{\'a}rik]{horvath2019stochastic}
S.~Horv{\'a}th, D.~Kovalev, K.~Mishchenko, S.~Stich, and P.~Richt{\'a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock \emph{arXiv preprint arXiv:1904.05115}, 2019.

\bibitem[Ivkin et~al.(2019)Ivkin, Rothchild, Ullah, Braverman, Stoica, and
  Arora]{ivkin2019communication}
N.~Ivkin, D.~Rothchild, E.~Ullah, V.~Braverman, I.~Stoica, and R.~Arora.
\newblock Communication-efficient distributed {SGD} with sketching.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 13144--13154, Vancouver, Canada, 2019.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2019advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
H.~Karimi, J.~Nutini, and M.~Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Proceedings of European Conference on Machine Learning and
  Knowledge Discovery in Databases ({ECML}-{PKDD})}, pages 795--811, Riva del
  Garda, Italy, 2016.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2019scaffold}
S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~J. Reddi, S.~U. Stich, and A.~T.
  Suresh.
\newblock Scaffold: Stochastic controlled averaging for on-device federated
  learning.
\newblock \emph{arXiv preprint arXiv:1910.06378}, 2019.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'{a}}rik]{bayoumi2020tighter}
A.~Khaled, K.~Mishchenko, and P.~Richt{\'{a}}rik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics (AISTATS)}, pages 4519--4529, Online [Palermo, Sicily, Italy],
  2020.

\bibitem[Kleinberg(2003)]{kleinberg2003bursty}
J.~Kleinberg.
\newblock Bursty and hierarchical structure in streams.
\newblock \emph{Data Mining and Knowledge Discovery}, 7\penalty0 (4):\penalty0
  373--397, 2003.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, F.~X. Yu, P.~Richt{\'a}rik, A.~T. Suresh,
  and D.~Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2008)Li, Church, and Hastie]{Proc:Li_Church_Hastie_NIPS08}
P.~Li, K.~W. Church, and T.~Hastie.
\newblock One sketch for all: Theory and application of conditional random
  sampling.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 953--960, Vancouver, Canada, 2008.

\bibitem[Li et~al.(2019)Li, Liu, Sekar, and Smith]{li2019privacy}
T.~Li, Z.~Liu, V.~Sekar, and V.~Smith.
\newblock Privacy for free: Communication-efficient learning with differential
  privacy using sketches.
\newblock \emph{arXiv preprint arXiv:1911.00972}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{li2019federated}
T.~Li, A.~K. Sahu, A.~Talwalkar, and V.~Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{{IEEE} Signal Process. Mag.}, 37\penalty0 (3):\penalty0 50--60,
  2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2018federated}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock In \emph{Proceedings of Machine Learning and Systems (MLSys)},
  Austin, TX, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Huang, Yang, Wang, and
  Zhang]{li2019convergence}
X.~Li, K.~Huang, W.~Yang, S.~Wang, and Z.~Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}, Addis Ababa, Ethiopia, 2020{\natexlab{c}}.

\bibitem[Liang et~al.(2019)Liang, Shen, Liu, Pan, Chen, and
  Cheng]{liang2019variance}
X.~Liang, S.~Shen, J.~Liu, Z.~Pan, E.~Chen, and Y.~Cheng.
\newblock Variance reduced local sgd with lower communication complexity.
\newblock \emph{arXiv preprint arXiv:1912.12844}, 2019.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Y.~Lin, S.~Han, H.~Mao, Y.~Wang, and B.~Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2016communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 1273--1282, Fort
  Lauderdale, FL, 2017.

\bibitem[McMahan et~al.(2018)McMahan, Ramage, Talwar, and
  Zhang]{mcmahan2017learning}
H.~B. McMahan, D.~Ramage, K.~Talwar, and L.~Zhang.
\newblock Learning differentially private recurrent language models.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[Philippenko and Dieuleveut(2020)]{philippenko2020artemis}
C.~Philippenko and A.~Dieuleveut.
\newblock Artemis: tight convergence guarantees for bidirectional compression
  in federated learning.
\newblock \emph{arXiv preprint arXiv:2006.14591}, 2020.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Mokhtari, Hassani, Jadbabaie, and
  Pedarsani]{reisizadeh2020fedpaq}
A.~Reisizadeh, A.~Mokhtari, H.~Hassani, A.~Jadbabaie, and R.~Pedarsani.
\newblock Fedpaq: {A} communication-efficient federated learning method with
  periodic averaging and quantization.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics (AISTATS)}, pages 2021--2031, Online [Palermo, Sicily, Italy],
  2020.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica,
  Braverman, Gonzalez, and Arora]{rothchild2020fetchsgd}
D.~Rothchild, A.~Panda, E.~Ullah, N.~Ivkin, I.~Stoica, V.~Braverman,
  J.~Gonzalez, and R.~Arora.
\newblock {FetchSGD}: Communication-efficient federated learning with
  sketching.
\newblock \emph{arXiv preprint arXiv:2007.07682}, 2020.

\bibitem[Sahu et~al.(2018)Sahu, Li, Sanjabi, Zaheer, Talwalkar, and
  Smith]{sahu2018convergence}
A.~K. Sahu, T.~Li, M.~Sanjabi, M.~Zaheer, A.~Talwalkar, and V.~Smith.
\newblock On the convergence of federated optimization in heterogeneous
  networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, 2018.

\bibitem[Stich(2019)]{stich2019local}
S.~U. Stich.
\newblock Local sgd converges fast and communicates little.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations ({ICLR})}, New Orleans, LA, 2019.

\bibitem[Stich and Karimireddy(2019)]{stich2019error}
S.~U. Stich and S.~P. Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed communication.
\newblock \emph{arXiv preprint arXiv:1909.05350}, 2019.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 4447--4458, Montr{\'{e}}al, Canada, 2018.

\bibitem[Tang et~al.(2018)Tang, Gan, Zhang, Zhang, and
  Liu]{tang2018communication}
H.~Tang, S.~Gan, C.~Zhang, T.~Zhang, and J.~Liu.
\newblock Communication compression for decentralized training.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 7652--7662, Montr{\'{e}}al, Canada, 2018.

\bibitem[Tang et~al.(2019)Tang, Yu, Lian, Zhang, and
  Liu]{tang2019doublesqueeze}
H.~Tang, C.~Yu, X.~Lian, T.~Zhang, and J.~Liu.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock In \emph{International Conference on Machine Learning}, pages
  6155--6165. PMLR, 2019.

\bibitem[Wang and Joshi(2018)]{wang2018cooperative}
J.~Wang and G.~Joshi.
\newblock Cooperative sgd: A unified framework for the design and analysis of
  communication-efficient sgd algorithms.
\newblock \emph{arXiv preprint arXiv:1808.07576}, 2018.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pages 1509--1519, Long Beach, CA, 2017.

\bibitem[Wu et~al.(2018)Wu, Huang, Huang, and Zhang]{wu2018error}
J.~Wu, W.~Huang, J.~Huang, and T.~Zhang.
\newblock Error compensated quantized sgd and its applications to large-scale
  distributed optimization.
\newblock \emph{arXiv preprint arXiv:1806.08054}, 2018.

\bibitem[Yu et~al.(2019{\natexlab{a}})Yu, Jin, and Yang]{yu2019linear}
H.~Yu, R.~Jin, and S.~Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 7184--7193, Long Beach, CA, 2019{\natexlab{a}}.

\bibitem[Yu et~al.(2019{\natexlab{b}})Yu, Yang, and Zhu]{yu2019parallel}
H.~Yu, S.~Yang, and S.~Zhu.
\newblock Parallel restarted {SGD} with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{The Thirty-Third {AAAI} Conference on Artificial
  Intelligence (AAAI)}, pages 5693--5700, Honolulu, HI, 2019{\natexlab{b}}.

\bibitem[Zheng et~al.(2019)Zheng, Huang, and Kwok]{zheng2019communication}
S.~Zheng, Z.~Huang, and J.~T. Kwok.
\newblock Communication-efficient distributed blockwise momentum sgd with
  error-feedback.
\newblock \emph{arXiv preprint arXiv:1905.10936}, 2019.

\bibitem[Zhou and Cong(2018)]{zhou2018convergence}
F.~Zhou and G.~Cong.
\newblock On the convergence properties of a k-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence (IJCAI)}, pages 3219--3227, Stockholm,
  Sweden, 2018.

\end{thebibliography}
