\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{chilimbi2014project,mcmahan2017communication}
\citation{alistarh2017qsgd,lin2017deep,wangni2018gradient,stich2018sparsified,wang2018atomo,tang2019doublesqueeze}
\citation{aji2017sparse}
\citation{chen2010approximate,ge2013optimized,jegou2010product}
\citation{duchi2011dual}
\citation{lian2017can}
\citation{duchi2011adaptive}
\citation{kingma2014adam}
\citation{reddi2019convergence}
\citation{robbins1951stochastic}
\citation{reddi2020adaptive}
\citation{reddi2020adaptive}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{nazari2019dadam}
\citation{nazari2019dadam}
\citation{boyd2011distributed}
\citation{duchi2011dual}
\citation{nedic2009distributed}
\citation{shi2015extra}
\citation{di2016next}
\citation{hong2017prox}
\citation{lu2019gnsd}
\citation{koloskova2019decentralized}
\citation{lian2017can}
\citation{tang2018d}
\citation{assran2019stochastic}
\citation{nazari2019dadam}
\citation{reddi2019convergence}
\citation{duchi2011adaptive}
\citation{kingma2014adam}
\citation{reddi2019convergence}
\citation{ward2019adagrad}
\citation{li2019convergence}
\citation{chen2018convergence}
\citation{zhou2018convergence}
\citation{zou2018convergence}
\citation{agarwal2019efficient,luo2019adaptive,zaheer2018adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {2}Decentralized Adaptive Training and Divergence of DADAM}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Decentralized Adaptive Training and Divergence of DADAM}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Related Work}{2}{subsection.2.1}}
\citation{chen2018convergence,ward2019adagrad}
\citation{nazari2019dadam}
\citation{nedic2009distributed,yuan2016convergence}
\citation{nazari2019dadam}
\citation{yuan2016convergence}
\citation{yuan2016convergence}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Decentralized Optimization }{3}{subsection.2.2}}
\newlabel{eq:minproblem}{{1}{3}{Decentralized Optimization}{equation.2.1}{}}
\newlabel{a:diff}{{1}{3}{}{assumptionA.1}{}}
\newlabel{a:boundsto}{{2}{3}{}{assumptionA.2}{}}
\newlabel{a:boundedvar}{{3}{3}{}{assumptionA.3}{}}
\newlabel{a:matrixW}{{4}{3}{}{assumptionA.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DADAM (with N nodes)\relax }}{3}{algorithm.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg: dadam}{{1}{3}{DADAM (with N nodes)\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Divergence of DADAM}{3}{subsection.2.3}}
\newlabel{sec:divergence}{{2.3}{3}{Divergence of DADAM}{subsection.2.3}{}}
\newlabel{thm: dadam_diverge}{{1}{3}{}{theorem.1}{}}
\citation{nazari2019dadam}
\citation{nazari2019dadam}
\citation{reddi2020adaptive}
\citation{luo2019adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {3}On the Convergence of Decentralized Adaptive Gradient Methods}{4}{section.3}}
\newlabel{sec:main}{{3}{4}{On the Convergence of Decentralized Adaptive Gradient Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Importance and Difficulties of Consensus on Adaptive Learning Rates}{4}{subsection.3.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Decentralized Adaptive Gradient Method (with N nodes)\relax }}{4}{algorithm.2}}
\newlabel{alg: dadaptive}{{2}{4}{Decentralized Adaptive Gradient Method (with N nodes)\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Unifying Decentralized Adaptive Gradient Framework}{4}{subsection.3.2}}
\citation{chen2018convergence}
\citation{boyd2004fastest}
\citation{boyd2009fastest}
\newlabel{thm: dagm_converge}{{2}{5}{}{theorem.2}{}}
\newlabel{eq: thm11}{{2}{5}{}{equation.3.2}{}}
\@writefile{loe}{\contentsline {corollary}{\numberline {2.1}Corollary}{5}{corollary.2.1}}
\newlabel{corl: adm_convergence}{{2.1}{5}{}{corollary.2.1}{}}
\newlabel{eq: thm1}{{3}{5}{}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Application to AMSGrad algorithm}{5}{subsection.3.3}}
\newlabel{sec:amsgrad}{{3.3}{5}{Application to AMSGrad algorithm}{subsection.3.3}{}}
\citation{yan2018unified,chen2018convergence}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Decentralized AMSGrad (N nodes)\relax }}{6}{algorithm.3}}
\newlabel{alg: damsgrad}{{3}{6}{Decentralized AMSGrad (N nodes)\relax }{algorithm.3}{}}
\newlabel{thm: dams_converge}{{3}{6}{}{theorem.3}{}}
\newlabel{eq: exp_telescope_sketchmain}{{5}{6}{Application to AMSGrad algorithm}{equation.3.5}{}}
\citation{duchi2011adaptive}
\citation{lian2017can}
\citation{lecun1998mnist}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Application to AdaGrad algorithm}{7}{subsection.3.4}}
\newlabel{sec:adagrad}{{3.4}{7}{Application to AdaGrad algorithm}{subsection.3.4}{}}
\newlabel{thm: dadagrad_converge}{{4}{7}{}{theorem.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Decentralized AdaGrad (with N nodes)\relax }}{7}{algorithm.4}}
\newlabel{alg: dadagrad}{{4}{7}{Decentralized AdaGrad (with N nodes)\relax }{algorithm.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{7}{section.4}}
\newlabel{sec:numerical}{{4}{7}{Numerical Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Effect of heterogeneity}{8}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training loss and Testing accuracy for homogeneous (top row) and heterogeneous data (bottow row)\relax }}{8}{figure.caption.2}}
\newlabel{fig: homo_data}{{1}{8}{Training loss and Testing accuracy for homogeneous (top row) and heterogeneous data (bottow row)\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sensitivity to the Learning Rate}{8}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training loss (left) and testing accuracy (right) comparison of different stepsizes for various methods. Top row: DP-SGD. {Middle row:} DADAM. {Bottom row:} DAMS. \textbf  {Left panel:} 1 local epoch. \textbf  {Right panel:} 3 local epochs. \relax }}{8}{figure.caption.3}}
\newlabel{fig: stepsize}{{2}{8}{Training loss (left) and testing accuracy (right) comparison of different stepsizes for various methods. Top row: DP-SGD. {Middle row:} DADAM. {Bottom row:} DAMS. \textbf {Left panel:} 1 local epoch. \textbf {Right panel:} 3 local epochs. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}}
\newlabel{sec:conclusion}{{5}{8}{Conclusion}{section.5}{}}
\bibstyle{plain}
\bibdata{reference}
\bibcite{agarwal2019efficient}{{1}{}{{}}{{}}}
\bibcite{aji2017sparse}{{2}{}{{}}{{}}}
\bibcite{alistarh2017qsgd}{{3}{}{{}}{{}}}
\bibcite{assran2019stochastic}{{4}{}{{}}{{}}}
\bibcite{boyd2009fastest}{{5}{}{{}}{{}}}
\bibcite{boyd2004fastest}{{6}{}{{}}{{}}}
\bibcite{boyd2011distributed}{{7}{}{{}}{{}}}
\bibcite{chen2018convergence}{{8}{}{{}}{{}}}
\bibcite{chen2010approximate}{{9}{}{{}}{{}}}
\bibcite{chilimbi2014project}{{10}{}{{}}{{}}}
\bibcite{duchi2011dual}{{11}{}{{}}{{}}}
\bibcite{duchi2011adaptive}{{12}{}{{}}{{}}}
\bibcite{ge2013optimized}{{13}{}{{}}{{}}}
\bibcite{hong2017prox}{{14}{}{{}}{{}}}
\bibcite{jegou2010product}{{15}{}{{}}{{}}}
\bibcite{kingma2014adam}{{16}{}{{}}{{}}}
\bibcite{koloskova2019decentralized}{{17}{}{{}}{{}}}
\bibcite{lecun1998mnist}{{18}{}{{}}{{}}}
\bibcite{li2019convergence}{{19}{}{{}}{{}}}
\bibcite{lian2017can}{{20}{}{{}}{{}}}
\bibcite{lin2017deep}{{21}{}{{}}{{}}}
\bibcite{di2016next}{{22}{}{{}}{{}}}
\bibcite{lu2019gnsd}{{23}{}{{}}{{}}}
\bibcite{luo2019adaptive}{{24}{}{{}}{{}}}
\bibcite{mcmahan2017communication}{{25}{}{{}}{{}}}
\bibcite{nazari2019dadam}{{26}{}{{}}{{}}}
\bibcite{nedic2009distributed}{{27}{}{{}}{{}}}
\bibcite{reddi2020adaptive}{{28}{}{{}}{{}}}
\bibcite{reddi2019convergence}{{29}{}{{}}{{}}}
\bibcite{robbins1951stochastic}{{30}{}{{}}{{}}}
\bibcite{shi2015extra}{{31}{}{{}}{{}}}
\bibcite{stich2018sparsified}{{32}{}{{}}{{}}}
\bibcite{tang2018d}{{33}{}{{}}{{}}}
\bibcite{tang2019doublesqueeze}{{34}{}{{}}{{}}}
\bibcite{wang2018atomo}{{35}{}{{}}{{}}}
\bibcite{wangni2018gradient}{{36}{}{{}}{{}}}
\bibcite{ward2019adagrad}{{37}{}{{}}{{}}}
\bibcite{yan2018unified}{{38}{}{{}}{{}}}
\bibcite{yuan2016convergence}{{39}{}{{}}{{}}}
\bibcite{zaheer2018adaptive}{{40}{}{{}}{{}}}
\bibcite{zhou2018convergence}{{41}{}{{}}{{}}}
\bibcite{zou2018convergence}{{42}{}{{}}{{}}}
\citation{yan2018unified,chen2018convergence}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Auxiliary Lemmas}{12}{appendix.A}}
\newlabel{app: proof_lemmas}{{A}{12}{Proof of Auxiliary Lemmas}{appendix.A}{}}
\newlabel{eq: seq_z_sketchapp}{{7}{12}{Proof of Auxiliary Lemmas}{equation.A.7}{}}
\newlabel{lem: z_diff}{{A.1}{12}{}{lemma.1}{}}
\newlabel{lem: mean_after_max}{{A.2}{13}{}{lemma.2}{}}
\newlabel{eq: r_decrease}{{8}{13}{}{equation.A.8}{}}
\newlabel{eq: r_reduce}{{9}{13}{}{equation.A.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Theorem \ref  {thm: dagm_converge}}{14}{appendix.B}}
\newlabel{app: proof_thm_adm}{{B}{14}{Proof of Theorem \ref {thm: dagm_converge}}{appendix.B}{}}
\newlabel{eq: seq_z}{{10}{14}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.10}{}}
\newlabel{eq: exp_lip}{{11}{14}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.11}{}}
\newlabel{eq: u_to_u_bar}{{13}{14}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.13}{}}
\newlabel{eq: split_1}{{14}{15}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.14}{}}
\newlabel{eq: exp_telescope}{{15}{15}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.15}{}}
\newlabel{eq: T_3_bound_first}{{17}{16}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.17}{}}
\newlabel{eq: update_X}{{18}{16}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.18}{}}
\newlabel{eq: t2_matrix}{{19}{16}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.19}{}}
\newlabel{eq: update_x_decom}{{20}{16}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.20}{}}
\newlabel{eq: x_ql}{{21}{16}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.21}{}}
\newlabel{eq: T_5_bound}{{22}{16}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.22}{}}
\newlabel{eq:T_2_bound}{{24}{17}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.24}{}}
\newlabel{eq: T_6_bound}{{25}{18}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.25}{}}
\newlabel{eq: T_1}{{26}{18}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.26}{}}
\newlabel{eq: diff_u_t}{{27}{19}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.27}{}}
\newlabel{eq: split_var}{{30}{20}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.30}{}}
\newlabel{eq: variance_bound_1}{{31}{21}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.31}{}}
\newlabel{eq: diff_u}{{32}{21}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.32}{}}
\newlabel{eq: diff_g}{{33}{22}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.33}{}}
\newlabel{eq: final_bound}{{34}{22}{Proof of Theorem \ref {thm: dagm_converge}}{equation.B.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proof of Theorem \ref  {thm: dams_converge}}{25}{appendix.C}}
\newlabel{app: proof_ams}{{C}{25}{Proof of Theorem \ref {thm: dams_converge}}{appendix.C}{}}
\newlabel{eq: rep_thm1}{{36}{25}{Proof of Theorem \ref {thm: dams_converge}}{equation.C.36}{}}
\newlabel{eq: sub_thm1}{{37}{25}{Proof of Theorem \ref {thm: dams_converge}}{equation.C.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proof of Theorem \ref  {thm: dadagrad_converge}}{26}{appendix.D}}
\newlabel{app: proof_adagrad}{{D}{26}{Proof of Theorem \ref {thm: dadagrad_converge}}{appendix.D}{}}
\newlabel{eq: rep_thm1bis}{{39}{26}{Proof of Theorem \ref {thm: dadagrad_converge}}{equation.D.39}{}}
\newlabel{eq: sub_thm1bis}{{40}{27}{Proof of Theorem \ref {thm: dadagrad_converge}}{equation.D.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Additional Experiments and Details}{28}{appendix.E}}
\newlabel{app: experiments}{{E}{28}{Additional Experiments and Details}{appendix.E}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance comparison of different stepsizes for D-PSGD\relax }}{29}{figure.caption.5}}
\newlabel{fig: sgd_curve}{{3}{29}{Performance comparison of different stepsizes for D-PSGD\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance comparison of different stepsizes for decentralized AMSGrad\relax }}{29}{figure.caption.5}}
\newlabel{fig: amsgrad_curve}{{4}{29}{Performance comparison of different stepsizes for decentralized AMSGrad\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance comparison of different stepsizes for DADAM\relax }}{29}{figure.caption.5}}
\newlabel{fig: adam_curve}{{5}{29}{Performance comparison of different stepsizes for DADAM\relax }{figure.caption.5}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {F}Improvements Made Over Last Submission}{30}{appendix.F}}
