\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{madigan1995bayesian}
\citation{hruschka2007bayesian}
\citation{koller2007graphical}
\citation{bilmes2005graphical}
\citation{shwe1990probabilistic}
\citation{jordan1999graphical}
\citation{sanner2012symbolic}
\citation{kahle2008junction}
\citation{jordan1999introduction}
\citation{hoffman2013stochastic}
\citation{kingma2013auto}
\citation{liu2016stein}
\citation{xing2012generalized}
\citation{bishop2003vibes}
\citation{winn2005variational}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kingma2013auto}
\citation{kingma2018glow}
\citation{rezende2015variational}
\citation{tabak2010density}
\citation{Dinh2016DensityEU}
\citation{rippel2013high}
\citation{rezende2015variational}
\citation{Dinh2016DensityEU}
\citation{dinh2014nice}
\citation{de2020block}
\citation{ho2019flow++}
\citation{papamakarios2019normalizing}
\citation{rezende2015variational}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\newlabel{eq:vi_elbo}{{1}{2}{Preliminaries}{equation.2.1}{}}
\newlabel{eq:vae_recon}{{2}{2}{Preliminaries}{equation.2.2}{}}
\newlabel{eq:flow}{{3}{2}{Preliminaries}{equation.2.3}{}}
\citation{rezende2015variational}
\citation{berg2018sylvester}
\citation{kingma2013auto}
\citation{rezende2014stochastic}
\newlabel{eq:flow2}{{4}{3}{Preliminaries}{equation.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variational Flow Graphical Model}{3}{section.3}}
\newlabel{sec:main}{{3}{3}{Variational Flow Graphical Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evidence Lower Bound of Variational Flow Graphical Models}{3}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  (Left) Node $\mathbf  {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf  {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation\nobreakspace  {}nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. Thin lines are identity functions, and thick lines are flow functions. \relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tree}{{1}{3}{\small (Left) Node $\mathbf {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation~nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. Thin lines are identity functions, and thick lines are flow functions. \relax }{figure.caption.2}{}}
\newlabel{eq:posterior}{{5}{3}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.5}{}}
\citation{kingma2013auto}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Forward and backward message passing to generate each node's hidden variable. Forward message passing approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions. \relax }}{4}{figure.caption.3}}
\newlabel{fig:tree_message}{{2}{4}{Forward and backward message passing to generate each node's hidden variable. Forward message passing approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions. \relax }{figure.caption.3}{}}
\newlabel{eq:elbo}{{6}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.6}{}}
\newlabel{eq:kl}{{7}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.7}{}}
\newlabel{eq:elbo_dag}{{8}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ELBO Calculation}{4}{subsection.3.2}}
\newlabel{eq:post_smp}{{9}{4}{ELBO Calculation}{equation.3.9}{}}
\newlabel{eq:prior_smp}{{10}{4}{ELBO Calculation}{equation.3.10}{}}
\newlabel{eq:KL_l}{{11}{4}{ELBO Calculation}{equation.3.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3} Aggregation Nodes}{5}{subsection.3.3}}
\newlabel{sec:node_aggr}{{3.3}{5}{Aggregation Nodes}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (Left) Aggregation node $\mathbf  {h}^{l+1,1}$ has three children, $\mathbf  {h}^{l,1}$, $\mathbf  {h}^{l,2}$, and $\mathbf  {h}^{l,3}$. (Right) A VFG model with one aggregation node, $\mathbf  {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }}{5}{figure.caption.4}}
\newlabel{fig:node_aggre}{{3}{5}{(Left) Aggregation node $\mathbf {h}^{l+1,1}$ has three children, $\mathbf {h}^{l,1}$, $\mathbf {h}^{l,2}$, and $\mathbf {h}^{l,3}$. (Right) A VFG model with one aggregation node, $\mathbf {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }{figure.caption.4}{}}
\newlabel{eq:one_agg_node}{{12}{5}{Aggregation Nodes}{equation.3.12}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inference model parameters with forward and backward message propagation\relax }}{5}{algorithm.1}}
\newlabel{alg:main}{{1}{5}{Inference model parameters with forward and backward message propagation\relax }{algorithm.1}{}}
\newlabel{line:for2}{{4}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.4}{}}
\newlabel{line:forward}{{6}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.6}{}}
\newlabel{line:backward}{{11}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.11}{}}
\newlabel{line:update}{{15}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Algorithms and Implementation}{5}{section.4}}
\newlabel{sec:algrithm}{{4}{5}{Algorithms and Implementation}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\relax \fontsize  {9}{10pt}\selectfont  (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }}{6}{figure.caption.5}}
\newlabel{fig:two_layer_infer}{{4}{6}{{\small (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Layer-wise Training}{6}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Random Masking}{6}{subsection.4.2}}
\newlabel{eq:elbo_tree_mask}{{15}{6}{Random Masking}{equation.4.15}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Inference model parameters with random masking\relax }}{6}{algorithm.2}}
\newlabel{alg:rand_mask}{{2}{6}{Inference model parameters with random masking\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Inference on VFG Models }{6}{section.5}}
\newlabel{sec:infer}{{5}{6}{Inference on VFG Models}{section.5}{}}
\newlabel{eq:aggr_obs_ch}{{17}{6}{Inference on VFG Models}{equation.5.17}{}}
\citation{bengio2013representation}
\citation{Dinh2016DensityEU}
\newlabel{lm:apprx}{{1}{7}{}{lemma.1}{}}
\newlabel{rmk:apprx_mul}{{1}{7}{}{remark.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Experiments}{7}{section.6}}
\newlabel{sec:numerical}{{6}{7}{Numerical Experiments}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Evaluation on Inference with Missing Entries Imputation}{7}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }}{7}{figure.caption.6}}
\newlabel{fig:sim}{{5}{7}{Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }{figure.caption.6}{}}
\citation{berg2018sylvester}
\citation{berg2018sylvester}
\citation{kingma2013auto}
\citation{rezende2015variational}
\citation{kingma2016improving}
\citation{berg2018sylvester}
\citation{Lecunmnist2010}
\citation{Sorrenson2020}
\citation{maaten2008visualizing}
\bibdata{ref}
\bibcite{bengio2013representation}{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ELBO and Likelihood}{8}{subsection.6.2}}
\newlabel{sec:exp:elbo}{{6.2}{8}{ELBO and Likelihood}{subsection.6.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }}{8}{table.1}}
\newlabel{tab:elbo}{{1}{8}{Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Latent Representation Learning on MNIST}{8}{subsection.6.3}}
\newlabel{sec:exp:mnist}{{6.3}{8}{Latent Representation Learning on MNIST}{subsection.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }}{8}{figure.caption.7}}
\newlabel{fig:reconst}{{6}{8}{(Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The tree structure for MNIST.\relax }}{8}{figure.caption.8}}
\newlabel{fig:struct}{{7}{8}{The tree structure for MNIST.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }}{8}{figure.caption.9}}
\newlabel{fig:z_tsne}{{8}{8}{MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}}
\newlabel{sec:conclusion}{{7}{8}{Conclusion}{section.7}{}}
\bibcite{berg2018sylvester}{2}
\bibcite{bilmes2005graphical}{3}
\bibcite{bishop2003vibes}{4}
\bibcite{de2020block}{5}
\bibcite{dinh2014nice}{6}
\bibcite{Dinh2016DensityEU}{7}
\bibcite{efron1975defining}{8}
\bibcite{ho2019flow++}{9}
\bibcite{hoffman2013stochastic}{10}
\bibcite{hruschka2007bayesian}{11}
\bibcite{jordan1999graphical}{12}
\bibcite{jordan1999introduction}{13}
\bibcite{kahle2008junction}{14}
\bibcite{Khemakhem20a}{15}
\bibcite{kingma2018glow}{16}
\bibcite{kingma2016improving}{17}
\bibcite{kingma2013auto}{18}
\bibcite{koller2007graphical}{19}
\bibcite{Lecunmnist2010}{20}
\bibcite{liu2016stein}{21}
\bibcite{maaten2008visualizing}{22}
\bibcite{madigan1995bayesian}{23}
\bibcite{papamakarios2019normalizing}{24}
\bibcite{rezende2015variational}{25}
\bibcite{rezende2014stochastic}{26}
\bibcite{rippel2013high}{27}
\bibcite{sanner2012symbolic}{28}
\bibcite{shwe1990probabilistic}{29}
\bibcite{Sorrenson2020}{30}
\bibcite{tabak2010density}{31}
\bibcite{winn2005variational}{32}
\bibcite{xing2012generalized}{33}
\bibstyle{abbrv}
\citation{Dinh2016DensityEU}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Numerical Experiments}{11}{appendix.A}}
\newlabel{sec:exp_supp}{{A}{11}{Additional Numerical Experiments}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}California Housing Dataset}{11}{subsection.A.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }}{11}{table.caption.11}}
\newlabel{tab:imp_arrhytmia}{{2}{11}{California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Representation Learning with MNIST}{11}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.1}Latent Representation Learning on MNIST}{11}{subsubsection.A.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }}{12}{figure.caption.12}}
\newlabel{fig:z_no_Y}{{9}{12}{MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }{figure.caption.12}{}}
\citation{berg2018sylvester}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.2}Latent structure learning on MNIST}{13}{subsubsection.A.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces MNIST: Increasing each latent variable from a small value to a larger one.\relax }}{13}{figure.caption.13}}
\newlabel{fig:mnist_dis}{{10}{13}{MNIST: Increasing each latent variable from a small value to a larger one.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}ELBO and Likelihood}{13}{subsection.A.3}}
\@writefile{toc}{\contentsline {section}{\numberline {B}ELBO Calculation}{14}{appendix.B}}
\newlabel{sec:elbo_cal_supp}{{B}{14}{ELBO Calculation}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  The recognition model consists of froward message from data to approximate the posterior distributions; the generative model is realized by backward message from the root node and generates the reconstruction in each layer. \relax }}{14}{figure.caption.14}}
\newlabel{fig:tree_message2}{{11}{14}{The recognition model consists of froward message from data to approximate the posterior distributions; the generative model is realized by backward message from the root node and generates the reconstruction in each layer. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Distributions of Latent Variables}{14}{subsection.B.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Generative Model}{14}{subsubsection.B.1.1}}
\newlabel{sec:generative}{{B.1.1}{14}{Generative Model}{subsubsection.B.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}Recognition Model}{14}{subsubsection.B.1.2}}
\newlabel{eq:posteriorapp}{{18}{14}{Recognition Model}{equation.B.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}$\mathbf  {KL}$ Term}{15}{subsection.B.2}}
\newlabel{eq:KL_lapp}{{19}{15}{$\mathbf {KL}$ Term}{equation.B.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Reconstruction Term}{15}{subsection.B.3}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Aggregation Node}{15}{appendix.C}}
\newlabel{sec:aggr_supp}{{C}{15}{Aggregation Node}{appendix.C}{}}
\newlabel{eq:child_avg}{{20}{15}{Aggregation Node}{equation.C.20}{}}
\newlabel{eq:parent_avg}{{21}{15}{Aggregation Node}{equation.C.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  {\relax \fontsize  {9}{10pt}\selectfont  Aggregation node on a DAG.}\relax }}{16}{figure.caption.15}}
\newlabel{fig:dag_aggr}{{12}{16}{{\small Aggregation node on a DAG.}\relax }{figure.caption.15}{}}
\newlabel{eq:i_child}{{22}{16}{Aggregation Node}{equation.C.22}{}}
\newlabel{eq:i_parent}{{23}{16}{Aggregation Node}{equation.C.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}More Details on Inference}{16}{appendix.D}}
\newlabel{sec:infer_supp}{{D}{16}{More Details on Inference}{appendix.D}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Derivation of the ELBOs for Trees and DAGs }{17}{appendix.E}}
\newlabel{sec:ebl_deri}{{E}{17}{Derivation of the ELBOs for Trees and DAGs}{appendix.E}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}ELBO of Tree Models}{17}{subsection.E.1}}
\newlabel{appd:tree_elbo}{{E.1}{17}{ELBO of Tree Models}{subsection.E.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A tree VFG with $L=5$ and three aggregation nodes.\relax }}{17}{figure.caption.16}}
\newlabel{fig:tree_vfg}{{13}{17}{A tree VFG with $L=5$ and three aggregation nodes.\relax }{figure.caption.16}{}}
\newlabel{eq:prior}{{24}{17}{ELBO of Tree Models}{equation.E.24}{}}
\newlabel{eq:posterior2}{{25}{17}{ELBO of Tree Models}{equation.E.25}{}}
\newlabel{eq:chain_post}{{26}{18}{ELBO of Tree Models}{equation.E.26}{}}
\newlabel{eq:chain_prior}{{27}{18}{ELBO of Tree Models}{equation.E.27}{}}
\newlabel{eq:elbo12L}{{28}{18}{ELBO of Tree Models}{equation.E.28}{}}
\newlabel{eq:kl_lL}{{29}{18}{ELBO of Tree Models}{equation.E.29}{}}
\newlabel{eq:elbo0}{{30}{18}{ELBO of Tree Models}{equation.E.30}{}}
\newlabel{eq:kl_l}{{31}{18}{ELBO of Tree Models}{equation.E.31}{}}
\newlabel{eq:elbo1}{{32}{18}{ELBO of Tree Models}{equation.E.32}{}}
\citation{rezende2015variational}
\citation{kingma2016improving}
\citation{berg2018sylvester}
\citation{rezende2015variational}
\citation{kingma2016improving}
\citation{berg2018sylvester}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Improve ELBO Estimation with Flows}{19}{subsection.E.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}ELBO of DAG Models}{19}{subsection.E.3}}
\newlabel{appd:dag_elbo}{{E.3}{19}{ELBO of DAG Models}{subsection.E.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces A DAG with inverse topology order {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \{\vcenter to\@ne \big@size {}\right .$}\box \z@ } \{1,2,3\}, \{4,5\}, \{6\}, \{7\} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \}\vcenter to\@ne \big@size {}\right .$}\box \z@ }, and they correspond to layers 0 to 3. \relax }}{19}{figure.caption.17}}
\newlabel{fig:dag}{{14}{19}{A DAG with inverse topology order \big \{ \{1,2,3\}, \{4,5\}, \{6\}, \{7\} \big \}, and they correspond to layers 0 to 3. \relax }{figure.caption.17}{}}
\newlabel{eq:dag_elbo}{{33}{19}{ELBO of DAG Models}{equation.E.33}{}}
\citation{Khemakhem20a}
\citation{Sorrenson2020}
\citation{efron1975defining}
\newlabel{eq:dag_chain_q}{{34}{20}{ELBO of DAG Models}{equation.E.34}{}}
\newlabel{eq:dag_chain_p}{{35}{20}{ELBO of DAG Models}{equation.E.35}{}}
\newlabel{eq:dag_kl_lL}{{36}{20}{ELBO of DAG Models}{equation.E.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Theoretical Justifications for Latent Representation Learning}{20}{appendix.F}}
\newlabel{sec:repr_theory}{{F}{20}{Theoretical Justifications for Latent Representation Learning}{appendix.F}{}}
\newlabel{eq:exp_h}{{37}{20}{Theoretical Justifications for Latent Representation Learning}{equation.F.37}{}}
\newlabel{eq:xt_gen}{{38}{20}{Theoretical Justifications for Latent Representation Learning}{equation.F.38}{}}
\citation{Khemakhem20a}
\newlabel{eq:u_diff}{{39}{21}{Theoretical Justifications for Latent Representation Learning}{equation.F.39}{}}
\newlabel{eq:A_sim}{{40}{21}{Theoretical Justifications for Latent Representation Learning}{equation.F.40}{}}
