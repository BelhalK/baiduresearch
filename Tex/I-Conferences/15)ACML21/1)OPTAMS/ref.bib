@inproceedings{mcmahan2017communication,
  author    = {Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Seth Hampson and
               Blaise Ag{\"{u}}era y Arcas},
  title     = {Communication-Efficient Learning of Deep Networks from Decentralized
               Data},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence
               and Statistics (AISTATS)},
  address   = {Fort Lauderdale,
               FL},
  pages     = {1273--1282},
  year      = {2017}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@inproceedings{lin2017deep,
  author    = {Yujun Lin and
               Song Han and
               Huizi Mao and
               Yu Wang and
               Bill Dally},
  title     = {Deep Gradient Compression: Reducing the Communication Bandwidth for
               Distributed Training},
  booktitle = {Proceedings of the 6th International Conference on Learning Representations (ICLR)},
  address   = {
               Vancouver, Canada},
  year      = {2018}
}

@article{chen2021convergent,
  title={On the Convergence of Decentralized Adaptive Gradient Methods},
  author={Chen, Xiangyi and Karimi, Belhal and Zhao, Weijie and Li, Ping},
   journal={arXiv preprint arXiv:2109.03194},
  year={2021}
}


@inproceedings{alistarh2017qsgd,
  author    = {Dan Alistarh and
               Demjan Grubic and
               Jerry Li and
               Ryota Tomioka and
               Milan Vojnovic},
  title     = {{QSGD:} Communication-Efficient {SGD} via Gradient Quantization and
               Encoding},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  address   = {
               Long Beach, CA},
  pages     = {1709--1720},
  year      = {2017}
}

@inproceedings{wangni2018gradient,
  author    = {Jianqiao Wangni and
               Jialei Wang and
               Ji Liu and
               Tong Zhang},
  title     = {Gradient Sparsification for Communication-Efficient Distributed Optimization},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  address   = {Montr{\'{e}}al, Canada},
  pages     = {1306--1316},
  year      = {2018}
}

@inproceedings{chen2020toward,
  author    = {Xiangyi Chen and
               Xiaoyun Li and
               Ping Li},
  title     = {Toward Communication Efficient Adaptive Gradient Method},
  booktitle = {Proceedings of the {ACM-IMS} Foundations of Data Science Conference (FODS)},
  address   = {Virtual
               Event, USA},
  pages     = {119--128},
  year      = {2020}
}
@inproceedings{you2019large,
  author    = {Yang You and
               Jing Li and
               Sashank J. Reddi and
               Jonathan Hseu and
               Sanjiv Kumar and
               Srinadh Bhojanapalli and
               Xiaodan Song and
               James Demmel and
               Kurt Keutzer and
               Cho{-}Jui Hsieh},
  title     = {Large Batch Optimization for Deep Learning: Training {BERT} in 76
               minutes},
  booktitle = {Proceedings of the 8th International Conference on Learning Representations (ICLR)},
  address   = {Addis Ababa, Ethiopia},
  year      = {2020}
}


@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}


@ARTICLE{RKK18,
  title={On the Convergence of Adam and Beyond },
  author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  journal={ICLR},
  year={2018}
}

@inproceedings{KB15,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  address   = {
               San Diego, CA},
  year      = {2015}
}
@ARTICLE{TH12,
  title = {RmsProp: Divide the gradient by a running average of its recent magnitude},
  author    = {T. Tieleman and G. Hinton},
  journal = {COURSERA: Neural Networks for Machine Learning},
  year = {2012}
}


@ARTICLE{Z12,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}


@ARTICLE{DHS11,
  author    = {John C. Duchi and
               Elad Hazan and
               Yoram Singer},
  title     = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal   = {J. Mach. Learn. Res.},
  volume    = {12},
  pages     = {2121--2159},
  year      = {2011}
}


@ARTICLE{N04,
    title = {Introductory Lectures on Convex Optimization:
A Basic Course},
    Author = {Yurii Nesterov},
    journal = {Springer},
    year = {2004},
}


@ARTICLE{P64,
  title = {Some methods of speeding up the convergence of iteration methods},
  author    = {B. T. Polyak},
  journal = {Mathematics and Mathematical Physics},
  year = {1964}
}



@inproceedings{li2014scaling,
  author    = {Mu Li and
               David G. Andersen and
               Jun Woo Park and
               Alexander J. Smola and
               Amr Ahmed and
               Vanja Josifovski and
               James Long and
               Eugene J. Shekita and
               Bor{-}Yiing Su},
  title     = {Scaling Distributed Machine Learning with the Parameter Server},
  booktitle = {Proceedings of the 11th {USENIX} Symposium on Operating Systems Design and Implementation (OSDI)},
  address   = {Broomfield, CO},
  pages     = {583--598},
  year      = {2014}
}

@inproceedings{zhao2020distributed,
  author    = {Weijie Zhao and
               Deping Xie and
               Ronglai Jia and
               Yulei Qian and
               Ruiquan Ding and
               Mingming Sun and
               Ping Li},
  title     = {Distributed Hierarchical {GPU} Parameter Server for Massive Scale
               Deep Learning Ads Systems},
  booktitle = {Proceedings of Machine Learning and Systems (MLSys)},
  address   = {Austin, TX},
  year      = {2020}
}

@article{recht2011hogwild,
  title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={693--701},
  year={2011}
}



@inproceedings{zhou2017convergence,
  author    = {Fan Zhou and
               Guojing Cong},
  title     = {On the Convergence Properties of a K-step Averaging Stochastic Gradient
               Descent Algorithm for Nonconvex Optimization},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence (IJCAI)},
  address   = {Stockholm,
               Sweden},
  pages     = {3219--3227},
  year      = {2018}
}

@inproceedings{stich2018local,
  author    = {Sebastian U. Stich},
  title     = {Local {SGD} Converges Fast and Communicates Little},
  booktitle = {Proceedings of the 7th International Conference on Learning Representations (ICLR)},
  address   = {
               New Orleans, LA},
  year      = {2019}
}

@inproceedings{yu2019linear,
  author    = {Hao Yu and
               Rong Jin and
               Sen Yang},
  title     = {On the Linear Speedup Analysis of Communication Efficient Momentum
               {SGD} for Distributed Non-Convex Optimization},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  address   = {Long Beach, CA},
  pages     = {7184--7193},
  year      = {2019}
}
@inproceedings{Proc:He-resnet16,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the 2016 {IEEE} Conference on Computer Vision and Pattern Recognition (CVPR)},
  address   = {Las Vegas, NV},
  pages     = {770--778},
  year      = {2016}
}

@article{ghadimi2013stochastic,
  author    = {Saeed Ghadimi and
               Guanghui Lan},
  title     = {Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic
               Programming},
  journal   = {{SIAM} J. Optim.},
  volume    = {23},
  number    = {4},
  pages     = {2341--2368},
  year      = {2013}
}
	
@article{karimireddy2019scaffold,
  title={SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:1910.06378},
  year={2019}
}

@inproceedings{reddi2020adaptive,
  author    = {Sashank J. Reddi and
               Zachary Charles and
               Manzil Zaheer and
               Zachary Garrett and
               Keith Rush and
               Jakub Kone{\v{c}}n{\'y} and
               Sanjiv Kumar and
               Hugh Brendan McMahan},
  title     = {Adaptive Federated Optimization},
  booktitle = {Proceedings of the 9th International Conference on Learning Representations (ICLR)},
  address   = {
               Virtual Event, Austria},
  year      = {2021}
}

@article{horvath2019stochastic,
  title={Stochastic distributed learning with gradient quantization and variance reduction},
  author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1904.05115},
  year={2019}
}

@article{li2019federated,
  author    = {Tian Li and
               Anit Kumar Sahu and
               Ameet Talwalkar and
               Virginia Smith},
  title     = {Federated Learning: Challenges, Methods, and Future Directions},
  journal   = {{IEEE} Signal Process. Mag.},
  volume    = {37},
  number    = {3},
  pages     = {50--60},
  year      = {2020}
}


@article{liang2019variance,
  title={Variance Reduced Local SGD with Lower Communication Complexity},
  author={Liang, Xianfeng and Shen, Shuheng and Liu, Jingchang and Pan, Zhen and Chen, Enhong and Cheng, Yifei},
  journal={arXiv preprint arXiv:1912.12844},
  year={2019}
}

@inproceedings{haddadpour2020federated,
  author    = {Farzin Haddadpour and
               Mohammad Mahdi Kamani and
               Aryan Mokhtari and
               Mehrdad Mahdavi},
  title     = {Federated Learning with Compression: Unified Analysis and Sharp Guarantees},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  address   = {Virtual Event},
  pages     = {2350--2358},
  year      = {2021}
}
@article{haddadpour2020fedsketch,
  title={Fedsketch: Communication-efficient and private federated learning via sketching},
  author={Haddadpour, Farzin and Karimi, Belhal and Li, Ping and Li, Xiaoyun},
  journal={arXiv preprint arXiv:2008.04975},
  year={2020}
}

@inproceedings{ivkin2019communication,
  author    = {Nikita Ivkin and
               Daniel Rothchild and
               Enayat Ullah and
               Vladimir Braverman and
               Ion Stoica and
               Raman Arora},
  title     = {Communication-efficient Distributed {SGD} with Sketching},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  address   = {Vancouver,  Canada},
  pages     = {13144--13154},
  year      = {2019}
}

@article{li2019privacy,
  title={Privacy for Free: Communication-Efficient Learning with Differential Privacy Using Sketches},
  author={Li, Tian and Liu, Zaoxing and Sekar, Vyas and Smith, Virginia},
  journal={arXiv preprint arXiv:1911.00972},
  year={2019}
}

@article{lecun1998mnist,
	Author = {LeCun, Yann},
	Journal = {http://yann. lecun. com/exdb/mnist/},
	Title = {The MNIST database of handwritten digits},
	Year = {1998}}
	
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex},
  journal={Master's thesis, Department of Computer Science, University of Toronto},
  year={2009}
}

@article{Arxiv:Zhou_18,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}

@inproceedings{mcmahan2010adaptive,
  author    = {H. Brendan McMahan and
               Matthew J. Streeter},
  title     = {Adaptive Bound Optimization for Online Convex Optimization},
  booktitle = {Proceedings of the 23rd Conference on Learning Theory (COLT)}, 
  address   = {Haifa, Israel},
  pages     = {244--256},
  year      = {2010}
}

@article{wang2019optimistic,
  title={An optimistic acceleration of amsgrad for nonconvex optimization},
  author={Wang, Jun-Kun and Li, Xiaoyun and Karimi, Belhal and Li, Ping},
  journal={arXiv preprint arXiv:1903.01435},
  year={2019}
}

@inproceedings{zhou2020towards,
  author    = {Yingxue Zhou and
               Belhal Karimi and
               Jinxing Yu and
               Zhiqiang Xu and
               Ping Li},
  title     = {Towards Better Generalization of Adaptive Gradient Methods},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  address  = {virtual},
  year      = {2020}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}



@article{tieleman2012rmsprop,
  title={Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA Neural Networks Mach. Learn},
  year={2012}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}


@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}


@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{GMH13,
  title={Speech recognition with deep recurrent neural networks},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6645--6649},
  year={2013},
  organization={Ieee}
}

@article{MS10,
title={Adaptive bound optimization for online convex optimization},
  author={McMahan, H Brendan and Streeter, Matthew},
  journal={arXiv preprint arXiv:1002.4908},
  year={2010}
}


@inproceedings{CJ12,
  title={Online optimization with gradual variations},
  author={Chiang, Chao-Kai and Yang, Tianbao and Lee, Chia-Jung and Mahdavi, Mehrdad and Lu, Chi-Jen and Jin, Rong and Zhu, Shenghuo},
  booktitle={Conference on Learning Theory},
  pages={6--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}



@article{RS13b,
  title={Optimization, learning, and games with predictable sequences},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1311.1869},
  year={2013}
}

@article{SALS15,
 title={Fast convergence of regularized learning in games},
  author={Syrgkanis, Vasilis and Agarwal, Alekh and Luo, Haipeng and Schapire, Robert E},
  journal={arXiv preprint arXiv:1507.00407},
  year={2015}
}


@inproceedings{ALLW18,
  title={Faster rates for convex-concave games},
  author={Abernethy, Jacob and Lai, Kevin A and Levy, Kfir Y and Wang, Jun-Kun},
  booktitle={Conference On Learning Theory},
  pages={1595--1625},
  year={2018},
  organization={PMLR}
}

@article{mertikopoulos2018optimistic,
  title={Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile},
  author={Mertikopoulos, Panayotis and Lecouat, Bruno and Zenati, Houssam and Foo, Chuan-Sheng and Chandrasekhar, Vijay and Piliouras, Georgios},
  journal={arXiv preprint arXiv:1807.02629},
  year={2018}
}

@article{ZTYCG18,
    title = {On the convergence of adaptive gradient methods for nonconvex optimization.},
    Author = {Dongruo Zhou and Yiqi Tang and Ziyan Yang and Yuan Cao and Quanquan Gu},
    journal={arXiv:1808.05671},
    year = {2018},
}


@article{defossez2020convergence,
  title={On the convergence of adam and adagrad},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv e-prints},
  pages={arXiv--2003},
  year={2020}
}

@inproceedings{reddi2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Reddi, S and Zaheer, Manzil and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  booktitle={Proceeding of 32nd Conference on Neural Information Processing Systems (NIPS 2018)},
  year={2018}
}

@article{chen2018convergence,
  title={On the convergence of a class of adam-type algorithms for non-convex optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  journal={arXiv preprint arXiv:1808.02941},
  year={2018}
}

@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={International Conference on Machine Learning},
  pages={6677--6686},
  year={2019},
  organization={PMLR}
}

@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}

@article{zou2018convergence,
  title={On the convergence of adagrad with momentum for training deep neural networks},
  author={Zou, Fangyu and Shen, Li},
  journal={arXiv preprint arXiv:1808.03408},
  volume={2},
  number={3},
  pages={5},
  year={2018}
}

@article{daskalakis2017training,
  title={Training gans with optimism},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  journal={arXiv preprint arXiv:1711.00141},
  year={2017}
}

@article{hazan2019introduction,
  title={Introduction to online convex optimization},
  author={Hazan, Elad},
  journal={arXiv preprint arXiv:1909.05207},
  year={2019}
}

@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={983--992},
  year={2019},
  organization={PMLR}
}

@inproceedings{agarwal2019efficient,
  title={Efficient full-matrix adaptive regularization},
  author={Agarwal, Naman and Bullins, Brian and Chen, Xinyi and Hazan, Elad and Singh, Karan and Zhang, Cyril and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={102--110},
  year={2019},
  organization={PMLR}
}

@article{mohri2015accelerating,
  title={Accelerating optimization via adaptive prediction},
  author={Mohri, Mehryar and Yang, Scott},
  journal={arXiv preprint arXiv:1509.05760},
  year={2015}
}

@article{walker2011anderson,
  title={Anderson acceleration for fixed-point iterations},
  author={Walker, Homer F and Ni, Peng},
  journal={SIAM Journal on Numerical Analysis},
  volume={49},
  number={4},
  pages={1715--1735},
  year={2011},
  publisher={SIAM}
}

@article{scieur2020regularized,
  title={Regularized nonlinear acceleration},
  author={Scieur, Damien and dAspremont, Alexandre and Bach, Francis},
  journal={Mathematical Programming},
  volume={179},
  number={1},
  pages={47--83},
  year={2020},
  publisher={Springer}
}

@incollection{eddy1979extrapolating,
  title={Extrapolating to the limit of a vector sequence},
  author={Eddy, RP},
  booktitle={Information linkage between applied mathematics and industry},
  pages={387--396},
  year={1979},
  publisher={Elsevier}
}

@book{brezinski2013extrapolation,
  title={Extrapolation methods: theory and practice},
  author={Brezinski, Claude and Zaglia, M Redivo},
  year={2013},
  publisher={Elsevier}
}


@article{Scieur18,
  author    = {Damien Scieur and
               Edouard Oyallon and
               Alexandre dAspremont and
               Francis Bach},
  title     = {Nonlinear Acceleration of Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1805.09639},
  year      = {2018}
}

@inproceedings{larochelle2007empirical,
  title={An empirical evaluation of deep architectures on problems with many factors of variation},
  author={Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={473--480},
  year={2007}
}

@article{gers1999learning,
author = {Gers, Felix A. and Schmidhuber, J\"{u}rgen A. and Cummins, Fred A.},
title = {Learning to Forget: Continual Prediction with LSTM},
year = {2000},
issue_date = {October 2000},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {12},
number = {10},
issn = {0899-7667},
journal = {Neural Comput.},
month = oct,
pages = {2451-2471},
numpages = {21}
}


@article{tseng2008accelerated,
  title={On accelerated proximal gradient methods for convex-concave optimization},
  author={Tseng, Paul},
  journal={submitted to SIAM Journal on Optimization},
  volume={2},
  number={3},
  year={2008}
}

@article{yan2018unified,
  title={A unified analysis of stochastic momentum methods for deep learning},
  author={Yan, Yan and Yang, Tianbao and Li, Zhe and Lin, Qihang and Yang, Yi},
  journal={arXiv preprint arXiv:1808.10396},
  year={2018}
}

@article{chen2018universal,
  title={Universal stagewise learning for non-convex problems with convergence on averaged solutions},
  author={Chen, Zaiyi and Yuan, Zhuoning and Yi, Jinfeng and Zhou, Bowen and Chen, Enhong and Yang, Tianbao},
  journal={arXiv preprint arXiv:1808.06296},
  year={2018}
}

@article{cabay1976polynomial,
  title={A polynomial extrapolation method for finding limits and antilimits of vector sequences},
  author={Cabay, Stan and Jackson, LW},
  journal={SIAM Journal on Numerical Analysis},
  volume={13},
  number={5},
  pages={734--752},
  year={1976},
  publisher={SIAM}
}