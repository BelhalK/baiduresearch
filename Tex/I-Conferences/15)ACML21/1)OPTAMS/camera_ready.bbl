\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2018)Abernethy, Lai, Levy, and Wang]{ALLW18}
Jacob Abernethy, Kevin~A Lai, Kfir~Y Levy, and Jun-Kun Wang.
\newblock Faster rates for convex-concave games.
\newblock In \emph{Conference On Learning Theory}, pages 1595--1625. PMLR,
  2018.

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2019efficient}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock In \emph{International Conference on Machine Learning}, pages
  102--110. PMLR, 2019.

\bibitem[Brezinski and Zaglia(2013)]{brezinski2013extrapolation}
Claude Brezinski and M~Redivo Zaglia.
\newblock \emph{Extrapolation methods: theory and practice}.
\newblock Elsevier, 2013.

\bibitem[Cabay and Jackson(1976)]{cabay1976polynomial}
Stan Cabay and LW~Jackson.
\newblock A polynomial extrapolation method for finding limits and antilimits
  of vector sequences.
\newblock \emph{SIAM Journal on Numerical Analysis}, 13\penalty0 (5):\penalty0
  734--752, 1976.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Liu, Sun, and
  Hong]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.02941}, 2018{\natexlab{a}}.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Yuan, Yi, Zhou, Chen, and
  Yang]{chen2018universal}
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao
  Yang.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock \emph{arXiv preprint arXiv:1808.06296}, 2018{\natexlab{b}}.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and Zhu]{CJ12}
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
  Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock In \emph{Conference on Learning Theory}, pages 6--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Daskalakis et~al.(2017)Daskalakis, Ilyas, Syrgkanis, and
  Zeng]{daskalakis2017training}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock \emph{arXiv preprint arXiv:1711.00141}, 2017.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020convergence}
Alexandre D{\'e}fossez, L{\'e}on Bottou, Francis Bach, and Nicolas Usunier.
\newblock On the convergence of adam and adagrad.
\newblock \emph{arXiv e-prints}, pages arXiv--2003, 2020.

\bibitem[Dozat(2016)]{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Eddy(1979)]{eddy1979extrapolating}
RP~Eddy.
\newblock Extrapolating to the limit of a vector sequence.
\newblock In \emph{Information linkage between applied mathematics and
  industry}, pages 387--396. Elsevier, 1979.

\bibitem[Gers et~al.(2000)Gers, Schmidhuber, and Cummins]{gers1999learning}
Felix~A. Gers, J\"{u}rgen~A. Schmidhuber, and Fred~A. Cummins.
\newblock Learning to forget: Continual prediction with lstm.
\newblock \emph{Neural Comput.}, 12\penalty0 (10):\penalty0 2451--2471, October
  2000.
\newblock ISSN 0899-7667.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{{SIAM} J. Optim.}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{GMH13}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 6645--6649. Ieee, 2013.

\bibitem[Hazan(2019)]{hazan2019introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{arXiv preprint arXiv:1909.05207}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Larochelle et~al.(2007)Larochelle, Erhan, Courville, Bergstra, and
  Bengio]{larochelle2007empirical}
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua
  Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pages 473--480, 2007.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Li and Orabona(2019)]{li2019convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 983--992. PMLR, 2019.

\bibitem[McMahan and Streeter(2010)]{MS10}
H~Brendan McMahan and Matthew Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{arXiv preprint arXiv:1002.4908}, 2010.

\bibitem[Mertikopoulos et~al.(2018)Mertikopoulos, Lecouat, Zenati, Foo,
  Chandrasekhar, and Piliouras]{mertikopoulos2018optimistic}
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay
  Chandrasekhar, and Georgios Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock \emph{arXiv preprint arXiv:1807.02629}, 2018.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mohri and Yang(2015)]{mohri2015accelerating}
Mehryar Mohri and Scott Yang.
\newblock Accelerating optimization via adaptive prediction.
\newblock \emph{arXiv preprint arXiv:1509.05760}, 2015.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Ussr computational mathematics and mathematical physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Rakhlin and Sridharan(2013)]{RS13b}
Alexander Rakhlin and Karthik Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock \emph{arXiv preprint arXiv:1311.1869}, 2013.

\bibitem[Reddi et~al.(2018)Reddi, Zaheer, Sachan, Kale, and
  Kumar]{reddi2018adaptive}
S~Reddi, Manzil Zaheer, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Proceeding of 32nd Conference on Neural Information
  Processing Systems (NIPS 2018)}, 2018.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Scieur et~al.(2018)Scieur, Oyallon, dAspremont, and Bach]{Scieur18}
Damien Scieur, Edouard Oyallon, Alexandre dAspremont, and Francis Bach.
\newblock Nonlinear acceleration of deep neural networks.
\newblock \emph{CoRR}, abs/1805.09639, 2018.

\bibitem[Scieur et~al.(2020)Scieur, dAspremont, and
  Bach]{scieur2020regularized}
Damien Scieur, Alexandre dAspremont, and Francis Bach.
\newblock Regularized nonlinear acceleration.
\newblock \emph{Mathematical Programming}, 179\penalty0 (1):\penalty0 47--83,
  2020.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and Schapire]{SALS15}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock \emph{arXiv preprint arXiv:1507.00407}, 2015.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012rmsprop}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude. coursera: Neural networks for machine learning.
\newblock \emph{COURSERA Neural Networks Mach. Learn}, 2012.

\bibitem[Tseng(2008)]{tseng2008accelerated}
Paul Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock \emph{submitted to SIAM Journal on Optimization}, 2\penalty0 (3),
  2008.

\bibitem[Walker and Ni(2011)]{walker2011anderson}
Homer~F Walker and Peng Ni.
\newblock Anderson acceleration for fixed-point iterations.
\newblock \emph{SIAM Journal on Numerical Analysis}, 49\penalty0 (4):\penalty0
  1715--1735, 2011.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In \emph{International Conference on Machine Learning}, pages
  6677--6686. PMLR, 2019.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock \emph{arXiv preprint arXiv:1808.10396}, 2018.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zhou et~al.(2018)Zhou, Chen, Cao, Tang, Yang, and
  Gu]{zhou2018convergence}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Karimi, Yu, Xu, and Li]{zhou2020towards}
Yingxue Zhou, Belhal Karimi, Jinxing Yu, Zhiqiang Xu, and Ping Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\bibitem[Zou and Shen(2018)]{zou2018convergence}
Fangyu Zou and Li~Shen.
\newblock On the convergence of adagrad with momentum for training deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1808.03408}, 2\penalty0 (3):\penalty0 5,
  2018.

\end{thebibliography}
