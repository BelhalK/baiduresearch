\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{reddi2019convergence}
\citation{levine2016end}
\citation{he2016deep,goodfellow2014generative}
\citation{mnih2013playing}
\citation{GMH13}
\citation{reddi2019convergence}
\citation{kingma2014adam}
\citation{tieleman2012rmsprop}
\citation{zeiler2012adadelta}
\citation{dozat2016incorporating}
\citation{duchi2011adaptive,MS10}
\citation{nesterov2003introductory}
\citation{polyak1964some}
\citation{polyak1964some}
\citation{reddi2019convergence}
\citation{kingma2014adam}
\citation{CJ12,RS13b,SALS15,ALLW18,mertikopoulos2018optimistic}
\citation{daskalakis2017training}
\citation{RS13b}
\citation{goodfellow2014generative}
\citation{CJ12,RS13b,SALS15}
\citation{daskalakis2017training}
\providecommand \oddpage@label [2]{}
\newlabel{jmlrstart}{{}{}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{}{section.0.1}}
\citation{kingma2014adam,reddi2019convergence}
\citation{CJ12,RS13b,SALS15,ALLW18}
\citation{hazan2019introduction}
\citation{SALS15}
\citation{SALS15}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{RS13b}
\citation{kingma2014adam}
\citation{polyak1964some}
\citation{duchi2011adaptive}
\citation{duchi2011adaptive}
\citation{kingma2014adam}
\citation{kingma2014adam}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{zhou2020towards}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{}{section.0.2}}
\newlabel{sec:prelim}{{2}{}{Preliminaries}{section.0.2}{}}
\newlabel{optFTRL}{{1}{}{Preliminaries}{equation.0.2.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces AMSGrad \citep  {reddi2019convergence}\relax }}{}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:amsgrad}{{1}{}{AMSGrad \citep {reddi2019convergence}\relax }{figure.caption.1}{}}
\newlabel{line:maxop}{{7}{}{AMSGrad \citep {reddi2019convergence}\relax }{ALC@unique.7}{}}
\citation{RS13b}
\citation{CJ12}
\citation{duchi2011adaptive}
\citation{nesterov2003introductory}
\citation{polyak1964some}
\citation{CJ12,RS13b,SALS15}
\@writefile{toc}{\contentsline {section}{\numberline {3}\textsc  {OPT-AMSGRAD} Algorithm}{}{section.0.3}}
\newlabel{sec:opt}{{3}{}{\textsc {OPT-AMSGRAD} Algorithm}{section.0.3}{}}
\citation{reddi2019convergence,kingma2014adam}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textsc  {OPT-AMSGrad}\relax }}{}{algorithm.2}}
\newlabel{alg:optamsgrad}{{2}{}{\textsc {OPT-AMSGrad}\relax }{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textsc  {OPT-AMSGrad} underlying structure.\relax }}{}{figure.caption.2}}
\newlabel{fig:scheme}{{1}{}{\textsc {OPT-AMSGrad} underlying structure.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}On the Convergence of \textsc  {OPT-AMSGrad}}{}{section.0.4}}
\newlabel{sec:analysis}{{4}{}{On the Convergence of \textsc {OPT-AMSGrad}}{section.0.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Convex Regret Analysis}{}{subsection.0.4.1}}
\newlabel{sec:convex}{{4.1}{}{Convex Regret Analysis}{subsection.0.4.1}{}}
\newlabel{THM:MAINCONVEX}{{1}{}{}{Theorem.1}{}}
\citation{reddi2019convergence}
\citation{RS13b}
\citation{ghadimi2013stochastic}
\newlabel{COR:COROLLARY}{{1}{}{}{Corollary.1}{}}
\newlabel{eq:boundAMS}{{2}{}{Convex Regret Analysis}{equation.0.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Nonconvex Finite-Time Analysis}{}{subsection.0.4.2}}
\newlabel{eq:minproblem}{{3}{}{Nonconvex Finite-Time Analysis}{equation.0.4.3}{}}
\newlabel{eq:random}{{4}{}{Nonconvex Finite-Time Analysis}{equation.0.4.4}{}}
\newlabel{ASS:BOUNDEDPARAM}{{1}{}{}{assumption.1}{}}
\citation{ghadimi2013stochastic}
\citation{ghadimi2013stochastic}
\citation{zhou2018convergence}
\newlabel{ass:smooth}{{2}{}{}{assumption.2}{}}
\newlabel{ass:guessbound}{{3}{}{}{assumption.3}{}}
\newlabel{ass:bounded}{{4}{}{}{assumption.4}{}}
\newlabel{LEM:BOUND}{{1}{}{}{Lemma.1}{}}
\newlabel{THM:BOUNDOPT}{{2}{}{}{Theorem.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Checking H\ref  {ASS:BOUNDEDPARAM} for a Deep Neural Network}{}{subsection.0.4.3}}
\citation{defossez2020convergence}
\newlabel{eq:dnnmodel}{{5}{}{Checking H\ref {ASS:BOUNDEDPARAM} for a Deep Neural Network}{equation.0.4.5}{}}
\newlabel{LEM:DNNH2}{{2}{}{}{Lemma.2}{}}
\newlabel{eq:boundderivativeloss}{{6}{}{Checking H\ref {ASS:BOUNDEDPARAM} for a Deep Neural Network}{equation.0.4.6}{}}
\newlabel{eq:decrease}{{7}{}{Checking H\ref {ASS:BOUNDEDPARAM} for a Deep Neural Network}{equation.0.4.7}{}}
\citation{reddi2018adaptive,chen2018convergence,ward2019adagrad,zhou2018convergence,zou2018convergence,li2019convergence}
\citation{chen2018convergence}
\citation{agarwal2019efficient}
\citation{chen2018universal}
\citation{mohri2015accelerating}
\citation{mohri2015accelerating}
\citation{mohri2015accelerating}
\citation{mohri2015accelerating}
\newlabel{eq:gradientatell}{{8}{}{Checking H\ref {ASS:BOUNDEDPARAM} for a Deep Neural Network}{equation.0.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Comparison to related methods}{}{subsection.0.4.4}}
\newlabel{sec:related}{{4.4}{}{Comparison to related methods}{subsection.0.4.4}{}}
\newlabel{OPT-DISZ}{{3}{}{Optimistic-Adam+$\hat {v}_t$. \relax }{figure.caption.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Optimistic-Adam+$\mathaccentV {hat}05E{v}_t$. \relax }}{}{figure.caption.3}}
\citation{daskalakis2017training}
\citation{goodfellow2014generative}
\citation{daskalakis2017training}
\citation{daskalakis2017training}
\citation{walker2011anderson}
\citation{cabay1976polynomial}
\citation{eddy1979extrapolating}
\citation{scieur2020regularized}
\citation{scieur2020regularized}
\citation{scieur2020regularized}
\citation{brezinski2013extrapolation}
\citation{Scieur18}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{}{section.0.5}}
\newlabel{sec:numerical}{{5}{}{Numerical Experiments}{section.0.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Gradient Prediction Process}{}{subsection.0.5.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Regularized Approximated Minimal Polynomial Extrapolation \citep  {scieur2020regularized} \relax }}{}{figure.caption.4}}
\newlabel{alg:algex}{{4}{}{Regularized Approximated Minimal Polynomial Extrapolation \citep {scieur2020regularized} \relax }{figure.caption.4}{}}
\newlabel{nox}{{9}{}{Gradient Prediction Process}{equation.0.5.9}{}}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{kingma2014adam}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. \relax }}{}{figure.caption.5}}
\newlabel{simu}{{2}{}{\small (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. \relax }{figure.caption.5}{}}
\citation{reddi2019convergence}
\citation{daskalakis2017training}
\citation{reddi2019convergence}
\citation{kingma2014adam}
\citation{larochelle2007empirical}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Choice of parameter $r$}{}{subsection.0.5.2}}
\newlabel{sec:choicer}{{5.2}{}{Choice of parameter $r$}{subsection.0.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training loss w.r.t. $r$.\relax }}{}{figure.caption.6}}
\newlabel{fig:compare}{{3}{}{Training loss w.r.t. $r$.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Classification Experiments}{}{subsection.0.5.3}}
\citation{he2016deep}
\citation{gers1999learning}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training loss vs. Number of iterations for fully connected NN, CNN, LSTM and ResNet.\relax }}{}{figure.caption.7}}
\newlabel{fig:train_loss}{{4}{}{Training loss vs. Number of iterations for fully connected NN, CNN, LSTM and ResNet.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{}{section.0.6}}
\bibdata{ref}
\bibcite{ALLW18}{{1}{2018}{{Abernethy et~al.}}{{Abernethy, Lai, Levy, and Wang}}}
\bibcite{agarwal2019efficient}{{2}{2019}{{Agarwal et~al.}}{{Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and Zhang}}}
\bibcite{brezinski2013extrapolation}{{3}{2013}{{Brezinski and Zaglia}}{{}}}
\bibcite{cabay1976polynomial}{{4}{1976}{{Cabay and Jackson}}{{}}}
\bibcite{chen2018convergence}{{5}{2018{a}}{{Chen et~al.}}{{Chen, Liu, Sun, and Hong}}}
\bibcite{chen2018universal}{{6}{2018{b}}{{Chen et~al.}}{{Chen, Yuan, Yi, Zhou, Chen, and Yang}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textit  {MNIST-back-image} + CNN, \textit  {CIFAR10} + Res-18 and \textit  {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy.\relax }}{}{figure.caption.8}}
\newlabel{fig:testandtrain}{{5}{}{\textit {MNIST-back-image} + CNN, \textit {CIFAR10} + Res-18 and \textit {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy.\relax }{figure.caption.8}{}}
\bibcite{CJ12}{{7}{2012}{{Chiang et~al.}}{{Chiang, Yang, Lee, Mahdavi, Lu, Jin, and Zhu}}}
\bibcite{daskalakis2017training}{{8}{2017}{{Daskalakis et~al.}}{{Daskalakis, Ilyas, Syrgkanis, and Zeng}}}
\bibcite{defossez2020convergence}{{9}{2020}{{D{\'e}fossez et~al.}}{{D{\'e}fossez, Bottou, Bach, and Usunier}}}
\bibcite{dozat2016incorporating}{{10}{2016}{{Dozat}}{{}}}
\bibcite{duchi2011adaptive}{{11}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{eddy1979extrapolating}{{12}{1979}{{Eddy}}{{}}}
\bibcite{gers1999learning}{{13}{2000}{{Gers et~al.}}{{Gers, Schmidhuber, and Cummins}}}
\bibcite{ghadimi2013stochastic}{{14}{2013}{{Ghadimi and Lan}}{{}}}
\bibcite{goodfellow2014generative}{{15}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{GMH13}{{16}{2013}{{Graves et~al.}}{{Graves, Mohamed, and Hinton}}}
\bibcite{hazan2019introduction}{{17}{2019}{{Hazan}}{{}}}
\bibcite{he2016deep}{{18}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{kingma2014adam}{{19}{2014}{{Kingma and Ba}}{{}}}
\bibcite{larochelle2007empirical}{{20}{2007}{{Larochelle et~al.}}{{Larochelle, Erhan, Courville, Bergstra, and Bengio}}}
\bibcite{levine2016end}{{21}{2016}{{Levine et~al.}}{{Levine, Finn, Darrell, and Abbeel}}}
\bibcite{li2019convergence}{{22}{2019}{{Li and Orabona}}{{}}}
\bibcite{MS10}{{23}{2010}{{McMahan and Streeter}}{{}}}
\bibcite{mertikopoulos2018optimistic}{{24}{2018}{{Mertikopoulos et~al.}}{{Mertikopoulos, Lecouat, Zenati, Foo, Chandrasekhar, and Piliouras}}}
\bibcite{mnih2013playing}{{25}{2013}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller}}}
\bibcite{mohri2015accelerating}{{26}{2015}{{Mohri and Yang}}{{}}}
\bibcite{nesterov2003introductory}{{27}{2003}{{Nesterov}}{{}}}
\bibcite{polyak1964some}{{28}{1964}{{Polyak}}{{}}}
\bibcite{RS13b}{{29}{2013}{{Rakhlin and Sridharan}}{{}}}
\bibcite{reddi2018adaptive}{{30}{2018}{{Reddi et~al.}}{{Reddi, Zaheer, Sachan, Kale, and Kumar}}}
\bibcite{reddi2019convergence}{{31}{2019}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{Scieur18}{{32}{2018}{{Scieur et~al.}}{{Scieur, Oyallon, dAspremont, and Bach}}}
\bibcite{scieur2020regularized}{{33}{2020}{{Scieur et~al.}}{{Scieur, dAspremont, and Bach}}}
\bibcite{SALS15}{{34}{2015}{{Syrgkanis et~al.}}{{Syrgkanis, Agarwal, Luo, and Schapire}}}
\bibcite{tieleman2012rmsprop}{{35}{2012}{{Tieleman and Hinton}}{{}}}
\bibcite{tseng2008accelerated}{{36}{2008}{{Tseng}}{{}}}
\bibcite{walker2011anderson}{{37}{2011}{{Walker and Ni}}{{}}}
\bibcite{ward2019adagrad}{{38}{2019}{{Ward et~al.}}{{Ward, Wu, and Bottou}}}
\bibcite{yan2018unified}{{39}{2018}{{Yan et~al.}}{{Yan, Yang, Li, Lin, and Yang}}}
\bibcite{zeiler2012adadelta}{{40}{2012}{{Zeiler}}{{}}}
\bibcite{zhou2018convergence}{{41}{2018}{{Zhou et~al.}}{{Zhou, Chen, Cao, Tang, Yang, and Gu}}}
\bibcite{zhou2020towards}{{42}{2020}{{Zhou et~al.}}{{Zhou, Karimi, Yu, Xu, and Li}}}
\bibcite{zou2018convergence}{{43}{2018}{{Zou and Shen}}{{}}}
\citation{tseng2008accelerated}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Theorem\nobreakspace  {}\ref  {THM:MAINCONVEX}}{}{section.0.A}}
\newlabel{app:thmmainconvex}{{A}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{section.0.A}{}}
\newlabel{nn1}{{11}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.11}{}}
\newlabel{ii}{{12}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.12}{}}
\newlabel{nc1}{{13}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.13}{}}
\newlabel{nn2}{{14}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.14}{}}
\newlabel{nc2}{{15}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.15}{}}
\newlabel{nn3}{{16}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.16}{}}
\newlabel{nnn}{{17}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.17}{}}
\newlabel{nnnn}{{18}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.18}{}}
\newlabel{nn5}{{19}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.19}{}}
\newlabel{nn4}{{20}{}{Proof of Theorem~\ref {THM:MAINCONVEX}}{equation.0.A.20}{}}
\citation{yan2018unified}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Corollary\nobreakspace  {}\ref  {COR:COROLLARY}}{}{section.0.B}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proofs of Auxiliary Lemmas}{}{section.0.C}}
\newlabel{eq:deftilde}{{21}{}{Proofs of Auxiliary Lemmas}{equation.0.C.21}{}}
\newlabel{lem:momentum}{{3}{}{}{Lemma.3}{}}
\newlabel{lem:squarev}{{4}{}{}{Lemma.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Proof of Lemma\nobreakspace  {}\ref  {LEM:BOUND}}{}{subsection.0.C.1}}
\newlabel{app:lembound}{{C.1}{}{Proof of Lemma~\ref {LEM:BOUND}}{subsection.0.C.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proof of Theorem\nobreakspace  {}\ref  {THM:BOUNDOPT}}{}{section.0.D}}
\newlabel{app:thmboundopt}{{D}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{section.0.D}{}}
\newlabel{eq:smoothness}{{30}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.30}{}}
\newlabel{eq:termA1}{{31}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.31}{}}
\newlabel{eq:termA2}{{32}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.32}{}}
\newlabel{eq:termA}{{33}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.33}{}}
\newlabel{eq:termB1}{{34}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.34}{}}
\newlabel{eq:termB2}{{35}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.35}{}}
\newlabel{eq:termB3}{{37}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.37}{}}
\newlabel{eq:termB}{{38}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.38}{}}
\newlabel{eq:term3}{{39}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.39}{}}
\newlabel{eq:expectationtildegrad}{{40}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.40}{}}
\newlabel{eq:bound1}{{41}{}{Proof of Theorem~\ref {THM:BOUNDOPT}}{equation.0.D.41}{}}
\citation{zhou2018convergence}
\newlabel{jmlrend}{{D}{}{end of OPT-AMS for Nonconvex Optimization}{section*.10}{}}
