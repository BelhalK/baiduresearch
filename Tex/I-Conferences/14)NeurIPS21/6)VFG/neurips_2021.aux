\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{koller2007graphical}
\citation{bilmes2005graphical}
\citation{shwe1990probabilistic}
\citation{jordan1999graphical}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{sanner2012symbolic,kahle2008junction}
\citation{jordan1999introduction,hoffman2013stochastic,kingma2013auto,liu2016stein}
\citation{xing2012generalized}
\citation{bishop2003vibes,winn2005variational}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kingma2013auto}
\citation{kingma2018glow,rezende2015variational}
\citation{tabak2010density}
\citation{Dinh2016DensityEU,rippel2013high}
\citation{rezende2015variational}
\citation{Dinh2016DensityEU,dinh2014nice,de2020block,ho2019flow++,papamakarios2019normalizing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\newlabel{eq:vae_recon}{{1}{2}{Preliminaries}{equation.2.1}{}}
\citation{rezende2015variational}
\citation{rezende2015variational,berg2018sylvester}
\citation{kingma2013auto,rezende2014stochastic}
\newlabel{eq:flow}{{2}{3}{Preliminaries}{equation.2.2}{}}
\newlabel{eq:flow2}{{3}{3}{Preliminaries}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variational Flow Graphical Model}{3}{section.3}}
\newlabel{sec:main}{{3}{3}{Variational Flow Graphical Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evidence Lower Bound of Variational Flow Graphical Models}{3}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ (Left) Node $\mathbf  {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf  {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation\nobreakspace  {}nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. Thin lines are identity functions, and thick lines are flow functions. \relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tree}{{1}{3}{\small (Left) Node $\mathbf {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation~nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. Thin lines are identity functions, and thick lines are flow functions. \relax }{figure.caption.2}{}}
\citation{kingma2013auto}
\newlabel{eq:posterior}{{4}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.4}{}}
\newlabel{eq:elbo}{{5}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.5}{}}
\newlabel{eq:kl}{{6}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.6}{}}
\newlabel{eq:elbo_dag}{{7}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Forward and backward message passing to generate each node's hidden variable. Forward message passing approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions. \relax }}{4}{figure.caption.3}}
\newlabel{fig:tree_message}{{2}{4}{Forward and backward message passing to generate each node's hidden variable. Forward message passing approximates the posterior distribution of latent variables, and backward message passing generates the reconstructions. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ELBO Calculation}{4}{subsection.3.2}}
\newlabel{eq:post_smp}{{8}{5}{ELBO Calculation}{equation.3.8}{}}
\newlabel{eq:prior_smp}{{9}{5}{ELBO Calculation}{equation.3.9}{}}
\newlabel{eq:KL_l}{{10}{5}{ELBO Calculation}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3} Aggregation Nodes}{5}{subsection.3.3}}
\newlabel{sec:node_aggr}{{3.3}{5}{Aggregation Nodes}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (Left) Aggregation node $\mathbf  {h}^{l+1,1}$ has three children, $\mathbf  {h}^{l,1}$, $\mathbf  {h}^{l,2}$, and $\mathbf  {h}^{l,3}$. (Right) A VFG model with one aggregation node, $\mathbf  {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }}{5}{figure.caption.4}}
\newlabel{fig:node_aggre}{{3}{5}{(Left) Aggregation node $\mathbf {h}^{l+1,1}$ has three children, $\mathbf {h}^{l,1}$, $\mathbf {h}^{l,2}$, and $\mathbf {h}^{l,3}$. (Right) A VFG model with one aggregation node, $\mathbf {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }{figure.caption.4}{}}
\newlabel{eq:one_agg_node}{{11}{6}{Aggregation Nodes}{equation.3.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inference model parameters with forward and backward message propagation\relax }}{6}{figure.caption.5}}
\newlabel{alg:main}{{1}{6}{Inference model parameters with forward and backward message propagation\relax }{figure.caption.5}{}}
\newlabel{line:for2}{{4}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.4}{}}
\newlabel{line:forward}{{6}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.6}{}}
\newlabel{line:backward}{{11}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.11}{}}
\newlabel{line:update}{{15}{6}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Algorithms and Implementation}{6}{section.4}}
\newlabel{sec:algrithm}{{4}{6}{Algorithms and Implementation}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Layer-wise Training}{6}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }}{6}{figure.caption.6}}
\newlabel{fig:two_layer_infer}{{4}{6}{{\small (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }{figure.caption.6}{}}
\citation{bengio2013representation}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Random Masking}{7}{subsection.4.2}}
\newlabel{eq:elbo_tree_mask}{{13}{7}{Random Masking}{equation.4.13}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Inference model parameters with random masking\relax }}{7}{figure.caption.7}}
\newlabel{alg:rand_mask}{{2}{7}{Inference model parameters with random masking\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Inference on VFG Models }{7}{section.5}}
\newlabel{sec:infer}{{5}{7}{Inference on VFG Models}{section.5}{}}
\newlabel{eq:aggr_obs_ch}{{14}{7}{Inference on VFG Models}{equation.5.14}{}}
\newlabel{lm:apprx}{{1}{7}{}{lemma.1}{}}
\newlabel{rmk:apprx_mul}{{1}{7}{}{remark.1}{}}
\citation{Dinh2016DensityEU}
\citation{berg2018sylvester}
\citation{berg2018sylvester}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Experiments}{8}{section.6}}
\newlabel{sec:numerical}{{6}{8}{Numerical Experiments}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Evaluation on Inference with Missing Entries Imputation}{8}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }}{8}{figure.caption.8}}
\newlabel{fig:sim}{{5}{8}{Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ELBO and Likelihood}{8}{subsection.6.2}}
\newlabel{sec:exp:elbo}{{6.2}{8}{ELBO and Likelihood}{subsection.6.2}{}}
\citation{kingma2013auto}
\citation{rezende2015variational}
\citation{kingma2016improving}
\citation{berg2018sylvester}
\citation{Lecunmnist2010}
\citation{Sorrenson2020}
\citation{maaten2008visualizing}
\bibdata{ref}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }}{9}{table.1}}
\newlabel{tab:elbo}{{1}{9}{Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Latent Representation Learning on MNIST}{9}{subsection.6.3}}
\newlabel{sec:exp:mnist}{{6.3}{9}{Latent Representation Learning on MNIST}{subsection.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }}{9}{figure.caption.9}}
\newlabel{fig:reconst}{{6}{9}{(Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The tree structure for MNIST.\relax }}{9}{figure.caption.10}}
\newlabel{fig:struct}{{7}{9}{The tree structure for MNIST.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }}{9}{figure.caption.11}}
\newlabel{fig:z_tsne}{{8}{9}{MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{9}{section.7}}
\newlabel{sec:conclusion}{{7}{9}{Conclusion}{section.7}{}}
\bibcite{chsklearn}{{1}{}{{}}{{}}}
\bibcite{bengio2013representation}{{2}{}{{}}{{}}}
\bibcite{berg2018sylvester}{{3}{}{{}}{{}}}
\bibcite{bilmes2005graphical}{{4}{}{{}}{{}}}
\bibcite{bishop2003vibes}{{5}{}{{}}{{}}}
\bibcite{de2020block}{{6}{}{{}}{{}}}
\bibcite{dinh2014nice}{{7}{}{{}}{{}}}
\bibcite{Dinh2016DensityEU}{{8}{}{{}}{{}}}
\bibcite{efron1975defining}{{9}{}{{}}{{}}}
\bibcite{Book:Hanson_1994}{{10}{}{{}}{{}}}
\bibcite{ho2019flow++}{{11}{}{{}}{{}}}
\bibcite{hoffman2013stochastic}{{12}{}{{}}{{}}}
\bibcite{hruschka2007bayesian}{{13}{}{{}}{{}}}
\bibcite{jordan1999graphical}{{14}{}{{}}{{}}}
\bibcite{jordan1999introduction}{{15}{}{{}}{{}}}
\bibcite{kahle2008junction}{{16}{}{{}}{{}}}
\bibcite{Khemakhem20a}{{17}{}{{}}{{}}}
\bibcite{kingma2018glow}{{18}{}{{}}{{}}}
\bibcite{kingma2016improving}{{19}{}{{}}{{}}}
\bibcite{kingma2013auto}{{20}{}{{}}{{}}}
\bibcite{koller2007graphical}{{21}{}{{}}{{}}}
\bibcite{Article:Krantz_2008}{{22}{}{{}}{{}}}
\bibcite{Lecunmnist2010}{{23}{}{{}}{{}}}
\bibcite{liu2016stein}{{24}{}{{}}{{}}}
\bibcite{maaten2008visualizing}{{25}{}{{}}{{}}}
\bibcite{madigan1995bayesian}{{26}{}{{}}{{}}}
\bibcite{papamakarios2019normalizing}{{27}{}{{}}{{}}}
\bibcite{rezende2015variational}{{28}{}{{}}{{}}}
\bibcite{rezende2014stochastic}{{29}{}{{}}{{}}}
\bibcite{rippel2013high}{{30}{}{{}}{{}}}
\bibcite{sanner2012symbolic}{{31}{}{{}}{{}}}
\bibcite{shwe1990probabilistic}{{32}{}{{}}{{}}}
\bibcite{Sorrenson2020}{{33}{}{{}}{{}}}
\bibcite{tabak2010density}{{34}{}{{}}{{}}}
\bibcite{winn2005variational}{{35}{}{{}}{{}}}
\bibcite{xing2012generalized}{{36}{}{{}}{{}}}
\bibstyle{abbrv}
\citation{Dinh2016DensityEU}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Numerical Experiments}{13}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}California Housing Dataset}{13}{subsection.A.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }}{13}{table.caption.14}}
\newlabel{tab:imp_arrhytmia}{{2}{13}{California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Inference on DAGs}{13}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}MNIST}{13}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.1}Latent Representation Learning on MNIST}{13}{subsubsection.A.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }}{14}{figure.caption.15}}
\newlabel{fig:z_no_Y}{{9}{14}{MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.2}Disentanglement on MNIST}{14}{subsubsection.A.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces MNIST: Increasing each latent variable from a small value to a larger one.\relax }}{15}{figure.caption.16}}
\newlabel{fig:mnist_dis}{{10}{15}{MNIST: Increasing each latent variable from a small value to a larger one.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Derivation of the ELBO for both Tree and DAG structures}{15}{appendix.B}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}ELBO of Tree Models}{15}{subsection.B.1}}
\newlabel{appd:tree_elbo}{{B.1}{15}{ELBO of Tree Models}{subsection.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A VFG tree with $L=3$.\relax }}{16}{figure.caption.17}}
\newlabel{fig:tree3}{{11}{16}{A VFG tree with $L=3$.\relax }{figure.caption.17}{}}
\newlabel{eq:prior}{{15}{16}{ELBO of Tree Models}{equation.B.15}{}}
\newlabel{eq:posterior2}{{16}{16}{ELBO of Tree Models}{equation.B.16}{}}
\newlabel{eq:chain_post}{{17}{16}{ELBO of Tree Models}{equation.B.17}{}}
\newlabel{eq:chain_prior}{{18}{16}{ELBO of Tree Models}{equation.B.18}{}}
\newlabel{eq:elbo12L}{{19}{16}{ELBO of Tree Models}{equation.B.19}{}}
\citation{rezende2015variational,kingma2016improving,berg2018sylvester}
\citation{rezende2015variational,kingma2016improving,berg2018sylvester}
\newlabel{eq:kl_lL}{{20}{17}{ELBO of Tree Models}{equation.B.20}{}}
\newlabel{eq:elbo0}{{21}{17}{ELBO of Tree Models}{equation.B.21}{}}
\newlabel{eq:kl_l}{{22}{17}{ELBO of Tree Models}{equation.B.22}{}}
\newlabel{eq:elbo1}{{23}{17}{ELBO of Tree Models}{equation.B.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Improve ELBO Estimation with Flows}{17}{subsection.B.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}ELBO of DAG Models}{17}{subsection.B.3}}
\newlabel{appd:dag_elbo}{{B.3}{17}{ELBO of DAG Models}{subsection.B.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces DAG structure. The inverse topology order is {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \{\vcenter to\@ne \big@size {}\right .$}\box \z@ } \{1,2,3\}, \{4,5\}, \{6\}, \{7\} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \}\vcenter to\@ne \big@size {}\right .$}\box \z@ }, and it corresponds to layers 0 to 3. \relax }}{18}{figure.caption.18}}
\newlabel{fig:dag}{{12}{18}{DAG structure. The inverse topology order is \big \{ \{1,2,3\}, \{4,5\}, \{6\}, \{7\} \big \}, and it corresponds to layers 0 to 3. \relax }{figure.caption.18}{}}
\newlabel{eq:dag_elbo}{{24}{18}{ELBO of DAG Models}{equation.B.24}{}}
\newlabel{eq:dag_chain_q}{{25}{18}{ELBO of DAG Models}{equation.B.25}{}}
\newlabel{eq:dag_chain_p}{{26}{18}{ELBO of DAG Models}{equation.B.26}{}}
\newlabel{eq:dag_kl_lL}{{27}{18}{ELBO of DAG Models}{equation.B.27}{}}
\citation{rezende2015variational,berg2018sylvester}
\@writefile{toc}{\contentsline {section}{\numberline {C}Model the Distributions of Latent Latent Representations at Nodes}{19}{appendix.C}}
\newlabel{sec:vfg_inference}{{C}{19}{Model the Distributions of Latent Latent Representations at Nodes}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Distributions at Nodes}{19}{subsection.C.1}}
\newlabel{eq:elbo_dag_2}{{28}{19}{Distributions at Nodes}{equation.C.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.1}KL Computation for Nodes}{19}{subsubsection.C.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Layer-wise sampling of the posterior and the prior corresponds to forward\nobreakspace  {}(encoding) and backward\nobreakspace  {}(decoding) message passing, respectively.}\relax }}{19}{figure.caption.19}}
\newlabel{fig:message}{{13}{19}{{\small Layer-wise sampling of the posterior and the prior corresponds to forward~(encoding) and backward~(decoding) message passing, respectively.}\relax }{figure.caption.19}{}}
\newlabel{eq:kl_est}{{29}{19}{KL Computation for Nodes}{equation.C.29}{}}
\citation{Book:Hanson_1994,Article:Krantz_2008}
\citation{Khemakhem20a,Sorrenson2020}
\citation{efron1975defining}
\@writefile{toc}{\contentsline {section}{\numberline {D}More Details on Inference}{20}{appendix.D}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Inference on DAGs}{20}{subsection.D.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Aggregation node in a DAG.}\relax }}{20}{figure.caption.20}}
\newlabel{fig:dag_aggr}{{14}{20}{{\small Aggregation node in a DAG.}\relax }{figure.caption.20}{}}
\newlabel{lm:apprxapp}{{2}{20}{}{lemma.2}{}}
\newlabel{eq:cond_llk}{{30}{20}{}{equation.D.30}{}}
\newlabel{rmk:apprx_mulapp}{{2}{20}{}{remark.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Proof of Lemma\nobreakspace  {}\ref  {lm:apprx}}{20}{subsection.D.2}}
\newlabel{appd:proof_lm1}{{D.2}{20}{Proof of Lemma~\ref {lm:apprx}}{subsection.D.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Theoretical Justifications for Latent Representation Learning}{21}{appendix.E}}
\newlabel{sec:theory}{{E}{21}{Theoretical Justifications for Latent Representation Learning}{appendix.E}{}}
\newlabel{eq:exp_h}{{31}{21}{Theoretical Justifications for Latent Representation Learning}{equation.E.31}{}}
\newlabel{eq:xt_gen}{{32}{21}{Theoretical Justifications for Latent Representation Learning}{equation.E.32}{}}
\newlabel{eq:u_diff}{{33}{21}{Theoretical Justifications for Latent Representation Learning}{equation.E.33}{}}
\citation{Khemakhem20a}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{eq:A_sim}{{34}{22}{Theoretical Justifications for Latent Representation Learning}{equation.E.34}{}}
