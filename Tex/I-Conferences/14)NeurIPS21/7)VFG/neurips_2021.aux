\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{koller2007graphical}
\citation{bilmes2005graphical}
\citation{shwe1990probabilistic}
\citation{jordan1999graphical}
\citation{madigan1995bayesian,hruschka2007bayesian}
\citation{sanner2012symbolic,kahle2008junction}
\citation{jordan1999introduction,hoffman2013stochastic,kingma2013auto,liu2016stein}
\citation{xing2012generalized}
\citation{bishop2003vibes,winn2005variational}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kingma2013auto}
\citation{kingma2018glow,rezende2015variational}
\citation{tabak2010density}
\citation{Dinh2016DensityEU,rippel2013high}
\citation{rezende2015variational}
\citation{Dinh2016DensityEU,dinh2014nice,de2020block,ho2019flow++,papamakarios2019normalizing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\newlabel{eq:vae_recon}{{1}{2}{Preliminaries}{equation.2.1}{}}
\citation{rezende2015variational}
\citation{rezende2015variational,berg2018sylvester}
\newlabel{eq:flow}{{2}{3}{Preliminaries}{equation.2.2}{}}
\newlabel{eq:flow2}{{3}{3}{Preliminaries}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Variational Flow Graphical Model}{3}{section.3}}
\newlabel{sec:main}{{3}{3}{Variational Flow Graphical Model}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Evidence Lower Bound of Variational Flow Graphical Models}{3}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ (Left) Node $\mathbf  {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf  {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation\nobreakspace  {}nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. \relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tree}{{1}{3}{\small (Left) Node $\mathbf {h}^{2, 1}$ connects its children with invertible functions. Messages from the children are aggregated at the parent, $\mathbf {h}^{2,1}$; $\oplus $ is an aggregation node, and circles stand for non-aggregation~nodes.(Right) An illustration of the latent structure from layer $l-1$ to $l+1$. \relax }{figure.caption.2}{}}
\newlabel{eq:posterior}{{4}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.4}{}}
\newlabel{eq:elbo}{{5}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.5}{}}
\newlabel{eq:kl}{{6}{4}{Evidence Lower Bound of Variational Flow Graphical Models}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2} Aggregation Nodes}{4}{subsection.3.2}}
\newlabel{sec:node_aggr}{{3.2}{4}{Aggregation Nodes}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (Left) Aggregation node $\mathbf  {h}^{l+1,1}$ has three children, $\mathbf  {h}^{l,1}$, $\mathbf  {h}^{l,2}$, and $\mathbf  {h}^{l,3}$. Thin lines are identity functions, and thick lines are flow functions. (Right) A VFG model with one aggregation node, $\mathbf  {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }}{4}{figure.caption.3}}
\newlabel{fig:node_aggre}{{2}{4}{(Left) Aggregation node $\mathbf {h}^{l+1,1}$ has three children, $\mathbf {h}^{l,1}$, $\mathbf {h}^{l,2}$, and $\mathbf {h}^{l,3}$. Thin lines are identity functions, and thick lines are flow functions. (Right) A VFG model with one aggregation node, $\mathbf {h}^{(r)}$. Solid circles are nodes with observed values, and the diamond is the prior for the root node.\relax }{figure.caption.3}{}}
\newlabel{eq:one_agg_node}{{7}{4}{Aggregation Nodes}{equation.3.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inference model parameters with forward and backward message propagation\relax }}{5}{figure.caption.4}}
\newlabel{alg:main}{{1}{5}{Inference model parameters with forward and backward message propagation\relax }{figure.caption.4}{}}
\newlabel{line:for2}{{4}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.4}{}}
\newlabel{line:forward}{{6}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.6}{}}
\newlabel{line:backward}{{11}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.11}{}}
\newlabel{line:update}{{15}{5}{Inference model parameters with forward and backward message propagation\relax }{ALC@unique.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Algorithms and Implementation}{5}{section.4}}
\newlabel{sec:algrithm}{{4}{5}{Algorithms and Implementation}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Layer-wise Training}{5}{subsection.4.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Inference model parameters with random masking\relax }}{5}{figure.caption.5}}
\newlabel{alg:rand_mask}{{2}{5}{Inference model parameters with random masking\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Random Masking}{5}{subsection.4.2}}
\newlabel{eq:elbo_tree_mask}{{9}{5}{Random Masking}{equation.4.9}{}}
\citation{bengio2013representation}
\@writefile{toc}{\contentsline {section}{\numberline {5}Inference on VFG Models }{6}{section.5}}
\newlabel{sec:infer}{{5}{6}{Inference on VFG Models}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }}{6}{figure.caption.6}}
\newlabel{fig:two_layer_infer}{{3}{6}{{\small (Left) Inference on model with single aggregation node. Node 7 aggregates information from node 1 and 2, and pass down the updated state to node 3 for prediction. (Right) Inference on a tree model. Observed node states are gathered at node 7 to predict the state of node 4. Red and green lines are forward and backward messages, respectively.}\relax }{figure.caption.6}{}}
\newlabel{eq:aggr_obs_ch}{{10}{6}{Inference on VFG Models}{equation.5.10}{}}
\newlabel{lm:apprx}{{1}{6}{}{lemma.1}{}}
\newlabel{eq:cond_llk}{{11}{6}{}{equation.5.11}{}}
\newlabel{rmk:apprx_mul}{{1}{6}{}{remark.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Experiments}{6}{section.6}}
\newlabel{sec:numerical}{{6}{6}{Numerical Experiments}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Evaluation on Inference with Missing Entries Imputation}{6}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }}{6}{figure.caption.7}}
\newlabel{fig:sim}{{4}{6}{Synthetic datasets: MSE boxplots of VFG and baseline methods.\relax }{figure.caption.7}{}}
\citation{Dinh2016DensityEU}
\citation{berg2018sylvester}
\citation{kingma2013auto}
\citation{rezende2015variational}
\citation{kingma2016improving}
\citation{berg2018sylvester}
\citation{Lecunmnist2010}
\citation{Sorrenson2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ELBO and Likelihood}{7}{subsection.6.2}}
\newlabel{sec:exp:elbo}{{6.2}{7}{ELBO and Likelihood}{subsection.6.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }}{7}{table.1}}
\newlabel{tab:elbo}{{1}{7}{Negative log-likelihood and free energy (negative evidence lower bound) for static MNIST, Caltech101, and Omniglot.\relax }{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Latent Representation Learning on MNIST}{7}{subsection.6.3}}
\newlabel{sec:exp:mnist}{{6.3}{7}{Latent Representation Learning on MNIST}{subsection.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }}{7}{figure.caption.8}}
\newlabel{fig:reconst}{{5}{7}{(Top row) original MNIST digits. (Bottom row) reconstructed images using VFG.\relax }{figure.caption.8}{}}
\citation{maaten2008visualizing}
\bibdata{ref}
\bibcite{chsklearn}{{1}{}{{}}{{}}}
\bibcite{bengio2013representation}{{2}{}{{}}{{}}}
\bibcite{berg2018sylvester}{{3}{}{{}}{{}}}
\bibcite{bilmes2005graphical}{{4}{}{{}}{{}}}
\bibcite{bishop2003vibes}{{5}{}{{}}{{}}}
\bibcite{de2020block}{{6}{}{{}}{{}}}
\bibcite{dinh2014nice}{{7}{}{{}}{{}}}
\bibcite{Dinh2016DensityEU}{{8}{}{{}}{{}}}
\bibcite{efron1975defining}{{9}{}{{}}{{}}}
\bibcite{Book:Hanson_1994}{{10}{}{{}}{{}}}
\bibcite{ho2019flow++}{{11}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The tree structure for MNIST.\relax }}{8}{figure.caption.9}}
\newlabel{fig:struct}{{6}{8}{The tree structure for MNIST.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }}{8}{figure.caption.10}}
\newlabel{fig:z_tsne}{{7}{8}{MNIST: t-SNE plot of latent variables from VFG learned with labels.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}}
\newlabel{sec:conclusion}{{7}{8}{Conclusion}{section.7}{}}
\bibcite{hoffman2013stochastic}{{12}{}{{}}{{}}}
\bibcite{hruschka2007bayesian}{{13}{}{{}}{{}}}
\bibcite{jordan1999graphical}{{14}{}{{}}{{}}}
\bibcite{jordan1999introduction}{{15}{}{{}}{{}}}
\bibcite{kahle2008junction}{{16}{}{{}}{{}}}
\bibcite{Khemakhem20a}{{17}{}{{}}{{}}}
\bibcite{kingma2018glow}{{18}{}{{}}{{}}}
\bibcite{kingma2016improving}{{19}{}{{}}{{}}}
\bibcite{kingma2013auto}{{20}{}{{}}{{}}}
\bibcite{koller2007graphical}{{21}{}{{}}{{}}}
\bibcite{Article:Krantz_2008}{{22}{}{{}}{{}}}
\bibcite{Lecunmnist2010}{{23}{}{{}}{{}}}
\bibcite{liu2016stein}{{24}{}{{}}{{}}}
\bibcite{maaten2008visualizing}{{25}{}{{}}{{}}}
\bibcite{madigan1995bayesian}{{26}{}{{}}{{}}}
\bibcite{papamakarios2019normalizing}{{27}{}{{}}{{}}}
\bibcite{rezende2015variational}{{28}{}{{}}{{}}}
\bibcite{rippel2013high}{{29}{}{{}}{{}}}
\bibcite{sanner2012symbolic}{{30}{}{{}}{{}}}
\bibcite{shwe1990probabilistic}{{31}{}{{}}{{}}}
\bibcite{Sorrenson2020}{{32}{}{{}}{{}}}
\bibcite{tabak2010density}{{33}{}{{}}{{}}}
\bibcite{winn2005variational}{{34}{}{{}}{{}}}
\bibcite{xing2012generalized}{{35}{}{{}}{{}}}
\bibstyle{abbrv}
\citation{Dinh2016DensityEU}
\citation{chsklearn}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Numerical Experiments}{10}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}California Housing Dataset}{10}{subsection.A.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }}{10}{table.caption.12}}
\newlabel{tab:imp_arrhytmia}{{2}{10}{California Housing dataset: Imputation Mean Squared Error (MSE) results.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Inference on DAGs}{10}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}MNIST}{10}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.1}Latent Representation Learning on MNIST}{11}{subsubsection.A.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }}{11}{figure.caption.13}}
\newlabel{fig:z_no_Y}{{8}{11}{MNIST: t-SNE plot of latent variables from VFG learned without labels.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.2}Disentanglement on MNIST}{11}{subsubsection.A.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MNIST: Increasing each latent variable from a small value to a larger one.\relax }}{12}{figure.caption.14}}
\newlabel{fig:mnist_dis}{{9}{12}{MNIST: Increasing each latent variable from a small value to a larger one.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Derivation of the ELBO for both Tree and DAG structures}{12}{appendix.B}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}ELBO of Tree Models}{12}{subsection.B.1}}
\newlabel{appd:tree_elbo}{{B.1}{12}{ELBO of Tree Models}{subsection.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A VFG tree with $L=3$.\relax }}{13}{figure.caption.15}}
\newlabel{fig:tree3}{{10}{13}{A VFG tree with $L=3$.\relax }{figure.caption.15}{}}
\newlabel{eq:prior}{{12}{13}{ELBO of Tree Models}{equation.B.12}{}}
\newlabel{eq:posterior2}{{13}{13}{ELBO of Tree Models}{equation.B.13}{}}
\newlabel{eq:chain_post}{{14}{13}{ELBO of Tree Models}{equation.B.14}{}}
\newlabel{eq:chain_prior}{{15}{13}{ELBO of Tree Models}{equation.B.15}{}}
\newlabel{eq:elbo12L}{{16}{13}{ELBO of Tree Models}{equation.B.16}{}}
\citation{rezende2015variational,kingma2016improving,berg2018sylvester}
\citation{rezende2015variational,kingma2016improving,berg2018sylvester}
\newlabel{eq:kl_lL}{{17}{14}{ELBO of Tree Models}{equation.B.17}{}}
\newlabel{eq:elbo0}{{18}{14}{ELBO of Tree Models}{equation.B.18}{}}
\newlabel{eq:kl_l}{{19}{14}{ELBO of Tree Models}{equation.B.19}{}}
\newlabel{eq:elbo1}{{20}{14}{ELBO of Tree Models}{equation.B.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Improve ELBO Estimation with Flows}{14}{subsection.B.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}ELBO of DAG Models}{14}{subsection.B.3}}
\newlabel{appd:dag_elbo}{{B.3}{14}{ELBO of DAG Models}{subsection.B.3}{}}
\citation{kingma2013auto}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces DAG structure. The inverse topology order is {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \{\vcenter to\@ne \big@size {}\right .$}\box \z@ } \{1,2,3\}, \{4,5\}, \{6\}, \{7\} {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left \}\vcenter to\@ne \big@size {}\right .$}\box \z@ }, and it corresponds to layers 0 to 3. \relax }}{15}{figure.caption.16}}
\newlabel{fig:dag}{{11}{15}{DAG structure. The inverse topology order is \big \{ \{1,2,3\}, \{4,5\}, \{6\}, \{7\} \big \}, and it corresponds to layers 0 to 3. \relax }{figure.caption.16}{}}
\newlabel{eq:dag_elbo}{{21}{15}{ELBO of DAG Models}{equation.B.21}{}}
\newlabel{eq:dag_chain_q}{{22}{15}{ELBO of DAG Models}{equation.B.22}{}}
\newlabel{eq:dag_chain_p}{{23}{15}{ELBO of DAG Models}{equation.B.23}{}}
\newlabel{eq:dag_kl_lL}{{24}{15}{ELBO of DAG Models}{equation.B.24}{}}
\citation{kingma2013auto}
\@writefile{toc}{\contentsline {section}{\numberline {C}Model the Distributions of Latent Latent Representations at Nodes}{16}{appendix.C}}
\newlabel{sec:vfg_inference}{{C}{16}{Model the Distributions of Latent Latent Representations at Nodes}{appendix.C}{}}
\newlabel{eq:post_smp}{{25}{16}{Model the Distributions of Latent Latent Representations at Nodes}{equation.C.25}{}}
\newlabel{eq:prior_smp}{{26}{16}{Model the Distributions of Latent Latent Representations at Nodes}{equation.C.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Distributions at Nodes}{16}{subsection.C.1}}
\newlabel{eq:elbo_dag}{{27}{16}{Distributions at Nodes}{equation.C.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.1}KL Computation for Nodes}{16}{subsubsection.C.1.1}}
\citation{rezende2015variational,berg2018sylvester}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Layer-wise sampling of the posterior and the prior corresponds to forward\nobreakspace  {}(encoding) and backward\nobreakspace  {}(decoding) message passing, respectively.}\relax }}{17}{figure.caption.17}}
\newlabel{fig:message}{{12}{17}{{\small Layer-wise sampling of the posterior and the prior corresponds to forward~(encoding) and backward~(decoding) message passing, respectively.}\relax }{figure.caption.17}{}}
\newlabel{eq:kl_est}{{28}{17}{KL Computation for Nodes}{equation.C.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}More Details on Inference}{17}{appendix.D}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Inference on DAGs}{17}{subsection.D.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  {\relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Aggregation node in a DAG.}\relax }}{17}{figure.caption.18}}
\newlabel{fig:dag_aggr}{{13}{17}{{\small Aggregation node in a DAG.}\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Proof of Lemma\nobreakspace  {}\ref  {lm:apprx}}{17}{subsection.D.2}}
\newlabel{appd:proof_lm1}{{D.2}{17}{Proof of Lemma~\ref {lm:apprx}}{subsection.D.2}{}}
\citation{Book:Hanson_1994,Article:Krantz_2008}
\citation{Khemakhem20a,Sorrenson2020}
\citation{efron1975defining}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3} Inference Reliability and Expressive Power}{18}{subsection.D.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.3.1}Prediction Error }{18}{subsubsection.D.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.3.2} Probability Tractability}{18}{subsubsection.D.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Theoretical Justifications for Latent Representation Learning}{18}{appendix.E}}
\newlabel{sec:theory}{{E}{18}{Theoretical Justifications for Latent Representation Learning}{appendix.E}{}}
\newlabel{eq:exp_h}{{29}{18}{Theoretical Justifications for Latent Representation Learning}{equation.E.29}{}}
\newlabel{eq:xt_gen}{{30}{18}{Theoretical Justifications for Latent Representation Learning}{equation.E.30}{}}
\citation{Khemakhem20a}
\newlabel{eq:u_diff}{{31}{19}{Theoretical Justifications for Latent Representation Learning}{equation.E.31}{}}
\newlabel{eq:A_sim}{{32}{19}{Theoretical Justifications for Latent Representation Learning}{equation.E.32}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
