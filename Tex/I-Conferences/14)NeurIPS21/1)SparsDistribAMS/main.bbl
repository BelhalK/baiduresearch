\begin{thebibliography}{10}

\bibitem{Proc:Agrawal_NIPS19}
Naman Agarwal, Ananda~Theertha Suresh, Felix~X. Yu, Sanjiv Kumar, and Brendan
  McMahan.
\newblock cpsgd: Communication-efficient and differentially-private distributed
  {SGD}.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, pages 7575--7586, 2018.

\bibitem{ajalloeian2020analysis}
Ahmad Ajalloeian and Sebastian~U Stich.
\newblock Analysis of sgd with biased gradient estimators.
\newblock {\em arXiv preprint arXiv:2008.00051}, 2020.

\bibitem{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
  Konstantinov, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock {\em arXiv preprint arXiv:1809.10505}, 2018.

\bibitem{Proc:Basu_NIPS19}
Debraj Basu, Deepesh Data, Can Karakus, and Suhas~N. Diggavi.
\newblock Qsparse-local-sgd: Distributed {SGD} with quantization,
  sparsification and local computations.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 14668--14679, 2019.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem{Proc:Bernstein_ICLR19}
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock signsgd with majority vote is communication efficient and fault
  tolerant.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem{Arxiv:Beznosikov20}
Aleksandr Beznosikov, Samuel Horv{\'{a}}th, Peter Richt{\'{a}}rik, and Mher
  Safaryan.
\newblock On biased compression for distributed learning.
\newblock {\em CoRR}, abs/2002.12410, 2020.

\bibitem{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning},
  3(1):1--122, 2011.

\bibitem{Proc:Chang18}
Ken Chang, Niranjan Balachandar, Carson~K. Lam, Darvin Yi, James~M. Brown,
  Andrew Beers, Bruce~R. Rosen, Daniel~L. Rubin, and Jayashree
  Kalpathy{-}Cramer.
\newblock Distributed deep learning networks among institutions for medical
  imaging.
\newblock {\em J. Am. Medical Informatics Assoc.}, 25(8):945--954, 2018.

\bibitem{Proc:Charikar_ICALP02}
Moses Charikar, Kevin~C. Chen, and Martin Farach{-}Colton.
\newblock Finding frequent items in data streams.
\newblock In {\em Automata, Languages and Programming, 29th International
  Colloquium, {ICALP} 2002, Malaga, Spain, July 8-13, 2002, Proceedings},
  volume 2380 of {\em Lecture Notes in Computer Science}, pages 693--703.
  Springer, 2002.

\bibitem{chen2020quantized}
Congliang Chen, Li~Shen, Haozhi Huang, Qi~Wu, and Wei Liu.
\newblock Quantized adam with error feedback.
\newblock {\em arXiv preprint arXiv:2004.14180}, 2020.

\bibitem{Proc:Chen_ICLR19}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem{chilimbi2014project}
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In {\em Symposium on Operating Systems Design and Implementation},
  pages 571--582, 2014.

\bibitem{Arxiv:Choi_2019}
Dami Choi, Christopher~J. Shallue, Zachary Nado, Jaehoon Lee, Chris~J.
  Maddison, and George~E. Dahl.
\newblock On empirical comparisons of optimizers for deep learning.
\newblock {\em CoRR}, abs/1910.05446, 2019.

\bibitem{Proc:Covington_2016}
Paul Covington, Jay Adams, and Emre Sargin.
\newblock Deep neural networks for youtube recommendations.
\newblock In {\em Proceedings of the 10th {ACM} Conference on Recommender
  Systems, Boston, MA, USA, September 15-19, 2016}, pages 191--198. {ACM},
  2016.

\bibitem{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In {\em Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pages 561--574, 2017.

\bibitem{Proc:Dean_NIPS12}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc~V. Le,
  Mark~Z. Mao, Marc'Aurelio Ranzato, Andrew~W. Senior, Paul~A. Tucker, Ke~Yang,
  and Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in Neural Information Processing Systems 25: 26th
  Annual Conference on Neural Information Processing Systems 2012. Proceedings
  of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States},
  pages 1232--1240, 2012.

\bibitem{Proc:8-bit_ICLR16}
Tim Dettmers.
\newblock 8-bit approximations for parallelism in deep learning.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem{Proc:BERT}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
  1 (Long and Short Papers)}, pages 4171--4186. Association for Computational
  Linguistics, 2019.

\bibitem{duchi2011dual}
John~C Duchi, Alekh Agarwal, and Martin~J Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock {\em IEEE Transactions on Automatic control}, 57(3):592--606, 2011.

\bibitem{Duchi10-adagrad}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In {\em {COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010}, pages 257--269, 2010.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{Proc:GAN_NIPS14}
Ian~J. Goodfellow, Jean Pouget{-}Abadie, Mehdi Mirza, Bing Xu, David
  Warde{-}Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems 27: Annual
  Conference on Neural Information Processing Systems 2014, December 8-13 2014,
  Montreal, Quebec, Canada}, pages 2672--2680, 2014.

\bibitem{Arxiv:Goyal17}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock {\em CoRR}, abs/1706.02677, 2017.

\bibitem{Proc:Graves_ICASSP13}
Alex Graves, Abdel{-}rahman Mohamed, and Geoffrey~E. Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em {IEEE} International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP} 2013, Vancouver, BC, Canada, May 26-31, 2013},
  pages 6645--6649. {IEEE}, 2013.

\bibitem{Proc:Resnet_CVPR16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em 2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pages
  770--778. {IEEE} Computer Society, 2016.

\bibitem{hong2017prox}
Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1529--1538, 2017.

\bibitem{Proc:Ivkin_NIPS19}
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica,
  and Raman Arora.
\newblock Communication-efficient distributed {SGD} with sketching.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 13144--13154, 2019.

\bibitem{Proc:Jiang_SIGMOD18}
Jiawei Jiang, Fangcheng Fu, Tong Yang, and Bin Cui.
\newblock Sketchml: Accelerating distributed machine learning with data
  sketches.
\newblock In {\em Proceedings of the 2018 International Conference on
  Management of Data, {SIGMOD} Conference 2018, Houston, TX, USA, June 10-15,
  2018}, pages 1269--1284. {ACM}, 2018.

\bibitem{jiang2018linear}
Peng Jiang and Gagan Agrawal.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 2530--2541, 2018.

\bibitem{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian~U Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock {\em arXiv preprint arXiv:1901.09847}, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In {\em International Conference on Machine Learning}, pages
  3478--3487, 2019.

\bibitem{Proc:Lin_ICLR18}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and Bill Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.

\bibitem{lu2019gnsd}
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong.
\newblock Gnsd: A gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In {\em 2019 IEEE Data Science Workshop (DSW)}, pages 315--321, 2019.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{mikami2018massively}
Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, Yuichi Kageyama, et~al.
\newblock Massively distributed sgd: Imagenet/resnet-50 training in a flash.
\newblock {\em arXiv preprint arXiv:1811.05233}, 2018.

\bibitem{Arxiv:MnihKSGAWR13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em CoRR}, abs/1312.5602, 2013.

\bibitem{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock {\em arXiv preprint arXiv:1901.09109}, 2019.

\bibitem{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48, 2009.

\bibitem{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 19(4):1574--1609, 2009.

\bibitem{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Proc:Seide14}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em {INTERSPEECH} 2014, 15th Annual Conference of the
  International Speech Communication Association, Singapore, September 14-18,
  2014}, pages 1058--1062. {ISCA}, 2014.

\bibitem{Proc:Shen_ICML18}
Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, and Hui Qian.
\newblock Towards more efficient stochastic decentralized learning: Faster
  convergence and sparse communication.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  4631--4640. {PMLR}, 2018.

\bibitem{shi2019convergence}
Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu.
\newblock A convergence analysis of distributed sgd with
  communication-efficient gradient sparsification.
\newblock In {\em IJCAI}, pages 3411--3417, 2019.

\bibitem{AlphaGo_17}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  Yutian Chen, Timothy~P. Lillicrap, Fan Hui, Laurent Sifre, George van~den
  Driessche, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nat.}, 550(7676):354--359, 2017.

\bibitem{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem{Article:Stich_arxiv19}
Sebastian~U. Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock {\em CoRR}, abs/1909.05350, 2019.

\bibitem{CV_review18}
Athanasios Voulodimos, Nikolaos Doulamis, Anastasios~D. Doulamis, and Eftychios
  Protopapadakis.
\newblock Deep learning for computer vision: {A} brief review.
\newblock {\em Comput. Intell. Neurosci.}, 2018:7068349:1--7068349:13, 2018.

\bibitem{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1299--1309, 2018.

\bibitem{Article:Wei_2017}
Jian Wei, Jianhua He, Kai Chen, Yi~Zhou, and Zuoyin Tang.
\newblock Collaborative filtering and deep learning based recommendation system
  for cold start items.
\newblock {\em Expert Systems with Applications}, 69:29--39, 2017.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock {\em arXiv preprint arXiv:1705.07878}, 2017.

\bibitem{Proc:Wu_ICML18}
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.
\newblock Error compensated quantized {SGD} and its applications to large-scale
  distributed optimization.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of {\em Proceedings of Machine Learning Research}, pages
  5321--5329. {PMLR}, 2018.

\bibitem{yang2019swalp}
Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew~Gordon
  Wilson, and Chris De~Sa.
\newblock Swalp: Stochastic weight averaging in low precision training.
\newblock In {\em International Conference on Machine Learning}, pages
  7015--7024. PMLR, 2019.

\bibitem{Proc:LAMB_ICLR20}
Yang You, Jing Li, Sashank~J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho{-}Jui Hsieh.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem{NLP_review18}
Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria.
\newblock Recent trends in deep learning based natural language processing
  [review article].
\newblock {\em {IEEE} Comput. Intell. Mag.}, 13(3):55--75, 2018.

\bibitem{Proc:Yu_ICML19}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  7184--7193. {PMLR}, 2019.

\bibitem{Proc:Yu_AISTATS19}
Yue Yu, Jiaxiang Wu, and Junzhou Huang.
\newblock Exploring fast and communication-efficient algorithms in large-scale
  distributed networks.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2019, 16-18 April 2019, Naha, Okinawa, Japan},
  volume~89 of {\em Proceedings of Machine Learning Research}, pages 674--683.
  {PMLR}, 2019.

\bibitem{Proc:adadelta}
Matthew~D. Zeiler.
\newblock {ADADELTA:} an adaptive learning rate method.
\newblock {\em CoRR}, abs/1212.5701, 2012.

\bibitem{Proc:Zhang_ICML17}
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji~Liu, and Ce~Zhang.
\newblock Zipml: Training linear models with end-to-end low precision, and a
  little bit of deep learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, volume~70
  of {\em Proceedings of Machine Learning Research}, pages 4035--4043. {PMLR},
  2017.

\bibitem{sentiment_review18}
Lei Zhang, Shuai Wang, and Bing Liu.
\newblock Deep learning for sentiment analysis: {A} survey.
\newblock {\em Wiley Interdiscip. Rev. Data Min. Knowl. Discov.}, 8(4), 2018.

\bibitem{Arxiv:Zhang_ICLR21}
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian~Q. Weinberger, and Yoav Artzi.
\newblock Revisiting few-sample {BERT} fine-tuning.
\newblock {\em CoRR}, abs/2006.05987, 2020.

\bibitem{Proc:Zhao_MLsys20}
Weijie Zhao, Deping Xie, Ronglai Jia, Yulei Qian, Ruiquan Ding, Mingming Sun,
  and Ping Li.
\newblock Distributed hierarchical {GPU} parameter server for massive scale
  deep learning ads systems.
\newblock In {\em Proceedings of Machine Learning and Systems 2020, MLSys 2020,
  Austin, TX, USA, March 2-4, 2020}. mlsys.org, 2020.

\bibitem{Proc:Zheng_NIPS19}
Shuai Zheng, Ziyue Huang, and James~T. Kwok.
\newblock Communication-efficient distributed blockwise momentum {SGD} with
  error-feedback.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 11446--11456, 2019.

\bibitem{Arxiv:Zhou_18}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em CoRR}, abs/1808.05671, 2018.

\end{thebibliography}
