\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Proc:GAN_NIPS14,Proc:Resnet_CVPR16,CV_review18}
\citation{Proc:Graves_ICASSP13,NLP_review18,sentiment_review18}
\citation{Arxiv:MnihKSGAWR13,AlphaGo_17}
\citation{Proc:Covington_2016,Article:Wei_2017}
\citation{Proc:Chang18}
\citation{Proc:Dean_NIPS12}
\citation{boyd2011distributed,nedic2009distributed,duchi2011dual,Arxiv:Goyal17,hong2017prox,lu2019gnsd,koloskova2019decentralized}
\citation{chilimbi2014project,Proc:Agrawal_NIPS19,mikami2018massively}
\citation{Proc:Seide14}
\citation{alistarh2017qsgd,wen2017terngrad,wangni2018gradient,stich2018sparsified,aji2017sparse,bernstein2018signsgd,de2017understanding,yang2019swalp,Proc:Ivkin_NIPS19}
\citation{stich2018sparsified,ajalloeian2020analysis}
\citation{karimireddy2019error}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{Duchi10-adagrad}
\citation{kingma2014adam}
\citation{reddi2019convergence}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Our Contributions}{2}{subsection.1.1}}
\citation{Proc:8-bit_ICLR16}
\citation{Proc:Seide14,bernstein2018signsgd,karimireddy2019error,Proc:Bernstein_ICLR19}
\citation{alistarh2017qsgd,Proc:Wu_ICML18,Proc:Zhang_ICML17}
\citation{Proc:Yu_AISTATS19}
\citation{Proc:Lin_ICLR18}
\citation{wangni2018gradient}
\citation{stich2018sparsified,shi2019convergence}
\citation{Proc:Lin_ICLR18}
\citation{Proc:Ivkin_NIPS19}
\citation{Proc:Charikar_ICALP02}
\citation{jiang2018linear,Proc:Shen_ICML18,alistarh2018convergence,Proc:Basu_NIPS19,Proc:Jiang_SIGMOD18}
\citation{ajalloeian2020analysis,Arxiv:Beznosikov20}
\citation{stich2018sparsified,karimireddy2019error}
\citation{nemirovski2009robust,ghadimi2013stochastic}
\citation{Proc:Zheng_NIPS19,Article:Stich_arxiv19}
\citation{Duchi10-adagrad}
\citation{Proc:adadelta}
\citation{kingma2014adam}
\citation{reddi2019convergence}
\citation{Arxiv:Choi_2019,Proc:LAMB_ICLR20,Arxiv:Zhang_ICLR21}
\citation{nazari2019dadam}
\citation{chen2020quantized}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}}
\newlabel{sec:related}{{2}{3}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Distributed SGD with Compressed Gradients}{3}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Adaptive Optimization}{3}{subsection.2.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {AMSGrad} optimization method}}{3}{algorithm.1}}
\newlabel{alg:amsgrad}{{1}{3}{Adaptive Optimization}{algorithm.1}{}}
\newlabel{line:maxop}{{7}{3}{Adaptive Optimization}{ALC@unique.7}{}}
\citation{stich2018sparsified,Proc:Zheng_NIPS19}
\citation{Proc:Seide14,bernstein2018signsgd}
\citation{chen2020quantized}
\citation{chen2020quantized}
\citation{kingma2014adam}
\citation{chen2020quantized}
\citation{Proc:BERT,Proc:Zhao_MLsys20}
\@writefile{toc}{\contentsline {section}{\numberline {3}Communication-Efficient Adaptive Optimization}{4}{section.3}}
\newlabel{sec:main}{{3}{4}{Communication-Efficient Adaptive Optimization}{section.3}{}}
\newlabel{eq:opt}{{1}{4}{Communication-Efficient Adaptive Optimization}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient Compressors}{4}{subsection.3.1}}
\newlabel{ass:quant}{{1}{4}{}{assumption.1}{}}
\newlabel{def:topk}{{1}{4}{Top-$k$}{definition.1}{}}
\newlabel{def:sign}{{2}{4}{Block-Sign}{definition.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\textsc  {Comp-AMS}\ for Distributed Optimization}{4}{subsection.3.2}}
\citation{chen2020quantized}
\citation{reddi2019convergence,Proc:Chen_ICLR19}
\citation{chen2020quantized}
\citation{chen2020quantized}
\citation{reddi2019convergence,Arxiv:Zhou_18,Proc:Chen_ICLR19}
\citation{mcmahan2017communication}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Distributed \textsc  {Comp-AMS}\ with error-feedback}}{5}{algorithm.2}}
\newlabel{alg:sparsams}{{2}{5}{\algo \ for Distributed Optimization}{algorithm.2}{}}
\newlabel{line:topk}{{7}{5}{\algo \ for Distributed Optimization}{ALC@unique.16}{}}
\newlabel{line:v}{{15}{5}{\algo \ for Distributed Optimization}{ALC@unique.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Convergence Analysis}{5}{section.4}}
\newlabel{sec:theory}{{4}{5}{Convergence Analysis}{section.4}{}}
\newlabel{ass:smooth}{{2}{5}{}{assumption.2}{}}
\newlabel{ass:boundgrad}{{3}{5}{}{assumption.3}{}}
\newlabel{ass:var}{{4}{5}{}{assumption.4}{}}
\citation{karimireddy2019error}
\citation{Arxiv:Zhou_18}
\citation{karimireddy2019error}
\citation{Proc:Yu_ICML19}
\citation{Proc:Zheng_NIPS19,jiang2018linear}
\citation{Proc:Yu_ICML19}
\newlabel{theo:rate}{{1}{6}{}{Theorem.1}{}}
\newlabel{coro:mainsingle}{{1}{6}{}{Corollary.1}{}}
\newlabel{coro:linear speedup}{{2}{6}{}{Corollary.2}{}}
\newlabel{label:eq:linear speedup}{{2}{6}{}{equation.4.2}{}}
\citation{mnist}
\citation{wangni2018gradient}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}{section.5}}
\newlabel{sec:experiment}{{5}{7}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Error Feedback Fixes the Convergence of Compressed AMSGrad}{7}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training loss and test accuracy of \textsc  {Comp-AMS}\ using a single machine.}}{7}{figure.1}}
\newlabel{fig:mnist-1}{{1}{7}{Training loss and test accuracy of \algo \ using a single machine}{figure.1}{}}
\citation{cifar}
\citation{imdb}
\citation{chen2020quantized}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Linear Speedup of \textsc  {Comp-AMS}}{8}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training loss of \textsc  {Comp-AMS}\ with \textbf  {Top-$k$}-0.01 on MNIST.}}{8}{figure.2}}
\newlabel{fig:speedup}{{2}{8}{Training loss of \algo \ with \textbf {Top-$k$}-0.01 on MNIST}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}General Evaluation and Communication Efficiency}{8}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Discussion on \textsc  {Comp-AMS}\ and \textsc  {QAdam}}{8}{subsection.5.4}}
\citation{Proc:Liu_ICLR20}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Test accuracy of distributed \textsc  {Comp-AMS}. Top row: accuracy vs. epochs. Bottom row: accuracy vs. number of bits transmitted per worker.}}{9}{figure.3}}
\newlabel{fig:test accuracy dist}{{3}{9}{Test accuracy of distributed \algo . Top row: accuracy vs. epochs. Bottom row: accuracy vs. number of bits transmitted per worker}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}{section.6}}
\newlabel{sec:conclusion}{{6}{9}{Conclusion}{section.6}{}}
\bibstyle{plain}
\bibdata{ref}
\bibcite{Proc:Agrawal_NIPS19}{{1}{}{{}}{{}}}
\bibcite{ajalloeian2020analysis}{{2}{}{{}}{{}}}
\bibcite{aji2017sparse}{{3}{}{{}}{{}}}
\bibcite{alistarh2017qsgd}{{4}{}{{}}{{}}}
\bibcite{alistarh2018convergence}{{5}{}{{}}{{}}}
\bibcite{Proc:Basu_NIPS19}{{6}{}{{}}{{}}}
\bibcite{bernstein2018signsgd}{{7}{}{{}}{{}}}
\bibcite{Proc:Bernstein_ICLR19}{{8}{}{{}}{{}}}
\bibcite{Arxiv:Beznosikov20}{{9}{}{{}}{{}}}
\bibcite{boyd2011distributed}{{10}{}{{}}{{}}}
\bibcite{Proc:Chang18}{{11}{}{{}}{{}}}
\bibcite{Proc:Charikar_ICALP02}{{12}{}{{}}{{}}}
\bibcite{chen2020quantized}{{13}{}{{}}{{}}}
\bibcite{Proc:Chen_ICLR19}{{14}{}{{}}{{}}}
\bibcite{chilimbi2014project}{{15}{}{{}}{{}}}
\bibcite{Arxiv:Choi_2019}{{16}{}{{}}{{}}}
\bibcite{Proc:Covington_2016}{{17}{}{{}}{{}}}
\bibcite{de2017understanding}{{18}{}{{}}{{}}}
\bibcite{Proc:Dean_NIPS12}{{19}{}{{}}{{}}}
\bibcite{Proc:8-bit_ICLR16}{{20}{}{{}}{{}}}
\bibcite{Proc:BERT}{{21}{}{{}}{{}}}
\bibcite{duchi2011dual}{{22}{}{{}}{{}}}
\bibcite{Duchi10-adagrad}{{23}{}{{}}{{}}}
\bibcite{ghadimi2013stochastic}{{24}{}{{}}{{}}}
\bibcite{Proc:GAN_NIPS14}{{25}{}{{}}{{}}}
\bibcite{Arxiv:Goyal17}{{26}{}{{}}{{}}}
\bibcite{Proc:Graves_ICASSP13}{{27}{}{{}}{{}}}
\bibcite{Proc:Resnet_CVPR16}{{28}{}{{}}{{}}}
\bibcite{hong2017prox}{{29}{}{{}}{{}}}
\bibcite{Proc:Ivkin_NIPS19}{{30}{}{{}}{{}}}
\bibcite{Proc:Jiang_SIGMOD18}{{31}{}{{}}{{}}}
\bibcite{jiang2018linear}{{32}{}{{}}{{}}}
\bibcite{karimireddy2019error}{{33}{}{{}}{{}}}
\bibcite{kingma2014adam}{{34}{}{{}}{{}}}
\bibcite{koloskova2019decentralized}{{35}{}{{}}{{}}}
\bibcite{cifar}{{36}{}{{}}{{}}}
\bibcite{mnist}{{37}{}{{}}{{}}}
\bibcite{Proc:Lin_ICLR18}{{38}{}{{}}{{}}}
\bibcite{Proc:Liu_ICLR20}{{39}{}{{}}{{}}}
\bibcite{lu2019gnsd}{{40}{}{{}}{{}}}
\bibcite{imdb}{{41}{}{{}}{{}}}
\bibcite{mcmahan2017communication}{{42}{}{{}}{{}}}
\bibcite{mikami2018massively}{{43}{}{{}}{{}}}
\bibcite{Arxiv:MnihKSGAWR13}{{44}{}{{}}{{}}}
\bibcite{nazari2019dadam}{{45}{}{{}}{{}}}
\bibcite{nedic2009distributed}{{46}{}{{}}{{}}}
\bibcite{nemirovski2009robust}{{47}{}{{}}{{}}}
\bibcite{reddi2019convergence}{{48}{}{{}}{{}}}
\bibcite{Proc:Seide14}{{49}{}{{}}{{}}}
\bibcite{Proc:Shen_ICML18}{{50}{}{{}}{{}}}
\bibcite{shi2019convergence}{{51}{}{{}}{{}}}
\bibcite{AlphaGo_17}{{52}{}{{}}{{}}}
\bibcite{stich2018sparsified}{{53}{}{{}}{{}}}
\bibcite{Article:Stich_arxiv19}{{54}{}{{}}{{}}}
\bibcite{CV_review18}{{55}{}{{}}{{}}}
\bibcite{wangni2018gradient}{{56}{}{{}}{{}}}
\bibcite{Article:Wei_2017}{{57}{}{{}}{{}}}
\bibcite{wen2017terngrad}{{58}{}{{}}{{}}}
\bibcite{Proc:Wu_ICML18}{{59}{}{{}}{{}}}
\bibcite{yang2019swalp}{{60}{}{{}}{{}}}
\bibcite{Proc:LAMB_ICLR20}{{61}{}{{}}{{}}}
\bibcite{NLP_review18}{{62}{}{{}}{{}}}
\bibcite{Proc:Yu_ICML19}{{63}{}{{}}{{}}}
\bibcite{Proc:Yu_AISTATS19}{{64}{}{{}}{{}}}
\bibcite{Proc:adadelta}{{65}{}{{}}{{}}}
\bibcite{Proc:Zhang_ICML17}{{66}{}{{}}{{}}}
\bibcite{sentiment_review18}{{67}{}{{}}{{}}}
\bibcite{Arxiv:Zhang_ICLR21}{{68}{}{{}}{{}}}
\bibcite{Proc:Zhao_MLsys20}{{69}{}{{}}{{}}}
\bibcite{Proc:Zheng_NIPS19}{{70}{}{{}}{{}}}
\bibcite{Arxiv:Zhou_18}{{71}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of the Convergence Result}{16}{appendix.A}}
\newlabel{app:proof}{{A}{16}{Proof of the Convergence Result}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Intermidiary Lemmas}{16}{subsection.A.1}}
\newlabel{app:lemmas}{{A.1}{16}{Intermidiary Lemmas}{subsection.A.1}{}}
\newlabel{lemma:m_t,m_t'}{{1}{16}{}{Lemma.1}{}}
\newlabel{lemma:bound e_t}{{2}{17}{}{Lemma.2}{}}
\newlabel{eq:e_t 0}{{3}{17}{Intermidiary Lemmas}{equation.A.3}{}}
\newlabel{lemma:bound big E_t}{{3}{17}{}{Lemma.3}{}}
\newlabel{lemma:bound v_t}{{4}{18}{}{Lemma.4}{}}
\newlabel{lemma:bound difference}{{5}{18}{}{Lemma.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proof of Theorem\nobreakspace  {}\ref  {theo:rate}}{19}{subsection.A.2}}
\newlabel{app:thm}{{A.2}{19}{Proof of Theorem~\ref {theo:rate}}{subsection.A.2}{}}
\newlabel{eq0}{{4}{20}{Proof of Theorem~\ref {theo:rate}}{equation.A.4}{}}
\newlabel{eq:I}{{5}{21}{Proof of Theorem~\ref {theo:rate}}{equation.A.5}{}}
\newlabel{eq:II}{{6}{21}{Proof of Theorem~\ref {theo:rate}}{equation.A.6}{}}
\newlabel{eq:III}{{7}{21}{Proof of Theorem~\ref {theo:rate}}{equation.A.7}{}}
\newlabel{eq:IV}{{8}{21}{Proof of Theorem~\ref {theo:rate}}{equation.A.8}{}}
\newlabel{eq:IV error}{{9}{22}{Proof of Theorem~\ref {theo:rate}}{equation.A.9}{}}
\citation{mnist}
\citation{cifar}
\@writefile{toc}{\contentsline {section}{\numberline {B}Model Architecture of the Experiments}{23}{appendix.B}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Model architectures used in the experiments. Left: MNIST + CNN. Middle: CIFAR-10 + CNN. Right: IMDB + LSTM. In the last figure, the penultimate linear layer takes the last hidden state (64-dim vector) as the input.}}{23}{figure.4}}
\newlabel{fig:model arch}{{4}{23}{Model architectures used in the experiments. Left: MNIST + CNN. Middle: CIFAR-10 + CNN. Right: IMDB + LSTM. In the last figure, the penultimate linear layer takes the last hidden state (64-dim vector) as the input}{figure.4}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {C}Additional content}{24}{appendix.C}}
\newlabel{app:add}{{C}{24}{Additional content}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Extension to the single-machine setting}{24}{subsection.C.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \textsc  {Comp-AMS}\ with error-feedback for a single-machine}}{24}{algorithm.3}}
\newlabel{alg:sparsamssingle}{{3}{24}{Extension to the single-machine setting}{algorithm.3}{}}
\newlabel{line:stochgrad}{{4}{24}{Extension to the single-machine setting}{ALC@unique.30}{}}
\newlabel{line:topksingle}{{5}{24}{Extension to the single-machine setting}{ALC@unique.31}{}}
\newlabel{line:vsingle}{{9}{24}{Extension to the single-machine setting}{ALC@unique.35}{}}
