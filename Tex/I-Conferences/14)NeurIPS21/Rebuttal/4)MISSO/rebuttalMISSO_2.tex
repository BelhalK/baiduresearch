\documentclass{article}
\usepackage{xcolor,bm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{lipsum}


\begin{document}






\textbf{Reviewer J9Ez (7;4):}

Weaknesses:

The practical performance of the method is reasonably strong, but I find the experiment on CIFAR-10 a bit puzzling; for the proposed MISSO algorithm, the ELBO starts increasing, while MC-Adam seems to converge stably. Is this due to a perhaps suboptimal choice of parameters in the MISSO algorithm?

The method requires one to store the previous surrogate models, which could be problematic in large-scale applications. I am surprised it was feasible to apply the algorithm to a ResNet-18 on CIFAR-10. Or am I missing something here?

Note that there are stronger baselines than "Bayes by Backprop" for Bayesian deep learning. Since the main contribution of this work is the theoretical analysis, I don't expect a comparison to those (but it still would be nice). But Perhaps they could be cited in the paper:

Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam, Khan et al., ICML 2018
Practical Deep Learning with Bayesian Principles, Osawa et al., NeurIPS 2020


It is unclear to what extend the theoretical analysis can be applied to understand the considered applications, e.g. I am not sure whether the Bayesian deep learning setting fulfills assumption H1-H4.

Note that when using a location-scale parametrization for the variational Gaussian distribution, it is known that the expectation in the cost function is L-smooth and convex for convex and L-smooth integrands, see "Provable Smoothness Guarantees for Black-Box Variational Inference", Domke, ICML 2020. Perhaps this fact could be used to rigorously verify the assumptions for the considered applications?

In general, my suggestion is to address in the experimental section to what extend the theory applies to the considered experiments.





\textcolor{purple}{Our reply:}
Thank you for your detailed review and suggestions.



\textbf{Reviewer N978 (5;2):}

My rating is based on several concerns listed below.

A subproblem needs to be solved accurately at each iteration, which has a high complexity itself and should be taken account into the complexity analysis.

I am not clear about the technical difficulty to incorporate Monte Carlo and how it helps the theory.

What would go wrong if we abstractly merge the n distributions into one distribution, and draw a minibatch each iteration to make updates? I am not sure how this would affect the theory but it may not be too bad in practice.

\textcolor{purple}{Our reply:}

Thank you for your review.
We would like to clarify that our method applies when those surrogate functions are not directly computable but rather require an approximation (as explaine example 1 and 2).
The subproblem that you rightly mentioned is always easily solvable at each MISSO iteration specifically because the surrogate functions are user designed and hence are tractable and convex. Just like we explain in example 2 (VI), the surrogate functions are quadratic and the subproblem you are mentioning is solved using a single gradient step. As for general Majorization-Minimization methods, the surrogate minimization does not add substantial cost to the overall, and complex, optimization problem.





\textbf{Reviewer 2PVe (4;4):}

Cons and my concerns:

The author(s) stated that “MISSO is a general framework for nonconvex and nonsmooth finite sum problems”. I believe that mention nonsmoothness here is misleading. It is hard to imagine a nonsmooth function (such as the absolute value function at the origin) that could satisfy H2 in the paper. Also, the original MISO paper did not say it works for nonsmooth problems. I think it is necessary to remove emphasis on nonsmoothness in this paper.
In line 87, the author(s) stated that H2 can be implied by assuming 
 to be -smooth from [1, Proposition 1]. I checked [1, Proposition 1] and did find much connection to H2. Can you double-check if this is the right reference? I think [2, Theorem 2.1.5, eq (2.1.7)] is more related, but even though I how to prove H2 by assuming smoothness. Can you provide more detail on how to obtain H2? I am currently a little bit skeptical about your statement.
Using Monte-Carlo sampling seems to be a straightforward approach when the surrogate function is an expectation. The algorithm itself cannot claim much novelty, I think the major contribution of this paper is the motivating example and the convergence rates.
The notation is heavy, make the theory a little bit hard to follow.
I did not find the definition of , I guess it is the number of samples at the k-th iteration? Can the author(s) point me to the definition of ?
Eq (15), the definition of 
 is wrong.
Eq (16), $\bar{M}{(k)}\bar{M}{K_{max}}$?
In line 181, why does the term $M_k$
 disappear?
Better to have a precise definition of -stationary point, so readers can better understand line 180-185.
Line 183, $g$ is a scalar, why use $|g|$?
Line 187, set 
$M_k = k^2/n^2$, then $\sum M_k^{-1/2}$ goes to infinity?
Overall, the presentation of this paper should be improved.

[1] A unified convergence analysis of block successive minimization methods for nonsmooth optimization. Razaviyayn et al. SIOPT 2013. [2] Introductory Lectures on Convex optimization: A basic course. Yurii Nesterov. 2014.



\textcolor{purple}{Our reply:}




\textbf{Reviewer 1meD (6;4):}

Clarity: The paper is well-motivated and clearly written. For example, I can follow the intro very easily and understand the main idea of the paper and the distinction from prior works. The main theorems and experiments are also clear to me.

Quality: The theoretical development is solid and interesting. For example, Theorem 1 is derived for MISSO under stochastic setting but it can be specialized for MISO. From the experiments, it is not obvious that MISSO improves prior works for logistic regression with missing values or fitting Bayesian networks (as the authors themselves confessed).

Originality: The originality is moderate. Examples 1 and 2 are well-known. The MISSO algorithm is a minor modification of MISO that incoporates Monte Carlo sampling, the later technique being already well-known.

Significance: Researchers working on stochastic optimization and (non-)convex analysis might be interested in the paper. The paper might not attract practitioners in machine learning applications mentioned in the paper.

Other Comments: Does it make sense to specialize Theorems 1-2 to Examples 1-2, or to logistic regression or fitting Bayesian networks? Please make a clarification here. It is of interest to see how large L is in those applications, as Theorem 1 requires $(nL^3)/\epsilon$ samples.

\textcolor{purple}{Our reply:}


\end{document}

