\relax 
\citation{RKK18}
\citation{LFDA17}
\citation{Rnet16,goodfellow2014generative}
\citation{Atari13}
\citation{GMH13}
\citation{RKK18}
\citation{KB15}
\citation{TH12}
\citation{Z12}
\citation{D16}
\citation{DHS11,MS10}
\citation{N04}
\citation{P64}
\citation{P64}
\citation{RKK18}
\citation{KB15}
\citation{CJ12,RS13b,SALS15,ALLW18,mertikopoulos2018optimistic}
\citation{DISZ18}
\citation{RS13b}
\citation{goodfellow2014generative}
\citation{CJ12,RS13b,SALS15}
\citation{DISZ18}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\citation{KB15,RKK18}
\citation{CJ12,RS13b,SALS15,ALLW18}
\citation{H14}
\citation{SALS15}
\citation{SALS15}
\citation{RS13b}
\citation{KB15}
\citation{P64}
\citation{DHS11}
\citation{RKK18}
\citation{DHS11}
\citation{KB15}
\citation{KB15}
\citation{RKK18}
\citation{RKK18}
\citation{RS13b}
\@writefile{toc}{\contentsline {section}{\numberline {II}Preliminaries}{2}}
\newlabel{sec:prelim}{{II}{2}}
\newlabel{optFTRL}{{1}{2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces \textsc  {AMSGrad} \citep  {RKK18}\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:amsgrad}{{1}{2}}
\newlabel{line:maxop}{{7}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}\textsc  {OPT-AMSGRAD} Algorithm}{2}}
\newlabel{sec:opt}{{III}{2}}
\citation{CJ12}
\citation{DHS11}
\citation{N04}
\citation{P64}
\citation{CJ12,RS13b,SALS15}
\citation{RKK18,KB15}
\citation{RKK18}
\citation{RS13b}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces \textsc  {OPT-AMSGrad}\relax }}{3}}
\newlabel{alg:optamsgrad}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textsc  {OPT-AMSGrad} underlying structure.\relax }}{3}}
\newlabel{fig:scheme}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Convergence Analysis}{3}}
\newlabel{sec:analysis}{{IV}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Convex Regret Analysis}{3}}
\newlabel{sec:convex}{{\unhbox \voidb@x \hbox {IV-A}}{3}}
\newlabel{THM:MAINCONVEX}{{1}{3}}
\newlabel{COR:COROLLARY}{{1}{3}}
\newlabel{eq:boundAMS}{{2}{3}}
\citation{ghadimi2013stochastic}
\citation{ghadimi2013stochastic}
\citation{ghadimi2013stochastic}
\citation{ZTYCG18}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Finite-Time Analysis in the Nonconvex Case}{4}}
\newlabel{eq:minproblem}{{3}{4}}
\newlabel{eq:random}{{4}{4}}
\newlabel{ASS:BOUNDEDPARAM}{{1}{4}}
\newlabel{ass:smooth}{{2}{4}}
\newlabel{ass:guessbound}{{3}{4}}
\newlabel{ass:bounded}{{4}{4}}
\newlabel{LEM:BOUND}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Assumption H3\hbox {} on gradient prediction.\relax }}{4}}
\newlabel{fig:assumption}{{2}{4}}
\newlabel{THM:BOUNDOPT}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Checking H1\hbox {} for a Deep Neural Network}{4}}
\newlabel{eq:dnnmodel}{{5}{4}}
\citation{ZRSKK18,CLSH19,WWB18,ZTYCG18,ZS18,LO18}
\citation{CLSH19}
\citation{Princeton18}
\citation{CYYZC19}
\citation{MY16}
\citation{MY16}
\citation{MY16}
\citation{MY16}
\citation{DISZ18}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{DISZ18}
\citation{goodfellow2014generative}
\citation{DISZ18}
\citation{WN11}
\citation{CJ76}
\citation{E79}
\citation{SAB16}
\citation{BZ13}
\citation{SAB16}
\citation{Scieur18}
\newlabel{LEM:DNNH2}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Comparison to related methods}{5}}
\newlabel{sec:related}{{V}{5}}
\newlabel{OPT-DISZ}{{3}{5}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces \textsc  {Optimistic-Adam\nobreakspace  {}\citep  {DISZ18}+$\mathaccentV {hat}05E{v}_t$}. \relax }}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Numerical Experiments}{5}}
\newlabel{sec:numerical}{{VI}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-A}}Gradient Estimation}{5}}
\newlabel{nox}{{6}{5}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Regularized Approximated Minimal Polynomial Extrapolation \citep  {SAB16} \relax }}{5}}
\newlabel{alg:algex}{{4}{5}}
\citation{RKK18}
\citation{RKK18}
\citation{KB15}
\citation{RKK18}
\citation{DISZ18}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. \relax }}{6}}
\newlabel{simu}{{3}{6}}
\citation{RKK18}
\citation{KB15}
\citation{MNIST07}
\citation{Rnet16}
\citation{gers1999learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-B}}Classification Experiments}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training loss vs. Number of iterations for fully connected NN, CNN, LSTM and ResNet.\relax }}{7}}
\newlabel{fig:train_loss}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-C}}Choice of parameter \lowercase {$r$}}{7}}
\newlabel{sec:choicer}{{\unhbox \voidb@x \hbox {VI-C}}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusion}{7}}
\bibstyle{plain}
\bibdata{reference}
\bibcite{ALLW18}{{1}{}{{}}{{}}}
\bibcite{Princeton18}{{2}{}{{}}{{}}}
\bibcite{BZ13}{{3}{}{{}}{{}}}
\bibcite{CJ76}{{4}{}{{}}{{}}}
\bibcite{CLSH19}{{5}{}{{}}{{}}}
\bibcite{CYYZC19}{{6}{}{{}}{{}}}
\bibcite{CJ12}{{7}{}{{}}{{}}}
\bibcite{DISZ18}{{8}{}{{}}{{}}}
\bibcite{D16}{{9}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textit  {MNIST-back-image} + CNN, \textit  {CIFAR10} + Res-18 and \textit  {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy.\relax }}{8}}
\newlabel{fig:testandtrain}{{5}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training loss w.r.t. different $r$ values.\relax }}{8}}
\newlabel{fig:compare}{{6}{8}}
\bibcite{DHS11}{{10}{}{{}}{{}}}
\bibcite{E79}{{11}{}{{}}{{}}}
\bibcite{gers1999learning}{{12}{}{{}}{{}}}
\bibcite{ghadimi2013stochastic}{{13}{}{{}}{{}}}
\bibcite{goodfellow2014generative}{{14}{}{{}}{{}}}
\bibcite{GMH13}{{15}{}{{}}{{}}}
\bibcite{H14}{{16}{}{{}}{{}}}
\bibcite{Rnet16}{{17}{}{{}}{{}}}
\bibcite{KB15}{{18}{}{{}}{{}}}
\bibcite{MNIST07}{{19}{}{{}}{{}}}
\bibcite{LFDA17}{{20}{}{{}}{{}}}
\bibcite{LO18}{{21}{}{{}}{{}}}
\bibcite{MS10}{{22}{}{{}}{{}}}
\bibcite{mertikopoulos2018optimistic}{{23}{}{{}}{{}}}
\bibcite{Atari13}{{24}{}{{}}{{}}}
\bibcite{MY16}{{25}{}{{}}{{}}}
\bibcite{N04}{{26}{}{{}}{{}}}
\bibcite{P64}{{27}{}{{}}{{}}}
\bibcite{RS13b}{{28}{}{{}}{{}}}
\bibcite{RKK18}{{29}{}{{}}{{}}}
\bibcite{SAB16}{{30}{}{{}}{{}}}
\bibcite{Scieur18}{{31}{}{{}}{{}}}
\bibcite{SALS15}{{32}{}{{}}{{}}}
\bibcite{TH12}{{33}{}{{}}{{}}}
\bibcite{WN11}{{34}{}{{}}{{}}}
\bibcite{WWB18}{{35}{}{{}}{{}}}
\bibcite{ZRSKK18}{{36}{}{{}}{{}}}
\bibcite{Z12}{{37}{}{{}}{{}}}
\bibcite{ZTYCG18}{{38}{}{{}}{{}}}
\bibcite{ZS18}{{39}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Appendix}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VIII-A}}Sketch Proof of Theorem\nobreakspace  {}1\hbox {}}{10}}
\newlabel{app:thmmainconvex}{{\unhbox \voidb@x \hbox {VIII-A}}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VIII-B}}Sketch Proof of Theorem\nobreakspace  {}2\hbox {}}{10}}
\newlabel{app:BOUNDOPT}{{\unhbox \voidb@x \hbox {VIII-B}}{10}}
