\begin{thebibliography}{10}

\bibitem{ALLW18}
Jacob Abernethy, Kevin~A. Lai, Kfir~Y. Levy, and Jun-Kun Wang.
\newblock Faster rates for convex-concave games.
\newblock {\em COLT}, 2018.

\bibitem{Princeton18}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock {\em ICML}, 2019.

\bibitem{BZ13}
C.~Brezinski and M.~R. Zaglia.
\newblock Extrapolation methods: theory and practice.
\newblock {\em Elsevier}, 2013.

\bibitem{CJ76}
S.~Cabay and L.~Jackson.
\newblock A polynomial extrapolation method for finding limits and antilimits
  of vector sequences.
\newblock {\em SIAM Journal on Numerical Analysis}, 1976.

\bibitem{CLSH19}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock {\em ICLR}, 2019.

\bibitem{CYYZC19}
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao
  Yang.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock {\em ICLR}, 2019.

\bibitem{CJ12}
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
  Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock {\em COLT}, 2012.

\bibitem{DISZ18}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock {\em ICLR}, 2018.

\bibitem{D16}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock {\em ICLR (Workshop Track)}, 2016.

\bibitem{DHS11}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research (JMLR)}, 2011.

\bibitem{E79}
R.~Eddy.
\newblock Extrapolating to the limit of a vector sequence.
\newblock {\em Information linkage between applied mathematics and industry,
  Elsevier}, 1979.

\bibitem{gers1999learning}
Felix~A. Gers, J\"{u}rgen~A. Schmidhuber, and Fred~A. Cummins.
\newblock Learning to forget: Continual prediction with lstm.
\newblock {\em Neural Comput.}, 12(10):2451--2471, October 2000.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock {\em NIPS}, 2014.

\bibitem{GMH13}
Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock {\em ICASSP}, 2013.

\bibitem{H14}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock {\em Foundations and Trends in Optimization}, 2016.

\bibitem{Rnet16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em CVPR}, 2016.

\bibitem{KB15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em ICLR}, 2015.

\bibitem{MNIST07}
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua
  Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock {\em ICML}, 2007.

\bibitem{LFDA17}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em NIPS}, 2017.

\bibitem{LO18}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock {\em AISTAT}, 2019.

\bibitem{MS10}
H.~Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock {\em COLT}, 2010.

\bibitem{mertikopoulos2018optimistic}
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay
  Chandrasekhar, and Georgios Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock {\em arXiv preprint arXiv:1807.02629}, 2018.

\bibitem{Atari13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em NIPS (Deep Learning Workshop)}, 2013.

\bibitem{MY16}
Mehryar Mohri and Scott Yang.
\newblock Accelerating optimization via adaptive prediction.
\newblock {\em AISTATS}, 2016.

\bibitem{N04}
Yurii Nesterov.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock {\em Springer}, 2004.

\bibitem{P64}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em Mathematics and Mathematical Physics}, 1964.

\bibitem{RS13b}
Sasha Rakhlin and Karthik Sridharan.
\newblock Optimization, learning, and games with predictable sequences.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3066--3074, 2013.

\bibitem{RKK18}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock {\em ICLR}, 2018.

\bibitem{SAB16}
Damien Scieur, Alexandre d'Aspremont, and Francis Bach.
\newblock Regularized nonlinear acceleration.
\newblock {\em NIPS}, 2016.

\bibitem{Scieur18}
Damien Scieur, Edouard Oyallon, Alexandre d'Aspremont, and Francis Bach.
\newblock Nonlinear acceleration of deep neural networks.
\newblock {\em CoRR}, abs/1805.09639, 2018.

\bibitem{SALS15}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock {\em NIPS}, 2015.

\bibitem{TH12}
T.~Tieleman and G.~Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock {\em COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem{WN11}
H.~F. Walker and P.~Ni.
\newblock Anderson acceleration for fixed-point iterations.
\newblock {\em SIAM Journal on Numerical Analysis}, 2011.

\bibitem{WWB18}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from
  any initialization.
\newblock {\em ICML}, 2019.

\bibitem{ZRSKK18}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock {\em NeurIPS}, 2018.

\bibitem{Z12}
Matthew~D. Zeiler.
\newblock Adadelta: An adaptive learning rate method.
\newblock {\em arXiv:1212.5701}, 2012.

\bibitem{ZTYCG18}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock {\em arXiv:1808.05671}, 2018.

\bibitem{ZS18}
Fangyu Zou and Li~Shen.
\newblock On the convergence of adagrad with momentum for training deep neural
  networks.
\newblock {\em arXiv:1808.03408}, 2018.

\end{thebibliography}
