\documentclass{article}

\usepackage{aistats2021_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % define colors in text
\usepackage{xspace}         % fix spacing around commands


\begin{document}

We would like to thank the four reviewers for their insightful and constructive feedback. 

\textbf{\textcolor{blue}{R1}\&\textcolor{red}{R2}\&\textbf{\color{yellow!50!black}R3}\&\textbf{\textcolor{green!50!black}{R5:}} -Further comparison with other related schemes}: We note that privacy property is one of the main reasons that (unbiased) sketching is used in FL algorithms, and we would like to emphasize that the main contribution of this work is to improve the convergence analysis of FL algorithms that use sketching in distributed setting. 
Therefore, our main baseline are algorithms based on sketching which also {benefit from privacy property}. 
Yet, per reviewers request we provide more comparison as follows. 
First, we note that there are two main approaches to reduce communication cost that is based on, firstly, periodic averaging or, secondly, gradient compression. In this direction, the state-of-the-art of FL algorithm regarding reducing communication rounds comes from [1]. 
In addition, as pointed out correctly by \textbf{\textcolor{yellow!50!black}{R3}}, since sketching is a special case of gradient compression based algorithm we compare with [2]. 
Nonetheless, we emphasize that both number of communication rounds and number of bits per communication rounds corresponding to [1] and [2], \textcolor{red!50!black}{with only focusing on unbiased compression (which does not require extra error correction step but have higher compression error compared to biased compression scheme)}, has been further improved in [3]. 
As discussed in Section.~3, we improve results in [3] by \textcolor{red!50!black}{1)  extending them to unbiased compression using \texttt{HEAPRIX} which has similar compression rate to biased schemes such as $top_k$} and \textcolor{red!50!black}{2)  bi-directional communication property due to lower dimension of sketching and not communicating models, unlike [3]}.  
The comparison with recent work in [4] which uses bi-directional compression based scheme is also provided in Section~B in the Appendix. We will add these details to a subsequent version.




\textbf{\textcolor{blue}{R1:}} We thank the reviewer for valuable comments and references. We would like to make the following clarification:\vspace{-1.5pt} 


\textbf{-Discussion on the assumptions:} Thanks for this point. We will gladly add more clarification to the compression ratio. Yet, we note that the compression ratio of the used sketching is not an assumption but it is rather a property induced by using count sketching and all the constants are discussed in detail in main theorems and comparison with prior methods, Section 4.3. 
The assumptions we use for theorems are standard in the relevant literature. 
\textbf{-Notation:} Thanks for the comment. We will move algorithm to the bottom of the page in a subsequent version.

\textbf{\textcolor{red}{R2:}} We thank the reviewer for the useful comments and typos. Our point-to-point response is as follows:\vspace{-5pt}

\textbf{-Numerical Runs:} We present in Section D of the Appendix, additional runs on CIFAR-10 showing similar performance of our method. The number of local updates $\tau$ has been set to $1$ and $5$ in the main text and we added runs with $\tau = 2$ in the Section D of the Appendix as well. Larger number of local updates $\tau$ tend to undermine the learning performance as we have observed empirically. In the heterogeneous setting, increasing $\tau$ can present a risk of learning bad local models. We acknowledge that there is a trade-off to be found here between speed of convergence and the quality of the local models (to obtain a good global one). \textbf{-Typos:} Thanks for mentioning typos, we will fix them. The index $\ell$ indicates randomly selected entries in Algorithm 2, we will fix this in a subsequent version.  


\textbf{\textcolor{yellow!50!black}{R3:}} We thank the reviewer for valuable comments. \textbf{We clarify the point regarding the comparisons as follows:}\vspace{-5pt}

 Your comment regarding the reference [2] is valid as sketching is a special case of compression, and we will definitely cite this paper and discuss it in a subsequent version. Besides, we would like to highlight that recently [3] improves the communication cost of [2] focusing on unbiased compressor. 
When using sketching, our work extends the performance of unbiased compression schemes in [3] to biased compressor \textcolor{red!50!black}{due to the use of \texttt{HEAPRIX}, and benefiting from bi-directional compression property of sketching, via lower dimension of the communicated sketches and not sharing models}.  
Hence, in addition to having privacy property of using sketching, we also improve the communication cost of [2], while \textcolor{red!50!black}{removing error feedback framework}. We will include these discussions in a subsequent version.    



\textbf{\textcolor{green!50!black}{R5:}} We thank the reviewer for valuable comments. Below we address your concerns:\vspace{-5pt}

\textbf{Additional Numerical Experiments:} Additional runs on CIFAR-10 are presented in the Appendix (Section D). While runs with different ratio of active devices at each iteration are interesting, we reported results with a practical one (half of the devices) for illustrative purposes.
We agree that rigorously comparing the number of bits transmitted between FedSGD and our methods is important.
Yet, we give the important values of $12$ and $75$ compressing ratio yielding a good order of magnitude on this latter quantity. 
Our method being almost as fast as FedSGD, despite the high compressing ratio, shows its benefits.

[1] Karimireddy, Sai Praneeth, et al. "Scaffold: Stochastic controlled averaging for federated learning." International Conference on Machine Learning. PMLR, 2020.\vspace{-1.95pt} 

[2] Basu, Debraj, et al. "Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations." Advances in Neural Information Processing Systems. 2019.\vspace{-1.95pt} 

[3] Haddadpour, Farzin, et al. "Federated learning with compression: Unified analysis and sharp guarantees." arXiv preprint arXiv:2007.01154 (2020).\vspace{-1.95pt} 

[4] Philippenko, Constantin, and Aymeric Dieuleveut. "Artemis: tight convergence guarantees for bidirectional compression in federated learning." arXiv preprint arXiv:2006.14591 (2020).

\end{document}
