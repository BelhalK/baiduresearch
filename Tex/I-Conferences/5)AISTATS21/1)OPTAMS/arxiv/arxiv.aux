\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Proc:Reddi_ICLR18}
\citation{Article:Levine_JMLR16}
\citation{Proc:He_CVPR16}
\citation{Proc:Goodfellow_NIPS14}
\citation{mnih2013playing}
\citation{Proc:Graves_ICASSP13}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Kingma_ICLR15}
\citation{Tieleman2012}
\citation{zeiler2012adadelta}
\citation{dozat2016incorporating}
\citation{Proc:Chen_FODS20}
\citation{Proc:Zhou_NeurIPS20}
\citation{Proc:McMahan_COLT10}
\citation{Proc:Duchi_JMLR11}
\citation{Book:Nesterov_2004}
\citation{Article:Polyak_1964}
\citation{Article:Polyak_1964}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Kingma_ICLR15}
\citation{Proc:Chiang_COLT12}
\citation{Proc:Rakhlin_NIPS13}
\citation{Proc:Syrgkanis_NIPS15}
\citation{Proc:Abernethy_COLT18}
\citation{Proc:Mertikopoulos_ICLR19}
\citation{Proc:Daskalakis_ICLR18}
\citation{Proc:Rakhlin_NIPS13}
\citation{Proc:Goodfellow_NIPS14}
\citation{Proc:Chiang_COLT12}
\citation{Proc:Rakhlin_NIPS13}
\citation{Proc:Syrgkanis_NIPS15}
\citation{Proc:Daskalakis_ICLR18}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Proc:Kingma_ICLR15}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Chiang_COLT12}
\citation{Proc:Rakhlin_NIPS13}
\citation{Proc:Syrgkanis_NIPS15}
\citation{Proc:Abernethy_COLT18}
\citation{hazan2019introduction}
\citation{Proc:Syrgkanis_NIPS15}
\citation{Proc:Syrgkanis_NIPS15}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:prelim}{{2}{2}{Preliminaries}{section.2}{}}
\citation{Proc:Rakhlin_NIPS13}
\citation{Proc:Kingma_ICLR15}
\citation{Article:Polyak_1964}
\citation{Proc:Duchi_JMLR11}
\citation{Proc:Duchi_JMLR11}
\citation{Proc:Kingma_ICLR15}
\citation{Proc:Kingma_ICLR15}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Rakhlin_NIPS13}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {AMSGrad}\nobreakspace  {}\cite  {Proc:Reddi_ICLR18}}}{3}{algorithm.1}}
\newlabel{alg:amsgrad}{{1}{3}{Preliminaries}{algorithm.1}{}}
\newlabel{line:maxop}{{7}{3}{Preliminaries}{ALC@unique.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\textsc  {OPT-AMSGRAD} Algorithm}{3}{section.3}}
\newlabel{sec:opt}{{3}{3}{\textsc {OPT-AMSGRAD} Algorithm}{section.3}{}}
\citation{Proc:Chiang_COLT12}
\citation{Proc:Duchi_JMLR11}
\citation{Book:Nesterov_2004}
\citation{Article:Polyak_1964}
\citation{Proc:Chiang_COLT12}
\citation{Proc:Rakhlin_NIPS13}
\citation{Proc:Syrgkanis_NIPS15}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Kingma_ICLR15}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \textsc  {OPT-AMSGrad}}}{4}{algorithm.2}}
\newlabel{alg:optamsgrad}{{2}{4}{\textsc {OPT-AMSGRAD} Algorithm}{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textsc  {OPT-AMSGrad} underlying structure.}}{4}{figure.1}}
\newlabel{fig:scheme}{{1}{4}{\textsc {OPT-AMSGrad} underlying structure}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Convergence Analysis}{4}{section.4}}
\newlabel{sec:analysis}{{4}{4}{Convergence Analysis}{section.4}{}}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Rakhlin_NIPS13}
\citation{Article:Ghadimi_SJOPT13}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Convex Regret Analysis}{5}{subsection.4.1}}
\newlabel{sec:convex}{{4.1}{5}{Convex Regret Analysis}{subsection.4.1}{}}
\newlabel{thm:mainconvex}{{1}{5}{}{Theorem.1}{}}
\newlabel{cor:corollary}{{1}{5}{}{Corollary.1}{}}
\newlabel{eq:boundAMS}{{1}{5}{Convex Regret Analysis}{equation.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Finite-Time Analysis in Nonconvex Case}{5}{subsection.4.2}}
\newlabel{eq:minproblem}{{2}{5}{Finite-Time Analysis in Nonconvex Case}{equation.4.2}{}}
\newlabel{eq:random}{{3}{5}{Finite-Time Analysis in Nonconvex Case}{equation.4.3}{}}
\newlabel{ass:boundedparam}{{1}{5}{}{assumption.1}{}}
\citation{Article:Ghadimi_SJOPT13}
\citation{Article:Ghadimi_SJOPT13}
\citation{zhou2018convergence}
\newlabel{ass:smooth}{{2}{6}{}{assumption.2}{}}
\newlabel{ass:guessbound}{{3}{6}{}{assumption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of H\ref  {ass:guessbound}.}}{6}{figure.2}}
\newlabel{fig:assumption}{{2}{6}{Illustration of H\ref {ass:guessbound}}{figure.2}{}}
\newlabel{ass:bounded}{{4}{6}{}{assumption.4}{}}
\newlabel{lem:bound}{{1}{6}{}{Lemma.1}{}}
\newlabel{thm:boundopt}{{2}{6}{}{Theorem.2}{}}
\citation{Proc:Mohri_AISTATS16}
\citation{Proc:Daskalakis_ICLR18}
\citation{Proc:Zaheer_NeurIPS18}
\citation{Proc:Chen_ICLR19}
\citation{Proc:Ward_ICML19}
\citation{zhou2018convergence}
\citation{zou2018convergence}
\citation{Proc:Li_AISTATS19}
\citation{Proc:Chen_ICLR19}
\citation{Proc:Agarwal_ICML19}
\citation{Proc:Chen_Yuan_ICLR19}
\citation{Proc:Mohri_AISTATS16}
\citation{Proc:Mohri_AISTATS16}
\citation{Proc:Mohri_AISTATS16}
\citation{Proc:Mohri_AISTATS16}
\citation{Proc:Daskalakis_ICLR18}
\citation{Proc:Goodfellow_NIPS14}
\citation{Proc:Daskalakis_ICLR18}
\citation{Proc:Goodfellow_NIPS14}
\citation{Proc:Daskalakis_ICLR18}
\citation{Proc:Daskalakis_ICLR18}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Checking H\ref  {ass:boundedparam} for a Deep Neural Network}{7}{subsection.4.3}}
\newlabel{eq:dnnmodel}{{4}{7}{Checking H\ref {ass:boundedparam} for a Deep Neural Network}{equation.4.4}{}}
\newlabel{lem:dnnh2}{{2}{7}{}{Lemma.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison to related methods}{7}{section.5}}
\newlabel{sec:related}{{5}{7}{Comparison to related methods}{section.5}{}}
\citation{Article:Walker_SJNA11}
\citation{cabay1976polynomial}
\citation{eddy1979extrapolating}
\citation{Article:Scieur_MP20}
\citation{brezinski2013extrapolation}
\citation{Article:Scieur_MP20}
\citation{scieur2018nonlinear}
\newlabel{OPT-DISZ}{{3}{8}{Comparison to related methods}{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces \textsc  {Optimistic-Adam\nobreakspace  {}\cite  {Proc:Daskalakis_ICLR18}+$\mathaccentV {hat}05E{v}_t$}. }}{8}{algorithm.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Experiments}{8}{section.6}}
\newlabel{sec:numerical}{{6}{8}{Numerical Experiments}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Gradient Estimation}{8}{subsection.6.1}}
\newlabel{nox}{{5}{8}{Gradient Estimation}{equation.6.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Regularized Approximated Minimal Polynomial Extrapolation\nobreakspace  {}\cite  {Article:Scieur_MP20} }}{8}{algorithm.4}}
\newlabel{alg:algex}{{4}{8}{Gradient Estimation}{algorithm.4}{}}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Daskalakis_ICLR18}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Kingma_ICLR15}
\citation{Proc:Larochelle_ICML07}
\citation{Proc:ABC_UAI10}
\citation{Proc:He_CVPR16}
\citation{Article:Gers_NC00}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Classification Experiments}{9}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training loss vs. number of iterations for fully connected NN, CNN, LSTM and ResNet.}}{9}{figure.3}}
\newlabel{fig:train_loss}{{3}{9}{Training loss vs. number of iterations for fully connected NN, CNN, LSTM and ResNet}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textit  {MNIST-back-image} + CNN, \textit  {CIFAR10} + Res-18 and \textit  {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy.}}{10}{figure.4}}
\newlabel{fig:testandtrain}{{4}{10}{\textit {MNIST-back-image} + CNN, \textit {CIFAR10} + Res-18 and \textit {CIFAR100} + Res-50 . We compare three methods in terms of training (cross-entropy) loss and accuracy, testing loss and accuracy}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Choice of parameter $r$}{11}{subsection.6.3}}
\newlabel{sec:choicer}{{6.3}{11}{Choice of parameter $r$}{subsection.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training loss w.r.t. different $r$ values.}}{11}{figure.5}}
\newlabel{fig:compare}{{5}{11}{Training loss w.r.t. different $r$ values}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{11}{section.7}}
\bibstyle{plain}
\bibdata{standard}
\bibcite{Proc:Abernethy_COLT18}{1}
\bibcite{Proc:Agarwal_ICML19}{2}
\bibcite{brezinski2013extrapolation}{3}
\bibcite{cabay1976polynomial}{4}
\bibcite{Proc:Chen_FODS20}{5}
\bibcite{Proc:Chen_ICLR19}{6}
\bibcite{Proc:Chen_Yuan_ICLR19}{7}
\bibcite{Proc:Chiang_COLT12}{8}
\bibcite{Proc:Daskalakis_ICLR18}{9}
\bibcite{defossez2020convergence}{10}
\bibcite{dozat2016incorporating}{11}
\bibcite{Proc:Duchi_JMLR11}{12}
\bibcite{eddy1979extrapolating}{13}
\bibcite{Article:Gers_NC00}{14}
\bibcite{Article:Ghadimi_SJOPT13}{15}
\bibcite{Proc:Goodfellow_NIPS14}{16}
\bibcite{Proc:Graves_ICASSP13}{17}
\bibcite{hazan2019introduction}{18}
\bibcite{Proc:He_CVPR16}{19}
\bibcite{Proc:Kingma_ICLR15}{20}
\bibcite{Proc:Larochelle_ICML07}{21}
\bibcite{Article:Levine_JMLR16}{22}
\bibcite{Proc:ABC_UAI10}{23}
\bibcite{Proc:Li_AISTATS19}{24}
\bibcite{Proc:McMahan_COLT10}{25}
\bibcite{Proc:Mertikopoulos_ICLR19}{26}
\bibcite{mnih2013playing}{27}
\bibcite{Proc:Mohri_AISTATS16}{28}
\bibcite{Book:Nesterov_2004}{29}
\bibcite{Article:Polyak_1964}{30}
\bibcite{Proc:Rakhlin_NIPS13}{31}
\bibcite{Proc:Reddi_ICLR18}{32}
\bibcite{Article:Scieur_MP20}{33}
\bibcite{scieur2018nonlinear}{34}
\bibcite{Proc:Syrgkanis_NIPS15}{35}
\bibcite{Tieleman2012}{36}
\bibcite{tseng2008accelerated}{37}
\bibcite{Article:Walker_SJNA11}{38}
\bibcite{Proc:Ward_ICML19}{39}
\bibcite{Proc:Yan_IJCAI18}{40}
\bibcite{Proc:Zaheer_NeurIPS18}{41}
\bibcite{zeiler2012adadelta}{42}
\bibcite{zhou2018convergence}{43}
\bibcite{Proc:Zhou_NeurIPS20}{44}
\bibcite{zou2018convergence}{45}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Reddi_ICLR18}
\citation{Proc:Kingma_ICLR15}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Remarks on the Gradient Prediction Process}{15}{appendix.A}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. }}{15}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{15}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{15}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{15}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{15}{figure.6}}
\newlabel{simu}{{6}{15}{\small (a): The iterate $w_t$; the closer to the optimal point $0$ the better. (b): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better. (c): Distance to the optimal point $-1$. The smaller the better. (d): A scaled and clipped version of $m_t$: $w_t - w_{t-1/2}$, which measures how the prediction of $m_t$ drives the update towards the optimal point. In this scenario, the more negative the better}{figure.6}{}}
\citation{tseng2008accelerated}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Theorem\nobreakspace  {}\ref  {thm:mainconvex}}{16}{appendix.B}}
\newlabel{app:thmmainconvex}{{B}{16}{Proof of Theorem~\ref {thm:mainconvex}}{appendix.B}{}}
\newlabel{nn1}{{6}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.6}{}}
\newlabel{ii}{{7}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.7}{}}
\newlabel{nc1}{{8}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.8}{}}
\newlabel{nn2}{{9}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.9}{}}
\newlabel{nc2}{{10}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.10}{}}
\newlabel{nn3}{{11}{16}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.11}{}}
\newlabel{nnnn}{{12}{17}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.12}{}}
\newlabel{nn5}{{13}{17}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.13}{}}
\newlabel{nn4}{{14}{17}{Proof of Theorem~\ref {thm:mainconvex}}{equation.B.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proof of Corollary\nobreakspace  {}\ref  {cor:corollary}}{17}{appendix.C}}
\citation{Proc:Yan_IJCAI18}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proofs of Auxiliary Lemmas}{18}{appendix.D}}
\newlabel{eq:deftilde}{{15}{18}{Proofs of Auxiliary Lemmas}{equation.D.15}{}}
\newlabel{lem:momentum}{{3}{18}{}{Lemma.3}{}}
\newlabel{lem:squarev}{{4}{19}{}{Lemma.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Proof of Lemma\nobreakspace  {}\ref  {lem:bound}}{19}{subsection.D.1}}
\newlabel{app:lembound}{{D.1}{19}{Proof of Lemma~\ref {lem:bound}}{subsection.D.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Proof of Theorem\nobreakspace  {}\ref  {thm:boundopt}}{20}{appendix.E}}
\newlabel{app:thmboundopt}{{E}{20}{Proof of Theorem~\ref {thm:boundopt}}{appendix.E}{}}
\newlabel{eq:smoothness}{{16}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.16}{}}
\newlabel{eq:termA1}{{17}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.17}{}}
\newlabel{eq:termA2}{{18}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.18}{}}
\newlabel{eq:termA}{{19}{20}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.19}{}}
\newlabel{eq:termB1}{{20}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.20}{}}
\newlabel{eq:termB2}{{21}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.21}{}}
\newlabel{eq:termB3}{{22}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.22}{}}
\newlabel{eq:termB}{{23}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.23}{}}
\newlabel{eq:term3}{{24}{21}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.24}{}}
\newlabel{eq:bound1}{{25}{22}{Proof of Theorem~\ref {thm:boundopt}}{equation.E.25}{}}
\citation{zhou2018convergence}
\citation{defossez2020convergence}
\@writefile{toc}{\contentsline {section}{\numberline {F}Proof of Lemma\nobreakspace  {}\ref  {lem:dnnh2} (Boundedness of the iterates H\ref  {ass:boundedparam})}{24}{appendix.F}}
\newlabel{app:lemdnnh2}{{F}{24}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{appendix.F}{}}
\newlabel{eq:mildassumptions}{{26}{24}{}{equation.F.26}{}}
\newlabel{eq:boundderivativeloss}{{27}{24}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.F.27}{}}
\newlabel{eq:decrease}{{28}{24}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.F.28}{}}
\newlabel{eq:gradientatell}{{29}{25}{Proof of Lemma~\ref {lem:dnnh2} (Boundedness of the iterates H\ref {ass:boundedparam})}{equation.F.29}{}}
