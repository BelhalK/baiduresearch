\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{chilimbi2014project,mcmahan2017communication}
\citation{alistarh2017qsgd,lin2017deep,wangni2018gradient,stich2018sparsified,wang2018atomo,tang2019doublesqueeze}
\citation{aji2017sparse}
\citation{chen2010approximate,ge2013optimized,jegou2010product}
\citation{duchi2011dual}
\citation{lian2017can}
\citation{duchi2011adaptive}
\citation{kingma2014adam}
\citation{reddi2019convergence}
\citation{robbins1951stochastic}
\citation{reddi2020adaptive}
\citation{reddi2020adaptive}
\providecommand \oddpage@label [2]{}
\jmlr@author{author names withheld}{author names withheld}
\jmlr@workshop{34th Annual Conference on Learning Theory}
\jmlr@title{Decentralized Adaptive Gradient Methods}{On the Convergence of Decentralized Adaptive Gradient Methods}
\providecommand\tcolorbox@label[2]{}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{nazari2019dadam}
\citation{boyd2011distributed}
\citation{duchi2011dual}
\citation{nedic2009distributed}
\citation{shi2015extra}
\citation{di2016next}
\citation{hong2017prox}
\citation{lu2019gnsd}
\citation{koloskova2019decentralized}
\citation{lian2017can}
\citation{tang2018d}
\citation{assran2019stochastic}
\citation{nazari2019dadam}
\citation{reddi2019convergence}
\@writefile{toc}{\contentsline {section}{\numberline {2}Some Preliminary Background}{2}{section.0.2}}
\newlabel{sec:prelim}{{2}{2}{Some Preliminary Background}{section.0.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Related Work}{2}{subsection.0.2.1}}
\citation{duchi2011adaptive}
\citation{kingma2014adam}
\citation{reddi2019convergence}
\citation{ward2019adagrad}
\citation{li2019convergence}
\citation{chen2018convergence}
\citation{zhou2018convergence}
\citation{zou2018convergence}
\citation{agarwal2019efficient,luo2019adaptive,zaheer2018adaptive}
\citation{chen2018convergence,ward2019adagrad}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Decentralized Optimization Framework}{3}{subsection.0.2.2}}
\newlabel{eq:minproblem}{{1}{3}{Decentralized Optimization Framework}{equation.0.2.1}{}}
\newlabel{a:diff}{{1}{3}{Decentralized Optimization Framework}{assumptionA.1}{}}
\newlabel{a:boundsto}{{2}{3}{Decentralized Optimization Framework}{assumptionA.2}{}}
\citation{nazari2019dadam}
\citation{nazari2019dadam}
\citation{nedic2009distributed,yuan2016convergence}
\citation{nazari2019dadam}
\citation{yuan2016convergence}
\citation{yuan2016convergence}
\newlabel{a:boundedvar}{{3}{4}{Decentralized Optimization Framework}{assumptionA.3}{}}
\newlabel{a:matrixW}{{4}{4}{Decentralized Optimization Framework}{assumptionA.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Decentralized Optimization and Adaptive Gradient Methods}{4}{section.0.3}}
\newlabel{sec:dadam}{{3}{4}{Decentralized Optimization and Adaptive Gradient Methods}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Divergence of DADAM}{4}{subsection.0.3.1}}
\newlabel{sec:divergence}{{3.1}{4}{Divergence of DADAM}{subsection.0.3.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DADAM (with N nodes)\relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg: dadam}{{1}{4}{DADAM (with N nodes)\relax }{figure.caption.1}{}}
\zref@newlabel{mdf@pagelabel-1}{\default{3.1}\page{4}\abspage{4}\mdf@pagevalue{4}}
\newlabel{thm: dadam_diverge}{{1}{4}{Divergence of DADAM}{boxtheo.1}{}}
\citation{nazari2019dadam}
\citation{nazari2019dadam}
\citation{reddi2020adaptive}
\citation{chen2021cada}
\citation{reddi2020adaptive}
\citation{chen2021cada}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Importance and Difficulties of Consensus on Adaptive Learning Rates}{5}{subsection.0.3.2}}
\citation{luo2019adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {4}A Unifying Decentralized Adaptive Gradient Framework}{6}{section.0.4}}
\newlabel{sec:main}{{4}{6}{A Unifying Decentralized Adaptive Gradient Framework}{section.0.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Decentralized Adaptive Gradient Method (with N nodes)\relax }}{6}{figure.caption.2}}
\newlabel{alg: dadaptive}{{2}{6}{Decentralized Adaptive Gradient Method (with N nodes)\relax }{figure.caption.2}{}}
\zref@newlabel{mdf@pagelabel-2}{\default{4}\page{6}\abspage{6}\mdf@pagevalue{6}}
\newlabel{thm: dagm_converge}{{2}{6}{A Unifying Decentralized Adaptive Gradient Framework}{boxtheo.2}{}}
\newlabel{eq: thm11}{{2}{6}{A Unifying Decentralized Adaptive Gradient Framework}{equation.0.4.2}{}}
\citation{chen2018convergence}
\citation{boyd2004fastest}
\citation{boyd2009fastest}
\zref@newlabel{mdf@pagelabel-3}{\default{4}\page{7}\abspage{7}\mdf@pagevalue{7}}
\newlabel{corl: adm_convergence}{{1}{7}{A Unifying Decentralized Adaptive Gradient Framework}{boxcoro.1}{}}
\newlabel{eq: thm1}{{3}{7}{A Unifying Decentralized Adaptive Gradient Framework}{equation.0.4.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Decentralized AMSGrad (N nodes)\relax }}{7}{figure.caption.3}}
\newlabel{alg: damsgrad}{{3}{7}{Decentralized AMSGrad (N nodes)\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Application to AMSGrad algorithm}{7}{subsection.0.4.1}}
\newlabel{sec:amsgrad}{{4.1}{7}{Application to AMSGrad algorithm}{subsection.0.4.1}{}}
\citation{duchi2011adaptive}
\zref@newlabel{mdf@pagelabel-4}{\default{4.1}\page{8}\abspage{8}\mdf@pagevalue{8}}
\newlabel{thm: dams_converge}{{3}{8}{Application to AMSGrad algorithm}{boxtheo.3}{}}
\citation{yan2018unified,chen2018convergence}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Application to AdaGrad algorithm}{9}{subsection.0.4.2}}
\newlabel{sec:adagrad}{{4.2}{9}{Application to AdaGrad algorithm}{subsection.0.4.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Decentralized AdaGrad (N nodes)\relax }}{9}{figure.caption.4}}
\newlabel{alg: dadagrad}{{4}{9}{Decentralized AdaGrad (N nodes)\relax }{figure.caption.4}{}}
\zref@newlabel{mdf@pagelabel-5}{\default{4.2}\page{9}\abspage{9}\mdf@pagevalue{9}}
\newlabel{thm: dadagrad_converge}{{4}{9}{Application to AdaGrad algorithm}{boxtheo.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convergence Analysis}{9}{subsection.0.4.3}}
\@writefile{toc}{\contentsline {paragraph}{Proof of Theorem \ref  {thm: dagm_converge}}{9}{section*.5}}
\newlabel{eq: exp_telescope_sketchmain}{{4}{10}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Proof of Theorem \ref  {thm: dams_converge}}{10}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Proof of Theorem \ref  {thm: dadagrad_converge}}{10}{section*.7}}
\newlabel{eq: rep_thm1bis}{{5}{10}{Proof of Theorem \ref {thm: dadagrad_converge}}{equation.0.4.5}{}}
\citation{lian2017can}
\citation{lecun1998mnist}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical Experiments}{11}{section.0.5}}
\newlabel{sec:numerical}{{5}{11}{Numerical Experiments}{section.0.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Effect of heterogeneity}{11}{subsection.0.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training loss and Testing accuracy for homogeneous (top row) and heterogeneous data (bottow row)\relax }}{11}{figure.caption.8}}
\newlabel{fig: homo_data}{{1}{11}{Training loss and Testing accuracy for homogeneous (top row) and heterogeneous data (bottow row)\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Sensitivity to the Learning Rate}{12}{subsection.0.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Traing Loss (Top Row) and Testing Accuracy (Bottom Row) comparison of different stepsizes for various methods. Left: DP-SGD. {Middle:} DADAM. {Right:} DAMS. \relax }}{12}{figure.caption.9}}
\newlabel{fig: stepsize}{{2}{12}{Traing Loss (Top Row) and Testing Accuracy (Bottom Row) comparison of different stepsizes for various methods. Left: DP-SGD. {Middle:} DADAM. {Right:} DAMS. \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{12}{section.0.6}}
\newlabel{sec:conclusion}{{6}{12}{Conclusion}{section.0.6}{}}
\bibdata{reference}
\bibcite{agarwal2019efficient}{{1}{2019}{{Agarwal et~al.}}{{Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and Zhang}}}
\bibcite{aji2017sparse}{{2}{2017}{{Aji and Heafield}}{{}}}
\bibcite{alistarh2017qsgd}{{3}{2017}{{Alistarh et~al.}}{{Alistarh, Grubic, Li, Tomioka, and Vojnovic}}}
\bibcite{assran2019stochastic}{{4}{2019}{{Assran et~al.}}{{Assran, Loizou, Ballas, and Rabbat}}}
\bibcite{boyd2004fastest}{{5}{2004}{{Boyd et~al.}}{{Boyd, Diaconis, and Xiao}}}
\bibcite{boyd2009fastest}{{6}{2009}{{Boyd et~al.}}{{Boyd, Diaconis, Parrilo, and Xiao}}}
\bibcite{boyd2011distributed}{{7}{2011}{{Boyd et~al.}}{{Boyd, Parikh, Chu, Peleato, and Eckstein}}}
\bibcite{chen2021cada}{{8}{2021}{{Chen et~al.}}{{Chen, Guo, Sun, and Yin}}}
\bibcite{chen2018convergence}{{9}{2019}{{Chen et~al.}}{{Chen, Liu, Sun, and Hong}}}
\bibcite{chen2010approximate}{{10}{2010}{{Chen et~al.}}{{Chen, Guan, and Wang}}}
\bibcite{chilimbi2014project}{{11}{2014}{{Chilimbi et~al.}}{{Chilimbi, Suzue, Apacible, and Kalyanaraman}}}
\bibcite{duchi2011adaptive}{{12}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{duchi2011dual}{{13}{2012}{{Duchi et~al.}}{{Duchi, Agarwal, and Wainwright}}}
\bibcite{ge2013optimized}{{14}{2013}{{Ge et~al.}}{{Ge, He, Ke, and Sun}}}
\bibcite{hong2017prox}{{15}{2017}{{Hong et~al.}}{{Hong, Hajinezhad, and Zhao}}}
\bibcite{jegou2010product}{{16}{2011}{{J{\'{e}}gou et~al.}}{{J{\'{e}}gou, Douze, and Schmid}}}
\bibcite{kingma2014adam}{{17}{2015}{{Kingma and Ba}}{{}}}
\bibcite{koloskova2019decentralized}{{18}{2019}{{Koloskova et~al.}}{{Koloskova, Stich, and Jaggi}}}
\bibcite{lecun1998mnist}{{19}{1998}{{LeCun}}{{}}}
\bibcite{li2019convergence}{{20}{2019}{{Li and Orabona}}{{}}}
\bibcite{lian2017can}{{21}{2017}{{Lian et~al.}}{{Lian, Zhang, Zhang, Hsieh, Zhang, and Liu}}}
\bibcite{lin2017deep}{{22}{2018}{{Lin et~al.}}{{Lin, Han, Mao, Wang, and Dally}}}
\bibcite{di2016next}{{23}{2016}{{Lorenzo and Scutari}}{{}}}
\bibcite{lu2019gnsd}{{24}{2019}{{Lu et~al.}}{{Lu, Zhang, Sun, and Hong}}}
\bibcite{luo2019adaptive}{{25}{2019}{{Luo et~al.}}{{Luo, Xiong, Liu, and Sun}}}
\bibcite{mcmahan2017communication}{{26}{2017}{{McMahan et~al.}}{{McMahan, Moore, Ramage, Hampson, and y~Arcas}}}
\bibcite{nazari2019dadam}{{27}{2019}{{Nazari et~al.}}{{Nazari, Tarzanagh, and Michailidis}}}
\bibcite{nedic2009distributed}{{28}{2009}{{Nedic and Ozdaglar}}{{}}}
\bibcite{reddi2019convergence}{{29}{2018}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{reddi2020adaptive}{{30}{2021}{{Reddi et~al.}}{{Reddi, Charles, Zaheer, Garrett, Rush, Kone{\v {c}}n{\'y}, Kumar, and McMahan}}}
\bibcite{robbins1951stochastic}{{31}{1951}{{Robbins and Monro}}{{}}}
\bibcite{shi2015extra}{{32}{2015}{{Shi et~al.}}{{Shi, Ling, Wu, and Yin}}}
\bibcite{stich2018sparsified}{{33}{2018}{{Stich et~al.}}{{Stich, Cordonnier, and Jaggi}}}
\bibcite{tang2018d}{{34}{2018}{{Tang et~al.}}{{Tang, Lian, Yan, Zhang, and Liu}}}
\bibcite{tang2019doublesqueeze}{{35}{2019}{{Tang et~al.}}{{Tang, Yu, Lian, Zhang, and Liu}}}
\bibcite{wang2018atomo}{{36}{2018}{{Wang et~al.}}{{Wang, Sievert, Liu, Charles, Papailiopoulos, and Wright}}}
\bibcite{wangni2018gradient}{{37}{2018}{{Wangni et~al.}}{{Wangni, Wang, Liu, and Zhang}}}
\bibcite{ward2019adagrad}{{38}{2019}{{Ward et~al.}}{{Ward, Wu, and Bottou}}}
\bibcite{yan2018unified}{{39}{2018}{{Yan et~al.}}{{Yan, Yang, Li, Lin, and Yang}}}
\bibcite{yuan2016convergence}{{40}{2016}{{Yuan et~al.}}{{Yuan, Ling, and Yin}}}
\bibcite{zaheer2018adaptive}{{41}{2018}{{Zaheer et~al.}}{{Zaheer, Reddi, Sachan, Kale, and Kumar}}}
\bibcite{zhou2018convergence}{{42}{2018}{{Zhou et~al.}}{{Zhou, Tang, Yang, Cao, and Gu}}}
\bibcite{zou2018convergence}{{43}{2018}{{Zou and Shen}}{{}}}
\citation{yan2018unified,chen2018convergence}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proof of Auxiliary Lemmas}{17}{section.0.A}}
\newlabel{app: proof_lemmas}{{A}{17}{Proof of Auxiliary Lemmas}{section.0.A}{}}
\newlabel{eq: seq_z_sketchapp}{{6}{17}{Proof of Auxiliary Lemmas}{equation.0.A.6}{}}
\newlabel{lem: z_diff}{{1}{17}{Proof of Auxiliary Lemmas}{theorem.0.A.1}{}}
\newlabel{lem: mean_after_max}{{2}{18}{Proof of Auxiliary Lemmas}{theorem.0.A.2}{}}
\newlabel{eq: r_decrease}{{7}{18}{Proof of Auxiliary Lemmas}{equation.0.A.7}{}}
\newlabel{eq: r_reduce}{{8}{18}{Proof of Auxiliary Lemmas}{equation.0.A.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proof of Theorem \ref  {thm: dagm_converge}}{19}{section.0.B}}
\newlabel{app: proof_thm_adm}{{B}{19}{Proof of Theorem \ref {thm: dagm_converge}}{section.0.B}{}}
\newlabel{eq: seq_z}{{9}{19}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.9}{}}
\newlabel{eq: exp_lip}{{10}{19}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.10}{}}
\newlabel{eq: u_to_u_bar}{{12}{19}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.12}{}}
\newlabel{eq: split_1}{{13}{20}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.13}{}}
\newlabel{eq: exp_telescope}{{14}{20}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.14}{}}
\newlabel{eq: T_3_bound_first}{{16}{21}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.16}{}}
\newlabel{eq: update_X}{{17}{21}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.17}{}}
\newlabel{eq: t2_matrix}{{18}{21}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.18}{}}
\newlabel{eq: update_x_decom}{{19}{21}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.19}{}}
\newlabel{eq: x_ql}{{20}{21}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.20}{}}
\newlabel{eq: T_5_bound}{{21}{22}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.21}{}}
\newlabel{eq:T_2_bound}{{23}{22}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.23}{}}
\newlabel{eq: T_6_bound}{{24}{24}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.24}{}}
\newlabel{eq: T_1}{{25}{24}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.25}{}}
\newlabel{eq: diff_u_t}{{26}{25}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.26}{}}
\newlabel{eq: split_var}{{29}{27}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.29}{}}
\newlabel{eq: variance_bound_1}{{30}{27}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.30}{}}
\newlabel{eq: diff_u}{{31}{27}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.31}{}}
\newlabel{eq: diff_g}{{33}{28}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.33}{}}
\newlabel{eq: final_bound}{{34}{29}{Proof of Theorem \ref {thm: dagm_converge}}{equation.0.B.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proof of Theorem \ref  {thm: dams_converge}}{31}{section.0.C}}
\newlabel{app: proof_ams}{{C}{31}{Proof of Theorem \ref {thm: dams_converge}}{section.0.C}{}}
\newlabel{eq: rep_thm1}{{36}{31}{Proof of Theorem \ref {thm: dams_converge}}{equation.0.C.36}{}}
\newlabel{eq: sub_thm1}{{37}{32}{Proof of Theorem \ref {thm: dams_converge}}{equation.0.C.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proof of Theorem \ref  {thm: dadagrad_converge}}{33}{section.0.D}}
\newlabel{app: proof_adagrad}{{D}{33}{Proof of Theorem \ref {thm: dadagrad_converge}}{section.0.D}{}}
\newlabel{eq: rep_thm1bis}{{39}{33}{Proof of Theorem \ref {thm: dadagrad_converge}}{equation.0.D.39}{}}
\newlabel{eq: sub_thm1bis}{{40}{34}{Proof of Theorem \ref {thm: dadagrad_converge}}{equation.0.D.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Additional Experiments and Details}{35}{section.0.E}}
\newlabel{app: experiments}{{E}{35}{Additional Experiments and Details}{section.0.E}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training loss and Testing accuracy for different stepsizes for D-PSGD\relax }}{35}{figure.caption.11}}
\newlabel{fig: sgd_curve}{{3}{35}{Training loss and Testing accuracy for different stepsizes for D-PSGD\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training loss and Testing accuracy for different stepsizes for AMSGrad\relax }}{35}{figure.caption.12}}
\newlabel{fig: amsgrad_curve}{{4}{35}{Training loss and Testing accuracy for different stepsizes for AMSGrad\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training loss and Testing accuracy for different stepsizes for DADAM\relax }}{36}{figure.caption.13}}
\newlabel{fig: adam_curve}{{5}{36}{Training loss and Testing accuracy for different stepsizes for DADAM\relax }{figure.caption.13}{}}
\newlabel{jmlrend}{{E}{36}{end of Decentralized Adaptive Gradient Methods}{section*.14}{}}
