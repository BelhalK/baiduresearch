\BOOKMARK [1][-]{section.0.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.0.2}{Decentralized Adaptive Training and Divergence of DADAM}{}% 2
\BOOKMARK [2][-]{subsection.0.2.1}{Related Work}{section.0.2}% 3
\BOOKMARK [2][-]{subsection.0.2.2}{Decentralized Optimization }{section.0.2}% 4
\BOOKMARK [2][-]{subsection.0.2.3}{Divergence of DADAM}{section.0.2}% 5
\BOOKMARK [1][-]{section.0.3}{On the Convergence of Decentralized Adaptive Gradient Methods}{}% 6
\BOOKMARK [2][-]{subsection.0.3.1}{Importance and Difficulties of Consensus on Adaptive Learning Rates}{section.0.3}% 7
\BOOKMARK [2][-]{subsection.0.3.2}{Unifying Decentralized Adaptive Gradient Framework}{section.0.3}% 8
\BOOKMARK [2][-]{subsection.0.3.3}{Application to AMSGrad algorithm}{section.0.3}% 9
\BOOKMARK [2][-]{subsection.0.3.4}{Application to AdaGrad algorithm}{section.0.3}% 10
\BOOKMARK [1][-]{section.0.4}{Numerical Experiments}{}% 11
\BOOKMARK [2][-]{subsection.0.4.1}{Effect of heterogeneity}{section.0.4}% 12
\BOOKMARK [2][-]{subsection.0.4.2}{Sensitivity to the Learning Rate}{section.0.4}% 13
\BOOKMARK [1][-]{section.0.5}{Conclusion}{}% 14
