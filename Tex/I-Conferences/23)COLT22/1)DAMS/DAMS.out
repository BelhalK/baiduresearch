\BOOKMARK [1][-]{section.0.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.0.2}{Decentralized Optimization and Adaptive Gradient Methods}{}% 2
\BOOKMARK [2][-]{subsection.0.2.1}{Related Work}{section.0.2}% 3
\BOOKMARK [2][-]{subsection.0.2.2}{Decentralized Optimization Framework}{section.0.2}% 4
\BOOKMARK [1][-]{section.0.3}{On DADAM and Consensus on Learning Rates}{}% 5
\BOOKMARK [2][-]{subsection.0.3.1}{Divergence of DADAM}{section.0.3}% 6
\BOOKMARK [2][-]{subsection.0.3.2}{Importance and Difficulties of Consensus on Adaptive Learning Rates}{section.0.3}% 7
\BOOKMARK [1][-]{section.0.4}{Unifying Decentralized Adaptive Gradient Framework}{}% 8
\BOOKMARK [2][-]{subsection.0.4.1}{Application to AMSGrad algorithm}{section.0.4}% 9
\BOOKMARK [2][-]{subsection.0.4.2}{Application to AdaGrad algorithm}{section.0.4}% 10
\BOOKMARK [1][-]{section.0.5}{Numerical Experiments}{}% 11
\BOOKMARK [2][-]{subsection.0.5.1}{Effect of heterogeneity}{section.0.5}% 12
\BOOKMARK [2][-]{subsection.0.5.2}{Sensitivity to the Learning Rate}{section.0.5}% 13
\BOOKMARK [1][-]{section.0.6}{Conclusion}{}% 14
\BOOKMARK [1][-]{section.0.A}{Proof of Auxiliary Lemmas}{}% 15
\BOOKMARK [1][-]{section.0.B}{Proof of Theorem 2}{}% 16
\BOOKMARK [1][-]{section.0.C}{Proof of Theorem 3}{}% 17
\BOOKMARK [1][-]{section.0.D}{Proof of Theorem 4}{}% 18
\BOOKMARK [1][-]{section.0.E}{Additional Experiments and Details}{}% 19
