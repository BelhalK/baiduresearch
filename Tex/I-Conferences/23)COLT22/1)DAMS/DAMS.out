\BOOKMARK [1][-]{section.0.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.0.2}{Decentralized Optimization and Adaptive Gradient Methods}{}% 2
\BOOKMARK [2][-]{subsection.0.2.1}{Related Work}{section.0.2}% 3
\BOOKMARK [2][-]{subsection.0.2.2}{Decentralized Optimization }{section.0.2}% 4
\BOOKMARK [1][-]{section.0.3}{Decentralized Adaptive Gradient Methods and Their Convergence}{}% 5
\BOOKMARK [2][-]{subsection.0.3.1}{Divergence of DADAM}{section.0.3}% 6
\BOOKMARK [2][-]{subsection.0.3.2}{Importance and Difficulties of Consensus on Adaptive Learning Rates}{section.0.3}% 7
\BOOKMARK [1][-]{section.0.4}{Unifying Decentralized Adaptive Gradient Framework}{}% 8
\BOOKMARK [2][-]{subsection.0.4.1}{Application to AMSGrad algorithm}{section.0.4}% 9
\BOOKMARK [2][-]{subsection.0.4.2}{Application to AdaGrad algorithm}{section.0.4}% 10
\BOOKMARK [1][-]{section.0.5}{Numerical Experiments}{}% 11
\BOOKMARK [2][-]{subsection.0.5.1}{Effect of heterogeneity}{section.0.5}% 12
\BOOKMARK [2][-]{subsection.0.5.2}{Sensitivity to the Learning Rate}{section.0.5}% 13
\BOOKMARK [1][-]{section.0.6}{Conclusion}{}% 14
