\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2019efficient}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 102--110, Long Beach, CA, 2019.

\bibitem[Aji and Heafield(2017)]{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 440--445, Copenhagen, Denmark,
  2017.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD:} communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1709--1720, Long Beach, CA, 2017.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and
  Rabbat]{assran2019stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael~G. Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 344--353, Long Beach, CA, 2019.

\bibitem[Boyd et~al.(2004)Boyd, Diaconis, and Xiao]{boyd2004fastest}
Stephen~P. Boyd, Persi Diaconis, and Lin Xiao.
\newblock Fastest mixing markov chain on a graph.
\newblock \emph{{SIAM} Rev.}, 46\penalty0 (4):\penalty0 667--689, 2004.

\bibitem[Boyd et~al.(2009)Boyd, Diaconis, Parrilo, and Xiao]{boyd2009fastest}
Stephen~P. Boyd, Persi Diaconis, Pablo~A. Parrilo, and Lin Xiao.
\newblock Fastest mixing markov chain on graphs with symmetries.
\newblock \emph{{SIAM} J. Optim.}, 20\penalty0 (2):\penalty0 792--819, 2009.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and
  Eckstein]{boyd2011distributed}
Stephen~P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Found. Trends Mach. Learn.}, 3\penalty0 (1):\penalty0 1--122,
  2011.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem[Chen et~al.(2010)Chen, Guan, and Wang]{chen2010approximate}
Yongjian Chen, Tao Guan, and Cheng Wang.
\newblock Approximate nearest neighbor search by residual vector quantization.
\newblock \emph{Sensors}, 10\penalty0 (12):\penalty0 11259--11273, 2010.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
Trishul~M. Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{Proceedings of the 11th {USENIX} Symposium on Operating
  Systems Design and Implementation (OSDI)}, pages 571--582, Broomfield, CO,
  2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2121--2159, 2011.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, and Wainwright]{duchi2011dual}
John~C. Duchi, Alekh Agarwal, and Martin~J. Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 57\penalty0 (3):\penalty0
  592--606, 2012.

\bibitem[Ge et~al.(2013)Ge, He, Ke, and Sun]{ge2013optimized}
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun.
\newblock Optimized product quantization for approximate nearest neighbor
  search.
\newblock In \emph{Proceedings of the 2013 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 2946--2953, Portland, OR, 2013.

\bibitem[Hong et~al.(2017)Hong, Hajinezhad, and Zhao]{hong2017prox}
Mingyi Hong, Davood Hajinezhad, and Ming{-}Min Zhao.
\newblock Prox-pda: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pages 1529--1538, Sydney, Australia, 2017.

\bibitem[J{\'{e}}gou et~al.(2011)J{\'{e}}gou, Douze, and
  Schmid]{jegou2010product}
Herv{\'{e}} J{\'{e}}gou, Matthijs Douze, and Cordelia Schmid.
\newblock Product quantization for nearest neighbor search.
\newblock \emph{{IEEE} Trans. Pattern Anal. Mach. Intell.}, 33\penalty0
  (1):\penalty0 117--128, 2011.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U. Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 3478--3487, Long Beach, CA, 2019.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Li and Orabona(2019)]{li2019convergence}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 983--992, Naha,
  Japan, 2019.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 5330--5340, 2017.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and Bill Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[Lorenzo and Scutari(2016)]{di2016next}
Paolo~Di Lorenzo and Gesualdo Scutari.
\newblock {NEXT:} in-network nonconvex optimization.
\newblock \emph{{IEEE} Trans. Signal Inf. Process. over Networks}, 2\penalty0
  (2):\penalty0 120--136, 2016.

\bibitem[Lu et~al.(2019)Lu, Zhang, Sun, and Hong]{lu2019gnsd}
Songtao Lu, Xinwei Zhang, Haoran Sun, and Mingyi Hong.
\newblock {GNSD:} a gradient-tracking based nonconvex stochastic algorithm for
  decentralized optimization.
\newblock In \emph{Proceedings of the {IEEE} Data Science Workshop (DSW)},
  pages 315--321, Minneapolis, MN, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adaptive}
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
  Blaise~Ag{\"{u}}era y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 1273--1282, Fort
  Lauderdale, FL, 2017.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock {DADAM:} {A} consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{CoRR}, abs/1901.09109, 2019.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
Angelia Nedic and Asuman~E. Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{{IEEE} Trans. Autom. Control.}, 54\penalty0 (1):\penalty0
  48--61, 2009.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {Adam} and beyond.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2020adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In \emph{Proceedings of the 9th International Conference on Learning
  Representations (ICLR)}, Virtual Event, Austria, 2021.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock {EXTRA:} an exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{{SIAM} J. Optim.}, 25\penalty0 (2):\penalty0 944--966, 2015.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Sebastian~U. Stich, Jean{-}Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified {SGD} with memory.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 4452--4463, Montr{\'{e}}al, Canada, 2018.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D\({}^{\mbox{2}}\): Decentralized training over decentralized data.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 4855--4863, Stockholmsm{\"{a}}ssan, Sweden, 2018.

\bibitem[Tang et~al.(2019)Tang, Yu, Lian, Zhang, and
  Liu]{tang2019doublesqueeze}
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji~Liu.
\newblock Doublesqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 6155--6165, Long Beach, CA, 2019.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{wang2018atomo}
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary~B. Charles, Dimitris~S.
  Papailiopoulos, and Stephen~J. Wright.
\newblock {ATOMO:} communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9872--9883, Montr{\'{e}}al, Canada, 2018.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 1306--1316, Montr{\'{e}}al, Canada, 2018.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{ward2019adagrad}
Rachel Ward, Xiaoxia Wu, and L{\'{e}}on Bottou.
\newblock Adagrad stepsizes: sharp convergence over nonconvex landscapes.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pages 6677--6686, Long Beach, CA, 2019.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{yan2018unified}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence (IJCAI)}, pages 2955--2961, Stockholm,
  Sweden, 2018.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
Kun Yuan, Qing Ling, and Wotao Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{{SIAM} J. Optim.}, 26\penalty0 (3):\penalty0 1835--1854, 2016.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank~J. Reddi, Devendra~Singh Sachan, Satyen Kale, and Sanjiv
  Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9815--9825, Montr{\'{e}}al, Canada, 2018.

\bibitem[Zhou et~al.(2018)Zhou, Tang, Yang, Cao, and Gu]{zhou2018convergence}
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{CoRR}, abs/1808.05671, 2018.

\bibitem[Zou and Shen(2018)]{zou2018convergence}
Fangyu Zou and Li~Shen.
\newblock On the convergence of weighted adagrad with momentum for training
  deep neural networks.
\newblock \emph{CoRR}, abs/1808.03408, 2018.

\end{thebibliography}
