\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allassonniere \& Kuhn(2015)Allassonniere and
  Kuhn]{allassonniere2015convergent}
Allassonniere, S. and Kuhn, E.
\newblock Convergent stochastic expectation maximization algorithm with
  efficient sampling in high dimension. application to deformable template
  model estimation.
\newblock \emph{CSDA}, 91:\penalty0 4--19, 2015.

\bibitem[Atchad{\'e}(2006)]{atchade2006adaptive}
Atchad{\'e}, Y.~F.
\newblock An adaptive version for the metropolis adjusted langevin algorithm
  with a truncated drift.
\newblock \emph{Methodology and Computing in applied Probability}, 8\penalty0
  (2):\penalty0 235--254, 2006.

\bibitem[Bottou \& Bousquet(2008)Bottou and Bousquet]{bottou2008}
Bottou, L. and Bousquet, O.
\newblock The tradeoffs of large scale learning.
\newblock In Platt, J.~C., Koller, D., Singer, Y., and Roweis, S.~T. (eds.),
  \emph{Advances in Neural Information Processing Systems 20}, pp.\  161--168.
  Curran Associates, Inc., 2008.

\bibitem[Brosse et~al.(2017)Brosse, Durmus, Moulines, and
  Sabanis]{brosse2017tamed}
Brosse, N., Durmus, A., Moulines, {\'E}., and Sabanis, S.
\newblock The tamed unadjusted langevin algorithm.
\newblock \emph{arXiv preprint arXiv:1710.05559}, 2017.

\bibitem[Cotter et~al.(2013)Cotter, Roberts, Stuart, and White]{cotter2013mcmc}
Cotter, S.~L., Roberts, G.~O., Stuart, A.~M., and White, D.
\newblock Mcmc methods for functions: modifying old algorithms to make them
  faster.
\newblock \emph{Statistical Science}, pp.\  424--446, 2013.

\bibitem[De~Freitas et~al.(2001)De~Freitas, H{\o}jen-S{\o}rensen, Jordan, and
  Russell]{freitas}
De~Freitas, N., H{\o}jen-S{\o}rensen, P., Jordan, M.~I., and Russell, S.
\newblock Variational mcmc.
\newblock \emph{Proceedings of the Seventeenth Conference on Uncertainty in
  Artificial Intelligence}, 2001.

\bibitem[Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and
  Ranzato]{deng2020residual}
Deng, Y., Bakhtin, A., Ott, M., Szlam, A., and Ranzato, M.
\newblock Residual energy-based models for text generation.
\newblock \emph{arXiv preprint arXiv:2004.11714}, 2020.

\bibitem[Doucet et~al.(2000)Doucet, Godsill, and Andrieu]{doucet2000sequential}
Doucet, A., Godsill, S., and Andrieu, C.
\newblock On sequential monte carlo sampling methods for bayesian filtering.
\newblock \emph{Statistics and computing}, 10\penalty0 (3):\penalty0 197--208,
  2000.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du2019implicit}
Du, Y. and Mordatch, I.
\newblock Implicit generation and modeling with energy based models.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d~Alche-Buc, F.,
  Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf}.

\bibitem[Du et~al.(2019)Du, Meier, Ma, Fergus, and Rives]{du2019energy}
Du, Y., Meier, J., Ma, J., Fergus, R., and Rives, A.
\newblock Energy-based models for atomic-resolution protein conformations.
\newblock In \emph{ICLR}, 2019.

\bibitem[Du et~al.(2020)Du, Li, Tenenbaum, and Mordatch]{du2020improved}
Du, Y., Li, S., Tenenbaum, J., and Mordatch, I.
\newblock Improved contrastive divergence training of energy based models.
\newblock \emph{arXiv preprint arXiv:2012.01316}, 2020.

\bibitem[Durmus et~al.(2017)Durmus, Roberts, Vilmart, and
  Zygalakis]{durmus2017fast}
Durmus, A., Roberts, G.~O., Vilmart, G., and Zygalakis, K.~C.
\newblock Fast langevin based algorithm for mcmc in high dimensions.
\newblock \emph{Ann. Appl. Probab.}, 27\penalty0 (4):\penalty0 2195--2237, 08
  2017.
\newblock \doi{10.1214/16-AAP1257}.

\bibitem[Gao et~al.(2018)Gao, Lu, Zhou, Zhu, and Wu]{gao2018learning}
Gao, R., Lu, Y., Zhou, J., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning generative convnets via multi-grid modeling and sampling.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  9155--9164, 2018.

\bibitem[Gao et~al.(2020)Gao, Nijkamp, Kingma, Xu, Dai, and Wu]{gao2020flow}
Gao, R., Nijkamp, E., Kingma, D.~P., Xu, Z., Dai, A.~M., and Wu, Y.~N.
\newblock Flow contrastive estimation of energy-based models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  7518--7528, 2020.

\bibitem[Girolami \& Calderhead(2011)Girolami and
  Calderhead]{girolami2011riemann}
Girolami, M. and Calderhead, B.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock \emph{Journal of the Royal Statistical Society: Series B},
  73\penalty0 (2):\penalty0 123--214, 2011.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
  Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1406.2661}, 2014.

\bibitem[Grenander \& Miller(1994)Grenander and
  Miller]{grenander1994representations}
Grenander, U. and Miller, M.~I.
\newblock Representations of knowledge in complex systems.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 56\penalty0 (4):\penalty0 549--581, 1994.

\bibitem[Gustafsson et~al.(2020)Gustafsson, Danelljan, Bhat, and
  Sch{\"o}n]{gustafsson2020energy}
Gustafsson, F.~K., Danelljan, M., Bhat, G., and Sch{\"o}n, T.~B.
\newblock Energy-based models for deep probabilistic regression.
\newblock In \emph{ECCV}, pp.\  325--343. Springer, 2020.

\bibitem[Gutmann \& Hyv{\"a}rinen(2010)Gutmann and
  Hyv{\"a}rinen]{gutmann2010noise}
Gutmann, M. and Hyv{\"a}rinen, A.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, 2010.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1352--1361, 2017.

\bibitem[Han et~al.(2019)Han, Nijkamp, Fang, Hill, Zhu, and
  Wu]{han2019divergence}
Han, T., Nijkamp, E., Fang, X., Hill, M., Zhu, S.-C., and Wu, Y.~N.
\newblock Divergence triangle for joint training of generator model,
  energy-based model, and inferential model.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  8670--8679, 2019.

\bibitem[Han et~al.(2020)Han, Nijkamp, Zhou, Pang, Zhu, and Wu]{han2020joint}
Han, T., Nijkamp, E., Zhou, L., Pang, B., Zhu, S.-C., and Wu, Y.~N.
\newblock Joint training of variational auto-encoder and latent energy-based
  model.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  7978--7987, 2020.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{arXiv preprint arXiv:1706.08500}, 2017.

\bibitem[Hinton(2002)]{hinton2002training}
Hinton, G.~E.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8), 2002.

\bibitem[Ingraham et~al.(2018)Ingraham, Riesselman, Sander, and
  Marks]{ingraham2018learning}
Ingraham, J., Riesselman, A., Sander, C., and Marks, D.
\newblock Learning protein structure with a differentiable simulator.
\newblock In \emph{ICLR}, 2018.

\bibitem[Jacob et~al.(2020)Jacob, O~Leary, and Atchad{\'e}]{jacob2020unbiased}
Jacob, P.~E., O~Leary, J., and Atchad{\'e}, Y.~F.
\newblock Unbiased markov chain monte carlo methods with couplings.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82\penalty0 (3):\penalty0 543--600, 2020.

\bibitem[Jarner \& Hansen(2000)Jarner and Hansen]{jarner2000geometric}
Jarner, S.~F. and Hansen, E.
\newblock Geometric ergodicity of metropolis algorithms.
\newblock \emph{Stochastic processes and their applications}, 85\penalty0
  (2):\penalty0 341--361, 2000.

\bibitem[Jin et~al.(2017)Jin, Lazarow, and Tu]{jin2017introspective}
Jin, L., Lazarow, J., and Tu, Z.
\newblock Introspective classification with convolutional nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  823--833, 2017.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KB15}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Lazarow et~al.(2017)Lazarow, Jin, and Tu]{lazarow2017introspective}
Lazarow, J., Jin, L., and Tu, Z.
\newblock Introspective neural networks for generative modeling.
\newblock In \emph{ICCV}, pp.\  2774--2783, 2017.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting structured data}, 1\penalty0 (0), 2006.

\bibitem[Lee et~al.(2018)Lee, Xu, Fan, and Tu]{lee2018wasserstein}
Lee, K., Xu, W., Fan, F., and Tu, Z.
\newblock Wasserstein introspective neural networks.
\newblock In \emph{ICCV}, pp.\  3702--3711, 2018.

\bibitem[Liu et~al.(2020)Liu, Wang, Owens, and Li]{ebmood2020}
Liu, W., Wang, X., Owens, J.~D., and Li, Y.
\newblock Energy-based out-of-distribution detection.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  3730--3738, 2015.

\bibitem[Marshall \& Roberts(2012)Marshall and Roberts]{marshall2012adaptive}
Marshall, T. and Roberts, G.
\newblock An adaptive approach to langevin mcmc.
\newblock \emph{Statistics and Computing}, 22\penalty0 (5), 2012.

\bibitem[Meyn \& Tweedie(2012)Meyn and Tweedie]{meyn2012markov}
Meyn, S.~P. and Tweedie, R.~L.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and
  Dean]{mikolov2013distributed}
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock \emph{arXiv preprint arXiv:1310.4546}, 2013.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Neal, R.~M. et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Ngiam et~al.(2011)Ngiam, Chen, Koh, and Ng]{ngiam2011learning}
Ngiam, J., Chen, Z., Koh, P.~W., and Ng, A.~Y.
\newblock Learning deep energy models.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, 2011.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Zhu, and Wu]{nijkamp2019learning}
Nijkamp, E., Hill, M., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock \emph{arXiv preprint arXiv:1904.09770}, 2019.

\bibitem[Nijkamp et~al.(2020)Nijkamp, Hill, Han, Zhu, and
  Wu]{nijkamp2020anatomy}
Nijkamp, E., Hill, M., Han, T., Zhu, S., and Wu, Y.~N.
\newblock On the anatomy of mcmc-based maximum likelihood learning of
  energy-based models.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence}, 2020.
\newblock URL \url{https://aaai.org/ojs/index.php/AAAI/article/view/5973}.

\bibitem[Nilsback \& Zisserman(2008)Nilsback and
  Zisserman]{nilsback2008automated}
Nilsback, M.-E. and Zisserman, A.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pp.\  722--729. IEEE, 2008.

\bibitem[Qiu et~al.(2019)Qiu, Zhang, and Wang]{qiu2019unbiased}
Qiu, Y., Zhang, L., and Wang, X.
\newblock Unbiased contrastive divergence algorithm for training energy-based
  latent variable models.
\newblock In \emph{ICLR}, 2019.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951A}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{Annals of Mathematical Statistics}, 22, 1951.

\bibitem[Robert \& Casella(2010)Robert and Casella]{mh:robert}
Robert, C.~P. and Casella, G.
\newblock \emph{Metropolis--Hastings Algorithms}, pp.\  167--197.
\newblock Springer New York, New York, NY, 2010.
\newblock ISBN 978-1-4419-1576-4.
\newblock \doi{10.1007/978-1-4419-1576-4_6}.

\bibitem[Roberts \& Rosenthal(1997)Roberts and Rosenthal]{roberts}
Roberts, G.~O. and Rosenthal, J.~S.
\newblock Optimal scaling of discrete approximations to langevin diffusions.
\newblock \emph{J. R. Statist. Soc. B}, 60:\penalty0 255--268, 1997.

\bibitem[Roberts \& Tweedie(1996)Roberts and Tweedie]{robertsmala}
Roberts, G.~O. and Tweedie, R.~L.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock \emph{Bernoulli}, 2\penalty0 (4):\penalty0 341--363, 12 1996.

\bibitem[Roberts et~al.(2004)Roberts, Rosenthal, et~al.]{roberts2004general}
Roberts, G.~O., Rosenthal, J.~S., et~al.
\newblock General state space markov chains and mcmc algorithms.
\newblock \emph{Probability surveys}, 1, 2004.

\bibitem[Rue et~al.(2009)Rue, Martino, and Chopin]{rue2009approximate}
Rue, H., Martino, S., and Chopin, N.
\newblock Approximate bayesian inference for latent gaussian models by using
  integrated nested laplace approximations.
\newblock \emph{Journal of the royal statistical society: Series b (statistical
  methodology)}, 2009.

\bibitem[Song et~al.(2020)Song, Garg, Shi, and Ermon]{song2020sliced}
Song, Y., Garg, S., Shi, J., and Ermon, S.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  574--584,
  2020.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tieleman, T.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{ICML}, 2008.

\bibitem[Wainwright \& Jordan(2008)Wainwright and Jordan]{jordanvi}
Wainwright, M.~J. and Jordan, M.~I.
\newblock Graphical models, exponential families, and variational inference.
\newblock \emph{Found. Trends Mach. Learn.}, 1\penalty0 (1-2):\penalty0 1--305,
  January 2008.
\newblock ISSN 1935-8237.
\newblock \doi{10.1561/2200000001}.

\bibitem[Welling \& Hinton(2002)Welling and Hinton]{welling2002new}
Welling, M. and Hinton, G.~E.
\newblock A new learning algorithm for mean field boltzmann machines.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  351--357, 2002.

\bibitem[Welling \& Teh(2011)Welling and Teh]{welling2011bayesian}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{ICML}, pp.\  681--688, 2011.

\bibitem[Wenliang et~al.(2019)Wenliang, Sutherland, Strathmann, and
  Gretton]{wenliang2019learning}
Wenliang, L., Sutherland, D., Strathmann, H., and Gretton, A.
\newblock Learning deep kernels for exponential family densities.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Wolfinger(1993)]{wolfinger}
Wolfinger, R.
\newblock Laplace's approximation for nonlinear mixed models.
\newblock \emph{Biometrika}, 80\penalty0 (4):\penalty0 791--795, 1993.

\bibitem[Xie et~al.(2016)Xie, Lu, Zhu, and Wu]{xie2016theory}
Xie, J., Lu, Y., Zhu, S.-C., and Wu, Y.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2635--2644. PMLR, 2016.

\bibitem[Xie et~al.(2017)Xie, Zhu, and Wu]{XieCVPR17}
Xie, J., Zhu, S.-C., and Wu, Y.~N.
\newblock Synthesizing dynamic patterns by spatial-temporal generative convnet.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp.\  7093--7101, 2017.

\bibitem[Xie et~al.(2018)Xie, Zheng, Gao, Wang, Zhu, and
  Nian~Wu]{xie2018learning}
Xie, J., Zheng, Z., Gao, R., Wang, W., Zhu, S.-C., and Nian~Wu, Y.
\newblock Learning descriptor networks for 3d shape synthesis and analysis.
\newblock In \emph{CVPR}, pp.\  8629--8638, 2018.

\bibitem[Xie et~al.(2019)Xie, Zhu, and Wu]{xie2019learning}
Xie, J., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning energy-based spatial-temporal generative convnets for
  dynamic patterns.
\newblock \emph{IEEE TPAMI}, 2019.

\bibitem[Xie et~al.(2020)Xie, Zheng, Gao, Wang, Zhu, and Wu]{xie2020generative}
Xie, J., Zheng, Z., Gao, R., Wang, W., Zhu, S.-C., and Wu, Y.~N.
\newblock Generative voxelnet: Learning energy-based models for 3d shape
  synthesis and analysis.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2020.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Zheng, Fang, Zhu, and
  Wu]{xie2021cooperative}
Xie, J., Zheng, Z., Fang, X., Zhu, S.-C., and Wu, Y.~N.
\newblock Cooperative training of fast thinking initializer and slow thinking
  solver for conditional learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Zheng, Fang, Zhu, and
  Wu]{xie2021cycleCoopNets}
Xie, J., Zheng, Z., Fang, X., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning cycle-consistent cooperative networks via alternating mcmc
  teaching for unsupervised cross-domain translation.
\newblock In \emph{AAAI}, 2021{\natexlab{b}}.

\bibitem[Xie et~al.(2021{\natexlab{c}})Xie, Zheng, and Li]{xie2020learning}
Xie, J., Zheng, Z., and Li, P.
\newblock Learning energy-based model with variational auto-encoder as
  amortized sampler.
\newblock In \emph{AAAI}, 2021{\natexlab{c}}.

\bibitem[Xu et~al.(2019)Xu, Xie, Zhao, Baker, Zhao, and Wu]{xu2019energy}
Xu, Y., Xie, J., Zhao, T., Baker, C., Zhao, Y., and Wu, Y.~N.
\newblock Energy-based continuous inverse optimal control.
\newblock \emph{arXiv preprint arXiv:1904.05453}, 2019.

\bibitem[Yu et~al.(2020)Yu, Song, Song, and Ermon]{yu2020training}
Yu, L., Song, Y., Song, J., and Ermon, S.
\newblock Training deep energy-based models with f-divergence minimization.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Zhu et~al.(1998)Zhu, Wu, and Mumford]{zhu1998filters}
Zhu, S.~C., Wu, Y., and Mumford, D.
\newblock Filters, random fields and maximum entropy (frame): Towards a unified
  theory for texture modeling.
\newblock \emph{International Journal of Computer Vision}, 27\penalty0
  (2):\penalty0 107--126, 1998.

\end{thebibliography}
