\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock {QSGD:} communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1709--1720, Long Beach, CA, 2017.

\bibitem[Chen et~al.(2020)Chen, Li, and Li]{chen2020toward}
Chen, X., Li, X., and Li, P.
\newblock Toward communication efficient adaptive gradient method.
\newblock In \emph{Proceedings of the {ACM-IMS} Foundations of Data Science
  Conference (FODS)}, pp.\  119--128, Virtual Event, USA, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei{-}Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei{-}Fei, L.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \emph{Proceedings of the 2009 {IEEE} Computer Society Conference
  on Computer Vision and Pattern Recognition (CVPR)}, pp.\  248--255, Miami,
  FL, 2009.

\bibitem[Dozat(2016)]{dozat2016incorporating}
Dozat, T.
\newblock Incorporating nesterov momentum into {Adam}.
\newblock In \emph{Proceedings of the 4th International Conference on Learning
  Representations (ICLR Workshop)}, San Juan, Puerto Rico, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{DHS11}
Duchi, J.~C., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2121--2159, 2011.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{{SIAM} J. Optim.}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Proc:He-resnet16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the 2016 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  770--778, Las Vegas, NV, 2016.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2019scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S.~J., Stich, S.~U., and
  Suresh, A.~T.
\newblock Scaffold: Stochastic controlled averaging for on-device federated
  learning.
\newblock \emph{arXiv preprint arXiv:1910.06378}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KB15}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
Kone{\v{c}}n{\`y}, J., McMahan, H.~B., Yu, F.~X., Richt{\'a}rik, P., Suresh,
  A.~T., and Bacon, D.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
Li, M., Andersen, D.~G., Park, J.~W., Smola, A.~J., Ahmed, A., Josifovski, V.,
  Long, J., Shekita, E.~J., and Su, B.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{Proceedings of the 11th {USENIX} Symposium on Operating
  Systems Design and Implementation (OSDI)}, pp.\  583--598, Broomfield, CO,
  2014.

\bibitem[Li et~al.(2020)Li, Sahu, Talwalkar, and Smith]{li2019federated}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{{IEEE} Signal Process. Mag.}, 37\penalty0 (3):\penalty0 50--60,
  2020.

\bibitem[Liang et~al.(2019)Liang, Shen, Liu, Pan, Chen, and
  Cheng]{liang2019variance}
Liang, X., Shen, S., Liu, J., Pan, Z., Chen, E., and Cheng, Y.
\newblock Variance reduced local sgd with lower communication complexity.
\newblock \emph{arXiv preprint arXiv:1912.12844}, 2019.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, B.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[McMahan \& Streeter(2010)McMahan and Streeter]{mcmahan2010adaptive}
McMahan, B. and Streeter, M.~J.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In \emph{Proceedings of the 23rd Conference on Learning Theory
  (COLT)}, pp.\  244--256, Haifa, Israel, 2010.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pp.\  1273--1282, Fort
  Lauderdale, FL, 2017.

\bibitem[Nesterov(2004)]{N04}
Nesterov, Y.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock \emph{Springer}, 2004.

\bibitem[Polyak(1964)]{P64}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Mathematics and Mathematical Physics}, 1964.

\bibitem[Recht et~al.(2011)Recht, R{\'{e}}, Wright, and Niu]{recht2011hogwild}
Recht, B., R{\'{e}}, C., Wright, S.~J., and Niu, F.
\newblock Hogwild: {A} lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  693--701, Granada, Spain, 2011.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2020adaptive}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
  Kone{\v{c}}n{\'y}, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{Proceedings of the 9th International Conference on Learning
  Representations (ICLR)}, Virtual Event, Austria, 2021.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica,
  Braverman, Gonzalez, and Arora]{Proc:Rothchild_ICML20}
Rothchild, D., Panda, A., Ullah, E., Ivkin, N., Stoica, I., Braverman, V.,
  Gonzalez, J., and Arora, R.
\newblock Fetchsgd: Communication-efficient federated learning with sketching.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8253--8265. PMLR, 2020.

\bibitem[Stich(2019)]{stich2018local}
Stich, S.~U.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, New Orleans, LA, 2019.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{TH12}
Tieleman, T. and Hinton, G.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Wangni, J., Wang, J., Liu, J., and Zhang, T.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  1306--1316, Montr{\'{e}}al, Canada, 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock {Fashion-MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
You, Y., Li, J., Reddi, S.~J., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}, Addis Ababa, Ethiopia, 2020.

\bibitem[Yu et~al.(2019)Yu, Jin, and Yang]{yu2019linear}
Yu, H., Jin, R., and Yang, S.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, pp.\  7184--7193, Long Beach, CA, 2019.

\bibitem[Zeiler(2012)]{Z12}
Zeiler, M.~D.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zhao et~al.(2020)Zhao, Xie, Jia, Qian, Ding, Sun, and
  Li]{zhao2020distributed}
Zhao, W., Xie, D., Jia, R., Qian, Y., Ding, R., Sun, M., and Li, P.
\newblock Distributed hierarchical {GPU} parameter server for massive scale
  deep learning ads systems.
\newblock In \emph{Proceedings of Machine Learning and Systems (MLSys)},
  Austin, TX, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Chen, Cao, Tang, Yang, and Gu]{Arxiv:Zhou_18}
Zhou, D., Chen, J., Cao, Y., Tang, Y., Yang, Z., and Gu, Q.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018.

\bibitem[Zhou \& Cong(2018)Zhou and Cong]{zhou2017convergence}
Zhou, F. and Cong, G.
\newblock On the convergence properties of a k-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence (IJCAI)}, pp.\  3219--3227, Stockholm,
  Sweden, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Karimi, Yu, Xu, and Li]{zhou2020towards}
Zhou, Y., Karimi, B., Yu, J., Xu, Z., and Li, P.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\end{thebibliography}
