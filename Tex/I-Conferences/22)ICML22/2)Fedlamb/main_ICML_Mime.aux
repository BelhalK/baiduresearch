\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{karimireddy2020mime}
\citation{chen2020toward}
\citation{konevcny2016federated,mcmahan2017communication}
\citation{mcmahan2017communication}
\citation{Proc:YuJY_ICML19}
\citation{KB15}
\citation{reddi2019convergence}
\citation{chen2020toward}
\citation{karimireddy2020mime}
\citation{li2019federated,liang2019variance,karimireddy2019scaffold}
\newlabel{sec:introduction}{{1}{1}{}{section.1}{}}
\newlabel{eq:opt}{{1}{1}{}{equation.1.1}{}}
\newlabel{rule:adam}{{2}{1}{}{equation.1.2}{}}
\citation{you2019large}
\citation{deng2009imagenet}
\citation{bert19}
\citation{chen2020toward}
\citation{TH12}
\citation{Z12}
\citation{KB15}
\citation{dozat2016incorporating}
\citation{reddi2019convergence}
\citation{DHS11,mcmahan2010adaptive}
\citation{N04}
\citation{P64}
\citation{DHS11}
\citation{KB15}
\citation{reddi2019convergence}
\citation{zhou2018convergence,Proc:Chen_ICLR19,zhou2020towards}
\citation{Proc:LARS18}
\citation{you2019large}
\citation{you2019large}
\citation{konevcny2016federated,mcmahan2017communication}
\citation{Article:YangLCT19,Proc:Leroy_ICASSP19,bonawitz2019towards,Article:NiknamDR20,Proc:XuGSWBW21}
\citation{Proc:YuJY_ICML19,karimireddy2019scaffold,Proc:Khaled_AISTATS20,Proc:Li_ICLR20,Proc:Woodworth_ICML20,Proc:WangTBR_ICLR20}
\newlabel{sec:related}{{2}{2}{}{section.2}{}}
\citation{reddi2020adaptive}
\citation{chen2020toward,karimireddy2020mime}
\citation{karimireddy2020mime}
\citation{karimireddy2020mime}
\citation{karimireddy2019scaffold}
\citation{Article:Sahu_arxiv18}
\citation{reddi2019convergence}
\citation{karimireddy2020mime}
\newlabel{sec:main}{{3}{3}{}{section.3}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:illustrate}{{1}{3}{Illustration of Fed-LAMB framework (Algorithm~\ref {alg:ldams}), with a three-layer network and $\phi (x)=x$ as an example. For device $i$ and each local iteration in round $r$, the adaptive ratio of $j$-th layer $\psi _{r,i}^j$ is normalized according to $\Vert \theta _{r,i}^j\Vert $, and then used for updating the local model. At the end of each round $r$, client $i$ sends $\theta _{r,i} = [\theta _{r,i}^{\ell }]_{\ell =1}^{\tot }$ and $v_{r,i}$ to the central server, which transmits back aggregated $\theta $ and $\hat v$ to devices to complete a round of training.\relax }{figure.caption.1}{}}
\citation{you2019large}
\newlabel{alg:ldams}{{1}{4}{\colorbox {blue!20!white}{Fed-LAMB} and \colorbox {green!20!white}{Mime-LAMB} \relax }{algorithm.1}{}}
\newlabel{line:scale}{{1}{4}{\colorbox {blue!20!white}{Fed-LAMB} and \colorbox {green!20!white}{Mime-LAMB} \relax }{algorithm.1}{}}
\newlabel{line:layer}{{1}{4}{\colorbox {blue!20!white}{Fed-LAMB} and \colorbox {green!20!white}{Mime-LAMB} \relax }{algorithm.1}{}}
\newlabel{eq:updatelayer}{{3}{4}{\colorbox {blue!20!white}{Fed-LAMB} and \colorbox {green!20!white}{Mime-LAMB} \relax }{equation.3.3}{}}
\newlabel{sec:theory}{{4}{4}{}{section.4}{}}
\newlabel{ass:smooth}{{4.1}{4}{}{theo.4.1}{}}
\newlabel{ass:boundgrad}{{4.2}{4}{}{theo.4.2}{}}
\newlabel{ass:var}{{4.3}{4}{}{theo.4.3}{}}
\newlabel{ass:phi}{{4.4}{4}{}{theo.4.4}{}}
\newlabel{th:multiple update}{{4.5}{4}{}{theo.4.5}{}}
\newlabel{bound1multiple}{{4}{4}{}{equation.4.4}{}}
\citation{you2019large}
\citation{you2019large}
\citation{ghadimi2013stochastic}
\citation{you2019large}
\citation{chen2020toward}
\citation{chen2020toward}
\citation{chen2020toward}
\citation{Arxiv:Zhou_18}
\newlabel{lemma:iterates}{{4.6}{5}{}{theo.4.6}{}}
\newlabel{lemma:ratio}{{4.7}{5}{}{theo.4.7}{}}
\newlabel{coro:main}{{4.8}{5}{}{theo.4.8}{}}
\newlabel{coro:rate}{{5}{5}{}{equation.4.5}{}}
\newlabel{thm:chen}{{4.9}{5}{Corollary 5.2 in~\citet {chen2020toward}}{theo.4.9}{}}
\newlabel{eqn:chen rate}{{6}{5}{Corollary 5.2 in~\citet {chen2020toward}}{equation.4.6}{}}
\citation{chen2020toward}
\citation{karimireddy2019scaffold}
\citation{reddi2020adaptive}
\citation{KB15}
\citation{reddi2020adaptive}
\citation{karimireddy2019scaffold}
\citation{mcmahan2017communication}
\citation{reddi2020adaptive}
\citation{chen2020toward}
\citation{karimireddy2020mime}
\citation{reddi2019convergence}
\newlabel{sec:numerical}{{5}{6}{}{section.5}{}}
\citation{lecun1998mnist}
\citation{xiao2017fashion}
\citation{krizhevsky2009learning}
\citation{deng2009imagenet}
\citation{Proc:He-resnet16}
\newlabel{fig:iid}{{2}{7}{\textbf {i.i.d. data setting}. Test accuracy on MNIST and FMNIST against the number of communication rounds. \textbf {Left Column:} 1 local epoch. \textbf {Right Column:} 3 local epochs. \textbf {1st row:} MNIST + MLP. \textbf {2nd row:} MNIST + CNN. \textbf {3rd row:} FMNIST + CNN. \relax }{figure.caption.2}{}}
\newlabel{fig:noniid}{{3}{7}{\textbf {non-i.i.d. data setting.} Test accuracy on MNIST and FMNIST against the number of communication rounds. \textbf {Left:} 1 local epoch. \textbf {Right:} 3 local epochs. \textbf {1st row:} MNIST + MLP. \textbf {2nd row:} MNIST + CNN. \textbf {3rd row:} FMNIST + CNN.\relax }{figure.caption.3}{}}
\citation{mcmahan2017communication}
\newlabel{tab:acc}{{1}{8}{Test Accuracy with ResNet-18 Network.\relax }{table.caption.4}{}}
\newlabel{fig:noniidresnet18}{{4}{8}{\textbf {non-i.i.d. data setting.} Test accuracy on CIFAR-10 + ResNet-18 and TinyImagenet + ResNet-18. \relax }{figure.caption.5}{}}
\newlabel{sec:conclusion}{{6}{8}{}{section.6}{}}
\bibstyle{plainnat}
\bibdata{ref}
\bibcite{bonawitz2019towards}{{1}{2019}{{Bonawitz et~al.}}{{Bonawitz, Eichner, Grieskamp, Huba, Ingerman, Ivanov, Kiddon, Kone{\v {c}}n{\`y}, Mazzocchi, McMahan, et~al.}}}
\bibcite{Proc:Chen_ICLR19}{{2}{2019}{{Chen et~al.}}{{Chen, Liu, Sun, and Hong}}}
\bibcite{chen2020toward}{{3}{2020}{{Chen et~al.}}{{Chen, Li, and Li}}}
\bibcite{deng2009imagenet}{{4}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei{-}Fei}}}
\bibcite{bert19}{{5}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dozat2016incorporating}{{6}{2016}{{Dozat}}{{}}}
\bibcite{DHS11}{{7}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{ghadimi2013stochastic}{{8}{2013}{{Ghadimi and Lan}}{{}}}
\bibcite{Proc:He-resnet16}{{9}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{karimireddy2019scaffold}{{10}{2019}{{Karimireddy et~al.}}{{Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh}}}
\bibcite{karimireddy2020mime}{{11}{2020}{{Karimireddy et~al.}}{{Karimireddy, Jaggi, Kale, Mohri, Reddi, Stich, and Suresh}}}
\bibcite{Proc:Khaled_AISTATS20}{{12}{2020}{{Khaled et~al.}}{{Khaled, Mishchenko, and Richt{\'{a}}rik}}}
\bibcite{KB15}{{13}{2015}{{Kingma and Ba}}{{}}}
\bibcite{konevcny2016federated}{{14}{2016}{{Kone{\v {c}}n{\`y} et~al.}}{{Kone{\v {c}}n{\`y}, McMahan, Yu, Richt{\'a}rik, Suresh, and Bacon}}}
\bibcite{krizhevsky2009learning}{{15}{2009}{{Krizhevsky}}{{}}}
\bibcite{lecun1998mnist}{{16}{1998}{{LeCun}}{{}}}
\bibcite{Proc:Leroy_ICASSP19}{{17}{2019}{{Leroy et~al.}}{{Leroy, Coucke, Lavril, Gisselbrecht, and Dureau}}}
\bibcite{li2019federated}{{18}{2020{a}}{{Li et~al.}}{{Li, Sahu, Talwalkar, and Smith}}}
\bibcite{Proc:Li_ICLR20}{{19}{2020{b}}{{Li et~al.}}{{Li, Huang, Yang, Wang, and Zhang}}}
\bibcite{liang2019variance}{{20}{2019}{{Liang et~al.}}{{Liang, Shen, Liu, Pan, Chen, and Cheng}}}
\bibcite{mcmahan2010adaptive}{{21}{2010}{{McMahan and Streeter}}{{}}}
\bibcite{mcmahan2017communication}{{22}{2017}{{McMahan et~al.}}{{McMahan, Moore, Ramage, Hampson, and y~Arcas}}}
\bibcite{N04}{{23}{2004}{{Nesterov}}{{}}}
\bibcite{Article:NiknamDR20}{{24}{2020}{{Niknam et~al.}}{{Niknam, Dhillon, and Reed}}}
\bibcite{P64}{{25}{1964}{{Polyak}}{{}}}
\bibcite{reddi2019convergence}{{26}{2018}{{Reddi et~al.}}{{Reddi, Kale, and Kumar}}}
\bibcite{reddi2020adaptive}{{27}{2021}{{Reddi et~al.}}{{Reddi, Charles, Zaheer, Garrett, Rush, Kone{\v {c}}n{\'y}, Kumar, and McMahan}}}
\bibcite{Article:Sahu_arxiv18}{{28}{2018}{{Sahu et~al.}}{{Sahu, Li, Sanjabi, Zaheer, Talwalkar, and Smith}}}
\bibcite{TH12}{{29}{2012}{{Tieleman and Hinton}}{{}}}
\bibcite{Proc:WangTBR_ICLR20}{{30}{2020}{{Wang et~al.}}{{Wang, Tantia, Ballas, and Rabbat}}}
\bibcite{Proc:Woodworth_ICML20}{{31}{2020}{{Woodworth et~al.}}{{Woodworth, Patel, Stich, Dai, Bullins, McMahan, Shamir, and Srebro}}}
\bibcite{xiao2017fashion}{{32}{2017}{{Xiao et~al.}}{{Xiao, Rasul, and Vollgraf}}}
\bibcite{Proc:XuGSWBW21}{{33}{2021}{{Xu et~al.}}{{Xu, Glicksberg, Su, Walker, Bian, and Wang}}}
\bibcite{Article:YangLCT19}{{34}{2019}{{Yang et~al.}}{{Yang, Liu, Chen, and Tong}}}
\bibcite{Proc:LARS18}{{35}{2018}{{You et~al.}}{{You, Zhang, Hsieh, Demmel, and Keutzer}}}
\bibcite{you2019large}{{36}{2020}{{You et~al.}}{{You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song, Demmel, Keutzer, and Hsieh}}}
\bibcite{Proc:YuJY_ICML19}{{37}{2019}{{Yu et~al.}}{{Yu, Jin, and Yang}}}
\bibcite{Z12}{{38}{2012}{{Zeiler}}{{}}}
\bibcite{Arxiv:Zhou_18}{{39}{2018{a}}{{Zhou et~al.}}{{Zhou, Chen, Cao, Tang, Yang, and Gu}}}
\bibcite{zhou2018convergence}{{40}{2018{b}}{{Zhou et~al.}}{{Zhou, Chen, Cao, Tang, Yang, and Gu}}}
\bibcite{zhou2020towards}{{41}{2020}{{Zhou et~al.}}{{Zhou, Karimi, Yu, Xu, and Li}}}
\citation{reddi2020adaptive}
\citation{chen2020toward}
\citation{reddi2020adaptive}
\citation{reddi2020adaptive}
\newlabel{app:experiment}{{A}{11}{}{appendix.A}{}}
\newlabel{alg:adp-fed}{{2}{11}{Adp-Fed: Adaptive Federated Optimization~\citep {reddi2020adaptive}\relax }{algorithm.2}{}}
\newlabel{adpfed line:local SGD}{{2}{11}{Adp-Fed: Adaptive Federated Optimization~\citep {reddi2020adaptive}\relax }{algorithm.2}{}}
\newlabel{adpfed line:global adam}{{2}{11}{Adp-Fed: Adaptive Federated Optimization~\citep {reddi2020adaptive}\relax }{algorithm.2}{}}
\newlabel{tab:tuning}{{2}{11}{Search grids of the learning rate.\relax }{table.caption.7}{}}
\newlabel{app:proofs}{{B}{12}{}{appendix.B}{}}
\newlabel{tab:notationsapp}{{3}{12}{Summary of notations used in the paper.\relax }{table.caption.8}{}}
\citation{reddi2019convergence}
\newlabel{app:proofmain}{{B.2}{13}{}{subsection.B.2}{}}
\newlabel{eq:main}{{8}{13}{}{equation.B.8}{}}
\newlabel{eq:defseq}{{9}{13}{}{equation.B.9}{}}
\newlabel{eq:gap}{{10}{13}{}{equation.B.10}{}}
\newlabel{eq:main2}{{11}{13}{}{equation.B.11}{}}
\newlabel{eqn1}{{12}{14}{}{equation.B.12}{}}
\newlabel{eqn:x1}{{13}{14}{}{equation.B.13}{}}
\newlabel{eq:inter}{{14}{14}{}{equation.B.14}{}}
\newlabel{eqn:B1}{{15}{14}{}{equation.B.15}{}}
