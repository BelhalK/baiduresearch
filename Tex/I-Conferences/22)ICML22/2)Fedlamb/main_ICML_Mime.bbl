\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bonawitz et~al.(2019)Bonawitz, Eichner, Grieskamp, Huba, Ingerman,
  Ivanov, Kiddon, Kone{\v{c}}n{\`y}, Mazzocchi, McMahan,
  et~al.]{bonawitz2019towards}
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone{\v{c}}n{\`y}, Stefano
  Mazzocchi, H~Brendan McMahan, et~al.
\newblock Towards federated learning at scale: System design.
\newblock \emph{arXiv preprint arXiv:1902.01046}, 2019.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{Proc:Chen_ICLR19}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of {A} class of adam-type algorithms for
  non-convex optimization.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Chen et~al.(2020)Chen, Li, and Li]{chen2020toward}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In \emph{Proceedings of the {ACM-IMS} Foundations of Data Science
  Conference (FODS)}, pages 119--128, Virtual Event, USA, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei{-}Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li{-}Jia Li, Kai Li, and Li~Fei{-}Fei.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \emph{Proceedings of the 2009 {IEEE} Computer Society Conference
  on Computer Vision and Pattern Recognition (CVPR)}, pages 248--255, Miami,
  FL, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert19}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
  1 (Long and Short Papers)}, pages 4171--4186. Association for Computational
  Linguistics, 2019.

\bibitem[Dozat(2016)]{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating nesterov momentum into {Adam}.
\newblock In \emph{Proceedings of the 4th International Conference on Learning
  Representations (ICLR Workshop)}, San Juan, Puerto Rico, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{DHS11}
John~C. Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2121--2159, 2011.

\bibitem[Haddadpour et~al.(2020)Haddadpour, Karimi, Li, and
  Li]{haddadpour2020fedsketch}
Farzin Haddadpour, Belhal Karimi, Ping Li, and Xiaoyun Li.
\newblock Fedsketch: Communication-efficient and private federated learning via
  sketching.
\newblock \emph{arXiv preprint arXiv:2008.04975}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Proc:He-resnet16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the 2016 {IEEE} Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 770--778, Las Vegas, NV, 2016.

\bibitem[Ivkin et~al.(2019)Ivkin, Rothchild, Ullah, Braverman, Stoica, and
  Arora]{ivkin2019communication}
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica,
  and Raman Arora.
\newblock Communication-efficient distributed {SGD} with sketching.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 13144--13154, Vancouver, Canada, 2019.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2019scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J Reddi,
  Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for on-device federated
  learning.
\newblock \emph{arXiv preprint arXiv:1910.06378}, 2019.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Jaggi, Kale, Mohri, Reddi, Stich,
  and Suresh]{karimireddy2020mime}
Sai~Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank~J
  Reddi, Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Mime: Mimicking centralized stochastic algorithms in federated
  learning.
\newblock \emph{arXiv preprint arXiv:2008.03606}, 2020.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'{a}}rik]{Proc:Khaled_AISTATS20}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'{a}}rik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2020, 26-28 August 2020, Online [Palermo, Sicily,
  Italy]}, volume 108 of \emph{Proceedings of Machine Learning Research}, pages
  4519--4529. {PMLR}, 2020.

\bibitem[Kingma and Ba(2015)]{KB15}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, San Diego, CA, 2015.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Leroy et~al.(2019)Leroy, Coucke, Lavril, Gisselbrecht, and
  Dureau]{Proc:Leroy_ICASSP19}
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph
  Dureau.
\newblock Federated learning for keyword spotting.
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and
  Signal Processing, {ICASSP} 2019, Brighton, United Kingdom, May 12-17, 2019},
  pages 6341--6345. {IEEE}, 2019.

\bibitem[Li et~al.(2019)Li, Liu, Sekar, and Smith]{li2019privacy}
Tian Li, Zaoxing Liu, Vyas Sekar, and Virginia Smith.
\newblock Privacy for free: Communication-efficient learning with differential
  privacy using sketches.
\newblock \emph{arXiv preprint arXiv:1911.00972}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{li2019federated}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{{IEEE} Signal Process. Mag.}, 37\penalty0 (3):\penalty0 50--60,
  2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Huang, Yang, Wang, and
  Zhang]{Proc:Li_ICLR20}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net,
  2020{\natexlab{b}}.

\bibitem[Liang et~al.(2019)Liang, Shen, Liu, Pan, Chen, and
  Cheng]{liang2019variance}
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei
  Cheng.
\newblock Variance reduced local sgd with lower communication complexity.
\newblock \emph{arXiv preprint arXiv:1912.12844}, 2019.

\bibitem[McMahan and Streeter(2010)]{mcmahan2010adaptive}
Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In \emph{Proceedings of the 23rd Conference on Learning Theory
  (COLT)}, pages 244--256, Haifa, Israel, 2010.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
  Blaise~Ag{\"{u}}era y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 1273--1282, Fort
  Lauderdale, FL, 2017.

\bibitem[Nesterov(2004)]{N04}
Yurii Nesterov.
\newblock Introductory lectures on convex optimization: A basic course.
\newblock \emph{Springer}, 2004.

\bibitem[Niknam et~al.(2020)Niknam, Dhillon, and Reed]{Article:NiknamDR20}
Solmaz Niknam, Harpreet~S. Dhillon, and Jeffrey~H. Reed.
\newblock Federated learning for wireless communications: Motivation,
  opportunities, and challenges.
\newblock \emph{{IEEE} Commun. Mag.}, 58\penalty0 (6):\penalty0 46--51, 2020.

\bibitem[Polyak(1964)]{P64}
B.~T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Mathematics and Mathematical Physics}, 1964.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{Proceedings of the 6th International Conference on Learning
  Representations (ICLR)}, Vancouver, Canada, 2018.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2020adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In \emph{Proceedings of the 9th International Conference on Learning
  Representations (ICLR)}, Virtual Event, Austria, 2021.

\bibitem[Sahu et~al.(2018)Sahu, Li, Sanjabi, Zaheer, Talwalkar, and
  Smith]{Article:Sahu_arxiv18}
Anit~Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
  Virginia Smith.
\newblock On the convergence of federated optimization in heterogeneous
  networks.
\newblock \emph{CoRR}, abs/1812.06127, 2018.

\bibitem[Tieleman and Hinton(2012)]{TH12}
T.~Tieleman and G.~Hinton.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Wang et~al.(2020)Wang, Tantia, Ballas, and
  Rabbat]{Proc:WangTBR_ICLR20}
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael~G. Rabbat.
\newblock Slowmo: Improving communication-efficient distributed {SGD} with slow
  momentum.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, McMahan,
  Shamir, and Srebro]{Proc:Woodworth_ICML20}
Blake~E. Woodworth, Kumar~Kshitij Patel, Sebastian~U. Stich, Zhen Dai, Brian
  Bullins, H.~Brendan McMahan, Ohad Shamir, and Nathan Srebro.
\newblock Is local {SGD} better than minibatch sgd?
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 10334--10343. {PMLR},
  2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock {Fashion-MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xu et~al.(2021)Xu, Glicksberg, Su, Walker, Bian, and
  Wang]{Proc:XuGSWBW21}
Jie Xu, Benjamin~S. Glicksberg, Chang Su, Peter~B. Walker, Jiang Bian, and Fei
  Wang.
\newblock Federated learning for healthcare informatics.
\newblock \emph{J. Heal. Informatics Res.}, 5\penalty0 (1):\penalty0 1--19,
  2021.

\bibitem[Yang et~al.(2019)Yang, Liu, Chen, and Tong]{Article:YangLCT19}
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.
\newblock Federated machine learning: Concept and applications.
\newblock \emph{{ACM} Trans. Intell. Syst. Technol.}, 10\penalty0 (2):\penalty0
  12:1--12:19, 2019.

\bibitem[You et~al.(2018)You, Zhang, Hsieh, Demmel, and Keutzer]{Proc:LARS18}
Yang You, Zhao Zhang, Cho{-}Jui Hsieh, James Demmel, and Kurt Keutzer.
\newblock Imagenet training in minutes.
\newblock In \emph{Proceedings of the 47th International Conference on Parallel
  Processing, {ICPP} 2018, Eugene, OR, USA, August 13-16, 2018}, pages
  1:1--1:10. {ACM}, 2018.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
Yang You, Jing Li, Sashank~J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho{-}Jui Hsieh.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}, Addis Ababa, Ethiopia, 2020.

\bibitem[Yu et~al.(2019)Yu, Jin, and Yang]{Proc:YuJY_ICML19}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  7184--7193. {PMLR}, 2019.

\bibitem[Zeiler(2012)]{Z12}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Chen, Cao, Tang, Yang, and
  Gu]{Arxiv:Zhou_18}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018{\natexlab{a}}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Chen, Cao, Tang, Yang, and
  Gu]{zhou2018convergence}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018{\natexlab{b}}.

\bibitem[Zhou et~al.(2020)Zhou, Karimi, Yu, Xu, and Li]{zhou2020towards}
Yingxue Zhou, Belhal Karimi, Jinxing Yu, Zhiqiang Xu, and Ping Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, virtual, 2020.

\end{thebibliography}
