\documentclass{article}

% Please use the following line and do not change the style file.
\usepackage{icml2022_author_response}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}       % hyperlinks
\usepackage{booktabs} % for professional tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.

%\usepackage{lipsum}

\begin{document}
% Uncomment the following line if you prefer a single-column format
%\onecolumn
\textbf{================Reply to R\#1===============}\\
Thanks for pointing out our paper is significantly novel with competitive performance. We address your~concerns~below. \\
\textbf{Q1: Far behind SOTA. Is it because of computational limits?}\\
\textit{A1: }We would like to clarify on the originality and goal of our contribution. 
In this paper, we zqnt to show the benefits of using adaptive stepsize for learning a ConvNet-based EBM where the energy landscape is highly nonconvex, not only via experiments but with a rigorous non-asymptotic theoretical analysis.
As our method aims at accelerating the convergence of the model in the first iterations, we argue that our paper does not aim at proving an additional SOTA in terms of generated outputs. Rather, we tackle this problem from an optimization point of view with the motivation of improving how the latent samples find rapidly a good enough maxima in the target conditional distribution. Our numerical experiments and theorem do provide insights on that regards as the first epochs show.

\textbf{Q2: About inpainting the same images for better comparison in Figure 6}\\
\textit{A2: We will follow your suggestion to revise Figure 6.}\\

\textbf{Q3: How to choose the anistropic step size?}\\
\textit{A3:} The simplicity of our stepsize comes in the fact that it is based on the gradient of the distribution at each iteration. The only tuning parameter is the threshold which can be found empirically.\\
\textbf{Q4: How the STANLEY alleviates the issues of the existing methods?}\\
\textit{A4: } See A1. We mostly focus on the convergence of the sampling scheme in the first epochs. This contribution is focussing on the transition regime and do not tackle the asymptotic convergence nor the final accuracy of the model.\\
\textbf{Q5: How would the NUTS ampler compare?}\\
\textit{A5: } NUTS sampler is based on HMC sampler. We refer the reviewer to check our results for HMC in the numerical experiments.\\
\textbf{Q6: About Typos.}\\
\textit{A6: We will correct the typos. Thanks.}\\

\textbf{================Reply to R\#3===============}\\
\textbf{Q1: Not clear if the assumptions of Allassonniere and Kuhn hold for EBM.}\\
\textit{A1: }Our main contribution theory-wise is to extend their proof to the case of EBM, i.e. in the nonconvex case. Hence their assumptions do not suffice as they are stronger that our case.\\

\textbf{Q2: Figure 1 doesn't support the conclusion that the proposed method recovers the density earlier. }\\
\textit{A2: } We will improve the resolution of Figure 6 for the sake of clarity.\\

\textbf{Q3: How robust are the curves to different model initializations?}\\
\textit{A3:} This is an interesting point that we believe all papers in that realm should tackle in the future. Most of our runs are single runs and studying the robustness to the initialization by averaging multiple runs could be interesting. \\

\textbf{Q4: Could STANLEY be leading only at the beginning in Figure 5? How do the curves behaves as the training progresses and convergence?}\\
\textit{A4:} As we tackle early convergence speed, we do not focus on the heavy tail of the convergence and certainly, our contribution does not reside in the resulting model accuracy but more on the ability to improve the ability of the MCMC to obtain good samples in the first epochs, i.e. when the EBM model parameters are still far from the target parameters.\\

\textbf{Q5: Why use FID rather than PSNR to evaluate image inpainting?}\\
\textit{A5:} \textcolor{red}{Jianwen can you reply to that please?} \\

\textbf{Q6: It would be more informative to apply two algorithms to the same image in Figure 6.}\\
\textit{A6: we will follow suggestion in our revision.}\\

\textbf{Q7: How does the Langevin step subscript $k$ combine with the iteration step subscript $t$? Do we start the scheme with the input argument $z_0^m$ all the time or draw random initial states?}\\
\textit{A7:} The two subscripts are independent. $k$ monitors the MCMC chain and $t$ monitors the EBM training. The initialization of the chain is random at each new parameter hence at each $t$.\\

\textbf{Q8: In theorem 1 drift function $V_\theta$ does not explicitly occur nor in eq. 8, nor in eq. 9. Is it present somewhere implicitly? What does $\chi$ stand for? Is there supposed to be $\pi_theta(\cdot)$ instead of $\pi_theta u(\cdot)$ in corollary 1?}\\
\textit{A8:} We define the drift function in the proof in the appendix. We will fix the typo raised by the reviewer. Thank you.\\

\textbf{Q9: Proof of geometric ergodicity seems to follow the proof Allassonni`ere and Kuhn leading to limited novelty from the theoretical viewpoint}\\
\textit{A9:}Our main contribution theory-wise is to extend their proof to the case of EBM, i.e. in the nonconvex case. Hence their assumptions do not suffice as they are stronger that our case.\\

\textbf{================Reply to R\#4===============}\\
\textbf{Q1: The authors do not do a good job of distinguishing their own contribution from prior art.}\\
\textit{A1: } We will clarify that in the revision.
Drastically reducing the number of MCMC transitions would have a great impact on the energy consumption and speed of the whole training process.
Besides, we stress on the important theoretical contribution that is presented along our algorithm compared to prior art. 
To the best of our knowledge, EBM methods are presented mainly using empirical insights on their respective contribution.
In this paper, we wanted to show the benefits of using adaptive stepsize for learning a convent-based EBM where the energy landscape is highly nonconvex, not only via experiments but with a rigorous non-asymptotic convergence analysis.\\

\textbf{Q2: The authors use the Unadjusted Metropolis algorithm in the main text (see algorithm 1), but the convergence proof uses the metropolis adjustment (Line 625).}\\
\textit{A2: } For the sake of the proofs we use the MALA algorithm. From a convergence standpoint, there is no interest in considering the ULA method.\\

\textbf{Q2: Theorem 1 and Corollary 1 do not seem to depend on the main contribution of the paper, the anisotropic learning rate. In fact the proofs seem to exactly follow the existing literature, specifically Atchade (2006)/Roberts and Tweedie (1996)}\\
\textit{A2: }Our main contribution theory-wise is to extend the proof of Allassonniere and Kuhn to the case of EBM, i.e. in the nonconvex case. Hence our results are intrinsically related to our introduced scheme. As for Roberts and Tweedie, their works is seminal and has paved the way to a long series of theoretical MCMC papers using their proof techniques. Indeed ours is also built upon those tools introduced in their work.\\

\textbf{Q3: Equation 4 - RHS depends on m, LHS doesn't, is the step size dependent on the max norm? or is the step size defined per chain?}\\
\textit{A3: }The stepsize is anisotropic and depends on the max norm. Since it also depends on the norm of the gradient, this stepsize depends on the initialization of the chain and the iteration index $k$\.\

\textbf{Q4: Line 235 Right - V norm is recursively defined}\\
\textit{A4: }\\

\textbf{Q5: Section 5.1 - The step size for Langevin is not specified -- is it equal to "th"?. There are two learning rates mentioned. The reviewer sees no difference in the empirical results. If the true model is known why is log likelihood of the examples not reported?}\\
\textit{A5: } The learning rates for Vanilla Langevin is the classical constant stepsize used in the Langevin Dynamics and is fine tuned over a grid. 
Coupling the quantitative FID curves and the qualitative synthetic images is important to be able to compare the baselines with our scheme.\\

\textbf{Q6: Figure 4,5 - Confidence intervals over multiple runs are missing. Is this a mean over multiple runs or a single run? If single, the variance may explain why HMC seemingly works better in Figure 4 Right.}\\
\textit{A6: } This is an interesting point that we believe all papers in that realm should tackle in the future. Most of our runs are single runs and studying the robustness to the initialization by averaging multiple runs could be interesting.\\

\textbf{Q7: Figure 6 - The exemplar results shown are non consistent, a useful comparison would be to show how both methods perform on the same initial image example.}\\
\textit{A7: }We will take that into consideration. The images used right now are pretty similar and coupled with the FID curves, provide a good comparison of the baselines.\\


\textbf{Q8: Quality of writing/presentation. Other math}\\
\textit{A8: }We will improve the overall clarity of our paper in the revision.\\

\end{document}
