\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}
\usepackage[square,numbers]{natbib}
\bibliographystyle{plain}

\usepackage{jmlr2e}
\usepackage{graphicx}
\usepackage[ margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor={purple},citecolor={blue},urlcolor={red}}  

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{meila00a}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names
\usepackage{fancyhdr}%
\usepackage{lipsum}% Just for this example


\fancyhf{}% Clear all headers/footers
\fancyfoot[L]{}\fancyfoot[C]{Belhal Karimi, Research Statement}\fancyfoot[R]{}
\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
\rfoot{\thepage}
%\thispagestyle{plain}

%\ShortHeadings{Belhal Karimi, Research Statement}{Belhal Karimi, Research Statement}
\firstpageno{1}
\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}
\renewcommand{\refname}{}
\usepackage[dvipsnames]{xcolor}
\definecolor{mygray}{gray}{0.3}


\begin{document}

%\title{Research Statement}
%
%\author{\name Belhal Karimi \email belhal.karimi@gmail.com \\
%       \addr Cognitive Computing Lab\
%       Baidu Research\\
%       Seattle, WA 98195-4322, USA}
%
%
%
%\maketitle
%
%\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%...\end{abstract}
%
%\begin{keywords}
%...
%\end{keywords}
%
%\section{Introduction}


\textbf{\scalebox{2}{Research Statement}}  \hfill \textbf{\scalebox{1.5}{Belhal Karimi}} 

 \hfill \scalebox{1}{\textcolor{mygray}{\textsc{Baidu Research}}}

\hfill

Throughout my research, I focus on developing \emph{training}, also known as \emph{optimization}, methods for large-scale datasets.
There are several specificities to my work.

The broad panel of my work has application on various problems, datasets and domains.
To name a few, such learning task as stated above is crucial while fitting complex nonlinear models (mixed models, deep neural networks, mixture models) on tabular, image, textual data to tackle problems encountered in computer vision, drug development or natural language processing.

Based on the principled approach that consists in \emph{observing} the world, \emph{designing} a model describing the best those observations and \emph{training} it on the latter, my main focal point in the realm of machine learning resides in the \emph{training}, or \emph{learning}, phase.
With the sheer size of data and the high nonconvexity of the modern models, such as mutlilayer nerual network, used to describe complex human tasks, there is a rising interest and need for scalable, faster learning methods and their rigorous theoretical understanding.

Up to some observations, either fixed or streaming, and a well designed model, the definition of a loss/cost function and its optimization (minimization) are at the heart of this training phase.
Continuously improving those optimization algorithms is key for \emph{machine learning} in order to sustain the rapid growth in dimension, compositionality of the models and the high variety of input observations (sound, image, LIDAR, etc \dots).

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{fig}
%\caption{fhzuz}
%\label{fig:myresearch}
\end{figure}

While my work provides \emph{novel} methods for particularly deep neural networks (DNNs), one special case of the setting above, is when the input-output relationship of a phenomena is not completely characterized by the observations.
A set of latent variables is thus needed and the loss function accepts the latter as a third argument.

\textit{ Illustrative example of latent data model:} 
During clinical trials, the kinetics and dynamics of a drug being tested are modeled using nonlinear functions (or systems of ordinary differential equations) and observations from patients which comprise for instance their gender, height, concentration of the drug after injection.
While those observed covariates are necessary, they are not sufficient to describe well the biological pheonomena. 
A set of latent variables are used to quantify what can not be measured. 
In the special case of pharmacology, those latent variables describe the inter-individual variability among patients of a population (this is what makes us all different other than measurable signals). 
Therefore, the loss function, here the likelihood, is completed by simulations of those random effects and are then used to complete the observations before final optimization.

Thus, part of my research is at \emph{the intersection} of \textbf{sampling} and \textbf{optimization}, bridging the gap between sampling methods such as Markov Chain Monte Carlo (MCMC) or Variational Inference and optimization method such as gradient based learning algorithms or maximum likelihood estimation.

My research has been published in top-tier conferences in machine learning such as NeurIPS, COLT, ICML and made the object of contribution in statistics Journal such as CSDA. I also received a collection of awards from those conferences and a Jacques Hadamard grant for a summer visit the Russian leading group in Bayesian Deep Learning called \emph{BayesGroup}.

Some of my work are now implemented in the commercial modeling and simulation software for drug development \emph{Lixoft} and in its open-source counter part \emph{saemix}.


% ---------------------------------------------------Deep Learning: Training and Generalization-----------------------------------------------------------------------------------------
\vspace{0.2in}
\textbf{\scalebox{1.6}{(a) Deep Learning: Training and Generalization}}
\vspace{0.2in}

A particular interest of mine lies in the practical training and theoretical understanding of deep neural networks, widely used for most learning tasks in the past decade.
Scaling, speeding and improving existing training algorithms is of utmost importance and drive most of my existing publications.
Recently, I have been also interested in the generalization properties of such training algorithms. 
Speeding training and making sure the output parameter estimates lead to models generalizing well on unseen data are the two main challenges I am tackling today.

\vspace{0.15in}
\textbf{Training Acceleration} 
\vspace{0.08in}

Dealing with the speed of convergence of a given training algorithm is a classical problem in modern machine learning.
From a theoretical perspective, we define the convergence of an algorithm when this latter reaches a so-called $\epsilon$-stationary point.
In deep learning, and more generally in stochastic nonconvex optimization, the chosen suboptimality condition is the second order moment of the gradient of the objective function. 
Then, deriving the algorithm convergence rate simply consists in finding the number of iterations until that quantity is bounded by $\epsilon$.
In \citep{karimi2019misso}, we  establish that the classical Stochastic Gradient Descent (SGD) algorithm reaches an $\epsilon$-stationary point in $\mathcal{O}\left(c_{0}+\log (n) / \sqrt{n}\right)$ iterations. 
The results also hold when the stochastic gradient is biased, i.e., its expectation is not equal to the full gradient. This setting has not been studied before our contribution and yet is presented in numerous applications such as the online EM algorithm or the policy-gradient method for average reward maximization in reinforcement learning.

From a practical perspective, we propose a variant of the known AMSGrad algorithm, a popular adaptive gradient method, in order to facilitate its acceleration.
In \citep{kun2020}, we add prior knowledge about the sequence of consecutive mini-batch gradients and leverages its underlying structure making the gradients sequentially predictable. 
By exploiting the predictability and ideas from optimistic online learning, our proposed algorithm accelerate the convergence and increase sample efficiency.

In \citep{karimi2019misso}, we derive a unifying framework for the incremental optimization methods. 
Among others, our framework include stochastic variational inference and MISO.



\vspace{0.15in}
\textbf{Decentralized Training} 
\vspace{0.08in}


Given the need for distributed training procedures, distributed optimization algorithms are at the center of attention. 
With the growth of computing power and the need for using machine learning models on mobile devices, the communication cost of distributed training algorithms needs careful consideration. 
In that regard, more and more attention is shifted from the traditional parameter server training paradigm to the decentralized one, which usually requires lower communication costs.
We develop, in \citep{chen2020decent}, a general algorithmic framework that can convert existing adaptive gradient methods to their decentralized counterparts and thoroughly analyze the convergence behavior of the proposed algorithmic framework showing that if a given adaptive gradient method converges, under some specific conditions, then its decentralized counterpart is also convergent.

Apart from the focus on communication complexity, the privacy of the data stored on the devices on which distributed learning occurs is also critical.
In \citep{had2020}, we derive \textsc{FedSKETCH}, a method based on the compression of the accumulation of local gradients using count sketch.
Due to the lower dimension of sketching used, our method exhibits communication-efficiency property. We also deal with the case where the data is heterogeneous across device, which is commonly faced in federated learning, by developing \textsc{FedSKETCHGATE}.
In particular, we establish a communication complexity of order $\mathcal{O}(\log(d))$ per round, where $d$ is the dimension of the vector of parameters compared to $\mathcal{O}(d)$ complexity per round of baseline mini-batch SGD.

Another focus on the federated learning setting is made in our work \citep{karimi2020lars}, where we develop a local variant of AMSGrad by using layerwise and dimensionwise adaptive learning rates. The main contribution of the paper lies in the embedding of the LARS method in the local AMSGrad method.


\vspace{0.15in}
\textbf{Towards Better Generalizaiton} 
\vspace{0.08in}

The final aspect of my work on training DNNs pertain to improving their generalization performances.
Adaptive gradient methods have been optimizers of choice for deep learning due to their fast training speed, yet, their generalization performance is often worse than that of SGD for over-parameterized neural networks. 
To tackle this flaw, we propose in \citep{zhou2020towards} Stable Adaptive Gradient Descent (\textsc{SAGD}) which leverages differential privacy to boost the generalization performance of adaptive gradient methods.
Empirical runs on image classification or language modeling are backed with theoretical justifications to highlight the improved generalization properties of \textsc{SAGD}.



% ---------------------------------------------------When Sampling meets Optimization-----------------------------------------------------------------------------------------
\vspace{0.2in}
\textbf{\scalebox{1.6}{(b) When Sampling meets Optimization}}
\vspace{0.2in}

Mostly driven by the potential applications and as stated in the beginning of this statement, the models I am considering in my work are comprised of some latent variables.
Indeed either in medical applications, where latent variables may be missing values uninformed by the patients or random effects in the special case of pharmacology, or in computer vision applications, and more specifically generative modeling, where layers of latent variables are used to disentangle a better representation of the input data, being able to \emph{sample/infer} those latent variables is key during the \emph{optimization} phase.
I detail below different contributions where this setting is respected.

\vspace{0.15in}
\textbf{Hierarchical Latent Structure Based Models} 
\vspace{0.08in}


\citep{karimi2019global}
\citep{ren2020vfg}
\citep{karimi2020hwa} 


\lipsum[35]


\vspace{0.15in}
\textbf{Two-level Stochastic Optimization Methods} 
\vspace{0.08in}

\citep{karimi2019convergence}
\citep{karimi2020misso} 
\citep{karimi2020tts}


\lipsum[35]

\vspace{0.15in}
\textbf{MCMC Based Optimization} 
\vspace{0.08in}

We propose in \citep{karimi2018eff} an efficient MCMC procedure, namely \textsc{nlme-IMH}, for posterior sampling in nonlinear mixed effects models, based on the Laplace approximation.
This work was followed by \citep{karimi2018fsaem} where we embed \textsc{nlme-IMH} into a stochastic variant of the EM algorithm (SAEM) for maximum likelihood estimation.

\citep{karimi2020misso} 
\citep{karimi2020anila}







% ---------------------------------------------------Future Research Directions-----------------------------------------------------------------------------------------
\vspace{0.2in}
\textbf{\scalebox{1.6}{Future Research Directions}} 
\vspace{0.2in}

dajndaundadzandzanidazni

\vspace{0.08in}
\paragraph{Energy Based Models.} \lipsum[35]

\vspace{0.08in}
\paragraph{Federated Learning.} \lipsum[35]

\vspace{0.08in}
\paragraph{Bayesian Deep Learning.} \lipsum[35]

\vspace{0.08in}
\paragraph{Stochastic Optimization for DNNs.} \lipsum[35]


\newpage


% ---------------------------------------------------References-----------------------------------------------------------------------------------------

\textbf{\scalebox{1.6}{References}}
\vspace{-0.3in}
\nocite{*}


\bibliography{references}

\end{document}
