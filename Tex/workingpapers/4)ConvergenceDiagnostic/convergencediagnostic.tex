\documentclass[11pt]{article}
%\usepackage{fullpage,graphicx,algorithm,algorithmic,bm,amsmath,amsthm,amssymb,color,hyperref,cite,natbib}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers}{natbib}
\usepackage{natbib,fullpage}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{wrapfig,lipsum}
\usepackage[textwidth=1cm,textsize=footnotesize]{todonotes}

% ready for submission
\usepackage{neurips_2020}

\usepackage[colorlinks=true,
linkcolor=red,
urlcolor=blue,
citecolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{cleveref}

\setlength{\parskip}{.2cm}

\newtheorem{Fact}{Fact}
\newtheorem{Lemma}{Lemma}
\newtheorem{Prop}{Proposition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Conjecture}{Conjecture}
\newtheorem{Property}{Property}
\newtheorem{Observation}{Observation}
%\theorembodyfont{\rmfamily}
\newtheorem{Exa}{Example}
\newtheorem{assumption}{H\!\!}
\newtheorem{assumptionA}{S\!\!}
\newtheorem{assumptionL}{L\!\!}
\newtheorem{Remark}{Remark}
\newtheorem*{Lemma*}{Lemma}
\newtheorem*{Theorem*}{Theorem}
 \makeatletter
\renewenvironment{proof}[1][\proofname]{%
   \par\pushQED{\qed}\normalfont%
   \topsep6\p@\@plus6\p@\relax
   \trivlist\item[\hskip\labelsep\bfseries#1]%
   \ignorespaces
}{%
   \popQED\endtrivlist\@endpefalse
}
\makeatother

%%%%%%%%%%% Stuffs for Tikz %%%%%%%%%%%%%%%%%%
\usepackage{pgfplots}
\usepackage{xargs}
\usepackage{stmaryrd}
\usetikzlibrary{arrows,shapes,calc,tikzmark,backgrounds,matrix,decorations.markings}
\usepgfplotslibrary{fillbetween}

\pgfplotsset{compat=1.3}

\usepackage{relsize}
\tikzset{fontscale/.style = {font=\relsize{#1}}
    }

\definecolor{lavander}{cmyk}{0,0.48,0,0}
\definecolor{violet}{cmyk}{0.79,0.88,0,0}
\definecolor{burntorange}{cmyk}{0,0.52,1,0}

\def\lav{lavander!90}
\def\oran{orange!30}

\definecolor{asuorange}{rgb}{1,0.699,0.0625}
\definecolor{asured}{rgb}{0.598,0,0.199}
\definecolor{asuborder}{rgb}{0.953,0.484,0}
\definecolor{asugrey}{rgb}{0.309,0.332,0.340}
\definecolor{asublue}{rgb}{0,0.555,0.836}
\definecolor{asugold}{rgb}{1,0.777,0.008}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{shortcuts_OPT}

%\renewcommand{\textwidth}{5.5in}

% Here's the definition of Sb, stolen from amstex
    \makeatletter
    \def\multilimits@{\bgroup
  \Let@
  \restore@math@cr
  \default@tag
 \baselineskip\fontdimen10 \scriptfont\tw@
 \advance\baselineskip\fontdimen12 \scriptfont\tw@
 \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
 \lineskiplimit\lineskip
 \vbox\bgroup\ialign\bgroup\hfil$\m@th\scriptstyle{##}$\hfil\crcr}
    \def\Sb{_\multilimits@}
    \def\endSb{\crcr\egroup\egroup\egroup}
\makeatother

\newtheoremstyle{k}         %name
    {\baselineskip}{2\topsep}      %space above and below
    {\rm}                   %Body font
    {0pt}{\bfseries}  %Heading indent and font
    {}                      %after heading
    { }                      %head after space
    {\thmname{#1}\thmnumber{#2}.}

\theoremstyle{k}
\newtheorem{q}{Q}
\parindent=0pt

%\newcommand{\eric}[1]{\todo[color=red!20]{{\bf EM:} #1}}
%\newcommand{\erici}[1]{\todo[color=red!20,inline]{{\bf EM:} #1}}
%\newcommand{\belhal}[1]{\todo[color=green!20]{{\bf BK:} #1}}
%\newcommand{\belhali}[1]{\todo[color=green!20,inline]{{\bf BK:} #1}}
%\newcommand{\toco}[1]{\todo[color=yellow!20]{{\bf To:} #1}}



\makeatletter
\DeclareRobustCommand*\cal{\@fontswitch\relax\mathcal}
\makeatother

\begin{document}
\title{Stochastic Gradient Descent with Momentum Convergence Diagnostic for Nonconvex Optimization}
%\author{}
\date{\today}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Nonconvex case}
We recall the SGD with Momentum update we are analyzing here:
\beq
\theta_{n+1}=\theta_{n}-\gamma_{n+1} \nabla \ell\left(\theta_{n}, \xi_{n+1}\right)+\beta\left(\theta_{n}-\theta_{n-1}\right)
\eeq
where $\ell$ is the nonconvex loss function parametrized by $\theta \in \Theta \subset \rset^p$ and $\xi$ is some random noise. $\beta \in [0,1)$ is the momentum parameter and $\gamma_{n+1}$ the learning stepsize.

We also define $f(\theta) = \EE[\ell(\theta, \xi]$ the expected loss. 
Following Pflug convergence diagnostic test, we construct the following test statistics:
\beq \label{eq:test}
\nabla \ell\left(\theta_{n}, \xi_{n+1}\right)^{\top} \nabla \ell\left(\theta_{n-1}, \xi_{n}\right)
\eeq
and the goal will be to upperbound the expectation of this quantity in order to spot the two different phases through the iterates.

We make the following assumptions before analyzing \eqref{eq:test}.
\begin{assumption}\label{ass:nonconv}
The loss function $\ell(\theta, \xi)$ is nonconvex \wrt the parameter $\theta$.
\end{assumption}
We consider the very general setting where the loss function $\ell(\dot, \xi)$ is $(l, L)$-smooth, see \citep{allen2017natasha, zhou2019lower}
\begin{assumption}\label{ass:smooth}
There exist some constant $l \in \rset$ and $L > 0$ such that for $(\theta, \vartheta) \in \Theta^2$:
\beq
\frac{l}{2} \norm{\theta - \vartheta}^2 \leq \ell(\theta) - \ell(\vartheta) - \nabla \ell(\vartheta)^\top(\theta - \vartheta) \leq \frac{L}{2} \norm{\theta - \vartheta}^2
\eeq
\end{assumption}
Note that if $l = -L$ we recover the conventional $L$-smoothness definition and if $l \geq 0$ (resp. $l >0$) we have convexity (resp. strong convexity).
\begin{assumption} \label{ass:iterate}
There exists $K>1$ such that 
$$
\mathbb{E}\left[\left(\theta_{n}-\theta_{n-1}\right)^{\top}\left(\theta_{n-1}-\theta_{n-2}\right)\right] \geq -K \mathbb{E}\left[\left\|\theta_{n}-\theta_{n-1}\right\|^{2}\right] 
$$
for large enough iteration index $n$.
\end{assumption}
Finally and classically (see \citep{ghadimi2013stochastic}) in nonconvex optimization, we make an assumption on the magnitude of the gradient:
\begin{assumption}\label{ass:bounded}
There exists a constant $G >0$ such that 
$$
\|\nabla \ell(\theta, \xi)\| < G \quad \textrm{for any $\theta$ and $\xi$}
$$
\end{assumption}
We recall an important convergence result for the SGD with Momentum update from \citep{yan2018unified}:
\begin{Theorem}\label{th:ratesum}
\citep{yan2018unified} Under assumptions H~\ref{ass:nonconv}, H~\ref{ass:smooth}, H~\ref{ass:bounded} and the boundedness of the variance of the stochastic gradients, we have
\beq
\begin{split}
\min _{k=0, \ldots, n} \mathrm{E}\left[\left\|\nabla \ell \left(\mathbf{\theta}_{k}\right)\right\|^{2}\right] \leq & \frac{2\left(\ell \left(\mathbf{\theta}_{0}\right)-\ell _{*}\right)(1-\beta)}{n+1} \max \left\{\frac{2 L}{1-\beta}, \frac{\sqrt{n+1}}{C}\right\} \\
& +\frac{C}{\sqrt{n+1}} \frac{L \beta^{2}((1-\beta) s-1)^{2}\left(G^{2}+\sigma^{2}\right)+L \sigma^{2}(1-\beta)^{2}}{(1-\beta)^{3}}
\end{split}
\eeq
\end{Theorem}

We can easily check the following identity:
\beq
\nabla \ell\left(\theta_{n}, \xi_{n+1}\right)^{\top} \nabla \ell\left(\theta_{n-1}, \xi_{n}\right)=\frac{1}{\gamma} \nabla \ell\left(\theta_{n}, \xi_{n+1}\right)^{\top}\left(\theta_{n-1}-\theta_{n}\right)+\frac{\beta}{\gamma} \nabla \ell \left(\theta_{n}, \xi_{n+1}\right)^{\top}\left(\theta_{n-1}-\theta_{n-2}\right)
\eeq
Taking expectations on both sides and using Assumption H~\ref{ass:smooth}, we have:
\beq
\begin{split}
\EE \left[\nabla \ell\left(\theta_{n}, \xi_{n+1}\right)^{\top} \nabla \ell \left(\theta_{n-1}, \xi_{n}\right)\right] \leq & \frac{1}{\gamma} \left[ f(\theta_{n-1}) - f(\theta_{n}) - \frac{l}{2}\norm{\theta_{n-1} - \theta_n}^2 \right]\\
& + \frac{\beta}{\gamma}\left[ f(\theta_{n}) - f(\theta_{n} + \theta_{n-2} - \theta_{n-1}) + \frac{L}{2}\norm{\theta_{n-1} - \theta_{n-2}}^2 \right]
\end{split}
\eeq
which yields:
\beq
\begin{split}
 \EE \left[\nabla \ell\left(\theta_{n}, \xi_{n+1}\right)^{\top} \nabla \ell \left(\theta_{n-1}, \xi_{n}\right)\right] \leq & \frac{1}{\gamma} \left[ f(\theta_{n-1}) - f(\theta^*) - \frac{l}{2} \norm{\theta_{n-1} - \theta_n}^2 \right]\\
& + \frac{\beta}{\gamma}\left[ f(\theta_{n}) - f(\theta^*) + \frac{L}{2}\norm{\theta_{n-1} - \theta_{n-2}}^2 \right]
\end{split}
\eeq
where $\theta^*$ is the global minimizer of the expected loss.

Denote $\Delta_{n} = \theta_n - \theta_{n-1}$ and observe that:
\beq
\left\|\Delta_{n}\right\|^{2}=\gamma^{2}\left\|\nabla \ell \left(\theta_{n-1}, \xi_{n}\right)\right\|^{2}+2 \beta \Delta_{n}^{\top} \Delta_{n-1}-\beta^{2}\left\|\Delta_{n-1}\right\|^{2}
\eeq
Using assumptions H~\ref{ass:iterate} and using Theorem~\ref{th:ratesum} we obtain:
\beq
\EE \left\|\Delta_{n}\right\|^{2} \leq \gamma^{2}G^{2}-(2\beta K + \beta^{2}) \EE \left[ \left\|\Delta_{n-1}\right\|^{2} \right] 
\eeq
\newpage

\bibliographystyle{abbrvnat}
\bibliography{ref}



%-----------------------------------------------------------------------------
%\vspace{0.4cm}

\end{document} 