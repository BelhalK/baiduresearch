\begin{thebibliography}{1}

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{chen2020toward}
Xiangyi Chen, Xiaoyun Li, and Ping Li.
\newblock Toward communication efficient adaptive gradient method.
\newblock In {\em ACM-IMS Foundations of Data Science Conference (FODS)},
  Seattle, WA, 2020.

\bibitem{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock {\em arXiv preprint arXiv:1712.01887}, 2017.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1299--1309, 2018.

\bibitem{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock {\em arXiv preprint arXiv:1904.00962}, 2019.

\end{thebibliography}
