\documentclass[11pt]{article}
%\usepackage{fullpage,graphicx,algorithm,algorithmic,bm,amsmath,amsthm,amssymb,color,hyperref,cite,natbib}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers}{natbib}
\usepackage{natbib,fullpage}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{wrapfig,lipsum}
\usepackage[textwidth=1cm,textsize=footnotesize]{todonotes}

% ready for submission
\usepackage{neurips_2020}

\usepackage[colorlinks=true,
linkcolor=red,
urlcolor=blue,
citecolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{cleveref}

\setlength{\parskip}{.2cm}

\newtheorem{Fact}{Fact}
\newtheorem{Lemma}{Lemma}
\newtheorem{Prop}{Proposition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Conjecture}{Conjecture}
\newtheorem{Property}{Property}
\newtheorem{Observation}{Observation}
%\theorembodyfont{\rmfamily}
\newtheorem{Exa}{Example}
\newtheorem{assumption}{H\!\!}
\newtheorem{assumptionA}{S\!\!}
\newtheorem{assumptionL}{L\!\!}
\newtheorem{Remark}{Remark}
\newtheorem*{Lemma*}{Lemma}
\newtheorem*{Theorem*}{Theorem}
 \makeatletter
\renewenvironment{proof}[1][\proofname]{%
   \par\pushQED{\qed}\normalfont%
   \topsep6\p@\@plus6\p@\relax
   \trivlist\item[\hskip\labelsep\bfseries#1]%
   \ignorespaces
}{%
   \popQED\endtrivlist\@endpefalse
}
\makeatother

%%%%%%%%%%% Stuffs for Tikz %%%%%%%%%%%%%%%%%%
\usepackage{pgfplots}
\usepackage{xargs}
\usepackage{stmaryrd}
\usetikzlibrary{arrows,shapes,calc,tikzmark,backgrounds,matrix,decorations.markings}
\usepgfplotslibrary{fillbetween}

\pgfplotsset{compat=1.3}

\usepackage{relsize}
\tikzset{fontscale/.style = {font=\relsize{#1}}
    }

\definecolor{lavander}{cmyk}{0,0.48,0,0}
\definecolor{violet}{cmyk}{0.79,0.88,0,0}
\definecolor{burntorange}{cmyk}{0,0.52,1,0}

\def\lav{lavander!90}
\def\oran{orange!30}

\definecolor{asuorange}{rgb}{1,0.699,0.0625}
\definecolor{asured}{rgb}{0.598,0,0.199}
\definecolor{asuborder}{rgb}{0.953,0.484,0}
\definecolor{asugrey}{rgb}{0.309,0.332,0.340}
\definecolor{asublue}{rgb}{0,0.555,0.836}
\definecolor{asugold}{rgb}{1,0.777,0.008}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{shortcuts_OPT}

%\renewcommand{\textwidth}{5.5in}

% Here's the definition of Sb, stolen from amstex
    \makeatletter
    \def\multilimits@{\bgroup
  \Let@
  \restore@math@cr
  \default@tag
 \baselineskip\fontdimen10 \scriptfont\tw@
 \advance\baselineskip\fontdimen12 \scriptfont\tw@
 \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
 \lineskiplimit\lineskip
 \vbox\bgroup\ialign\bgroup\hfil$\m@th\scriptstyle{##}$\hfil\crcr}
    \def\Sb{_\multilimits@}
    \def\endSb{\crcr\egroup\egroup\egroup}
\makeatother

\newtheoremstyle{t}         %name
    {\baselineskip}{2\topsep}      %space above and below
    {\rm}                   %Body font
    {0pt}{\bfseries}  %Heading indent and font
    {}                      %after heading
    { }                      %head after space
    {\thmname{#1}\thmnumber{#2}.}

\theoremstyle{t}
\newtheorem{q}{Q}
\parindent=0pt

%\newcommand{\eric}[1]{\todo[color=red!20]{{\bf EM:} #1}}
%\newcommand{\erici}[1]{\todo[color=red!20,inline]{{\bf EM:} #1}}
%\newcommand{\belhal}[1]{\todo[color=green!20]{{\bf BK:} #1}}
%\newcommand{\belhali}[1]{\todo[color=green!20,inline]{{\bf BK:} #1}}
%\newcommand{\toco}[1]{\todo[color=yellow!20]{{\bf To:} #1}}



\makeatletter
\DeclareRobustCommand*\cal{\@fontswitch\relax\mathcal}
\makeatother

\begin{document}
\title{Variance Reduced Federated Learning}
%\author{}
\date{\today}

\maketitle

\begin{abstract}
To be completed
\end{abstract}


\section{SAGA-like Algorithms}

\subsection{Local SGD}
\begin{algorithm}[H]
\algsetup{indent=0.25em}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Local learning rate $\gamma$ and global learning rate $\eta$ and communication period $p$.
\STATE \textbf{Init:} $g^{(k)} = \frac{1}{n} \sum_{i=1}^n g^{(0)}$.
\FOR {$k=0,1,..., K$}
\STATE Draw two independent and distinct indices $i_k$ and $j_k$
\FOR {$\tau=k,...,k+p-1$}
\STATE Compute the following quantity
$$
v_{i_k}^{(\tau)}=v_{i_k}^{(\tau-1)} - \gamma( \nabla f_{i_{k}}\left(x^{(k)}\right)-\nabla f_{i_{k}}\left(\alpha_{i_{(k)}}^{t}\right)+g^{(k)})
$$
\ENDFOR
\STATE $v_{i_k}^{(k)} \leftarrow v_{i_k}^{(k+p-1)}$
\STATE Update the global model
$$
x^{(k+1)}=x^{(k)} - \eta v_{i_k}^{(k)}
$$
\STATE Update $\alpha_{j_{k}}^{(k+1)}=x^{(k)} \text { and } \alpha_{j}^{(k+1)}=\alpha_{j}^{(k)} \text { for } j \neq j_{k}$
\STATE Update $g^{(k+1)}=g^{(k)}-\frac{1}{n}\left(\nabla f_{j_{k}}\left(\alpha_{j_{k}}^{(k)}\right)-\nabla f_{j_{k}}\left(\alpha_{j_{k}}^{(k+1)}\right)\right)$
\ENDFOR
\end{algorithmic}
\caption{SAGA Local SGD}
\label{alg:miso}
        \end{algorithm}


\subsection{FedSVRG with Sketching}
\begin{algorithm}[H]
\caption{\texttt{FedSVRG}: SVRG Federated Learning algorithm with Sketching. }\label{alg:fedsvrgsketch}
\begin{algorithmic}[1]
\STATE{\textbf{Inputs:} $\boldsymbol{x}^{(0)}$ initial common model, communication rounds $R$, the number of local updates $K$, and global and local learning rates $\gamma$ and $\eta$}
%\STATE{Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\STATE{\textbf{for $r=0, \ldots, R-1$ do}}
\STATE{$\qquad$\textbf{parallel for device $j=1,\ldots,n$ do}:}
\STATE{$\qquad \quad$ Computes ${\mathbf{\Phi}}^{(r)}\triangleq  {\texttt{Q}}\left[{\mathbf{S}}^{(r-1)}\right]$  (${\texttt{Q}}$ is any query function)}
\STATE{$\qquad\quad$ Set $\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma{\mathbf{\Phi}}^{(r)}$}
\STATE{$\qquad\quad$ Set $\boldsymbol{x}_j^{(0,r)}=\boldsymbol{x}^{(r)}$ }
\STATE{$\qquad\quad$ Compute full gradient $\nabla{f}_j(\boldsymbol{x}^{(\kappa(r))}) = \frac{1}{n_j} \sum_{i = 1}^{n_j} \nabla{f}_j(\boldsymbol{x}^{(\kappa(r))},\xi_i) $ }
\STATE{$\qquad\quad $\textbf{for} $\ell=0,\ldots,K-1$ \textbf{do}}
\STATE{$\qquad\quad\quad$ Sample an index $i_\ell$ uniformly on $[n_j]$}
\STATE{$\qquad\quad\quad$ $\boldsymbol{x}^{(\ell+1,r)}_{j}=\boldsymbol{x}^{(\ell,r)}_j-\eta~ ( \nabla{f}_j(\boldsymbol{x}_j^{(\ell, r)},\xi_{j,i_{\ell}})- \nabla{f}_j(\boldsymbol{x}^{(\kappa(r))},\xi_{j,i_{\ell}}) + \nabla{f}_j(\boldsymbol{x}^{(\kappa(r))}))$ \label{eq:update-rule-alg}}
\STATE{$\qquad\quad$\textbf{end for}}
\STATE{$\qquad\quad\quad$Device $j$ sends $\mathbf{S}^{(r)}_{j}\triangleq\mathbf{S}_{j}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(K,r)}\right)$ back to the server.}
\STATE{$\qquad$Server \textbf{computes} }
\STATE{$\qquad\qquad {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}\mathbf{S}^{(r)}_{j}$ and \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.}
\vspace{0.1cm}

\STATE{$\qquad$\textbf{end parallel for}}
\STATE{\textbf{end}}
\STATE{\textbf{Output:} ${\boldsymbol{x}}^{(R-1)}$}
\vspace{- 0.1cm}
\end{algorithmic}
\end{algorithm}

\textbf{Particularity:} $\kappa(r)$ is the epoch number at which we compute both control variate terms. 
We can have $\kappa(r) = r$ or something else (to tune).


\subsection{FedSAGA with Sketching}
\begin{algorithm}[H]
\caption{\texttt{FedSAGA}: SAGA Federated Learning algorithm with Sketching. }\label{alg:fedsagasketch}
\begin{algorithmic}[1]
\STATE{\textbf{Inputs:} $\boldsymbol{x}^{(0)}$ initial common model, communication rounds $R$, the number of local updates $K$, and global and local learning rates $\gamma$ and $\eta$}
%\STATE{Server chooses a subset $\mathcal{P}_0$ of $K$ devices at random (device $j$ is chosen with probability $q_j$);
\STATE{\textbf{for $r=0, \ldots, R-1$ do}}
\STATE{$\qquad$\textbf{parallel for device $j=1,\ldots,n$ do}:}
\STATE{$\qquad \quad$ Computes ${\mathbf{\Phi}}^{(r)}\triangleq  {\texttt{Q}}\left[{\mathbf{S}}^{(r-1)}\right]$  (${\texttt{Q}}$ is any query function)}
\STATE{$\qquad\quad$ Set $\boldsymbol{x}^{(r)}=\boldsymbol{x}^{(r-1)}-\gamma{\mathbf{\Phi}}^{(r)}$}
\STATE{$\qquad\quad$ Set $\boldsymbol{x}_j^{(0,r)}=\boldsymbol{x}^{(r)}$ }
\STATE{$\qquad\quad$ Compute full gradient $\nabla{f}_j(\boldsymbol{x}^{(r)}) = \frac{1}{n_j} \sum_{i = 1}^{n_j} \nabla{f}_j(\boldsymbol{x}^{(r)},\xi_i) $ }
\STATE{$\qquad\quad $\textbf{for} $\ell=0,\ldots,K-1$ \textbf{do}}
\STATE{$\qquad\quad\quad$ Sample an indices $(i_\ell, q_\ell)$ independently and uniformly on $[n_j]$}
\STATE{$\qquad\quad\quad$ $\boldsymbol{x}^{(\ell+1,r)}_{j}=\boldsymbol{x}^{(\ell,r)}_j-\eta~ ( \nabla{f}_j(\boldsymbol{x}_j^{(\ell, r)},\xi_{j,i_{\ell}}) -  \nabla{f}_j(\boldsymbol{x}_j^{(t_{i_\ell},r)},\xi_{j,i_{\ell}}) + \overline{F}_j^{(r)})$ \label{eq:update-rule-alg}}
\STATE{$\qquad\quad\quad$ where $ \overline{F}_j^{(r)} = \frac{1}{n_j} \sum_{i = 1}^{n_j} \nabla{f}_j(\boldsymbol{x}^{(r)},\xi_i) + \frac{1}{n_j} ( \nabla{f}_j(\boldsymbol{x}_j^{(\ell, r)},\xi_{j,q_{\ell}}) -  \nabla{f}_j(\boldsymbol{x}_j^{(t_{q_\ell},r)},\xi_{j,q_{\ell}}))$}
\STATE{$\qquad\quad$\textbf{end for}}
\STATE{$\qquad\quad\quad$Device $j$ sends $\mathbf{S}^{(r)}_{j}\triangleq\mathbf{S}_{j}\left(\boldsymbol{x}_j^{(0,r)}-~{\boldsymbol{x}}_{j}^{(K,r)}\right)$ back to the server.}
\STATE{$\qquad$Server \textbf{computes} }
\STATE{$\qquad\qquad {\mathbf{S}}^{(r)}=\frac{1}{p}\sum_{j=1}\mathbf{S}^{(r)}_{j}$ and \textbf{broadcasts} ${\mathbf{S}}^{(r)}$ to all devices.}
\vspace{0.1cm}

\STATE{$\qquad$\textbf{end parallel for}}
\STATE{\textbf{end}}
\STATE{\textbf{Output:} ${\boldsymbol{x}}^{(R-1)}$}
\vspace{- 0.1cm}
\end{algorithmic}
\end{algorithm}

\textbf{Particularity:}  We need to store each local models on each device $j$ in order to compute the control variate terms. No epoch tuning though.
Indeed $ \boldsymbol{x}_j^{(t_{i_\ell},r)}$ is the value of local model on device $j$ when index $i_\ell$ was drawn last.


\section{Numerical Examples}

\newpage

\bibliographystyle{abbrvnat}
\bibliography{ref}



%-----------------------------------------------------------------------------
%\vspace{0.4cm}

\end{document} 