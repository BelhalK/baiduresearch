\documentclass[11pt]{article}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers}{natbib}
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=magenta,urlcolor=blue]{hyperref}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,enumitem,graphicx,subfigure}
\usepackage{xargs}
\usepackage{stmaryrd}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{natbib}

% ready for submission
\usepackage{neurips_2020}
\input{shortcuts.tex}
\newtheorem{definition}{Definition}
\newcommand{\algo}{\textsc{eff-EBM}}


%%% Coloring the comment as blue
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}   


\begin{document}
\title{Memory Efficient EBM Training}

\author{
  Belhal Karimi \\
  Cognitive And Computing Lab\\
  Baidu Research\\
  Beijing, China \\
  \texttt{v_karimibelhal@baidu.com} 
   \And
  Ping Li \\
  Cognitive And Computing Lab\\
  Baidu Research\\
  Beijing, China \\
  \texttt{liping@baidu.com} \\
}

\date{\today}

\maketitle

\begin{abstract}
To be completed...
\end{abstract}

\section{Introduction}\label{sec:introduction}




\section{Related Work}\label{sec:related}

\textbf{Energy Based Modeling} 

\textbf{Distributed Optimization} 

\textbf{Compression and Quantization} 




\section{Distributed and Private EBM Training}



\subsection{Compression Methods for Distributed and Private Optimization}

\begin{definition}[Top-$k$]\label{def:topk}
For $x\in\mathbb R^d$, denote $\mathcal S$ as the size-$k$ set of $i\in[d]$ with largest $k$ magnitude $|x_i|$. The \textbf{Top-$k$} compressor is defined as $\mathcal C(x)_i=x_i$, if $i\in\mathcal S$; $\mathcal C(x)_i=0$ otherwise.
\end{definition}

\begin{definition}[Block-Sign]\label{def:sign}
For $x\in\mathbb R^d$, define $M$ blocks indexed by $\mathcal B_i$, $i=1,...,M$, with $d_i\eqdef |\mathcal B_i|$. The \textbf{Block-Sign} compressor is defined as $\mathcal C(x)=[sign(x_{\mathcal B_1})\frac{\|x_{\mathcal B_1}\|_1}{d_1},..., sign(x_{\mathcal B_M}) \frac{\|x_{\mathcal B_M}\|_1}{d_M}]$. 
\end{definition}






\subsection{Main Algorithm}


%\begin{algorithm}[H]
%\caption{\algo\ } \label{alg:anila}
%\begin{algorithmic}[1]
%%\small
%\STATE \textbf{Input}: Total number of iterations $T$, number of MCMC transitions $K$ and of samples $M$, sequence of global learning rate $\{\eta_t\}_{t >0}$,  sequence of MCMC stepsizes ${\gamma_k}_{k >0}$, initial value $\theta_0$, MCMC initialization $\{ z_{0}^m \}_{m=1}^M$ and observations $\{ x_{i} \}_{i=1}^n$.
%\FOR{$t=1$ to $T$}
%\STATE Draw $M$ samples $\{ z_{t}^m \}_{m=1}^M$ from the objective potential via Langevin diffusion:\label{line:langevin}
%\FOR{$k=1$ to $K$}
%\STATE Use black box compression operators:
%$$
%\tilde{g}_{k-1}^m = \mathcal{C}(\nabla_z f_{\theta_t}(z_{k-1}^m) )
%$$
%\STATE Construct the Markov Chain as follows:
%\beq\label{eq:anila}
%z_{k}^{m} = z_{k-1}^m + \gamma_k/2 \tilde{g}_{k-1}^m+ \sqrt{\gamma_k} \mathsf{B}_k \eqsp,
%\eeq
%where $\mathsf{B}_t$ denotes the Brownian motion (Gaussian noise).
%\ENDFOR
%\STATE Assign $\{ z_{t}^m \}_{m=1}^M \leftarrow \{ z_{K}^m \}_{m=1}^M$.
%\STATE Sample $m$ positive observations $\{ x_{i} \}_{i=1}^m$ from the empirical data distribution.
%\STATE Compute the gradient of the empirical log-EBM:
%\beq\notag
%\begin{split}
%\nabla \log p(\theta_t) 
% = \mathbb{E}_{p_{\text {data }}}\left[\nabla_{\theta} f_{\theta_t}(x)\right]-\mathbb{E}_{p_{\theta}}\left[\nabla_{\theta_t} f_{\theta}(z_t)\right]\approx  \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} f_{\theta_t}\left(x_{i}\right)-\frac{1}{m} \sum_{i=1}^{m} \nabla_{\theta} f_{\theta_t}\left(z_t^m\right)\eqsp.
%\end{split}
%\eeq
%\STATE Update the vector of global parameters of the EBM:\label{line:gradient}
%\beq\notag
%\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t) \eqsp.
%\eeq
%\ENDFOR
%\STATE \textbf{Output:} Vector of fitted parameters $\theta_{T+1}$.
%\end{algorithmic}
%\end{algorithm}





\begin{algorithm}[H]
\DontPrintSemicolon
  
  \KwInput{Total number of iterations $T$, number of MCMC transitions $K$ and of samples $M$, sequence of global learning rate $\{\eta_t\}_{t >0}$,  sequence of MCMC stepsizes ${\gamma_k}_{k >0}$, initial value $\theta_0$, MCMC initialization $\{ z_{0}^m \}_{m=1}^M$. Set of selected devices $\mathcal{D}^t$.}
  \KwOutput{Vector of fitted parameters $\theta_{T+1}$.}
  \KwData{ $\{ x^p_{i} \}_{i=1}^{n_p}$, $n_p$ number of observations on device $p$. $n = \sum_{p=1}^P n_p$ total.}
\hrulefill

\For{$t=1$ to $T$}
{	
	\tcc{Happening on distributed devices}
	
	\For{For device $p \in \mathcal{D}^t$} 
	{
		{Draw $M$ negative samples $\{ z_{K}^{p,m} \}_{m=1}^M$} \tcp*{local langevin diffusion}
			\For{$k=1$ to $K$}
			{
			\beq\notag
			z_{k}^{p,m} = z_{k-1}^{p,m} + \gamma_k/2\nabla_z f_{\theta_t}( z_{k-1}^{p,m})  ^{p,m}+ \sqrt{\gamma_k} \mathsf{B}_k^p \eqsp,
			\eeq
			where $\mathsf{B}_k^p$ denotes the Brownian motion (Gaussian noise).
			}
		{Assign $\{ z_{t}^{p,m} \}_{m=1}^M \leftarrow \{ z_{K}^{p,m} \}_{m=1}^M$.}
		
		{Sample $M$ positive observations $\{ x^p_{i} \}_{i=1}^M$ from the empirical data distribution.}
		
		{Compute the gradient of the empirical log-EBM} \tcp*{local - and + gradients}
		{
		$$\delta^p = \frac{1}{M} \sum_{i=1}^{M} \nabla_{\theta} f_{\theta_t}\left(x^p_{i}\right)- \frac{1}{M} 	\sum_{m=1}^{M} \nabla_{\theta} f_{\theta_t}\left(z_K^{p,m}\right)$$
		}
		{Use black box compression operators}
		{
		$$\Delta^p = \mathcal{C}(\delta^p )$$
		}
		{Devices broadcast $\Delta^p$ to Server} 
	}
	
	  \tcc{Happening on the central server}
	  
	{Aggregation of devices gradients: $\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.$}
%	{
%	\beq\notag
%	\begin{split}
%	\nabla \log p(\theta_t) \approx  \frac{1}{|\mathcal{D}^t|} \sum_{p=1}^{|\mathcal{D}^t|} \Delta^p\eqsp.
%	\end{split}
%	\eeq
%	}

	{Update the vector of global parameters of the EBM: $\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t)$}
%	{
%	\beq\notag
%	\theta_{t+1} = \theta_{t} + \eta_t \nabla \log p(\theta_t) \eqsp.
%	\eeq
%	}
 }
 { \textbf{Output:} Vector of fitted parameters $\theta_{T+1}$} 
\caption{Distributed and private EBM}
\end{algorithm}



\section{Convergence Guarantees}\label{sec:theory}

Recall that the goal of this paper is to train an energy-based model where the data is distributed on $P$ devices.
Formally, given a stream of input data noted $x \in \rset^d$ such that $x = \{ x^p_{i} \}_{i=1}^{n_p}$, $n_p$ number of observations on device $p$. $n = \sum_{p=1}^P n_p$ total., the model reads:
\beq\label{eq:distebm}
p(x,\theta) = \prod_{p = 1}^P \frac{1}{Z^p_{\theta}} \mathrm{exp}(-U^p_{\theta}(x)) \eqsp,
\eeq
where $\theta \in \Theta \subset \rset^d$ denotes the global parameters vector of our model and $Z(\theta) = \prod_{p=1}^P Z^p_{\theta} \eqdef \prod_{p=1}^P \int_{x} \mathrm{exp}(-U^p_{\theta}(x)) \textrm{d}x$ is the normalizing constant with respect to $x$.
$U^p_{\theta}(x)$ denotes the energy function for device $p$ is parameterized by $\theta$ and takes as input an image $x$.




We now establish a non-asymptotic convergence result for the set of parameters $\{ \theta_{t}\}_{t=1}^T$.

Beforehand, we provide mild assumptions on our model 

\begin{assumption} (Smoothness of the energy function)
\end{assumption}

\begin{assumption} (Bounded MC noise)
\end{assumption}

\begin{assumption} (Geometric ergodicity of CD-1)
\end{assumption}

\section{Numerical Experiments}\label{sec:numericals}


\section{Conclusion}\label{sec:conclusion}



\newpage
\bibliographystyle{plain}
\bibliography{ref}

\newpage
\appendix 

\section{Appendix}\label{sec:appendix}


%-----------------------------------------------------------------------------

\end{document} 