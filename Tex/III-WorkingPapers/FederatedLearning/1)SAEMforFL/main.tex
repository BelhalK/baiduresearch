\documentclass[11pt]{article}
%\usepackage{fullpage,graphicx,algorithm,algorithmic,bm,amsmath,amsthm,amssymb,color,hyperref,cite,natbib}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers}{natbib}
\usepackage{natbib,fullpage}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{wrapfig,lipsum}
\usepackage[textwidth=1cm,textsize=footnotesize]{todonotes}

% ready for submission
\usepackage{neurips_2020}

\usepackage[colorlinks=true,
linkcolor=red,
urlcolor=blue,
citecolor=blue]{hyperref}
\usepackage{hyperref}
\usepackage{cleveref}

\setlength{\parskip}{.2cm}

\newtheorem{Fact}{Fact}
\newtheorem{Lemma}{Lemma}
\newtheorem{Prop}{Proposition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Conjecture}{Conjecture}
\newtheorem{Property}{Property}
\newtheorem{Observation}{Observation}
%\theorembodyfont{\rmfamily}
\newtheorem{Exa}{Example}
\newtheorem{assumption}{H\!\!}
\newtheorem{assumptionA}{S\!\!}
\newtheorem{assumptionL}{L\!\!}
\newtheorem{Remark}{Remark}
\newtheorem*{Lemma*}{Lemma}
\newtheorem*{Theorem*}{Theorem}
 \makeatletter
\renewenvironment{proof}[1][\proofname]{%
   \par\pushQED{\qed}\normalfont%
   \topsep6\p@\@plus6\p@\relax
   \trivlist\item[\hskip\labelsep\bfseries#1]%
   \ignorespaces
}{%
   \popQED\endtrivlist\@endpefalse
}
\makeatother

%%%%%%%%%%% Stuffs for Tikz %%%%%%%%%%%%%%%%%%
\usepackage{pgfplots}
\usepackage{xargs}
\usepackage{stmaryrd}
\usetikzlibrary{arrows,shapes,calc,tikzmark,backgrounds,matrix,decorations.markings}
\usepgfplotslibrary{fillbetween}

\pgfplotsset{compat=1.3}

\usepackage{relsize}
\tikzset{fontscale/.style = {font=\relsize{#1}}
    }

\definecolor{lavander}{cmyk}{0,0.48,0,0}
\definecolor{violet}{cmyk}{0.79,0.88,0,0}
\definecolor{burntorange}{cmyk}{0,0.52,1,0}

\def\lav{lavander!90}
\def\oran{orange!30}

\definecolor{asuorange}{rgb}{1,0.699,0.0625}
\definecolor{asured}{rgb}{0.598,0,0.199}
\definecolor{asuborder}{rgb}{0.953,0.484,0}
\definecolor{asugrey}{rgb}{0.309,0.332,0.340}
\definecolor{asublue}{rgb}{0,0.555,0.836}
\definecolor{asugold}{rgb}{1,0.777,0.008}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{shortcuts_OPT}

%\renewcommand{\textwidth}{5.5in}

% Here's the definition of Sb, stolen from amstex
    \makeatletter
    \def\multilimits@{\bgroup
  \Let@
  \restore@math@cr
  \default@tag
 \baselineskip\fontdimen10 \scriptfont\tw@
 \advance\baselineskip\fontdimen12 \scriptfont\tw@
 \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
 \lineskiplimit\lineskip
 \vbox\bgroup\ialign\bgroup\hfil$\m@th\scriptstyle{##}$\hfil\crcr}
    \def\Sb{_\multilimits@}
    \def\endSb{\crcr\egroup\egroup\egroup}
\makeatother

\newtheoremstyle{t}         %name
    {\baselineskip}{2\topsep}      %space above and below
    {\rm}                   %Body font
    {0pt}{\bfseries}  %Heading indent and font
    {}                      %after heading
    { }                      %head after space
    {\thmname{#1}\thmnumber{#2}.}

\theoremstyle{t}
\newtheorem{q}{Q}
\parindent=0pt

%\newcommand{\eric}[1]{\todo[color=red!20]{{\bf EM:} #1}}
%\newcommand{\erici}[1]{\todo[color=red!20,inline]{{\bf EM:} #1}}
%\newcommand{\belhal}[1]{\todo[color=green!20]{{\bf BK:} #1}}
%\newcommand{\belhali}[1]{\todo[color=green!20,inline]{{\bf BK:} #1}}
%\newcommand{\toco}[1]{\todo[color=yellow!20]{{\bf To:} #1}}



\makeatletter
\DeclareRobustCommand*\cal{\@fontswitch\relax\mathcal}
\makeatother

\begin{document}
\title{Distributed and Private Stochastic EM Methods\\
via Quantized and Compressed MCMC}
%\author{}
\date{\today}

\maketitle

\begin{abstract}
To be completed
\end{abstract}


\section{Notations}

We minimize the negated log incomplete data likelihood 
\begin{align} \label{eq:em_motivate}
\begin{split} 
 \min_{ \theta \in \Theta }~ \overline{L} ( \theta ) \eqdef  L ( \theta ) + r (\theta) \quad \text{with}~~L(\theta) = \frac{1}{n} \sum_{i=1}^n L_i( \theta) \eqdef  \frac{1}{n} \sum_{i=1}^n \big\{ - \log g( y_i ; \theta ) \big\}\eqs,
\end{split} 
\end{align}


Consider a curved exponential family
\beq \label{eq:exp}
f(z_i,y_i; \theta) = h  (z_i,y_i) \textrm{exp} ( \pscal{S(z_i,y_i)}{\phi(\theta)} - \psi(\theta) )\eqs,
\eeq

Then EM reads

\begin{align}\label{eq:definition-overline-bss}
\overline{s}_i(\theta) \eqdef \int_{\Zset} S(z_i,y_i) p(z_i|y_i;\theta) \mu(\rmd z_i) \eqsp,
\end{align}
%\begin{align}\label{eq:definition-overline-bss}
%\begin{split} 
%& \overline{s}(\theta)= \frac{1}{n} \sum_{i=1}^n \overline{s}_i(\theta) \\
%& \text{where}  \quad \overline{s}_i(\theta)= \int_{\Zset} S(z_i,y_i) p(z_i|y_i;\theta) \mu(\rmd z_i) \eqsp,
%\end{split} 
%\end{align}
and the \textit{M-step} is given by
\begin{align}\label{eq:mstep}
\overline{\theta}( \overline{s}(\theta) ) \eqdef \argmin_{ \vartheta \in \theta } ~\big\{ \Pen( \vartheta ) + \psi( \vartheta) - \pscal{ \overline{s}(\theta)}{ \phi ( \vartheta) } \big\} \eqsp.
\end{align}


In the case where the expectations are intractable, then \eqref{eq:definition-overline-bss} becomes:

\beq\label{eq:stats}
\begin{split}
 \tilde{S}^{(k+1)} \eqdef \frac{1}{n} \sum_{i=1}^n \tilde{S}^{(k+1)}_i = \frac{1}{n} \sum_{i=1}^n\frac{1}{M_k} \sum_{m=1}^{M_k} S(z_{i,m}^{(k)}, y_i) \eqs,
\end{split}
\eeq



%\subsection{Periodic averaging of the local models}
%
%\begin{algorithm}[H]
%\caption{FL-SAEM with parameter averaging} \label{alg:flsaem}
%\begin{algorithmic}[1]
%%\small
%\STATE \textbf{Input}: .
%\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
%\FOR{$r=1$ to $R$}
%\FOR{parallel for device $i \in D^{r}$}
%\STATE Set $\hat{\theta}^{(0,k)}_i = \hat{\theta}^{(k)}$.
%\FOR{$t=1$ to $T$}
%\STATE Draw M samples $\{z_{i,m}^{(t,k)}\}_{m=1}^{M}$ under model $\hat{\theta}^{(t,k)}_i$
%\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(t,k+1)}$
%\STATE Update local model:
%$$
%\hat{\theta}^{(t,k+1)}_i = \overline{\theta}( \tilde{S}_i^{(t,k+1)}) 
%$$
%\ENDFOR
%\STATE Devices send $\hat{\theta}^{(T,k+1)}_i$ to server.
%\ENDFOR
%\STATE Server computes \textbf{the average of the local models}:
%$$
%\hat{\theta}^{(k+1)} = \frac{1}{n} \sum_{i=1}^n \hat{\theta}^{(T,k+1)}_i
%$$ 
%and send global model back to the devices. \label{line:final}
%\ENDFOR
%\end{algorithmic}
%\end{algorithm}
%
%
%\subsection{Periodic averaging of the local statistics}
%
%\begin{algorithm}[H]
%\caption{FL-SAEM with statistics averaging} \label{alg:flsaem2}
%\begin{algorithmic}[1]
%%\small
%\STATE \textbf{Input}: .
%\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
%\FOR{$r=1$ to $R$}
%\FOR{parallel for device $i \in D^{r}$}
%\STATE Set $\hat{\theta}^{(0,k)}_i = \hat{\theta}^{(k)}$.
%\FOR{$t=1$ to $T$}
%\STATE \textcolor{red}{ Here one local iteration, $T=1$}
%\STATE Draw M samples $z_{i,m}^{(k)}$ under model $\hat{\theta}^{(t,k)}_i$
%\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(t,k+1)}$
%\ENDFOR
%\STATE Devices send local statistics $\tilde{S}_{i}^{(t,k+1)}$ to server.
%\ENDFOR
%\STATE Server computes \textbf{global model using the aggregated statistics}:
%$$
%\hat{\theta}^{(k+1)} = \overline{\theta}( \tilde{S}^{(t,k+1)}) 
%$$
%where $\tilde{S}^{(t,k+1)} = (\tilde{S}_i^{(t,k+1)}, i \in D_r)$  and send global model back to the devices. 
%\ENDFOR
%\end{algorithmic}
%\end{algorithm}


\clearpage
\section{Algorithms}

For computational purposes and privacy enhanced matter, I have chosen to study and develop the second algorithms that I proposed in my last week's report.
In that algorithm, one does not compute a periodic averaging of the local models (this would requires performing as many M-steps as there are workers).
Rather, workers compute local statistics and send them to the central server for a periodic averaging of those vectors and the latter computes one M-step to update the global model.

\begin{algorithm}[H]
\caption{FL-SAEM with Periodic Statistics Averaging} \label{alg:flsaem2}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: \textcolor{red}{TO COMPLETE}
\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
\FOR{$r=1$ to $R$}
\FOR{parallel for device $i \in D^{r}$}
\STATE Set $\hat{\theta}^{(0,r)}_i = \hat{\theta}^{(r)}$.
\STATE Draw M samples $z_{i,m}^{(r)}$ under model $\hat{\theta}^{(r)}_i$ \label{line:sampling}
\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(r+1)}$ \label{line:compute}
\STATE Workers send local statistics $\tilde{S}_{i}^{(k+1)}$ to server.
\ENDFOR
\STATE Server computes \textbf{global model using the aggregated statistics}:
$$
\hat{\theta}^{(r+1)} = \overline{\theta}( \tilde{S}^{(r+1)}) 
$$
where $\tilde{S}^{(r+1)} = (\tilde{S}_i^{(r+1)}, i \in D_r)$  and send global model back to the devices. 
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Challenges with Algorithm~\ref{alg:flsaem2}}
While Algorithm~\ref{alg:flsaem2} is a distributed variant of the SAEM, it is neither (a) \emph private nor (b) \emph communication-efficient.

\textbf{Privacy:} Indeed, we remark that broadcasting the vector of statistics are a potential breach to the data observations as their expression is related $y$ and the latent data $z$. With a simple knowledge of the model used, the data could be retrieved if one extracts those statistics.

\textbf{Communication bottlenecks:} Also regarding (b), the broadcast of $n$ vector of statistics $S(y_i,z_i)$ can be cumbersome when the size of the latent space and the parameter space of the model are huge.

\subsection{Algorithmic solutions}

\textbf{Line~\ref{line:sampling} -- Quantization:} 
The first step is to quantize the gradient in the Stochastic Langevin Dynamics step used in our sampling scheme Line~\ref{line:sampling} of Algorithm~\ref{alg:flsaem2}.
Inspired by \citep{alistarh2017qsgd}, we use an extension of the QSGD algorithm for our latent samples.
Define the quantization operator as follows:

\beq\label{eq:operator}
\mathsf{C}_{j}^{(\ell)}\left(g, \xi_{j}\right)=\|v\| \cdot \textrm{sign}\left(g_{j}\right) \cdot\left(\left\lfloor \ell \left|g_{j}\right| /\|v\|\right\rfloor+\mathbf{1}\left\{\xi_{j} \leq \ell \left|g_{j}\right| /\|v\|-\left\lfloor \ell \left|g_{j}\right| /\|v\|\right\rfloor\right\}\right) /\ell
\eeq
where $\ell$ is the level of quantization and $j \in [d]$ denotes the dimension of the gradient.

Hence, for the sampling step, Line~\ref{line:sampling}, we use the modified SGLD below, to be compliant with the privacy of our method.
\begin{algorithm}[H]
\caption{Langevin Dynamics with Quantization for worker $i$} \label{alg:quant}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: Current local model $\hat{\theta}^{(r)}_i$ for worker $i \in \inter$.

\STATE Draw $M$ samples $\{ z_{i}^{(r,m} \}_{m=1}^M$ from the posterior distribution $p(z_i| y_i; \hat{\theta}^{(k)}_i)$ via Langevin diffusion with a quantized gradient:\label{line:langevin}
\FOR{$k=1$ to $K$}
\STATE Compute the quantized gradient of $\nabla \log p(z_i| y_i; \hat{\theta}^{(k)}_i)$:
\beq\label{eq:grad}
g_i{(k,m)} = \mathsf{C}_{j}^{(\ell)}\left(\nabla_j f_{\theta_t}(z_i^{(k-1,m)}), \xi^{(k)}_{j}\right)
\eeq
where $\xi^{(k)}_{j}$ is a realization of a uniform random variable.
\STATE Sample the latent data using the following chain:
\beq\label{eq:lang}
z_i^{(k,m)} = z_i^{(k-1,m)} + \frac{\gamma_k}{2}  g_i{(k,m)} + \sqrt{\gamma_k}  \mathsf{B}_k \eqsp,
\eeq
where $\mathsf{B}_t$ denotes the Brownian motion and $m \in [M]$ denotes the MC sample.
\ENDFOR
\STATE Assign $\{ z_{i}^{(r,m} \}_{m=1}^M \leftarrow \{ z_i^{(K,m)} \}_{m=1}^M$.
\STATE \textbf{Output:} latent data $z_{i,m}^{(k)}$ under model $\hat{\theta}^{(t,k)}_i$ 
\end{algorithmic}
\end{algorithm}



\noindent \textbf{Line~\ref{line:compute} -- Compression MCMC output:}
We use the notorious \textbf{Top-$k$} operator that we define as $\mathcal C(x)_i=x_i$, if $i\in \mathcal S$; $\mathcal C(x)_i=0$ otherwise and where $\mathcal S$ is defined as the size-$k$ set of $i\in[p]$.
Recall that after Line~\ref{line:sampling} we compute the local statistics $\tilde{S}_{i}^{(k+1)}$ using the output latent variables from Algorithm~\ref{alg:quant}.
We now use those statistics and compress them using Algorithm~\ref{alg:spars} as follows:

\begin{algorithm}[H]
\caption{Sparsified Statistics with \textbf{Top-$k$}} \label{alg:spars}
\begin{algorithmic}[1]
%\small
\STATE \textbf{Input}: Current local statistics $\tilde{S}_{i}^{(k+1)}$ for worker $i \in \inter$. Sparsification level $k$.
\STATE Apply \textbf{Top-$k$}:
\beq\label{eq:topkstats}
\ddot{S}_{i}^{(k+1)} = \mathcal C \left( \tilde{S}_{i}^{(k+1)}\right)
\eeq
\STATE \textbf{Output:} Compressed local statistics for worker $i$ denoted $\ddot{S}_{i}^{(k+1)}$.
\end{algorithmic}
\end{algorithm}


%
%Final method:
%
%\begin{algorithm}[H]
%\caption{FL-SAEM with Periodic Compressed and Quantized Statistics Averaging} \label{alg:flsaem}
%\begin{algorithmic}[1]
%%\small
%\STATE \textbf{Input}: .
%\STATE Init: $\theta_{0} \in \Theta \subseteq \mathbb R^d $, as the global model and $\bar{\theta}_0 =  \frac{1}{n} \sum_{i=1}^n \theta_0$.
%\FOR{$r=1$ to $R$}
%\FOR{parallel for device $i \in D^{r}$}
%\STATE Set $\hat{\theta}^{(0,k)}_i = \hat{\theta}^{(k)}$.
%\FOR{$t=1$ to $T$}
%\STATE Draw M samples $z_{i,m}^{(k)}$ under model $\hat{\theta}^{(t,k)}_i$
%\STATE Compute the surrogate sufficient statistics $\tilde{S}_{i}^{(t,k+1)}$
%\ENDFOR
%\STATE Devices send local statistics $\tilde{S}_{i}^{(t,k+1)}$ to server.
%\ENDFOR
%\STATE Server computes \textbf{global model using the aggregated statistics}:
%$$
%\hat{\theta}^{(k+1)} = \overline{\theta}( \tilde{S}^{(t,k+1)}) 
%$$
%where $\tilde{S}^{(t,k+1)} = (\tilde{S}_i^{(t,k+1)}, i \in D_r)$  and send global model back to the devices. 
%\ENDFOR
%\end{algorithmic}
%\end{algorithm}






\clearpage


\section{Numerical Experiments}


\newpage

\bibliographystyle{abbrvnat}
\bibliography{ref}



%-----------------------------------------------------------------------------
%\vspace{0.4cm}

\end{document} 
