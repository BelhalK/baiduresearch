\begin{thebibliography}{10}

\bibitem{chen2020decent}
Xiangyi Chen, \textbf{B. Karimi}, Weijie Zhao, and Ping Li.
\newblock Convergent adaptive gradient methods in decentralized optimization.
\newblock {\em Submitted}, 2020.

\bibitem{had2020}
Farzin Haddadpour, \textbf{B. Karimi}, Ping Li, and Xiaoyun Li.
\newblock {FedSKETCH}: Communication-efficient federated learning via
  sketching.
\newblock {\em Submitted}, 2020.

\bibitem{ren2020vfg}
Shaogang Ren, Yang Zhao, \textbf{B. Karimi}, and Ping Li.
\newblock {VFG}: Variational flow graphical model with hierarchical latent
  structure.
\newblock {\em Submitted}, 2020.

\bibitem{karimi2018eff}
\textbf{B. Karimi} and Marc Lavielle.
\newblock {Efficient Metropolis-Hastings sampling for nonlinear mixed effects
  models}.
\newblock {\em {Proceedings of BAYSM 2018}}, 2018.

\bibitem{karimi2017non}
\textbf{B. Karimi}, Marc Lavielle, and {\'E}ric Moulines.
\newblock Bridging the gap between independent metropolis hastings and
  variational inference.
\newblock In {\em Implicit Models Workshop (ICML)}, 2017.

\bibitem{karimi2019convergence}
\textbf{B. Karimi}, Marc Lavielle, and {\'E}ric Moulines.
\newblock On the convergence properties of the mini-batch em and mcem
  algorithms.
\newblock {\em HAL preprint hal: 02334485}, 2019.

\bibitem{karimi2018fsaem}
\textbf{B. Karimi}, Marc Lavielle, and Eric Moulines.
\newblock {f-SAEM}: A fast stochastic approximation of the {EM} algorithm for
  nonlinear mixed effects models.
\newblock {\em Computational Statistics and Data Analysis, (CSDA)}, 2020.

\bibitem{karimi2020hwa}
\textbf{B. Karimi} and Ping Li.
\newblock {HWA}: Hyperparameters weight averaging bayesian neural networks.
\newblock {\em Submitted}, 2020.

\bibitem{karimi2020tts}
\textbf{B. Karimi} and Ping Li.
\newblock Two timescale stochastic em algorithms.
\newblock {\em Submitted}, 2020.

\bibitem{karimi2020lars}
\textbf{B. Karimi}, Xiaoyun Li, and Ping Li.
\newblock Layerwise and dimensionwise adaptive local ams method for federated
  learning.
\newblock {\em Work in progress}, 2020.

\bibitem{karimi2019non}
\textbf{B. Karimi}, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai.
\newblock Non-asymptotic analysis of biased stochastic approximation scheme.
\newblock In {\em Proceedings of the Thirty-Second Conference on Learning
  Theory (COLT) 2019}. PMLR, 2019.

\bibitem{karimi2019misso}
\textbf{B. Karimi}, Hoi-To Wai, and Eric Moulines.
\newblock A doubly stochastic surrogate optimization scheme for non-convex
  finite-sum problems.
\newblock {\em Adv. in Approx. Bayes. Inference (AABI)}, 2019.

\bibitem{karimi2019global}
\textbf{B. Karimi}, Hoi-To Wai, Eric Moulines, and Marc Lavielle.
\newblock On the global convergence of (fast) incremental expectation
  maximization methods.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 2837--2847, 2019.

\bibitem{karimi2020misso}
\textbf{B. Karimi}, Hoi-To Wai, Eric Moulines, and Ping Li.
\newblock {MISSO}: Minimization by incremental stochastic surrogate
  optimization for large scale nonconvex problems.
\newblock {\em Submitted}, 2020.

\bibitem{karimi2020anila}
\textbf{B. Karimi}, Jianwen Xie, and Ping Li.
\newblock Anila: Anisotropic langevin dynamics for training energy-based
  models.
\newblock {\em Work in progress}, 2020.

\bibitem{kun2020}
Jun-Kun Wang, Xiaoyun Li, \textbf{B. Karimi}, and Ping Li.
\newblock An optimistic acceleration of amsgrad for nonconvex optimization.
\newblock {\em Submitted}, 2020.

\bibitem{zhou2020towards}
Yingxue Zhou, \textbf{B. Karimi}, Jinxing Yu, Zhiqiang Xu, and Ping Li.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 1--10, 2020.

\end{thebibliography}
