\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{neal2012bayesian}
\citation{guo2017calibration,kendall2017uncertainties}
\citation{blundell2015weight,kingma2015variational}
\citation{neal2012bayesian}
\citation{graves2011practical,hoffman2013stochastic}
\citation{blundell2015weight}
\citation{polyak1992acceleration}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{neal2011mcmc}
\citation{hastings1970monte}
\citation{ma2015complete}
\citation{graves2011practical}
\citation{blundell2015weight}
\citation{kingma2015variational,blundell2015weight,molchanov2017variational}
\citation{louizos2017multiplicative}
\citation{wu2018deterministic}
\citation{gal2016dropout}
\citation{polyak1990sa}
\citation{ruppert1988efficient}
\citation{zhou2017convergence}
\citation{izmailov2018averaging}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hyperparameters Averaging in Bayesian Neural Networks}{2}{section.3}}
\newlabel{sec:main}{{3}{2}{Hyperparameters Averaging in Bayesian Neural Networks}{section.3}{}}
\citation{blei2017variational}
\citation{bottou2008tradeoffs}
\citation{kucukelbir2017automatic}
\citation{izmailov2018averaging}
\citation{garipov2018loss}
\citation{he2019asymmetric}
\citation{keskar2016large}
\citation{kirkpatrick2017overcoming,blundell2015weight}
\citation{maddox2019simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Neural Networks and ELBO Maximization}{3}{subsection.3.1}}
\newlabel{eq:vi}{{1}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.3.1}{}}
\newlabel{eq:VI}{{2}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.3.2}{}}
\newlabel{eq:variationalobjective}{{3}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Averaging model snapshots through hyperparameters loss landscapes}{3}{subsection.3.2}}
\citation{zhou2017convergence}
\citation{schmidt2017minimizing}
\citation{defazio2014saga}
\citation{mairal2015incremental}
\citation{mairal2015incremental}
\newlabel{eq:hwa_updates}{{4}{4}{Averaging model snapshots through hyperparameters loss landscapes}{equation.3.4}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{line:svi}{{4}{4}{Averaging model snapshots through hyperparameters loss landscapes}{ALC@unique.4}{}}
\newlabel{line:svisigma}{{5}{4}{Averaging model snapshots through hyperparameters loss landscapes}{ALC@unique.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces HWA: Hyperparameters Weight Averaging\relax }}{4}{algorithm.1}}
\newlabel{alg:hwa}{{1}{4}{HWA: Hyperparameters Weight Averaging\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Comparison with other classical averaging procedures in nonconvex optimization}{4}{subsection.3.3}}
\citation{maddox2019simple}
\citation{gal2016dropout}
\citation{gal2017concrete}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Variational Inference with HWA}{5}{subsection.3.4}}
\newlabel{eq:lowrankcov}{{5}{5}{Variational Inference with HWA}{equation.3.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Variational Inference with HWA for BNNs\relax }}{5}{algorithm.2}}
\newlabel{alg:trainingbnn}{{2}{5}{Variational Inference with HWA for BNNs\relax }{algorithm.2}{}}
\citation{blundell2015weight}
\citation{welling2011bayesian}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{krizhevsky2009learning}
\citation{simonyan2014very}
\citation{wen2018flipout}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{6}{section.4}}
\newlabel{sec:numerical}{{4}{6}{Numerical Experiments}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison for Bayesian LeNet CNN architecture on MNIST dataset (top) and Bayesian VGG architecture on CIFAR-10 dataset (bottom). The plots are averaged over 5 repetitions.\relax }}{7}{figure.caption.1}}
\newlabel{fig:all}{{1}{7}{Comparison for Bayesian LeNet CNN architecture on MNIST dataset (top) and Bayesian VGG architecture on CIFAR-10 dataset (bottom). The plots are averaged over 5 repetitions.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{7}{section.5}}
\newlabel{sec:conclusion}{{5}{7}{Conclusion}{section.5}{}}
\bibdata{ref}
\bibcite{blei2017variational}{{1}{2017}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{blundell2015weight}{{2}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{bottou2008tradeoffs}{{3}{2008}{{Bottou \& Bousquet}}{{Bottou and Bousquet}}}
\bibcite{defazio2014saga}{{4}{2014}{{Defazio et~al.}}{{Defazio, Bach, and Lacoste-Julien}}}
\bibcite{gal2016dropout}{{5}{2016}{{Gal \& Ghahramani}}{{Gal and Ghahramani}}}
\bibcite{gal2017concrete}{{6}{2017}{{Gal et~al.}}{{Gal, Hron, and Kendall}}}
\bibcite{garipov2018loss}{{7}{2018}{{Garipov et~al.}}{{Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson}}}
\bibcite{graves2011practical}{{8}{2011}{{Graves}}{{}}}
\bibcite{guo2017calibration}{{9}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{hastings1970monte}{{10}{1970}{{Hastings}}{{}}}
\bibcite{he2019asymmetric}{{11}{2019}{{He et~al.}}{{He, Huang, and Yuan}}}
\bibcite{hoffman2013stochastic}{{12}{2013}{{Hoffman et~al.}}{{Hoffman, Blei, Wang, and Paisley}}}
\bibcite{izmailov2018averaging}{{13}{2018}{{Izmailov et~al.}}{{Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson}}}
\bibcite{kendall2017uncertainties}{{14}{2017}{{Kendall \& Gal}}{{Kendall and Gal}}}
\bibcite{keskar2016large}{{15}{2016}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2015variational}{{16}{2015}{{Kingma et~al.}}{{Kingma, Salimans, and Welling}}}
\bibcite{kirkpatrick2017overcoming}{{17}{2017}{{Kirkpatrick et~al.}}{{Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.}}}
\bibcite{krizhevsky2009learning}{{18}{2009}{{Krizhevsky et~al.}}{{Krizhevsky, Hinton, et~al.}}}
\bibcite{kucukelbir2017automatic}{{19}{2017}{{Kucukelbir et~al.}}{{Kucukelbir, Tran, Ranganath, Gelman, and Blei}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{20}{2010}{{LeCun \& Cortes}}{{LeCun and Cortes}}}
\bibcite{louizos2017multiplicative}{{21}{2017}{{Louizos \& Welling}}{{Louizos and Welling}}}
\bibcite{ma2015complete}{{22}{2015}{{Ma et~al.}}{{Ma, Chen, and Fox}}}
\bibcite{maddox2019simple}{{23}{2019}{{Maddox et~al.}}{{Maddox, Izmailov, Garipov, Vetrov, and Wilson}}}
\bibcite{mairal2015incremental}{{24}{2015}{{Mairal}}{{}}}
\bibcite{molchanov2017variational}{{25}{2017}{{Molchanov et~al.}}{{Molchanov, Ashukha, and Vetrov}}}
\bibcite{neal2012bayesian}{{26}{2012}{{Neal}}{{}}}
\bibcite{neal2011mcmc}{{27}{2011}{{Neal et~al.}}{{}}}
\bibcite{polyak1990sa}{{28}{1990}{{Polyak}}{{}}}
\bibcite{polyak1992acceleration}{{29}{1992}{{Polyak \& Juditsky}}{{Polyak and Juditsky}}}
\bibcite{ruppert1988efficient}{{30}{1988}{{Ruppert}}{{}}}
\bibcite{schmidt2017minimizing}{{31}{2017}{{Schmidt et~al.}}{{Schmidt, Le~Roux, and Bach}}}
\bibcite{simonyan2014very}{{32}{2014}{{Simonyan \& Zisserman}}{{Simonyan and Zisserman}}}
\bibcite{welling2011bayesian}{{33}{2011}{{Welling \& Teh}}{{Welling and Teh}}}
\bibcite{wen2018flipout}{{34}{2018}{{Wen et~al.}}{{Wen, Vicol, Ba, Tran, and Grosse}}}
\bibcite{wu2018deterministic}{{35}{2018}{{Wu et~al.}}{{Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and Gaunt}}}
\bibcite{zhou2017convergence}{{36}{2017}{{Zhou \& Cong}}{{Zhou and Cong}}}
\bibstyle{iclr2020_conference}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{10}{appendix.A}}
