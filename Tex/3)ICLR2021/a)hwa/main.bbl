\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1505.05424}, 2015.

\bibitem[Bottou \& Bousquet(2008)Bottou and Bousquet]{bottou2008tradeoffs}
L{\'e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  161--168, 2008.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pp.\
  1050--1059, 2016.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry~P Vetrov, and
  Andrew~G Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8789--8798, 2018.

\bibitem[Graves(2011)]{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2348--2356, 2011.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock \emph{arXiv preprint arXiv:1706.04599}, 2017.

\bibitem[Hastings(1970)]{hastings1970monte}
W~Keith Hastings.
\newblock Monte carlo sampling methods using markov chains and their
  applications.
\newblock 1970.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{he2019asymmetric}
Haowei He, Gao Huang, and Yang Yuan.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2553--2564, 2019.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
Matthew~D Hoffman, David~M Blei, Chong Wang, and John Paisley.
\newblock Stochastic variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Kendall \& Gal(2017)Kendall and Gal]{kendall2017uncertainties}
Alex Kendall and Yarin Gal.
\newblock What uncertainties do we need in bayesian deep learning for computer
  vision?
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5574--5584, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Durk~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2575--2583, 2015.

\bibitem[Kucukelbir et~al.(2017)Kucukelbir, Tran, Ranganath, Gelman, and
  Blei]{kucukelbir2017automatic}
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David~M Blei.
\newblock Automatic differentiation variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 430--474, 2017.

\bibitem[Louizos \& Welling(2017)Louizos and
  Welling]{louizos2017multiplicative}
Christos Louizos and Max Welling.
\newblock Multiplicative normalizing flows for variational bayesian neural
  networks.
\newblock \emph{arXiv preprint arXiv:1703.01961}, 2017.

\bibitem[Ma et~al.(2015)Ma, Chen, and Fox]{ma2015complete}
Yi-An Ma, Tianqi Chen, and Emily Fox.
\newblock A complete recipe for stochastic gradient mcmc.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2917--2925, 2015.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock \emph{arXiv preprint arXiv:1701.05369}, 2017.

\bibitem[Neal(2012)]{neal2012bayesian}
Radford~M Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Polyak(1990)]{polyak1990sa}
Boris~T Polyak.
\newblock A new method of stochastic approximation type.
\newblock \emph{Avtomat. i Telemekh}, \penalty0 (7):\penalty0 98:107, 1990.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM journal on control and optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Ruppert(1988)]{ruppert1988efficient}
David Ruppert.
\newblock Efficient estimations from a slowly convergent robbins-monro process.
\newblock Technical report, Cornell University Operations Research and
  Industrial Engineering, 1988.

\bibitem[Wu et~al.(2018)Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and
  Gaunt]{wu2018deterministic}
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard~E Turner, Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato, and Alexander~L Gaunt.
\newblock Deterministic variational inference for robust bayesian neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.03958}, 2018.

\bibitem[Zhou \& Cong(2017)Zhou and Cong]{zhou2017convergence}
Fan Zhou and Guojing Cong.
\newblock On the convergence properties of a $ k $-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1708.01012}, 2017.

\end{thebibliography}
