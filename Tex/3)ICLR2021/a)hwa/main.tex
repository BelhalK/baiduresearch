\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage{natbib,fullpage}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{wrapfig,lipsum}
\usepackage{xargs}
\usepackage{hyperref}
\usepackage{url}

\input{shortcuts_OPT.sty}

\title{HWA: Averaging Hyperparameters in Bayesian Neural Networks Leads to Better Generalization.}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Belhal Karimi \& Ping Li \\
Cognitive Computing Lab\\
Baidu Research\\
Beijing and Seattle \\
\texttt{\{v_karimibelhal, liping\}@baidu.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Bayesian Deep Learning presents itself as the most useful tool for adding uncertainty estimation to traditional Deep Learning models that only produce point estimates predictions as outputs.
Confidence of the model and the predictions at inference time are left alone.
Applying randomness and Bayes Rule to the weights of a deep neural network is a step towards achieving this goal.
Current state of the art optimization method for training a Bayesian Neural Network are relatively slow and inefficient, compared to their deterministic counterparts.
In this paper, we propose HWA (Hyperparameters Weight Averaging) algorithm that leverages the averaging procedure of Polyak and Ruppert in order to train faster and achieve a better accuracy.
We develop our main algorithm using the simple averaging heuristic and demonstrate its effectiveness on the space of the hyperparameters of the neural networks random weights.
Numerical applications confirm the empirical benefits of our method.
\end{abstract}

\section{Introduction}
While Deep Learning methods have shown increasing efficiency in various domains such as natural language processing, computer vision or robotics, sensible areas including autonomous driving or medical imaging not only require accurate predictions but also uncertainty quantification.
In \citep{neal2012bayesian}, authors develop a bayesian variant of plain feedforward multilayer neural networks in which weights and biases are considered as random variables.
For supervised learning tasks, deterministic models are prone to overfitting and are not capable of estimating uncertainty in the training data resulting in making overly confident decisions about the correct class, \textit{i.e.} miscalibration \citep{guo2017calibration,kendall2017uncertainties}.
Nevertheless, representing that aforementioned uncertainty is crucial for decision making.

Bayesian methods display a hierarchical probabilistic model that assume a (prior) random distribution over the parameters of the parameters and are useful for assessing the uncertainty of the model via posterior predictive distribution quantification \citep{blundell2015weight,kingma2015variational}.
Current training methods for Bayesian Neural Networks (BNN) \citep{neal2012bayesian} include Variational Inference \citep{graves2011practical, hoffman2013stochastic} or BayesByBackprop \citep{blundell2015weight} based on Evidence Lower Bound (ELBO) maximization task.
Naturally, Bayesian methods, and in particular BNNs, are thus highly sensitive to the parameters choice of the prior distribution and current state-of-the-art models are not as efficient and robust as traditional deep learning models.

In this paper, we introduce a new \emph{optimization} algorithm to alleviate those challenges.
Our main contributions read as follows:
\begin{itemize}
\item We introduce Hyperparameter Weight Averaging (HWA), a training algorithm that leverages stochastic averaging techniques \citep{polyak1992acceleration} and posterior sampling methods.
\item Given the high nonconvexity of the loss landscape, our method finds heuristic explanation from theoretical works on averaging and generalization such as \citep{keskar2016large,he2019asymmetric} and more practical work on Deep Neural Networks (DNN) optimization such as \citep{izmailov2018averaging}.

\item \color{red}{ Plots to show how HWA adapt to the curvature and goes in a better testing accuracy (but worst training loss). Plots with hyperparameters landscape and HWA trajectory on PCA subspace.}

\item We provide numerical examples showcasing the effectiveness of our method on simple and complex supervised classification tasks.
\end{itemize}

The remaining of the paper is organized as follows.
 
\section{Related Work}
\textbf{Stochastic Averaging:}

\textbf{Variational Inference:}

\textbf{Posterior Prediction:}

\section{Hyperparameters Averaging in Bayesian Neural Networks}

\begin{algorithm}[H]
\algsetup{indent=0.25em}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Trained hyperparameters $\hat{\mu}_{\ell}$ and $\hat{\sigma}$. LR bounds $\gamma_1$ and $\gamma_2$. Cycle length $c$.
\STATE Initialize the hyperparameters of the weights and 
$\mu_{\ell} = \hat{\mu}_{\ell}$ and $\mu^{HWA}_{\ell} = \mu_{\ell}$.
\FOR {$k=0,1,...$}
\STATE $\gamma \leftarrow \gamma(k)$ (Cyclical LR for the iteration)
\STATE $\mu_{\ell}^{k+1} \leftarrow \mu_{\ell}^{k} - \gamma \nabla \mathcal{L}(\mu_{\ell}^{k})$ (regular SVI update)
\IF{$\textrm{mod}(k,c) = 0$}
	\STATE $n_{\textrm{models}} \leftarrow k/c$ (Number of models to average)
		\STATE $\mu_{\ell}^{HWA} \leftarrow \frac{n_{\textrm{models}}\mu_{\ell}^{HWA} + \mu_{\ell}^{k+1}}{n_{\textrm{models}}+1}$
		\STATE $\mu_{\ell}^{HWA} \leftarrow \frac{n_{\textrm{models}}\mu_{\ell}^{HWA} + \mu_{\ell}^{k+1}}{n_{\textrm{models}}+1}$
\ENDIF
\ENDFOR
\end{algorithmic}
\caption{HWA: Hyperparameters Weight Averaging}
\label{alg:miso}
        \end{algorithm}



\section{Numerical Experiments}
\section{Conclusion}

\newpage

\bibliography{ref}
\bibliographystyle{iclr2020_conference}

\newpage
\appendix
\section{Appendix}
You may include other additional sections here. 

\end{document}
