\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage{natbib,fullpage}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{wrapfig,lipsum}
\usepackage{xargs}
\usepackage{hyperref}
\usepackage{url}

\input{shortcuts_OPT.sty}

\title{HWA: Averaging Hyperparameters in Bayesian Neural Networks Leads to Better Generalization.}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Belhal Karimi \& Ping Li \\
Cognitive Computing Lab\\
Baidu Research\\
Beijing and Seattle \\
\texttt{\{v_karimibelhal, liping\}@baidu.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
T.B.C
\end{abstract}

\section{Introduction}


Our main contributions read as follows:
\begin{itemize}
\item ff 
\item ff 
\end{itemize}

The remaining of the paper is organized as follows.
 
\section{Related Work}
\textbf{Stochastic Averaging:}

\textbf{Variational Inference:}

\textbf{Posterior Prediction:}

\section{Hyperparameters Averaging in Bayesian Neural Networks}

\begin{algorithm}[H]
\algsetup{indent=0.25em}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Trained hyperparameters $\hat{\mu}_{\ell}$ and $\hat{\sigma}$. LR bounds $\gamma_1$ and $\gamma_2$. Cycle length $c$.
\STATE Initialize the hyperparameters of the weights and 
$\mu_{\ell} = \hat{\mu}_{\ell}$ and $\mu^{HWA}_{\ell} = \mu_{\ell}$.
\FOR {$k=0,1,...$}
\STATE $\gamma \leftarrow \gamma(k)$ (Cyclical LR for the iteration)
\STATE $\mu_{\ell}^{k+1} \leftarrow \mu_{\ell}^{k} - \gamma \nabla \mathcal{L}(\mu_{\ell}^{k})$ (regular SVI update)
\IF{$\textrm{mod}(k,c) = 0$}
	\STATE $n_{\textrm{models}} \leftarrow k/c$ (Number of models to average)
		\STATE $\mu_{\ell}^{HWA} \leftarrow \frac{n_{\textrm{models}}\mu_{\ell}^{HWA} + \mu_{\ell}^{k+1}}{n_{\textrm{models}}+1}$
		\STATE $\mu_{\ell}^{HWA} \leftarrow \frac{n_{\textrm{models}}\mu_{\ell}^{HWA} + \mu_{\ell}^{k+1}}{n_{\textrm{models}}+1}$
\ENDIF
\ENDFOR
\end{algorithmic}
\caption{HWA: Hyperparameters Weight Averaging}
\label{alg:miso}
        \end{algorithm}



\section{Numerical Experiments}
\section{Conclusion}

\newpage

\bibliography{ref}
\bibliographystyle{iclr2020_conference}

\newpage
\appendix
\section{Appendix}
You may include other additional sections here. 

\end{document}
