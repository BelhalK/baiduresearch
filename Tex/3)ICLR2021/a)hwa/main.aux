\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{neal2012bayesian}
\citation{guo2017calibration,kendall2017uncertainties}
\citation{blundell2015weight,kingma2015variational}
\citation{neal2012bayesian}
\citation{graves2011practical,hoffman2013stochastic}
\citation{blundell2015weight}
\citation{polyak1992acceleration}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{neal2011mcmc}
\citation{hastings1970monte}
\citation{ma2015complete}
\citation{graves2011practical}
\citation{blundell2015weight}
\citation{kingma2015variational,blundell2015weight,molchanov2017variational}
\citation{louizos2017multiplicative}
\citation{wu2018deterministic}
\citation{gal2016dropout}
\citation{polyak1990sa}
\citation{ruppert1988efficient}
\citation{zhou2017convergence}
\citation{izmailov2018averaging}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\citation{blei2017variational}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hyperparameters Averaging in Bayesian Neural Networks}{2}{section.3}}
\newlabel{sec:main}{{3}{2}{Hyperparameters Averaging in Bayesian Neural Networks}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Neural Networks and ELBO Maximization}{2}{subsection.3.1}}
\newlabel{eq:vi}{{1}{2}{Bayesian Neural Networks and ELBO Maximization}{equation.3.1}{}}
\citation{bottou2008tradeoffs}
\citation{kucukelbir2017automatic}
\citation{izmailov2018averaging}
\citation{garipov2018loss}
\citation{he2019asymmetric}
\citation{keskar2016large}
\citation{kirkpatrick2017overcoming,blundell2015weight}
\citation{maddox2019simple}
\newlabel{eq:VI}{{2}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.3.2}{}}
\newlabel{eq:variationalobjective}{{3}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Averaging iterates through hyperparmeter loss landscapes snapshots.}{3}{subsection.3.2}}
\citation{blundell2015weight}
\citation{welling2011bayesian}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{krizhevsky2009learning}
\citation{simonyan2014very}
\citation{wen2018flipout}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{line:svi}{{6}{4}{Averaging iterates through hyperparmeter loss landscapes snapshots}{ALC@unique.6}{}}
\newlabel{line:svisigma}{{7}{4}{Averaging iterates through hyperparmeter loss landscapes snapshots}{ALC@unique.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces HWA: Hyperparameters Weight Averaging\relax }}{4}{algorithm.1}}
\newlabel{alg:hwa}{{1}{4}{HWA: Hyperparameters Weight Averaging\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Through the Lens of Loss Landscapes}{4}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{4}{section.4}}
\newlabel{sec:numerical}{{4}{4}{Numerical Experiments}{section.4}{}}
\citation{lecun1998gradient}
\citation{krizhevsky2009learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training Bayesian LeNet-5 for MNIST\nobreakspace  {}\citep  {lecun1998gradient} classification}{5}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison for Bayesian LeNet CNN architecture on MNIST dataset.\relax }}{5}{figure.caption.1}}
\newlabel{fig:mnist}{{1}{5}{Comparison for Bayesian LeNet CNN architecture on MNIST dataset.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training Bayesian VGG for CIFAR10\nobreakspace  {}\citep  {krizhevsky2009learning} classification}{6}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison for Bayesian VGG CNN architecture on CIFAR-10 dataset.\relax }}{6}{figure.caption.2}}
\newlabel{fig:cifar}{{2}{6}{Comparison for Bayesian VGG CNN architecture on CIFAR-10 dataset.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}{section.5}}
\newlabel{sec:conclusion}{{5}{6}{Conclusion}{section.5}{}}
\bibdata{ref}
\bibcite{blei2017variational}{{1}{2017}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{blundell2015weight}{{2}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{bottou2008tradeoffs}{{3}{2008}{{Bottou \& Bousquet}}{{Bottou and Bousquet}}}
\bibcite{gal2016dropout}{{4}{2016}{{Gal \& Ghahramani}}{{Gal and Ghahramani}}}
\bibcite{garipov2018loss}{{5}{2018}{{Garipov et~al.}}{{Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson}}}
\bibcite{graves2011practical}{{6}{2011}{{Graves}}{{}}}
\bibcite{guo2017calibration}{{7}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{hastings1970monte}{{8}{1970}{{Hastings}}{{}}}
\bibcite{he2019asymmetric}{{9}{2019}{{He et~al.}}{{He, Huang, and Yuan}}}
\bibcite{hoffman2013stochastic}{{10}{2013}{{Hoffman et~al.}}{{Hoffman, Blei, Wang, and Paisley}}}
\bibcite{izmailov2018averaging}{{11}{2018}{{Izmailov et~al.}}{{Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson}}}
\bibcite{kendall2017uncertainties}{{12}{2017}{{Kendall \& Gal}}{{Kendall and Gal}}}
\bibcite{keskar2016large}{{13}{2016}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2015variational}{{14}{2015}{{Kingma et~al.}}{{Kingma, Salimans, and Welling}}}
\bibcite{kirkpatrick2017overcoming}{{15}{2017}{{Kirkpatrick et~al.}}{{Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.}}}
\bibcite{krizhevsky2009learning}{{16}{2009}{{Krizhevsky et~al.}}{{Krizhevsky, Hinton, et~al.}}}
\bibcite{kucukelbir2017automatic}{{17}{2017}{{Kucukelbir et~al.}}{{Kucukelbir, Tran, Ranganath, Gelman, and Blei}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{18}{2010}{{LeCun \& Cortes}}{{LeCun and Cortes}}}
\bibcite{lecun1998gradient}{{19}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, Haffner, et~al.}}}
\bibcite{louizos2017multiplicative}{{20}{2017}{{Louizos \& Welling}}{{Louizos and Welling}}}
\bibcite{ma2015complete}{{21}{2015}{{Ma et~al.}}{{Ma, Chen, and Fox}}}
\bibcite{maddox2019simple}{{22}{2019}{{Maddox et~al.}}{{Maddox, Izmailov, Garipov, Vetrov, and Wilson}}}
\bibcite{molchanov2017variational}{{23}{2017}{{Molchanov et~al.}}{{Molchanov, Ashukha, and Vetrov}}}
\bibcite{neal2012bayesian}{{24}{2012}{{Neal}}{{}}}
\bibcite{neal2011mcmc}{{25}{2011}{{Neal et~al.}}{{}}}
\bibcite{polyak1990sa}{{26}{1990}{{Polyak}}{{}}}
\bibcite{polyak1992acceleration}{{27}{1992}{{Polyak \& Juditsky}}{{Polyak and Juditsky}}}
\bibcite{ruppert1988efficient}{{28}{1988}{{Ruppert}}{{}}}
\bibcite{simonyan2014very}{{29}{2014}{{Simonyan \& Zisserman}}{{Simonyan and Zisserman}}}
\bibcite{welling2011bayesian}{{30}{2011}{{Welling \& Teh}}{{Welling and Teh}}}
\bibcite{wen2018flipout}{{31}{2018}{{Wen et~al.}}{{Wen, Vicol, Ba, Tran, and Grosse}}}
\bibcite{wu2018deterministic}{{32}{2018}{{Wu et~al.}}{{Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and Gaunt}}}
\bibcite{zhou2017convergence}{{33}{2017}{{Zhou \& Cong}}{{Zhou and Cong}}}
\bibstyle{iclr2020_conference}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{9}{appendix.A}}
