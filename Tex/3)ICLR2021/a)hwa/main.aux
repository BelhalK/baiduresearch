\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{neal2012bayesian}
\citation{guo2017calibration,kendall2017uncertainties}
\citation{blundell2015weight,kingma2015variational}
\citation{neal2012bayesian}
\citation{graves2011practical,hoffman2013stochastic}
\citation{blundell2015weight}
\citation{polyak1992acceleration}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{neal2011mcmc}
\citation{hastings1970monte}
\citation{ma2015complete}
\citation{graves2011practical}
\citation{blundell2015weight}
\citation{kingma2015variational,blundell2015weight,molchanov2017variational}
\citation{louizos2017multiplicative}
\citation{wu2018deterministic}
\citation{gal2016dropout}
\citation{polyak1990sa}
\citation{ruppert1988efficient}
\citation{zhou2017convergence}
\citation{izmailov2018averaging}
\citation{keskar2016large,he2019asymmetric}
\citation{izmailov2018averaging}
\citation{blei2017variational}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hyperparameters Averaging in Bayesian Neural Networks}{2}{section.3}}
\newlabel{sec:main}{{3}{2}{Hyperparameters Averaging in Bayesian Neural Networks}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bayesian Neural Networks and ELBO Maximization}{2}{subsection.3.1}}
\newlabel{eq:vi}{{1}{2}{Bayesian Neural Networks and ELBO Maximization}{equation.3.1}{}}
\citation{bottou2008tradeoffs}
\citation{kucukelbir2017automatic}
\citation{izmailov2018averaging}
\citation{garipov2018loss}
\citation{he2019asymmetric}
\citation{keskar2016large}
\newlabel{eq:VI}{{2}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.3.2}{}}
\newlabel{eq:variationalobjective}{{3}{3}{Bayesian Neural Networks and ELBO Maximization}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Averaging iterates through hyperparmeter loss landscapes snapshots.}{3}{subsection.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces HWA: Hyperparameters Weight Averaging}}{3}{algorithm.1}}
\newlabel{alg:miso}{{1}{3}{Averaging iterates through hyperparmeter loss landscapes snapshots}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Through the Lens of Loss Landscapes}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Experiments}{3}{section.4}}
\newlabel{sec:numerical}{{4}{3}{Numerical Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{4}{section.5}}
\newlabel{sec:conclusion}{{5}{4}{Conclusion}{section.5}{}}
\bibdata{ref}
\bibcite{blei2017variational}{{1}{2017}{{Blei et~al.}}{{Blei, Kucukelbir, and McAuliffe}}}
\bibcite{blundell2015weight}{{2}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{bottou2008tradeoffs}{{3}{2008}{{Bottou \& Bousquet}}{{Bottou and Bousquet}}}
\bibcite{gal2016dropout}{{4}{2016}{{Gal \& Ghahramani}}{{Gal and Ghahramani}}}
\bibcite{garipov2018loss}{{5}{2018}{{Garipov et~al.}}{{Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson}}}
\bibcite{graves2011practical}{{6}{2011}{{Graves}}{{}}}
\bibcite{guo2017calibration}{{7}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{hastings1970monte}{{8}{1970}{{Hastings}}{{}}}
\bibcite{he2019asymmetric}{{9}{2019}{{He et~al.}}{{He, Huang, and Yuan}}}
\bibcite{hoffman2013stochastic}{{10}{2013}{{Hoffman et~al.}}{{Hoffman, Blei, Wang, and Paisley}}}
\bibcite{izmailov2018averaging}{{11}{2018}{{Izmailov et~al.}}{{Izmailov, Podoprikhin, Garipov, Vetrov, and Wilson}}}
\bibcite{kendall2017uncertainties}{{12}{2017}{{Kendall \& Gal}}{{Kendall and Gal}}}
\bibcite{keskar2016large}{{13}{2016}{{Keskar et~al.}}{{Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang}}}
\bibcite{kingma2015variational}{{14}{2015}{{Kingma et~al.}}{{Kingma, Salimans, and Welling}}}
\bibcite{kucukelbir2017automatic}{{15}{2017}{{Kucukelbir et~al.}}{{Kucukelbir, Tran, Ranganath, Gelman, and Blei}}}
\bibcite{louizos2017multiplicative}{{16}{2017}{{Louizos \& Welling}}{{Louizos and Welling}}}
\bibcite{ma2015complete}{{17}{2015}{{Ma et~al.}}{{Ma, Chen, and Fox}}}
\bibcite{molchanov2017variational}{{18}{2017}{{Molchanov et~al.}}{{Molchanov, Ashukha, and Vetrov}}}
\bibcite{neal2012bayesian}{{19}{2012}{{Neal}}{{}}}
\bibcite{neal2011mcmc}{{20}{2011}{{Neal et~al.}}{{}}}
\bibcite{polyak1990sa}{{21}{1990}{{Polyak}}{{}}}
\bibcite{polyak1992acceleration}{{22}{1992}{{Polyak \& Juditsky}}{{Polyak and Juditsky}}}
\bibcite{ruppert1988efficient}{{23}{1988}{{Ruppert}}{{}}}
\bibcite{wu2018deterministic}{{24}{2018}{{Wu et~al.}}{{Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and Gaunt}}}
\bibcite{zhou2017convergence}{{25}{2017}{{Zhou \& Cong}}{{Zhou and Cong}}}
\bibstyle{iclr2020_conference}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{7}{appendix.A}}
