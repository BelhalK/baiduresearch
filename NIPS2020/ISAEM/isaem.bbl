\begin{thebibliography}{4}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pages
  1646--1654, 2014.

\bibitem[Delyon et~al.(1999)Delyon, Lavielle, and Moulines]{delyon1999}
B.~Delyon, M.~Lavielle, and E.~Moulines.
\newblock Convergence of a stochastic approximation version of the em
  algorithm.
\newblock \emph{Ann. Statist.}, 27\penalty0 (1):\penalty0 94--128, 03 1999.
\newblock \doi{10.1214/aos/1018031103}.
\newblock URL \url{https://doi.org/10.1214/aos/1018031103}.

\bibitem[Kuhn et~al.(2019)Kuhn, Matias, and Rebafka]{kuhn2019properties}
E.~Kuhn, C.~Matias, and T.~Rebafka.
\newblock Properties of the stochastic approximation em algorithm with
  mini-batch sampling.
\newblock \emph{arXiv preprint arXiv:1907.09164}, 2019.

\bibitem[Reddi et~al.(2016)Reddi, Sra, P{\'o}czos, and Smola]{reddi2016fast}
S.~J. Reddi, S.~Sra, B.~P{\'o}czos, and A.~Smola.
\newblock Fast incremental method for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1603.06159}, 2016.

\end{thebibliography}
