\documentclass[11pt]{article}
%\usepackage{fullpage,graphicx,algorithm,algorithmic,bm,amsmath,amsthm,amssymb,color,hyperref,cite,natbib}

% if you need to pass options to natbib, use, e.g.:
%\PassOptionsToPackage{numbers}{natbib}
\usepackage{natbib,fullpage}
\usepackage{bm,amsmath,amsthm,amssymb,multicol,algorithmic,algorithm,enumitem}
\usepackage{color,wrapfig}
\usepackage[textwidth=1cm,textsize=footnotesize]{todonotes}
% ready for submission
\usepackage{neurips_2020}

\usepackage{lipsum}
\usepackage[colorlinks=true,
linkcolor=red,
urlcolor=blue,
citecolor=blue]{hyperref}

\usepackage{xargs}
\usepackage{stmaryrd}

\setlength{\parskip}{.2cm}

\newtheorem{Fact}{Fact}
\newtheorem{Lemma}{Lemma}
\newtheorem{Prop}{Proposition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Conjecture}{Conjecture}
\newtheorem{Property}{Property}
\newtheorem{Observation}{Observation}
%\theorembodyfont{\rmfamily}
\newtheorem{Exa}{Example}
\newtheorem{assumption}{H\!\!}
\newtheorem{assumptionA}{G\!\!}
\newtheorem{Remark}{Remark}
\newtheorem*{Lemma*}{Lemma}
\newtheorem*{Theorem*}{Theorem}
 \makeatletter
\renewenvironment{proof}[1][\proofname]{%
   \par\pushQED{\qed}\normalfont%
   \topsep6\p@\@plus6\p@\relax
   \trivlist\item[\hskip\labelsep\bfseries#1]%
   \ignorespaces
}{%
   \popQED\endtrivlist\@endpefalse
}
\makeatother
\usepackage{shortcuts_OPT}

%\renewcommand{\textwidth}{5.5in}

% Here's the definition of Sb, stolen from amstex
    \makeatletter
    \def\multilimits@{\bgroup
  \Let@
  \restore@math@cr
  \default@tag
 \baselineskip\fontdimen10 \scriptfont\tw@
 \advance\baselineskip\fontdimen12 \scriptfont\tw@
 \lineskip\thr@@\fontdimen8 \scriptfont\thr@@
 \lineskiplimit\lineskip
 \vbox\bgroup\ialign\bgroup\hfil$\m@th\scriptstyle{##}$\hfil\crcr}
    \def\Sb{_\multilimits@}
    \def\endSb{\crcr\egroup\egroup\egroup}
\makeatother

\newtheoremstyle{t}         %name
    {\baselineskip}{2\topsep}      %space above and below
    {\rm}                   %Body font
    {0pt}{\bfseries}  %Heading indent and font
    {}                      %after heading
    { }                      %head after space
    {\thmname{#1}\thmnumber{#2}.}

\theoremstyle{t}
\newtheorem{q}{Q}
\parindent=0pt

\makeatletter
\DeclareRobustCommand*\cal{\@fontswitch\relax\mathcal}
\makeatother

\newcommand{\tcr}[1]{\textcolor{red}{#1}}
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}

\begin{document}
\title{Fast Bi-Level and Incremental Noisy EM Algorithms}
\author{
  Belhal Karimi \\
  Cognitive And Computing Lab\\
  Baidu Research\\
  Beijing, China \\
  \texttt{belhal.karimi@baidu.com} 
   \And
  Ping Li \\
  Cognitive And Computing Lab\\
  Baidu Research\\
  Beijing, China \\
  \texttt{liping@baidu.com} }
\date{\today}

\maketitle

\begin{abstract}
T.B.C
\end{abstract}


\section{Introduction}
We formulate the following empirical risk minimization as:
\beq \label{eq:em_motivate}
\min_{ \param \in \Param }~ \overline{\calL} ( \param ) \eqdef \Pen (\param) + \calL ( \param )~~\text{with}~~\calL ( \param ) = \frac{1}{n} \sum_{i=1}^n \calL_i( \param) \eqdef  \frac{1}{n} \sum_{i=1}^n \big\{ - \log g( y_i ; \param ) \big\}\eqs,
\eeq
where $\{y_i\}_{i=1}^n$ are the observations, $\Param$ is a convex subset of $\rset^d$ for the parameters,  $\Pen : \Param \rightarrow \rset$ is a smooth convex regularization function   and for each $\param \in \Param$, $g(y;\param)$ is the (incomplete) likelihood of each individual  observation.
The objective function $ \overline{\calL} ( \param )$ is possibly \emph{non-convex} and is assumed to be lower bounded $ \overline{\calL} ( \param ) > - \infty$ for all $\param \in \Param$.
%We assume that $ \overline{\calL} ( \param ) > - \infty$ for all $\param \in \Param$.
In the latent variable model,  $g(y_i ; \param)$, is the marginal of the
complete data likelihood defined as $f(z_i,y_i; \param)$, i.e. $g(y_i; \param) = \int_{\Zset} f (z_i,y_i;\param) \mu(\rmd z_i)$, where $\{ z_i \}_{i=1}^n$ are the (unobserved)
latent variables.   
We make the assumption of a complete model belonging to the curved exponential family, \ie
\beq \label{eq:exp}
f(z_i,y_i; \param) = h  (z_i,y_i) \exp \big( \pscal{S(z_i,y_i)}{\phi(\param)} - \psi(\param) \big)\eqs,
\eeq
where $\psi(\param)$, $h(z_i,y_i)$ are scalar functions, $\phi(\param) \in \rset^k$ is a vector function, and $S(z_i,y_i) \in \rset^k$ is the complete data sufficient statistics.

\paragraph{Prior Work} 
Cite Kuhn \citep{kuhn2019properties} (for ISAEM) and incremental EM like papers.
As well as Optim papers (Variance reduction, SAGA etc.)

\section{Expectation Maximization Algorithm}
Full batch EM is a two steps procedure. The {\sf E-step} amounts to computing the conditional expectation of the complete data sufficient statistics, 
\begin{equation}
\label{eq:definition-overline-bss}
\overline{\bss}(\param)= \frac{1}{n} \sum_{i=1}^n \overline{\bss}_i(\param) \quad  \text{where}  \quad \overline{\bss}_i(\param)= \int_{\Zset} S(z_i,y_i) p(z_i|y_i;\param) \mu(\rmd z_i) \,.
\end{equation}
The {\sf M-step} is given by
\beq \label{eq:mstep}
\textsf{M-step:}~~\hat{\param} = \overline{\param}( \overline{\bss}(\param) ) \eqdef \argmin_{ \vartheta \in \Param } ~\big\{ \Pen( \vartheta ) + \psi( \vartheta) - \pscal{ \overline{\bss}(\param)}{ \phi ( \vartheta) } \big\},
\eeq

\section{Monte Carlo Integration and Stochastic Approximation} \label{sec:sEM}
For complex and possibly nonlinear models, the expectation under the posterior distribution defined in \eqref{eq:definition-overline-bss} is not tractable. In that case, the first solution involves computing a Monte Carlo integration of that latter term. 
For all $ i \in \inter$, draw for $m \in \llbracket 1, M \rrbracket$, samples $z_{i,m} \sim p(z_i|y_i;\theta)$ and compute the MC integration $\hat{\bss}$ of the deterministic quantity $\overline{\bss}(\param)$:
$$
\textsf{MC-step}:~ \hat{\bss} = \frac{1}{n} \sum_{i=1}^n\frac{1}{M} \sum_{m=1}^M S(z_{i,m}, y_i)
$$
and compute $\hat{\param} = \overline{\param}( \hat{\bss} ) $.

This algorithm bypasses the intractable expectation issue but is rather computationally expensive in order to reach point wise convergence ($M$ needs to be large).

As a result, an alternative to that stochastic algorithm is to use a Robbins-Monro (RM) type of update.
We denote
\beq\label{eq:stats}
\hat{S}^{(k)} = \frac{1}{n} \sum_{i=1}^n \hat{S}^{(k)}_i = \frac{1}{n} \sum_{i=1}^n\frac{1}{M} \sum_{m=1}^M S(z_{i,m}^{(k)}, y_i)
\eeq
where $z_{i,m}^{(k)} \sim p(z_i|y_i;\theta^{(k-1)})$.
At iteration $k$, the sufficient statistics $\hat{\bss}^{(k)}$ is approximated as follows:

\beq\label{eq:rmstep}
\textsf{SA-step}:~ \hat{\bss}^{(k)} =  \hat{\bss}^{(k-1)}  + \gamma_k(\hat{S}^{(k)} - \hat{\bss}^{(k-1)} )
\eeq
where $\{ \gamma_{k} \}_{k=1}^\infty \in [0,1]$ is a sequence of decreasing step sizes to ensure asymptotic convergence.
This is called the Stochastic Approximation of the EM (SAEM), see \citep{delyon1999} and allows a smooth convergence to the target parameter.
It represents the \textit{first level} of our algorithm (needed to temper the variance and noise implied by MC integration).

In the next section, we derive variants of this algorithm to adapt of the sheer size of data of today's applications.

\section{Incremental and Bi-Level Inexact EM Methods} \label{sec:sEM}
Strategies to scale to large datasets include classical incremental and variance reduced variants.
We will explicit a general update that will cover those variants and that represents the \textit{second level} of our algorithm, namely the incremental update of the noisy statistics $\hat{S}^{(k)}$ inside the RM type of update.

\beq \label{eq:sestep}
\textsf{Inexact-step}:~\hat{S}^{(k)} = \hat{S}^{(k-1)} + \rho_{k+1} \big( \StocEstep^{(k)}- \hat{S}^{(k-1)}  \big),
\eeq
Note $\{ \rho_{k} \}_{k=1}^\infty \in [0,1]$ is a sequence of step sizes, $\StocEstep^{(k+1)}$ is a proxy for $\hat{S}^{(k)}$,
If the stepsize is equal to one and the proxy $\StocEstep^{(k+1)} = \hat{S}^{(k)}$, i.e., computed in a full batch manner as in \eqref{eq:stats}, then we recover the SAEM algorithm.
Also if $\rho_{k}=1$, $\gamma_{k}=1$ and $\StocEstep^{(k+1) = \hat{S}^{(k)}}$, then we recover the Monte Carlo EM algorithm.

We now introduce three variants of the SAEM update depending on different definitions of the proxy $\StocEstep^{(k)}$ and the choice of the stepsize $\rho_k$.
Let $i_k \in \inter$ be a random index drawn at iteration $k$ and $\tau_i^k = \max \{ k' : i_{k'} = i,~k' < k \}$ be the iteration index where $i \in \inter$ is last drawn prior to iteration $k$.
For iteration $k \geq 0$, the \FISAEM\ method draws \emph{two} indices \emph{independently} and uniformly as $i_k, j_k \in \inter$. In addition to $\tau_i^k$ which was defined \wrt $i_k$, we define $t_j^k = \{ k' : j_{k'} = j , k' < k \}$ to be the iteration index where the sample $j \in \inter$ is last drawn as $j_k$ prior to iteration $k$. With the initialization $\overline{\StocEstep}^{(0)} = \overline{\bss}^{(0)}$, we use a slightly different update rule from SAGA inspired by \citep{reddi2016fast}. Then, we obtain:
\begin{align}
&\emph{(\ISAEM\ \citep{kuhn2019properties})} & \StocEstep^{(k+1)} &= \StocEstep^{(k)} + {\textstyle \frac{1}{n}}\big( \hat{S}_{i_k}^{(k)}  - \hat{S}_{i_k}^{(\tau_{i_k}^k)} \big) \label{eq:isaem} \\
&\emph{(\SAEMVR\ This paper )} &\StocEstep^{(k+1)} &= \hat{S}^{(\ell(k))} +  \big( \hat{S}_{i_k}^{(k)}  -\hat{S}_{i_k}^{(\ell(k))}   \big) \label{eq:vrsaem}\\
&\emph{(\FISAEM\ This paper )} &\StocEstep^{(k+1)} &= \overline{\StocEstep}^{(k)} + \big( \hat{S}_{i_k}^{(k)}  - \hat{S}_{i_k}^{(t_{i_k}^k)} \big) \label{eq:fisaem}\\
&    &\overline{\StocEstep}^{(k+1)} &= \overline{\StocEstep}^{(k)} + n^{-1} \big( \hat{S}_{j_k}^{(k)}  - \hat{S}_{j_k}^{(t_{j_k}^k)} \big).
%\emph{(SAGA-EM)} & ~~~~\StocEstep^{(k+1)} = s & \gamma_{k+1} =
\end{align}
The stepsize is set to $\rho_{k+1} = 1$ for the \ISAEM\ method; $\rho_{k+1} = \gamma$ is  constant for the \SAEMVR\ and \FISAEM\ methods.
Moreover, for \ISAEM\ we initialize with $\StocEstep^{(0)} = \hat{S}^{(0)}$; for \SAEMVR\, we set an epoch size of $m$ and define $\ell(k) \eqdef m \lfloor k/m \rfloor$ as the first iteration number in the epoch that iteration $k$ is in. \vspace{-.2cm}



\begin{algorithm}[H]
\caption{Bi-Level Stochastic Approximation EM methods.}\label{alg:sem}
  \begin{algorithmic}[1]
  \STATE \textbf{Input:} initializations $\hat{\param}^{(0)} \leftarrow 0$, $\hat{\bss}^{(0)} \leftarrow \hat{S}^{(0)}$, $K_{\sf max}$ $\leftarrow$ max.~iteration number. \STATE Set the terminating iteration number, $K \in \{0,\dots,K_{\sf max}-1\}$, as a discrete r.v.~with:\vspace{-.1cm}
  \beq \label{eq:random}
   P( K = k ) = \frac{ \gamma_{k} }{\sum_{\ell=0}^{K_{\sf max}-1} \gamma_\ell}.\vspace{-.2cm}
  \eeq
  \FOR {$k=0,1,2,\dots, K$}
  \STATE Draw index $i_k \in \inter$ uniformly (and $j_k \in \inter$ for \FISAEM).
   \STATE Compute the surrogate sufficient statistics $\StocEstep^{(k+1)}$ using \eqref{eq:isaem} or \eqref{eq:vrsaem} or \eqref{eq:fisaem}.
   \STATE Compute $\hat{S}^{(k+1)}$ via the {\sf Inexact-step} \eqref{eq:sestep}.
      \STATE Compute $\hat{\bss}^{(k+1)}$ via the {\sf SA-step} \eqref{eq:rmstep}.
   \STATE Compute $\hat{\param}^{(k+1)}$ via the {\sf M-step} \eqref{eq:mstep}.
\ENDFOR
\STATE \textbf{Return}: $\hat{\param}^{(K)}$.
%\STATE \textbf{Return:} $\prm_t$.
  \end{algorithmic}
\end{algorithm}



\section{Finite Time Analysis} \label{sec:main}
Finite analysis of \ISAEM\, \SAEMVR\ and \FISAEM\ .

Analysis in the curved exponential family assumption.

Suboptimality condition would be: $\EE[ \| \grd V( \hs{K} ) \|^2 ]$ where 
\beq\label{eq:em_sspace}
\min_{ {\bss} \in \Sset }~  V ( {\bss} ) \eqdef \overline\calL( \op(\bss) ) = 
\Pen (  \op(\bss) ) + \frac{1}{n} \sum_{i=1}^n {\cal L}_i (  \op(\bss) )
,
\eeq
is the Lyapunov function minimized here.


\section{Numerical Examples}
\subsection{Gaussian Mixture Models}
Graphs obtained and relevant

\subsection{Logistic Regression with Missing values OR random effects}
To Be Done

\section{Conclusion}


\newpage
\linespread{1.1}
\normalsize

\bibliographystyle{abbrvnat}
\bibliography{references}

\linespread{1}
\newpage
\appendix
\section{Proof of Theorem}
\end{document}
