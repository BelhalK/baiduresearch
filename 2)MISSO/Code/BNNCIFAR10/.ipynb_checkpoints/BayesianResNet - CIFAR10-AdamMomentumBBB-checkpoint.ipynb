{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from models.bayesian_resnet import bayesian_resnet\n",
    "from models.bayesian_vgg import bayesian_vgg\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [32, 32, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                         batch_size, valid_size):\n",
    "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
    "  x_train = x_train.astype(\"float32\")\n",
    "  x_test = x_test.astype(\"float32\")\n",
    "\n",
    "  x_train /= 255\n",
    "  x_test /= 255\n",
    "\n",
    "  y_train = y_train.flatten()\n",
    "  y_test = y_test.flatten()\n",
    "\n",
    "  if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "  print(\"x_train shape:\" + str(x_train.shape))\n",
    "  print(str(x_train.shape[0]) + \" train samples\")\n",
    "  print(str(x_test.shape[0]) + \" test samples\")\n",
    "\n",
    "  # Build an iterator over training batches.\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_train, np.int32(y_train)))\n",
    "  training_batches = training_dataset.shuffle(\n",
    "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n",
    "\n",
    "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
    "  # i.e., return the entire heldout set as a constant.\n",
    "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_test, np.int32(y_test)))\n",
    "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
    "  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_batches)\n",
    "\n",
    "  # Combine these into a feedable iterator that can switch between training\n",
    "  # and validation inputs.\n",
    "  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
    "  feedable_iterator = tf.compat.v1.data.Iterator.from_string_handle(\n",
    "      handle, training_batches.output_types, training_batches.output_shapes)\n",
    "  images, labels = feedable_iterator.get_next()\n",
    "\n",
    "  return images, labels, handle, training_iterator, heldout_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fake_data():\n",
    "  \"\"\"Build fake CIFAR10-style data for unit testing.\"\"\"\n",
    "  num_examples = 10\n",
    "  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"bnnmodels/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate fake data for now before switching to CIFAR10\n",
    "fake_data = True\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "epochs = 10\n",
    "data_dir = \"data/\"\n",
    "eval_freq = 400\n",
    "num_monte_carlo = 50\n",
    "architecture = \"resnet\" # or \"vgg\"\n",
    "kernel_posterior_scale_mean = 0.9\n",
    "kernel_posterior_scale_constraint = 0.2\n",
    "kl_annealing = 50\n",
    "subtract_pixel_mean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_runs = 1\n",
    "seed0 = 23456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:(10, 32, 32, 3)\n",
      "10 train samples\n",
      "10 test samples\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if fake_data:\n",
    "        (x_train, y_train), (x_test, y_test) = build_fake_data()\n",
    "    else:\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    (images, labels, handle,\n",
    "     training_iterator,\n",
    "     heldout_iterator) = build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                                              batch_size, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(algo,fake_data, batch_size, epochs, verbose):\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        model_fn = bayesian_resnet\n",
    "        model = model_fn(\n",
    "            IMAGE_SHAPE,\n",
    "            num_classes=10,\n",
    "            kernel_posterior_scale_mean=kernel_posterior_scale_mean,\n",
    "            kernel_posterior_scale_constraint=kernel_posterior_scale_constraint)\n",
    "        logits = model(images)\n",
    "        labels_distribution = tfd.Categorical(logits=logits)\n",
    "        t = tf.compat.v2.Variable(0.0)\n",
    "        kl_regularizer = t / (kl_annealing * len(x_train) / batch_size)\n",
    "\n",
    "        log_likelihood = labels_distribution.log_prob(labels)\n",
    "        neg_log_likelihood = -tf.reduce_mean(input_tensor=log_likelihood)\n",
    "        kl = sum(model.losses) / len(x_train) * tf.minimum(1.0, kl_regularizer)\n",
    "        loss = neg_log_likelihood + kl\n",
    "\n",
    "        predictions = tf.argmax(input=logits, axis=1)\n",
    "\n",
    "        with tf.compat.v1.name_scope(\"train\"):\n",
    "          train_accuracy, train_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions)\n",
    "        if algo==\"adam\":\n",
    "          opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        if algo==\"bbb\":\n",
    "          opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        if algo==\"misso\":\n",
    "          opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        if algo==\"momentum\":\n",
    "          opt = tf.compat.v1.train.MomentumOptimizer(learning_rate=learning_rate, momentum=5e-5)\n",
    "\n",
    "        train_op = opt.minimize(loss)\n",
    "        update_step_op = tf.compat.v1.assign(t, t + 1)\n",
    "\n",
    "        with tf.compat.v1.name_scope(\"valid\"):\n",
    "          valid_accuracy, valid_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions)\n",
    "\n",
    "        init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n",
    "                           tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "        stream_vars_valid = [\n",
    "            v for v in tf.compat.v1.local_variables() if \"valid/\" in v.name\n",
    "        ]\n",
    "        reset_valid_op = tf.compat.v1.variables_initializer(stream_vars_valid)\n",
    "    \n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # Run the training loop\n",
    "        train_handle = sess.run(training_iterator.string_handle())\n",
    "        heldout_handle = sess.run(heldout_iterator.string_handle())\n",
    "        training_steps = int(\n",
    "          round(epochs * (len(x_train) / batch_size)))\n",
    "        listloss = []\n",
    "        listaccuracy = []\n",
    "        print(training_steps)\n",
    "        for step in range(training_steps):\n",
    "            _ = sess.run([train_op,\n",
    "                      train_accuracy_update_op,\n",
    "                      update_step_op],\n",
    "                     feed_dict={handle: train_handle})\n",
    "            # Print loss values\n",
    "            loss_value, accuracy_value, kl_value = sess.run(\n",
    "                  [loss, train_accuracy, kl], feed_dict={handle: train_handle})\n",
    "            print(\n",
    "                  \"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f} KL: {:.3f}\".format(\n",
    "                      step, loss_value, accuracy_value, kl_value))\n",
    "            listloss.append(loss_value)\n",
    "            listaccuracy.append(accuracy_value)\n",
    "\n",
    "            if (step + 1) % eval_freq == 0:\n",
    "              # Compute log prob of heldout set by averaging draws from the model:\n",
    "              # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
    "              #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
    "              # where model_i is a draw from the posterior\n",
    "              # p(model|train).\n",
    "              probs = np.asarray([sess.run((labels_distribution.probs),\n",
    "                                           feed_dict={handle: heldout_handle})\n",
    "                                  for _ in range(num_monte_carlo)])\n",
    "              mean_probs = np.mean(probs, axis=0)\n",
    "\n",
    "              _, label_vals = sess.run(\n",
    "                  (images, labels), feed_dict={handle: heldout_handle})\n",
    "              heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]),\n",
    "                                                     label_vals.flatten()]))\n",
    "              print(\" ... Held-out nats: {:.3f}\".format(heldout_lp))\n",
    "\n",
    "          # Calculate validation accuracy\n",
    "          #for _ in range(20):\n",
    "           # sess.run(\n",
    "            #    valid_accuracy_update_op, feed_dict={handle: heldout_handle})\n",
    "          #valid_value = sess.run(\n",
    "           #   valid_accuracy, feed_dict={handle: heldout_handle})\n",
    "\n",
    "    #      print(\" ... Validation Accuracy: {:.3f}\".format(valid_value))\n",
    "            print(loss_value)\n",
    "        sess.run(reset_valid_op)\n",
    "    return listloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Step:   0 Loss: 309593.406 Accuracy: 0.117 KL: 153682.656\n",
      "309593.4\n"
     ]
    }
   ],
   "source": [
    "adam = []\n",
    "for run in range(nb_runs):\n",
    "    tf.random.set_random_seed(_*seed0)\n",
    "    run = run_experiment(algo='adam', fake_data=fake_data, batch_size = batch_size, epochs=epochs, verbose= True)\n",
    "    adam.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-92b4b0f64102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseed0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bbb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mbbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-bbb83bb523f8>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(algo, fake_data, batch_size, epochs, verbose)\u001b[0m\n\u001b[1;32m     62\u001b[0m                       \u001b[0mtrain_accuracy_update_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                       update_step_op],\n\u001b[0;32m---> 64\u001b[0;31m                      feed_dict={handle: train_handle})\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Print loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             loss_value, accuracy_value, kl_value = sess.run(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 960\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1183\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1184\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1361\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1350\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1352\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1443\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1444\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bbb = []\n",
    "for run in range(nb_runs):\n",
    "    tf.random.set_random_seed(_*seed0)\n",
    "    run = run_experiment(algo='bbb', fake_data=fake_data, batch_size = batch_size, epochs=epochs, verbose= True)\n",
    "    bbb.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = []\n",
    "for run in range(nb_runs):\n",
    "    tf.random.set_random_seed(_*seed0)\n",
    "    run = run_experiment(algo='momentum', fake_data=fake_data, batch_size = batch_size, epochs=epochs, verbose= True)\n",
    "    momentum.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SAVE LOSSES\n",
    "with open('losses/adam', 'wb') as fp: \n",
    "    pickle.dump(adam, fp)\n",
    "with open('losses/momentum', 'wb') as fp: \n",
    "    pickle.dump(momentum, fp)\n",
    "with open('losses/bbb', 'wb') as fp: \n",
    "    pickle.dump(bbb, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD LOSSES\n",
    "with open ('losses/bbbloss', 'rb') as fp:\n",
    "    bbb = pickle.load(fp)\n",
    "with open ('losses/missoloss', 'rb') as fp:\n",
    "    misso = pickle.load(fp)\n",
    "with open ('losses/adamloss', 'rb') as fp:\n",
    "    adam = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baiduenv",
   "language": "python",
   "name": "baiduenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
