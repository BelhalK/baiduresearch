{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from models.bayesian_resnet import bayesian_resnet\n",
    "from models.bayesian_vgg import bayesian_vgg\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [32, 32, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                         batch_size, valid_size):\n",
    "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
    "  x_train = x_train.astype(\"float32\")\n",
    "  x_test = x_test.astype(\"float32\")\n",
    "\n",
    "  x_train /= 255\n",
    "  x_test /= 255\n",
    "\n",
    "  y_train = y_train.flatten()\n",
    "  y_test = y_test.flatten()\n",
    "\n",
    "  if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "  print(\"x_train shape:\" + str(x_train.shape))\n",
    "  print(str(x_train.shape[0]) + \" train samples\")\n",
    "  print(str(x_test.shape[0]) + \" test samples\")\n",
    "\n",
    "  # Build an iterator over training batches.\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_train, np.int32(y_train)))\n",
    "  training_batches = training_dataset.shuffle(\n",
    "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n",
    "\n",
    "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
    "  # i.e., return the entire heldout set as a constant.\n",
    "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_test, np.int32(y_test)))\n",
    "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
    "  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_batches)\n",
    "\n",
    "  # Combine these into a feedable iterator that can switch between training\n",
    "  # and validation inputs.\n",
    "  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
    "  feedable_iterator = tf.compat.v1.data.Iterator.from_string_handle(\n",
    "      handle, training_batches.output_types, training_batches.output_shapes)\n",
    "  images, labels = feedable_iterator.get_next()\n",
    "\n",
    "  return images, labels, handle, training_iterator, heldout_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fake_data():\n",
    "  \"\"\"Build fake CIFAR10-style data for unit testing.\"\"\"\n",
    "  num_examples = 10\n",
    "  x_train = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_train = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  x_test = np.random.rand(num_examples, *IMAGE_SHAPE).astype(np.float32)\n",
    "  y_test = np.random.permutation(np.arange(num_examples)).astype(np.int32)\n",
    "  return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"bnnmodels/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate fake data for now before switching to CIFAR10\n",
    "fake_data = True\n",
    "learning_rate = 0.0001\n",
    "data_dir = \"data/\"\n",
    "eval_freq = 400\n",
    "num_monte_carlo = 50\n",
    "architecture = \"resnet\" # or \"vgg\"\n",
    "kernel_posterior_scale_mean = 0.9\n",
    "kernel_posterior_scale_constraint = 0.2\n",
    "kl_annealing = 50\n",
    "subtract_pixel_mean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:(10, 32, 32, 3)\n",
      "10 train samples\n",
      "10 test samples\n",
      "WARNING:tensorflow:From <ipython-input-2-26c3b33fe1c3>:40: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
      "WARNING:tensorflow:From <ipython-input-2-26c3b33fe1c3>:40: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if fake_data:\n",
    "        (x_train, y_train), (x_test, y_test) = build_fake_data()\n",
    "    else:\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    (images, labels, handle,\n",
    "     training_iterator,\n",
    "     heldout_iterator) = build_input_pipeline(x_train, x_test, y_train, y_test,\n",
    "                                              batch_size, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_misso(algo,fake_data, batch_size, epochs, verbose):\n",
    "    with tf.Session() as sess:\n",
    "            \n",
    "        #instanciate model (bayesian resnet)\n",
    "        model_fn = bayesian_resnet\n",
    "        model = model_fn(\n",
    "            IMAGE_SHAPE,\n",
    "            num_classes=10,\n",
    "            kernel_posterior_scale_mean=kernel_posterior_scale_mean,\n",
    "            kernel_posterior_scale_constraint=kernel_posterior_scale_constraint)\n",
    "        logits = model(images)\n",
    "        labels_distribution = tfd.Categorical(logits=logits)\n",
    "        t = tf.compat.v2.Variable(0.0)\n",
    "        kl_regularizer = t / (kl_annealing * len(x_train) / batch_size)\n",
    "\n",
    "        log_likelihood = labels_distribution.log_prob(labels)\n",
    "        neg_log_likelihood = -tf.reduce_mean(input_tensor=log_likelihood)\n",
    "        kl = sum(model.losses) / len(x_train) * tf.minimum(1.0, kl_regularizer)\n",
    "        loss = neg_log_likelihood + kl\n",
    "\n",
    "        predictions = tf.argmax(input=logits, axis=1)\n",
    "\n",
    "        with tf.compat.v1.name_scope(\"train\"):\n",
    "            train_accuracy, train_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "                labels=labels, predictions=predictions)\n",
    "            opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        \n",
    "            train_op = opt.minimize(loss)\n",
    "            update_step_op = tf.compat.v1.assign(t, t + 1)\n",
    "\n",
    "        with tf.compat.v1.name_scope(\"valid\"):\n",
    "            valid_accuracy, valid_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "                labels=labels, predictions=predictions)\n",
    "\n",
    "            init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n",
    "                           tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "            stream_vars_valid = [v for v in tf.compat.v1.local_variables() if \"valid/\" in v.name]\n",
    "            reset_valid_op = tf.compat.v1.variables_initializer(stream_vars_valid)\n",
    "    \n",
    "    \n",
    "        sess.run(init_op)\n",
    "        indivgrads = []\n",
    "        indivvar = []\n",
    "        # Run the training loop\n",
    "        train_handle = sess.run(training_iterator.string_handle())\n",
    "        heldout_handle = sess.run(heldout_iterator.string_handle())\n",
    "        training_steps = int(\n",
    "          round(epochs * (len(x_train) / batch_size)))\n",
    "        listloss = []\n",
    "        listaccuracy = []\n",
    "        print(training_steps)\n",
    "        for indiv in range(0,total,batch_size):\n",
    "            print(indiv)\n",
    "            gradss = tf.gradients(loss, tf.trainable_variables())\n",
    "            grads = [x for x in gradss if x is not None]\n",
    "            #set_trace()\n",
    "            var_updates = []\n",
    "            var_list = tf.trainable_variables()\n",
    "            for grad, var in zip(grads, var_list):\n",
    "                var_updates.append(var.assign_sub(0.001 * grad))\n",
    "            train_op = tf.group(*var_updates)\n",
    "            indivgrads.append(grads)\n",
    "            indivvar.append(var_list)\n",
    "#        for step in range(training_steps):\n",
    "        for epoch in range(epochs):\n",
    "            for step in range(0,int(total/batch_size)):\n",
    "                gradss = tf.gradients(loss, tf.trainable_variables())\n",
    "                grads = [x for x in gradss if x is not None]\n",
    "                indivgrads[step] = grads\n",
    "                var_updates = []\n",
    "                var_list = tf.trainable_variables()\n",
    "                print('ok')\n",
    "                for gradstemp, varlist in zip(indivgrads, indivvar):\n",
    "                    for grad, var in zip(gradstemp, varlist):\n",
    "                        var_updates.append(var.assign_sub(0.001 * grad)) #\\theta^{\\tau_i^k} - \\grad f_{\\tau_i^k}\n",
    "                _ = sess.run([train_op,\n",
    "                              train_accuracy_update_op,\n",
    "                              update_step_op],\n",
    "                              feed_dict={handle: train_handle})\n",
    "                # Print loss values\n",
    "                loss_value, accuracy_value, kl_value = sess.run(\n",
    "                  [loss, train_accuracy, kl], feed_dict={handle: train_handle})\n",
    "                print(\n",
    "                  \"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f} KL: {:.3f}\".format(\n",
    "                      step, loss_value, accuracy_value, kl_value))\n",
    "                listloss.append(loss_value)\n",
    "                listaccuracy.append(accuracy_value)\n",
    "                \n",
    "        sess.run(reset_valid_op)\n",
    "    return listloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 1\n",
    "nb_runs = 1\n",
    "seed0 = 23456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "ok\n",
      "Step:   0 Loss: 1455060942848.000 Accuracy: 0.055 KL: 62.639\n",
      "ok\n",
      "Step:   1 Loss: 1469906812928.000 Accuracy: 0.094 KL: 125.278\n",
      "ok\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ffbe7da4a9de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseed0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_misso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'misso'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmisso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-6b357bd61cce>\u001b[0m in \u001b[0;36mrun_experiment_misso\u001b[0;34m(algo, fake_data, batch_size, epochs, verbose)\u001b[0m\n\u001b[1;32m     79\u001b[0m                       \u001b[0mtrain_accuracy_update_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                       update_step_op],\n\u001b[0;32m---> 81\u001b[0;31m                      feed_dict={handle: train_handle})\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# Print loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 loss_value, accuracy_value, kl_value = sess.run(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 960\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    961\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1183\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1184\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1361\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1352\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1388\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "misso = []\n",
    "for run in range(nb_runs):\n",
    "    tf.random.set_random_seed(_*seed0)\n",
    "    run = run_experiment_misso(algo='misso', fake_data=fake_data, batch_size = batch_size, epochs=epochs, verbose= True)\n",
    "    misso.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SAVE LOSSES\n",
    "with open('lossesFAKE/misso', 'wb') as fp: \n",
    "    pickle.dump(misso, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD LOSSES\n",
    "with open ('losses/misso', 'rb') as fp:\n",
    "    misso = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baiduenv",
   "language": "python",
   "name": "baiduenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
