\documentclass{article}
\usepackage{fullpage,graphicx,algorithm,algorithmic,bm,amsmath,amsthm,amssymb,color,hyperref,cite,natbib}
\usepackage{neurips_2019_author_response,xcolor,bm}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{lipsum}


\begin{document}

We would like to thank three reviewers for their feedback. Upon acceptance, we will include in the final version \emph{{\sf (a)} clarifications on important assumptions H1-H2} and \emph{{\sf (b)} emphasize on the generality of our scheme}. We first explain a few common concerns shared by \textbf{\color{blue}reviewer 1}, \textbf{\color{red} reviewer 2}, \textbf{\color{green!50!black}reviewer 3}.

${\color{blue}\bullet}\!~{\color{red}\bullet}\!~{\color{green!50!black}\bullet}$ \textbf{Generality of MISSO}: We want to stress on the generality of our incremental framework, which tackles a \emph{constrained}, \emph{non-convex} and \emph{non-smooth} optimization problem. The main contribution of this paper is to propose a unifying framework for the analysis of a large class of optimization algorithms. 
%that uses intractable surrogate functions which require to be approximated. 
The major goal here is to relax the class of surrogate functions used in MISO [Mairal, 2015] and replace that by the respective Monte-Carlo approximations.
We provide a general algorithm and global convergence analysis under mild assumptions on the model and show that two examples, MLE for general latent data models and Variational Inference, are its special instances.


${\color{blue}\bullet}\!~{\color{red}\bullet}$ \textbf{Comparison to Baselines}: The reviewers expressed concerns about the incomplete comparison in our experiments. We wish to clarify again that MISSO is a \textbf{general} framework that, for example, applies to both MLE and VI. Meanwhile, SAEM, MCEM (in Sec.~4.1), MC-SAG, MC-ADAM (in Sec.~4.2) are algorithms that are  designed only for the applications they appear in, which is why they didn't appear in all comparisons.
To our best knowledge, no other framework has the same level of generality as MISSO. 

${\color{red}\bullet}\!~{\color{green!50!black}\bullet}$ \textbf{Assumptions}: We can verify that \textbf{H1,H2} hold for the two given examples. First, \textbf{H1} refers to the \textbf{user-designed surrogate function}, which is  constructed at the user's will, e.g., to be convex and majorizes ${\cal L}_i$.  \textbf{H2} is a classical result which we realized that it needs to be
%  to be well known to the readers but it is important for the understanding of this paper. We agree that the ideas need to be 
developed in greater detail. For the case of i.i.d.~samples, using Example 19.7, Lemma 19.36 from `Asymptotic Statistics' by van der Vaart (2000), it can be shown that the constants $C_{\sf r}, C_{\sf gr}$ are ${\cal O}(\sqrt{p})$ for Lipschitz functions, i.e., sublinear in $p$. Meanwhile, the cases for Markov samples are not as obvious, though comparable results can be found for $\beta$-mixing processes [Thm.2, Doukhan+1995]. 
%Their dependence on dimension $p$ is not `\emph{bad}'.
%Similar results can be obtained for Markov samples, e.g., in [Boucheron+2013]. 





%is based on results from empirical processes where the fluctuations between a deterministic quantity and its MC approximation can be easily bounded. Examples and references will be included in the final version. {\color{red} References and Examples!!}

%${\color{red}\bullet}\!~{\color{green!50!black}\bullet}$ \textbf{Clarity:} 

\textbf{\textcolor{blue}{Reviewer 1:}} We thank the reviewer for valuable comments and references. We have the following clarifications:

\textbf{Related work:} The authors thank the reviewer for the references and will include them. 
[Nitanda+, 2017] focuses on difference-of-convex programs and [Song+, 2016] considers a specific model (Boltzmann machine). Our scheme is more general in the sense that it is valid for any type of surrogate function and applies to general non-smooth, non-convex optimization.
The incremental update of MISSO also allows to tackle large-scale problems.

\textbf{Comparison to vanilla SGD}: We remind  that there is a slight different in the metric used for measuring stationarity in this paper. SGD admits the rate $\mathbb{E}[ \| \nabla {\cal L}( \bm{\theta}^{(K)} )\|^2 ] = {\cal O}(\sigma^2/\sqrt{K})$, $\sigma^2$ is the variance, and it is defined on the \textbf{squared} norm of gradient [Ghadimi\&Lan,2013]. When applying SGD in a finite sum setting, in the worst case we have $\sigma^2 = {\cal O}(n)$. On the other hand, the rate in our (22) is defined on the negative part of $g(\bm{\theta})$ for non-smooth ${\cal L}$. For differentiable case,  $g_-(\bm{\theta}) \approx \| \nabla {\cal L}( \bm{\theta}) \|$. Therefore (22) gives $\mathbb{E}[ \| \nabla {\cal L}( \bm{\theta}^{(K)} )\|^2 ] = {\cal O}(n/K)$ which is better than SGD. 
%This discussion will be included in the final version.

%\textbf{Advantage of MISSO}: 
%While MCEM, MC-SAG, MC-ADAM are algorithms designed for different types of problem, our scheme and analysis applies to them all.
%Besides, there are no competitors to the MISSO scheme - at least there are no other frameworks with the same level of generality to our knowledge.

\textbf{\textcolor{red}{Reviewer 2:}} We thank the reviewer for useful comments.
Regarding the \textbf{contributions}, we would like to emphasize on the fact that MISSO (as well as MISO) is not only designed for EM methods - these frameworks are relevant for solving non-convex, non-smooth, large-scale optimization at large. We promise the following additional results:
%To our best knowledge, there are no comparable stochastic MM works in the literature.

%\textbf{Comparison to related work:} We will be pleased to answer this question if there are comparable stochastic MM works in the literature.

%\textbf{Assumption H1:} It is reasonable to use a convex surrogate to make line 8 easy to evaluate. The function $r$ is user-designed, e.g., it can be a quadratic surrogate which trivially satisfies assumption H1.

\textbf{Additional plots and experiments:} We will plot the incomplete log-likelihood for the 1st experiment. For the 2nd experiment, we refrain from plotting the convergence of  parameters since there are too many of them when training a Bayesian Neural Network (BNN).
Implementations details for each example can be found in the supplementary material. 
More datasets can be used for the Logistic regression (simulated datasets of binary output) and the BNN (CIFAR-10). 
%Those latter datasets and the MNIST we presented are benchmark datasets for training BNN.

\textbf{\textcolor{green!50!black}{Reviewer 3:}} We thank the reviewer for the comments. We would like to clarify that (1) is not necessarily a convex problem. In fact, the results in this paper apply to any non-convex, non-smooth problems with a convex constraint. 
%We also want to add that our scheme applies to non-smooth objective function.

\textbf{Relation between MISSO and EM:} Even though applying MISSO to the MLE in Example 1 results in an algorithm similar to EM, it differs from the latter in two aspects. First, the surrogate function (10) which normally corresponds to an E-step in EM now involves an intractable integration, and it can only be approximated using Monte-Carlo simulation. Second, an incremental update scheme is adopted as inherited from the MISSO framework. 
In fact, the resultant algorithm is an incremental variant of the Monte-Carlo EM method where there are two levels of stochasticity. 
We will strengthen this connection to EM/MCEM in the final version.

%\textbf{Constants in H2:} 
%We  classical results from empirical processes. 
%We did not want to spend too much time to discuss it due to space limitation. 
%It depends on the structure of the problem such as the Lipschitzness or the i.i.d. property of the data.
%These bounds are typically in the order of $O(p)$ where $p$ is the dimension of the problem.
%In the final version of this paper, we will include a few examples for the explicit constants. 

\textbf{Bounds in (22):} As per your suggestion, we will clarify the expectation operator in the final version. However, we would like to point out that the LHS of (22) evaluates to an average of the \textbf{gradient} norm of iterates, and bounding the latter quantity by ${\cal O}(1/K_{\sf max})$ is the main challenge of our analysis, see Appendix A. It is true that using the derived bound, one could also prove the same sublinear convergence of the best iterate. However, picking the best iterate is not practical in our setting of \textbf{non-smooth stochastic optimization} as the quantity $g_-( \bm{\theta}^{(k)} )$ is intractable. 
The LHS in (22) refers to solution quality when one simply terminates MISSO at a random iteration. For stochastic optimization, this is easier to implement than the best iterate scheme above. Such a random termination scheme is very common in stochastic non-convex optimization, see [Ghadimi\&Lan,2013]. 



\end{document}

